<section xml:id="ss_coordinate">
  <title>Coordinate vectors</title>
  <paragraphs>
    <title>Coordinate vectors relative to a basis $B$</title>
    <definition>
      <statement>
        <p>
          Let <m>B=\{\boldv_1,\dots ,\boldv_n\}</m> be a basis for <m>V</m>,
          and let <m>\boldv\in V</m>.
          According to <xref ref="th_basisunique">Theorem</xref>
          there is a unique choice of <m>c_i</m> such that
          <me>
            \boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_n\boldv_n
          </me>.
        </p>
        <p>
          We define the <term>coordinate vector of <m>\boldv</m> relative to the basis <m>B</m></term> to be the <m>n</m>-tuple
          <me>
            [\boldv]_B:=(c_1,c_2,\dots, c_n)
          </me>
        </p>
      </statement>
    </definition>
    <remark>
      <ol>
        <li>
          <p>
            To compute the coordinate vector of <m>\boldv</m> relative to
            <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}</m> we must <em>solve</em>
            the vector equation <m>\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n</m> for the coefficients <m>c_1, c_2,\dots, c_n</m>.
            For any given example this computation can usually be reduced to setting up and solving a certain system of linear equations.
          </p>
        </li>
        <li>
          <p>
            As usual, we will be flexible in terms of how we treat the <m>n</m>-vector <m>(c_1,c_2,\dots,
            c_n)</m>.
            For example,
            we will often interpret this as a <m>n\times 1</m> column vector.
          </p>
        </li>
      </ol>
    </remark>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=P_2</m>.
      <ol>
        <li>
          <p>
            Let <m>B=\{1,x, x^2\}</m> be the standard basis of <m>V</m>.
            Computing <m>[p(x)]_B</m> in this case is easy since any polynomial
            <m>p(x)=a+bx+cx^2</m> is already expressed as a linear combination of the elements of <m>B</m> !!  That is,
            we have <m>[a+bx+cx^2]_B=(a,b,c)</m> for any polynomial <m>p(x)=a+bx+cx^2</m>.
            Some explicit examples:
            <md>
              <mrow>_B\amp = (7, -\pi, \sqrt{2})</mrow>
              <mrow>[1+9x^2]_B\amp =(1,0,9)</mrow>
            </md>
          </p>
        </li>
        <li>
          <p>
            Let <m>B'=\{1+x+x^2, -x+x^2, -1+x^2\}=\{p_1,p_2,p_3\}</m> be the nonstandard basis we considered earlier.
            Now it requires more work to compute <m>[p(x)]_{B'}</m>.
            Some explicit examples:
            <md>
              <mrow>1+x+x^2=1p_1+0p_2+0p_3\amp \Rightarrow\amp [1+x+x^2]_{B'}=(1,0,0)</mrow>
              <mrow>-x+x^2=0p_1+1p_2+0p_3\amp \Rightarrow\amp [-x+x^2]_{B'}=(0,1,0)</mrow>
              <mrow>x^2=\frac{1}{3}p_1+\frac{1}{3}p_2+\frac{1}{3}p_3\amp \Rightarrow\amp  [x^2]_{B'}=(\frac{1}{3},\frac{1}{3},\frac{1}{3})</mrow>
            </md>
          </p>
        </li>
      </ol>
    </p>
    <p>
      The last example was computed by setting up the polynomial equation
      <m>x^2=c_1(1+x+x^2)+c_2(-x+x^2)+c_3(-1+x^2)</m> and solving for the <m>c_i</m>.
      Verify for yourself that <m>c_1=c_2=c_3=1/3</m> is the solution!
    </p>
  </paragraphs>
  <paragraphs>
    <title>Coordinate vector map</title>
    <theorem xml:id="th_coordinates">
      <statement>
        <p>
          Let <m>V</m> be a vector space,
          and suppose <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}</m> be a basis for <m>V</m>.
          Define
          <md>
            <mrow>T\colon V\amp \rightarrow \R^n</mrow>
            <mrow>\boldv\amp \mapsto [\boldv]_B</mrow>
          </md>.
          <ol>
            <li>
              <p>
                <m>T</m> is a linear transformation.
                We will call <m>T</m> the <term>coordinate vector map with respect to <m>B</m></term>.
              </p>
            </li>
            <li>
              <p>
                <m>T(\boldv_1)=T(\boldv_2)</m> if and only if <m>\boldv_1=\boldv_2</m>:
                i.e., <m>T</m> is <em>one-to-one</em>.
                In particular,
                <m>T(\boldv)=\boldzero</m> if and only if <m>\boldv=\boldzero_V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\range T=\R^n</m>: i.e.
                <m>T</m> is <em>onto</em>.
              </p>
            </li>
            <li>
              <p>
                A set <m>S=\{w_1,w_2,\dots,
                w_r\}\subseteq V</m> is linear independent if and only if
                <m>T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}\subseteq \R^n</m> is linearly independent.
              </p>
            </li>
            <li>
              <p>
                A set <m>S=\{w_1,w_2,\dots,
                w_r\}\subseteq V</m> spans <m>V</m> if and only if <m>T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}</m> spans <m>\R^n</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        Exercise.
      </p>
    </proof>
  </paragraphs>
  <p>
    Fix vector space <m>V</m> and basis <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}</m>.
    According to <xref ref="th_coordinates">Theorem</xref>,
    the coordinate vector map <m>\boldv\mapsto [\boldv]_B</m> is not only <em>linear</em>,
    it is also <em>1-1 and onto</em>.
    Such functions are called <em>isomorphisms</em>.
  </p>
  <definition>
    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
        An <term>isomorphism</term> is a linear transformation
        <m>T\colon V\rightarrow W</m> that is 1-1 and onto.
      </p>
      <p>
        The spaces <m>V</m> and <m>W</m> are called
        <term>isomorphic</term> in this case.
      </p>
    </statement>
  </definition>
  <p>
    We apply the general principle described in the previous slide to the special case of a coordinate vector isomorphism <m>[\hspace{10pt}]_B\colon V\rightarrow \R^n</m>.
  </p>
  <paragraphs>
    <title>Coordinate vector technique</title>
    <p>
      Suppose we have chosen a finite basis <m>B</m> for the vector space <m>V</m>.
      Let <m>[\hspace{10pt}]_B</m> denote the corresponding coordinate vector map,
      and let <m>[\hspace{10pt}]_B^{-1}</m> denote the inverse of this map.
    </p>
    <p>
      Any linear algebraic questions we want to answer about <m>V</m>
      (involving span, linear independence, etc.)
      can be answered as follows:
      <ol>
        <li>
          <p>
            First apply <m>[\hspace{10pt}]_B</m> to all vectors <m>\boldv</m> in question.
          </p>
        </li>
        <li>
          <p>
            Answer the corresponding question in <m>\R^n</m> about the column vectors <m>[\boldv]_B</m>.
          </p>
        </li>
        <li>
          <p>
            If necessary,
            translate your results back to the setting of <m>V</m> by applying the inverse <m>[\hspace{10pt}]_B^{-1}</m>.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      The set
      <me>
        S=\left \{ A_1=\begin{bmatrix}2\amp 1\\ 0\amp -2 \end{bmatrix} , A_2=\begin{bmatrix}1\amp 1\\ 1\amp -1 \end{bmatrix} , A_3=\begin{bmatrix}0\amp 1\\ 2\amp 0 \end{bmatrix} , A_4=\begin{bmatrix}-1\amp 0\\ 1\amp 1 \end{bmatrix} \right\}
      </me>
      is a subset of the space <m>W=\{ A\in M_{22}\colon \tr A=0\}</m>. (Recall:
      the trace of a matrix, <m>\tr A</m>,
      is defined as the sum of its diagonal elements. )
    </p>
    <p>
      Contract <m>S</m> to a basis of <m>\Span S</m>,
      and determine whether <m>\Span S=W</m>.
      Use the coordinate vector map technique!
    </p>
    <p>
      We must first pick a basis for <m>W</m>.
      Observe that the general element of of <m>W</m> can be described as
      <me>
        \begin{bmatrix}a\amp b\\ c\amp -a \end{bmatrix} =a\underset{B_1}{\underbrace{\begin{bmatrix}1\amp 0\\ 0\amp -1 \end{bmatrix} }}+b\underset{B_2}{\underbrace{\begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} }}+c\underset{B_3}{\underbrace{\begin{bmatrix}0\amp 0\\1\amp 0 \end{bmatrix} }}
      </me>.
    </p>
    <p>
      It follows easily that <m>B=\{B_1,B_2, B_3\}</m> is a basis for <m>W</m>.
    </p>
    <p>
      Apply <m>[\hspace{10pt}]_B</m> to the elements of the given <m>S</m> to get a corresponding set <m>S'\subseteq\R^3</m>:
      <me>
        S'=\left\{ [A_1]_B=(2,1,0), [A_2]_B=(1,1,1), [A_3]_B=(0,1,2), [A_4]_B=(-1,0,1)\right\}
      </me>
    </p>
    <p>
      Now use the
      <q>contract to basis</q>
      algorithm on <m>S'</m> to conclude that the set
      <m>\{(2,1,0), (1,1,1)\}</m> forms a basis for <m>\Span S'</m>.
      Translating back to <m>W</m>,
      we see that <m>A_1</m> and <m>A_2</m> form a basis for <m>\Span S</m>,
      that <m>\dim\Span S=2</m>,
      and hence that <m>\Span S\ne W</m>, since <m>\dim W=3</m>.
    </p>
  </paragraphs>
</section>