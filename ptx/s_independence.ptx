<section xml:id="ss_independence">
  <title>Linear independence</title>
  <subsection>
    <title>Span</title>
    <paragraphs>
      <title>Linear combinations and span</title>
      <p>
        Recall that a <em>linear combination</em>
        in a vector space <m>V</m> is a vector of the form
        <me>
          \boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r
        </me>,
        where <m>k_i\in \R</m> are scalars.
      </p>
      <p>
        We use this notion to define the
        <em>span</em> of a set of vectors.
      </p>
      <definition>
        <statement>
          <p>
            Let <m>V</m> be a vector space,
            and let <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> be a set of vectors of <m>V</m>.
            The <term>span</term> of <m>S</m> is the set
            <md>
              <mrow>\Span(\{\boldv_1,\boldv_2,\dots,\boldv_r\})\amp :=\amp \left(\begin{array}{c} \text{ the set of all linear }  \\ \text{ combinations of the \(\boldv_i\) } \end{array} \right)</mrow>
              <mrow>\amp =\amp \{\boldv\in V\colon \boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r, \text{ for some }  k_i\in\R \}</mrow>
            </md>.
          </p>
        </statement>
      </definition>
    </paragraphs>
    <theorem>
      <statement>
        <p>
          Let <m>V</m> be a vector space,
          <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> a set of vectors of <m>V</m>,
          and <m>W=\Span(S)</m>.
          Then
          <ol>
            <li>
              <p>
                <m>W</m> is a subspace of <m>V</m>;
              </p>
            </li>
            <li>
              <p>
                if <m>W'</m> is a subspace containing all the vectors <m>\boldv_i</m>,
                then <m>W\subseteq W'</m>.
              </p>
            </li>
          </ol>
        </p>
        <p>
          We paraphrase (ii) by saying <m>W=\Span(S)</m> is the
          <q>smallest</q>
          subspace of <m>V</m> containing all the <m>\boldv_i</m>.
        </p>
      </statement>
    </theorem>
    <paragraphs>
      <title>More terminology:</title>
      <p>
        in the spirit of the last theorem,
        given a set of vectors <m>S=\{\boldv_1,\dots, \boldv_r\}</m>,
        we call <m>W=\Span(S)</m> the subspace of <m>V</m> <term>generated by</term>
        the vectors <m>\boldv_i</m>.
        Similarly, given a subspace <m>W</m>,
        a set <m>S=\{\boldv_1,\dots, \boldv_r\}</m> for which <m>W=\Span(S)</m> is called a
        <term>spanning set</term> of <m>W</m>.
      </p>
    </paragraphs>
    <paragraphs>
    <title>Examples</title>
    <p>
      \alert{<m>M_{mn}</m>}. Define <m>E_{ij}</m> to be the matrix whose <m>(i,j)</m>-th entry is 1, and whose every other entry is 0.
      Then the set
      <me>
        \{ E_{ij}\colon 1\leq i\leq m, 1\leq j\leq n\}
      </me>
      is a spanning set for <m>M_{mn}</m>.
    </p>
    <paragraphs>
      <title><m>\R^n</m></title>
      <p>
        In a similar vein,
        define <m>\bolde_i</m> to be the <m>n</m>-tuple whose <m>i</m>-th entry is 1, and whose every other entry is 0.
      </p>
    </paragraphs>
    <p>
      Then <m>\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m> is a spanning set for <m>\R^n</m>.
    </p>
    <paragraphs>
      <title><m>P</m> and <m>P_n</m></title>
      <p>
        The set <m>\{1, x, x^2, \dots, \}</m> is a spanning set for <m>P</m>.
        The set <m>\{1, x, x^2, \dots,
        x^n\}</m> is a spanning set for <m>P_n</m>.
      </p>
    </paragraphs>
    <paragraphs>
      <title><m>\R^\infty</m></title>
      <p>
        As above we can define <m>\bolde_i\in \R^\infty</m> to be the infinite sequence whose <m>i</m>-th entry is 1, and whose every other entry is <m>0</m>.
      </p>
    </paragraphs>
    <p>
      Note, however,
      that the set <m>\{\bolde_1, \bolde_2, \bolde_3, \dots\}</m> is <em>not</em>
      a spanning set for <m>\R^\infty</m>.
    </p>
    <p>
      Indeed, the sequence <m>(1,1,1,\dots)</m> is not a (finite!) linear combination of the <m>\bolde_i</m>.
    </p>
    <p>
      \alert{<m>\R_{>0}</m>} Any
      <m>a\ne 1\in \R_{>0}</m> forms a spanning set for <m>\R_{>0}</m> as a vector space.
      This is because scalar multiplication by <m>r</m> in <m>\R_{>0}</m> is defined as exponentiation.
      Thus <m>\Span(\{a\})=\{a^r\colon r\in\R\}=\R_{>0}</m>.
      The last equality holds since the exponential function
      <m>f(x)=a^x</m> has image all positive reals for any base <m>a\ne 1</m>.
    </p>
    </paragraphs>
    <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=P_2</m> and let <m>S=\{p_1, p_2\}</m>,
      where <m>p_1(x)=x^2-1</m> and <m>p_2(x)=x^2-x</m>.
      Show that <m>W=\Span(S)</m> is the subspace of all polynomials
      <m>p(x)=a_2x^2+a_1x+a_0</m> for which <m>p(1)=0</m>.
      That is:
      <me>
        \Span(S)=\{p(x)\in P_2\colon p(1)=0\}
      </me>
    </p>
    <proof>
      <p>
        We wish to prove a <em>set equality</em>.
        We do so by showing the <m>\subseteq</m> and
        <m>\supseteq</m> relations separately. (See my proof technique guide!)
      </p>
      <paragraphs>
        <title><m>\subseteq</m></title>
        <p>
          Note first that <m>p_1(1)=p_2(1)=0</m>.
          Given an element <m>q(x)\in \Span\left(\{p_1(x), p_2(x)\}\right)</m>,
          we have <m>q(x)=ap_1(x)+bp_2(x)</m> for some <m>a, b\in \R</m>.
          But then <m>q(1)=ap_1(1)+bp_2(1)=0+0=0</m>.
          Thus <m>q(x)\in \{p(x)\colon p(1)=0\}</m>.
        </p>
      </paragraphs>
      <paragraphs>
        <title><m>\supseteq</m></title>
        <p>
          Now take <m>p(x)=a_2x^2+a_1x+a_0\in \{p(x)\in P_2\colon p(1)=0\}</m>.
          We must find <m>a, b\in \R</m> such that <m>p(x)=ap_1+bp_2</m>.
        </p>
      </paragraphs>
      <p>
        Since <m>p(1)=0</m>, we have <m>a_2+a_1+a_0=0</m>.
        I claim <m>p(x)=(-a_0)p_1+(-a_1)p_2</m>.
        Indeed we have
        <me>
          -a_0p_1-a_1p_2=(-a_0-a_1)x^2+a_1x+a_0=a_2x^2+a_1x+a_0
        </me>,
        since <m>a_2+a_1+a_0=0</m>.
        This shows that <m>p(x)\in\Span\left(\{p_1, p_2\}\right)</m>,
        as desired.
      </p>
    </proof>
    </paragraphs>
    <paragraphs>
      <title>Example</title>
      <p>
        Let <m>V=P_2</m> and let <m>S=\{p_1, p_2, p_3\}</m> where
        <me>
          p_1(x)=x^2+x+1, p_2(x)=x^2+x, p_3(x)=x^2+1
        </me>.
      </p>
      <p>
        Show that <m>\Span(S)=P_2</m>.
      </p>
      <proof>
        <p>
          Again, we are tasked with showing a <em>set equality</em>.
          It is clear that <m>\Span(S)=\{rp_1+sp_2+tp_3\colon r,s,t\in\R\}\subseteq P_2</m>.
          The harder direction is showing <m>P_2\subseteq \Span(S)</m>:
          i.e., given <em>any</em> <m>p(x)=a_2x^2+a_1x+a_0</m> we must show there are
          <m>r,s,t\in\R</m> such that <m>p(x)=rp_1+sp_2+tp_3</m>.
        </p>
        <p>
          We do so by setting up a system of equations.
          Combining like terms and equating coefficients in the polynomial expression
          <m>p(x)=rp_1+sp_2+tp_3</m> yields the linear system
          <md>
            \begin{linsys}{3} r\amp +\amp s\amp +\amp t\amp =\amp a_2\\ r\amp +\amp s\amp \amp \amp =\amp a_1\\ r\amp \amp \amp +\amp t\amp =\amp a_0 \end{linsys}
          </md>
        </p>
        <p>
          GE shows that the system has a solution for <em>any</em>
          choice of <m>a_2, a_1, a_0</m>:
          namely, <m>r=-a_2+a_1+a_0</m>,
          <m>s=a_2-a_0</m>, <m>t=a_2-a_1</m>.
          Thus given any <m>p(x)=a_2x^2+a_1x+a_0</m>,
          we can find <m>r,s,t</m> such that <m>p=rp_1+sp_2+tp_3</m>,
          showing <m>P_2\subseteq \Span(S)</m>.
        </p>
      </proof>
    </paragraphs>
  </subsection>
  <paragraphs>
    <title><xref ref="c_vectorspace"></xref>. <xref ref="ss_independence"></xref>: linear independence</title>
    <definition>
      <statement>
        <p>
          Let <m>V</m> be a vector space.
          A set <m>S=\{\boldv_1,\dots ,\boldv_r\}</m> of elements of <m>V</m> is
          <term>linear independent</term> if
          <me>
            k_1\boldv_1+k_2\boldv_2+\cdots k_r\boldv_r=\boldzero \Rightarrow k_i=0 \text{ for all \(i\) }
          </me>.
        </p>
        <p>
          In other words, the only linear combination of the
          <m>\boldv_i</m> yielding <m>\boldzero</m> is the
          <term>trivial linear combination</term>
          we get by setting <m>k_i=0</m> for all <m>i</m>.
        </p>
        <p>
          The set <m>S</m> is <term>linearly dependent</term>
          if it is not linearly independent;
          i.e., if we can find a nontrivial linear combination of the
          <m>\boldv_i</m> that yields <m>\boldzero</m>.
        </p>
      </statement>
    </definition>
    <theorem>
      <statement>
        <p>
          The set <m>S</m> is linearly independent if and only if no element
          <m>\boldv_i</m> can be written as a linear combination of the other <m>\boldv_j</m>.
          Similarly, <m>S</m> is linearly dependent if and only if one of the elements
          <m>\boldv_i</m> can be written as a linear combination of the remaining <m>\boldv_j</m>.
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <paragraphs>
  <title>Example in $P_n$</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\{x^2+x-2, 2x^2+1, x^2-x\}\subset P_2</m>.
      Decide whether <m>S</m> is linearly independent.
    </p>
  </paragraphs>
  <proof>
    <p>
      First observe that <m>\boldzero=0x^2+0x+0</m>, the zero polynomial.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em> combination
      <md>
        <mrow>a(x^2+x-2)+b(2x^2+1)+c(x^2-x)\amp =\amp 0x^2+0x+0</mrow>
        <mrow>(a+2b+c)x^2+(a-c)x+(-2a+b)\amp =\amp 0x^2+0x+0</mrow>
      </md>
    </p>
    <p>
      Equating like terms gives us the linear system
      <md>
        \begin{linsys}{3} a\amp +\amp 2b\amp +\amp c\amp =\amp 0\\ a\amp \amp \amp -\amp c\amp =\amp 0\\ -2a\amp +\amp b \amp \amp  \amp =\amp 0 \end{linsys}
      </md>
    </p>
    <p>
      Row reduction shows this system only has the trivial solution <m>a=b=c=0</m>.
      Thus <m>S</m> is <em>linearly independent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Example in $M_{mn}$</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\left\{ A_1=\begin{bmatrix}3\amp 1\\ 2\amp -3 \end{bmatrix} , A_2= \begin{bmatrix}0\amp 4\\ 2\amp 0 \end{bmatrix} , A_3=\begin{bmatrix}-2\amp -2\\ -2\amp 2 \end{bmatrix} \right\}\subset M_{22}</m>.
      Decide whether <m>S</m> is linearly independent.
    </p>
  </paragraphs>
  <proof>
    <p>
      First observe that <m>\boldzero=\begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix}</m>,
      the zero matrix.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em> combination
      <md>
        <mrow>a\begin{bmatrix}3\amp 1\\ 2\amp -3 \end{bmatrix} +b\begin{bmatrix}0\amp 4\\ 2\amp 0 \end{bmatrix} +c\begin{bmatrix}-2\amp -2\\ -2\amp 2 \end{bmatrix} \amp =\amp \begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix} \begin{bmatrix}3a-2c\amp a+4b-2c\\ 2a+2b-2c\amp -3a+2c \end{bmatrix}  \amp =\amp \begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix}</mrow>
      </md>
    </p>
    <p>
      Equating like terms gives us the linear system
      <md>
        \begin{linsys}{3} 3a\amp \amp \amp -\amp 2c\amp =\amp 0\\ a\amp +\amp 4b\amp -\amp 2c\amp =\amp 0\\ 2a\amp +\amp 2b \amp -\amp 2c \amp =\amp 0\\ -3a\amp \amp \amp +\amp 2c\amp =\amp 0 \end{linsys}
      </md>
    </p>
    <p>
      Row reduction shows this system has a free variable,
      and hence a nontrivial solution<ndash/>in fact infinitely many!
      One example is <m>a=2</m>, <m>b=-1</m>, <m>c=3</m>.
      Thus <m>S</m> is <em>linearly dependent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Example in function space</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\{f(x)=x, g(x)=\cos(x),
      h(x)=\sin(x)\}\subseteq C^\infty(\R)</m>.
    </p>
  </paragraphs>
  <p>
    Decide whether <m>S</m> is linearly independent.
  </p>
  <proof>
    <p>
      First observe that <m>\boldzero</m> is the zero function:
      the function that assigns 0 to all inputs <m>x</m>.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em>
      combination <m>af+bg+ch=\boldzero</m>.
    </p>
    <p>
      <em>Key observation:</em> the equality above is an equality of <em>functions</em>.
      Thus this is true if and only if
      <m>af(x)+bg(x)+ch(x)=0</m> \alert{for all <m>x</m>}.
    </p>
    <p>
      To get a linear system,
      we evaluate the above at a few clever choices of <m>x</m>:
      <md>
        <mrow>x=0\amp :\amp  a(0)+b\cos(0)+c\sin(0)=0\Rightarrow 0+b+0=0\Rightarrow b=0</mrow>
        <mrow>x=\pi\amp :\amp  a(\pi)+c\sin(\pi)=0\Rightarrow \pi a+0=0\Rightarrow a=0</mrow>
      </md>
    </p>
    <p>
      Having shown <m>a=b=0</m>,
      we are left with the equation <m>c\sin(x)=0</m> for all <m>x</m>,
      which is true iff <m>c=0</m>.
    </p>
    <p>
      Thus the only linear combination yielding <m>\boldzero</m> is <m>a=b=c=0</m>,
      the trivial one, showing <m>S</m> is
      <em>linearly independent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Linear independence in function spaces</title>
  <p>
    As the last example illustrates,
    a set of functions <m>S=\{f_1,f_2, \dots ,f_r\}</m> is linearly independent if
  </p>
  <p>
    <m>k_1f_1+k_2f_2+\cdots k_rf_r=\boldzero</m> implies <m>k_i=0</m> for all <m>i</m>.
  </p>
  <p>
    Recall <m>\boldzero</m> here stands for the <em>zero function</em>.
    Thus we must treat such a linear combination as a function identity!
    In other words to say
    <me>
      k_1f_1+k_2f_2+\cdots k_rf_r=\boldzero
    </me>
    is simply to say that
    <me>
      k_1f_1(x)+k_2f_2(x)+\cdots k_rf_r(x)=0
    </me>
  </p>
  <paragraphs>
    <title>for all <m>x</m></title>
    <p>
      in the given domain.
    </p>
  </paragraphs>
  <p>
    The beauty of this
    <q>for all <m>x</m></q>
    is that by picking say <m>r</m> actual examples of <m>x</m> and evaluating above,
    we generate <m>r</m> linear equations in the unknowns <m>k_i</m>.
  </p>
  <p>
    If this system has no nontrivial solutions,
    then we know the functions are independent.
  </p>
  <p>
    However, if this system DOES have a nontrivial solution we CANNOT conclude the functions are linearly dependent.
    Why?
    We would have shown the identity above holds only for these <m>r</m> choices of <m>x</m>,
    but not necessarily <em>all</em> <m>x</m>!
  </p>
  </paragraphs>
  <paragraphs>
  <title>$C^\infty(a,b)$ and the Wronskian</title>
  <p>
    Let's consider this observation in the special example of
    <em>differentiable</em> functions.
  </p>
  <definition>
    <statement>
      <p>
        Suppose <m>f_1,f_2,\dots f_n</m> are each <m>(n-1)</m>-differentiable functions on <m>(a,b)</m>.
        We define the Wronskian of the <m>f_i</m> as the function
        <md>
          W(x)=\begin{vmatrix}f_1(x)\amp f_2(x)\amp \cdots \amp f_n(x)\\ f_1'(x)\amp f_2'(x)\amp \cdots \amp f_n'(x)\\ \vdots\amp \cdots \amp  \amp \vdots \\ f_1^{(n-1)}(x)\amp f_2^{(n-1)}(x)\amp \cdots \amp f_n^{(n-1)}(x) \end{vmatrix}.
        </md>
      </p>
    </statement>
  </definition>
  <theorem>
    <title>Wronskian</title>
    <statement>
      <p>
        Let <m>V=C^{\infty}(X)</m>, where <m>X</m> is a fixed interval
        (usually <m>X=\R</m>),
        and let <m>S=\{f_1,f_2,\dots ,f_n\}</m> be a set of elements of <m>V</m>.
        Let <m>W(x)</m> be the Wronskian of the <m>f_i</m>.
        Then
        <me>
          W\ne\boldzero\Rightarrow \text{ \(S\) is linearly indendent }
        </me>.
      </p>
    </statement>
  </theorem>
  <paragraphs>
    <title>Comments</title>
  </paragraphs>
  <p>
    (1) Again <m>\boldzero</m> here is the <em>zero function</em>.
    So <m>W\ne\boldzero</m> means there is an <m>x</m> in <m>(a,b)</m> such that <m>W(x)\ne 0</m>.
  </p>
  <p>
    (2) This implication only goes one way!! In other words,
    <m>W(x)=0</m> for all <m>x</m> does not imply <m>S</m> is dependent!!
  </p>
  </paragraphs>
</section>
