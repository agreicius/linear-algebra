<section xml:id="ss_eigenvectors">
  <title>Eigenvectors and eigenvalues</title>
  <introduction>
    <p>
      As we saw in <xref ref="eg_matrixreps_proj"/>, different matrix representations <m>[T]_B</m> and <m>[T]_{B'}</m> for a linear transformation <m>T\colon V\rightarrow V</m> provide different insights into the nature of <m>T</m>. Furthermore, we see from this example that if the action of <m>T</m> on a given basis is simple to describe, then so too is the matrix representation of <m>T</m> with respect to that basis.
    </p>
    <p>
      A particularly agreeable situation arises when the basis <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> satisfies
      <me>
        T(\boldv_i)=c_i\boldv_i, c_i\in \R
      </me>
      for all <m>1\leq i\leq n</m>. Using recipe <xref ref="eq_matrixrep_formula"/> we in this case that the corresponding matrix representation
      <me>
        [T]_B=\begin{bmatrix}
          c_1 \amp 0\amp \dots \amp \amp 0\\
          0   \amp c_2\amp 0\amp \dots \amp 0\\
          0\amp 0\amp \ddots \amp \amp 0\\
          \vdots  \amp \amp \amp \amp \vdots \\
          0\amp 0\amp \dots \amp 0\amp c_n
      \end{bmatrix}
      </me>
      is diagonal! Diagonal matrices are about as simple as they come: they wear all of their properties (rank, nullity, invertibility, <etc />) on their sleeve.  If we hope to find a diagonal matrix representation of <m>T</m>, then we should seek nonzero vectors <m>\boldv</m> satisfying <m>T(\boldv)=c\boldv</m> for some <m>c\in \R</m>: these are called <em>eigenvectors</em> of <m>T</m>.
    </p>
  </introduction>

  <subsection>
    <title>Eigenvectors</title>
    <p>
      We further motivate the notion of an eigenvector with an illustrative example.
    </p>
    <example xml:id="eg_reflection_eigenvectors">
      <statement>
        <p>
          Consider <m>T_A\colon \R^2\rightarrow \R^2</m> where
           <me>
           A=\frac{1}{5}\begin{amatrix}[rr]-3\amp 4\\ 4\amp 3 \end{amatrix}
           </me>.
           It turns out that <m>T=T_A</m> has a simple geometric description, though you would not have guessed this from <m>A</m>. To reveal the geometry at play, we represent <m>T</m> with respect to the orthogonal basis <m>B'=(\boldv_1=(1,2), \boldv_2=(2,-1))</m>. Since
            <md>
              <mrow>T((1,2)) \amp=A\colvec{1 \\ 2 }=\colvec{1 \\ 2}=1(1,2)+0(2,-1) </mrow>
              <mrow>T_A((2,-1)) \amp=A\colvec{2 \\ -1}=-(2,-1)=0(1,2)+(-1)(2,-1) </mrow>
            </md>,
            it follows that
            <me>
              [T]_{B'}=\begin{amatrix}[rr] 1\amp 0\\ 0\amp -1  \end{amatrix}
            </me>.
            The alternative representation given by <m>A'=[T]_{B'}</m> reveals that <m>T</m> is none other than reflection through the line <m>\ell=\Span\{(1,2)\}</m>!  How? Given any vector <m>\boldv\in \R^2</m>, we can write
          <men xml:id="eq_reflection_decomposition">
            \boldv=c_1\boldv_1+c_2\boldv_2
          </men>.
          Note that since <m>\boldv_1</m> and <m>\boldv_2</m> are orthogonal, we have <m>c_1\boldv_1\in \ell</m> and <m>c_2\boldv_2\in \ell^\perp</m>: <ie />, <xref ref="eq_reflection_decomposition"/> is the orthogonal decomposition of <m>\boldv</m> with respect to <m>\ell</m>. Next, the representation <m>A'=[T]_{B'}</m> tells us that <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=-\boldv_2</m>. It follows that <m>T(\boldv)=c_1\boldv_1-c_2\boldv_2</m>.
          This is nothing more than a vector description of reflection through the line <m>\ell</m>, as <xref ref="fig_reflection_eigenvectors"/> makes clear.
          </p>
      </statement>
    </example>

        <figure xml:id="fig_reflection_eigenvectors">
          <title>Reflection through <m>\ell=\Span\{(1,2)\}</m></title>
          <image xml:id="im_reflection_eigenvectors" width="100%" source="images/im_reflection_eigenvectors"/>
          <caption>Reflection through <m>\ell=\Span\{(1,2)\}</m></caption>
        </figure>
    <p>
      The success of our analysis in <xref ref="eg_reflection_eigenvectors"/> depended on finding the vectors <m>\boldv_1</m> and <m>\boldv_2</m> satisfying <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=(-1)\boldv_2</m>. These are examples of eigenvectors, a concept we now officially define. For reasons that will become clear below, it is convenient to give separate definitions for linear transformations and matrices.
    </p>
    <definition xml:id="d_eigenvectors">
      <idx><h>eigenvector</h></idx>
      <idx><h>eigenvalue</h></idx>
      <title>Eigenvectors and eigenvalues</title>
      <statement>
        <case>
         <title>Eigenvectors of linear transformations</title>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation.
          A <em>nonzero</em> vector <m>\boldv\in V</m> satisfying
          <men xml:id="eq_eigenvector_def">
            T(\boldv)=\lambda\boldv
          </men> for some <m>\lambda\in\R</m> is called an <term>eigenvector</term> of <m>T</m> with <term>eigenvalue</term> <m>\lambda</m>.
        </p>
        </case>
        <case>
         <title>Eigenvectors of matrices</title>
        <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. A nonzero <m>\boldx\in \R^n</m> satisfying
        <men xml:id="eq_eigenvector_matrix_def">
          A\boldx=\lambda\boldx
        </men>
        for some <m>\lambda\in \R</m> is called an <term>eigenvector</term> of <m>A</m> with <term>eigenvalue</term> <m>\lambda</m>.
        </p>
        </case>
      </statement>
    </definition>
      <remark xml:id="rm_eigenvector_lambda">
      <statement>
        <p>
          You ask: Why use <m>\lambda</m> instead of <m>c</m> or <m>k</m>? My answer: tradition!
        </p>
      </statement>
    </remark>
        <remark xml:id="rm_eigenvector_nonzero">
      <statement>
        <p>
         Note well the important condition that an eigenvector must be nonzero. This means <m>\boldzero</m> by definition is <em>not</em> an eigenvector. If we allowed <m>\boldzero</m> as an eigenvector, then the notion of the eigenvalue <em>of an eigenvector</em> would no longer be well-defined. This is because for any linear transformation we have
         <me>
           T(\boldzero)=\boldzero,
         </me>
         which implies that
         <me>
           T(\boldzero)=\lambda\boldzero
         </me>
         for all <m>\lambda\in \R</m>.
        </p>
      </statement>
    </remark>
    <p>
      Given a linear transformation <m>T\colon V\rightarrow V</m> we wish to determine (a) which values <m>\lambda\in \R</m> are eigenvalues of <m>T</m>, and (b) for each eigenvalue <m>\lambda</m>, which vectors <m>\boldv\in V</m> are eigenvectors of <m>T</m> with eigenvalue <m>\lambda</m>. In the next examples we carry out such an investigation in an <foreign>ad hoc</foreign> manner. The first example is a generalization of our analysis in <xref ref="eg_reflection_eigenvectors"/>.
    </p>
    <example xml:id="eg_eigenvector_adhoc_reflection">
      <title>Eigenvectors of reflections</title>
      <statement>
        <p>
        Let <m>\ell=\Span\{\boldv\}</m> be a line in <m>\R^2</m> passing through the origin, and define <m>T\colon \R^2\rightarrow \R^2</m> to be reflection through <m>\ell</m>. (See <xref ref="d_reflection"/>.) Find all eigenvectors and eigenvalues of <m>T</m>. Use a geometric argument.
        </p>
      </statement>
      <solution>
        <p>
        Let <m>\ell^\perp</m>
        </p>
      </solution>
    </example>
    <example xml:id="eg_eigenvector_adhoc_rotation">
      <statement>
        <p>
          Fix <m>\theta\in (0,2\pi)</m> and define <m>T\colon \R^2\rightarrow \R^2</m> to be rotation by <m>\theta</m>. (See <xref ref="d_rotation"/>) Find all eigenvectors and eigenvalues of <m>T</m>. Use a geometric argument. Your answer will depend on the choice of <m>\theta</m>.
        </p>
      </statement>
      <solution>
        <p>

        </p>
      </solution>
    </example>
    <example xml:id="eg_eigenvector_adhoc_transposition">
      <title>Transposition</title>
      <statement>
        <p>
          Consider the linear transformation
          <md>
            <mrow>S\colon M_{22} \amp\rightarrow M_{22} </mrow>
            <mrow> A \amp\mapsto A^T </mrow>
          </md>.
          Determine all eigenvectors and eigenvalues of <m>S</m>.
        </p>
      </statement>
      <solution>
        <p>
          To be an eigenvector of <m>S</m> a nonzero matrix <m>A</m> must satisfy <m>S(A)=\lambda A</m> for some <m>\lambda\in \R</m>. Using the definition of <m>S</m>, this means
          <men xml:id="eq_eigenvectors_adhoc_transposition">
            A^T=\lambda A
          </men>.
          We ask for which scalars <m>\lambda\in \R</m> do there exist a nonzero matrix <m>A</m> satisfying <xref ref="eq_eigenvectors_adhoc_transposition"/>. Let's consider some specific choices of <m>\lambda</m>.
        </p>
        <case>
         <title>Case: <m>\lambda=1</m></title>
        <p>
        In this case <xref ref="eq_eigenvectors_adhoc_transposition"/> reads <m>A^T=1\,A=A</m>. Thus the eigenvectors of <m>S</m> with eigenvalue <m>1</m> are precisely the nonzero  <em>symmetric</em> matrices: <ie />,
        <me>
          A=\begin{amatrix}[rr]a\amp b\\ b\amp c  \end{amatrix}
        </me>.
        </p>
        </case>
        <case>
         <title>Case: <m>\lambda= -1</m></title>
        <p>
        For this choice of <m>\lambda</m> we seek nonzero matrices satisfying <m>S(A)=A^T=(-1)A=-A</m>. These are precisely the nonzero <em>skew-symmetric</em> matrices: <ie />,
        <me>
          A=\begin{amatrix}[rr]0\amp a\\ -a \amp 0  \end{amatrix}
        </me>.
        </p>
        </case>
        <case>
         <title>Case: <m>\lambda\ne \pm 1</m></title>
        <p>
        Suppose <m>A=\begin{amatrix}[cc]a\amp b \\ c\amp d  \end{amatrix}</m> satisfies <m>A^T=\lambda A</m>. Equating the entries of these two matrices yields the system
        <md>
          <mrow> a \amp =\lambda a </mrow>
          <mrow> d \amp = \lambda d</mrow>
          <mrow> b \amp =\lambda c </mrow>
          <mrow> c \amp =\lambda b </mrow>
        </md>.
        The first two equations imply <m>a=d=0</m>, using the fact that <m>\lambda\ne 1</m>. The second two equations imply further that <m>b=\lambda^2 b</m> and <m>c=\lambda^2 c</m>. Since <m>\lambda\ne \pm 1</m>, <m>\lambda^2\ne 1</m>. It follows that <m>b=c=0</m>. We conclude that for <m>\lambda\ne \pm 1</m>, if <m>A^T=\lambda A</m>, then <m>A=\boldzero</m>. It follows that <m>\lambda</m> is not an eigenvalue of <m>S</m> in this case.
        </p>
        </case>
        In summation, our analysis shows that the transposition operator <m>S</m> has exactly two eigenvalues, <m>\lambda_1=1</m> and <m>\lambda_2=-1</m>, that the eigenvectors of <m>S</m> with eigenvalue 1 are the nonzero symmetric matrices, and that the eigenvalues of <m>S</m> with eigenvalue <m>-1</m> are the nonzero skew-symmetric matrices.
      </solution>
    </example>
  </subsection>

  <subsection>
    <title>Finding eigenvalues of <m>T</m> systematically</title>
    <p>
      You can imagine that our <foreign>ad hoc</foreign> approach to finding eigenvalues and eigenvectors will break down once the linear transformation under consideration becomes complicated enough. As such it is vital to have a systematic method of finding all eigenvalues and eigenvectors of a linear transformation <m>T\colon V\rightarrow V</m>. The rest of this section is devoted to describing just such a method in the special case where <m>\dim V=n\lt\infty</m>. The first key observation is that we can answer the eigenvalues/eigenvectors of <m>T</m> by answering the same question about <m>A=[T]_B</m>, where <m>B</m> is an ordered basis of <m>V</m>.
    </p>
    <theorem xml:id="th_eigenvectors_transformation_matrixrep">
      <title>Eigenvectors of a linear transformation</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation, let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> be an ordered basis of <m>V</m>, and let <m>A=[T]_B</m>.
        </p>
        <ol>
          <li>
            <p>
             A value <m>\lambda\in \R</m> is an eigenvalue of <m>T</m> if and only if it is an eigenvalue of <m>A</m>. Thus <m>T</m> and <m>A</m> have the same set of eigenvalues.
            </p>
          </li>
          <li>
            <p>
              A vector <m>\boldv\in V</m> is an eigenvector of <m>T</m> with eigenvalue <m>\lambda</m> if and only if <m>\boldx=[\boldv]_B</m> is an eigenvector of <m>A</m> with eigenvalue <m>\lambda</m>.
            </p>
          </li>
        </ol>
      </statement>
    </theorem>
    <p>
      Thanks to <xref ref="th_eigenvectors_transformation_matrixrep"/>, we can boil down the eigenvector/eigenvalue question for linear transformations of finite vector spaces to the analogous question about square matrices. The next theorem is the key result.
    </p>
    <theorem xml:id="th_eigenvectors_matrices">
      <title>Eigenvectors of matrices</title>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> matrix. Fix <m>\lambda\in \R</m>.
          <ol>
            <li>
              <p>
                The eigenvectors of <m>A</m> with eigenvalue <m>\lambda</m> are the nonzero elements of the subspace <m>\NS(\lambda I-A)</m>.
              </p>
            </li>
            <li>
              <p>
                As a consequence, <m>\lambda\in \R</m> is an eigenvalue of <m>A</m> if and only if the matrix <m>\lambda I-A</m> is singular (<ie />, noninvertible).
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
<definition xml:id="d_eigenspace">
  <title>Eigenspaces</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix. Given <m>\lambda\in \R</m> the <term><m>\lambda</m>-eigenspace</term> of <m>A</m> is the subspace <m>W_\lambda</m> defined as
      <me>
        W_\lambda=\NS (\lambda I -A)
      </me>.
      The nonzero elements of <m>W_\lambda</m> are precisely the eigenvectors of <m>A</m> with eigenvalue <m>\lambda</m>.
    </p>
  </statement>
</definition>
<p>
  We have made some significant progress. Finding all eigenvectors with eigenvalue <m>\lambda</m> amounts to finding the nonzero elements of the space <m>\NS (\lambda I-A)</m>, and our fundamental space algorithms make this an easy task. However, for there to be any eigenvectors whatsoever, the matrix <m>\NS(\lambda I-A)</m> must be singular, or equivalently, <m>\det (\lambda I-A)=0</m>. Thus the eigenvalues of <m>A</m> are precisely the <em>roots</em> of the function <m>p(t)=\det(tI-A)</m>. Clearly this function deserves a name.
</p>
<definition>
  <idx><h>characteristic polynomial</h><h>of a matrix</h></idx>
  <title>Characteristic polynomial of a matrix</title>
  <statement>
    <p>
      Let <m>A</m> be <m>n\times n</m>.
      The <term>characteristic polynomial</term>
      of <m>A</m> is the function
      <me>
      p(t)=\det(tI-A)
      </me>.
    </p>
  </statement>
</definition>
<corollary xml:id="cor_eigenvalues">
  <title>Eigenvalues of a matrix</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix, and let <m>p(t)=\det(tI-A)</m>. The following are equivalent:
    </p>
    <ol>
      <li>
        <p>
          <m>\lambda</m> is an eigenvalue of <m>A</m>;
        </p>
      </li>
      <li>
        <p>
          <m>\lambda I-A</m> is singular;
        </p>
      </li>
      <li>
        <p>
          <m>\det(\lambda I-A)=0</m>;
        </p>
      </li>
      <li>
        <p>
         <m>p(\lambda)=0</m>.
        </p>
      </li>
    </ol>
  </statement>
</corollary>
<corollary xml:id="cor_eigenvalue_invertible">
  <statement>
    <p>
      A square matrix <m>A</m> is invertible if and only if <m>0</m> is not an eigenvalue of <m>A</m>.
    </p>
  </statement>
</corollary>
<p>
  The function <m>p(t)=\det(tI-A)</m> is indeed a polynomial. In fact, as we we will see below (<xref ref="th_characteristic_polynomial"/>), if <m>A</m> is <m>n\times n</m>, then <m>p(t)</m> is a degree <m>n</m> polynomial.
</p>
<theorem xml:id="th_characteristic_polynomial">
  <title>Characteristic polynomial</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix.
    </p>
    <ol>
      <li>
        <p>
          The function <m>p(t)=\det (tI-A)</m> is a monic polynomial of degree <m>n</m>: <ie />,
          <men xml:id="eq_char_poly">
          p(t)=t^n+a_{n-1}t^{n-1}+\cdots +a_1t+a_0
        </men>.
        </p>
      </li>
      <li>
        <p>
          Over <m>\C</m> we can factor <m>p(t)</m> completely as
          <me>
            p(t)=(t-\lambda_1)(t-\lambda_2)\cdots (t-\lambda_n),
          </me>
          where the <m>\lambda_i\in \C</m> are the (not necessarily distinct) roots of <m>p(t)</m>. We call the <m>\lambda_i</m> the <term>complex eigenvalues</term> of <m>A</m>.
        </p>
      </li>
      <li>
        <p>
          The (usual) eigenvalues of <m>A</m> are precisely the real roots of <m>p(t)</m>: <ie />, the <m>\lambda_i</m> for which <m>\lambda_i\in \R</m>.
        </p>
      </li>
      <li>
        <p>
          The coefficient <m>a_{n-1}</m> in <xref ref="eq_char_poly"/> satisfies
          <men xml:id="eq_char_poly_trace">
            a_{n-1}=-\tr A=-(\lambda_1+\lambda_2+\cdots +\lambda_n)
          </men>.
        </p>
      </li>
      <li>
        <p>
          The coefficient <m>a_0</m> in <xref ref="eq_char_poly"/> satisfies
        </p>
        <men xml:id="eq_char_poly_det">
          a_0=(-1)^n\det A=(-1)^n\lambda_1\lambda_2\cdots \lambda_n
        </men>.
      </li>
      <li>
        <p>
          We conclude that
          <mdn>
            <mrow xml:id="eq_tr_sum_eigenvalues"> \tr A \amp = \lambda_1+\lambda_2+\cdots +\lambda_n </mrow>
            <mrow xml:id="eq_det_prod_eigenvalues"> \det A\amp = \lambda_1\lambda_2\cdots \lambda_n </mrow>
          </mdn>.
        </p>
      </li>
    </ol>
  </statement>
</theorem>
<corollary xml:id="cor_eigenvalues_atmost_n">
  <statement>
    <p>
      An <m>n\times n</m> matrix <m>A</m> has at most <m>n</m> distinct eigenvalues.
    </p>
  </statement>
</corollary>
</subsection>

  <subsection>
    <title>Example</title>
    <p>
      Find all eigenvalues of <m>A=\begin{bmatrix}1\amp 1\\ 2\amp -1 \end{bmatrix}</m>. \begin{bsolution} Compute:
    </p>
  </subsection>
  <p>
    <m>p(t)=\det(tI-A)=\val{\begin{array}{cc}t-1\amp -1\\ -2\amp t+1 \end{array} }=t^2-3</m>.
  </p>
  <p>
    The eigenvalues are the roots of <m>p(t)</m>:
    <m>\lambda_1=\sqrt{3}, \lambda_2=-\sqrt{3}</m>. \end{bsolution}
  </p>
  <subsection>
    <title>Example</title>
    <p>
      Find all eigenvalues of <m>A=\begin{bmatrix}2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{bmatrix}</m>. \begin{bsolution} Compute:
    </p>
  </subsection>
  <p>
    <m>p(t)=\det(tI-A)=\val{\begin{array}{ccc}t-2\amp 1\amp 1\\ 1\amp t-2\amp 1\\ 1\amp 1\amp t-2 \end{array} }=t^3-6t^2+9t=t(t-3)^2</m>.
  </p>
  <p>
    The eigenvalues are the roots of <m>p(t)</m>:
    <m>\lambda_1=0</m>, <m>\lambda_2=3</m>. \end{bsolution}
  </p>

  <subsection>
    <title>Computing eigenspaces of a matrix </title>
    <p>
      Now suppose we have found an eigenvalue <m>\lambda</m> of <m>A</m>.
    </p>
    <p>
      Our chain of equivalences from earlier told us that the eigenvectors of <m>A</m> with eigenvalue <m>\lambda</m> are precisely the <em>nonzero</em>
      solutions to the matrix equation <m>(\lambda I_n-A)\boldx=\boldzero</m>.
    </p>
    <p>
      Thus to find all eigenvectors with eigenvalue <m>\lambda</m> we simply compute
      <m>\NS(\lambda I_n-A)</m> using Gaussian elimination.
    </p>

  </subsection>
  <algorithm xml:id="proc_eigenspaces_matrix">
    <title>Computing eigenspaces of a matrix</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. To find all real eigenvalues <m>\lambda</m> of <m>A</m> and compute a basis for the corresponding eigenspace <m>W_\lambda</m>, proceed as follows.
      </p>
      <ol>
        <li>
          <p>
            Compute <m>p(t)=\det(tI-A)</m>. Let
            <m>\lambda_1, \lambda_2, \dots, \lambda_r</m> be the distinct <em>real</em> roots of <m>p(t)</m>. These are the eigenvalues of <m>A</m>.
          </p>
        </li>
        <li>
          <p>
            For each <m>\lambda\in \{\lambda_1, \lambda_2,\dots, \lambda_r\}</m>, the corresponding eigenspace is
            <me>
            W_{\lambda}=\NS(\lambda I-A)
            </me>.
            Use the null space algorithm (<xref ref="proc_fund_spaces" text="global"/>) to compute a basis for <m>W_\lambda</m>.
          </p>
        </li>
      </ol>
    </statement>
  </algorithm>
  <algorithm xml:id="proc_eigenspaces_transformation">
    <title>Computing eigenspaces of a linear transformation</title>
    <statement>
      <p>
        Let <m>T\colon V\rightarrow V</m> be a linear transformation of a finite-dimensional vector space <m>V</m>. To compute the eigenvalues and eigenspaces of <m>T</m>, proceed as follows.
      </p>
      <ol>
        <li>
          <p>
            Pick an ordered basis <m>B</m> of <m>V</m> and compute <m>A=[T]_B</m>.
          </p>
        </li>
        <li>
          <p>
            Apply <xref ref="proc_eigenspaces_matrix"/> to <m>A</m> to compute bases of <m>W_\lambda</m> for each eigenvalue <m>\lambda</m> of <m>A</m>.
          </p>
        </li>
        <li>
          <p>
            For each eigenvalue <m>\lambda</m>, <q>lift</q> the basis of <m>W_\lambda\subseteq \R^n</m> back up to <m>V</m> using the coordinate transformation <m>[\phantom{\boldv}]_B</m>. The result is the basis for the <m>\lambda</m>-eigenspace of <m>T</m>.
          </p>
        </li>
      </ol>
    </statement>
  </algorithm>
  <p>
    Let's compute the eigenspaces of our previous examples.
  </p>
  <subsection>
    <title>Example</title>
    <p>
      <m>A=\begin{bmatrix}1\amp 1\\ 2\amp -1 \end{bmatrix}</m>,
      <m>\lambda_1=\sqrt{3}</m>, <m>\lambda_2=-\sqrt{3}</m>. \begin{bsolution}
    </p>
  </subsection>
  <p>
    Compute:
  </p>
  <p>
    <m>W_{\sqrt{3}}=\NS(\sqrt{3}I-A)=\NS\left(\begin{bmatrix}\sqrt{3}-1\amp -1\\ -2\amp \sqrt{3}+1 \end{bmatrix} \right) =\Span(\left\{ \begin{bmatrix}\sqrt{3}+1\\2 \end{bmatrix} \right\})</m>
  </p>
  <p>
    <m>W_{-\sqrt{3}}=\NS(-\sqrt{3}I-A)=\NS\left(\begin{bmatrix}-\sqrt{3}-1\amp -1\\ -2\amp -\sqrt{3}+1 \end{bmatrix} \right) =\Span(\left\{ \begin{bmatrix}-\sqrt{3}+1\\ 2 \end{bmatrix} \right\})</m>
  </p>
  <p>
    Thus any multiple of <m>\boldv_1=(\sqrt{3}+1,2)</m> is an eigenvector of <m>A</m> with eigenvalue <m>\sqrt{3}</m>,
    and any multiple of <m>\boldv_2=(-\sqrt{3}+1,2)</m> is an eigenvector of <m>A</m> with eigenvalue <m>-\sqrt{3}</m>.
  </p>
  <p>
    Check: <m>A\begin{bmatrix}\sqrt{3}+1\\ 2 \end{bmatrix} =\begin{bmatrix}3\sqrt{3}+3\\ 2\sqrt{3} \end{bmatrix} =\sqrt{3}\begin{bmatrix}\sqrt{3}+1\\ 2 \end{bmatrix} \checkmark</m>.
    <m>A\begin{bmatrix}-\sqrt{3}+1\\ 2 \end{bmatrix} =\begin{bmatrix}-3\sqrt{3}+3\\ -2\sqrt{3} \end{bmatrix} =-\sqrt{3}\begin{bmatrix}-\sqrt{3}+1\\ 2 \end{bmatrix} \checkmark</m>.
  </p>
  <p>
    \end{bsolution}
  </p>
  <p>
    Let's compute the eigenspaces of our previous examples.
  </p>
  <subsection>
    <title>Example</title>
    <p>
      <m>A=\begin{bmatrix}2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{bmatrix}</m>,
      <m>\lambda_1=0</m>, <m>\lambda_2=3</m>. \begin{bsolution} Compute:
    </p>
  </subsection>
  <p>
    <m>W_0=\NS(0I-A)=\NS\left(\begin{bmatrix}2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{bmatrix} \right)=\Span\left(\left\{\boldv_1=\begin{bmatrix}1 \\ 1 \\ 1 \end{bmatrix} \right\}\right)</m>.
  </p>
  <p>
    <m>W_3=\NS(3I-A)=\NS(\left( \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{bmatrix} \right) =\Span\left(\left\{\boldv_2= \begin{bmatrix}1 \\ -1 \\ 0 \end{bmatrix} , \boldv_3=\begin{bmatrix}1 \\ 0 \\ -1 \end{bmatrix} \right\}\right)</m>.
  </p>
  <p>
    Thus the eigenvectors of <m>A</m> with eigenvalue 0 are vectors of the form <m>c\boldv_1</m>,
    and the eigenvectors of <m>A</m> with eigenvalue 3 are vectors of the form <m>c\boldv_2+d\boldv_3</m>.
  </p>
  <subsection>
    <title>Comment</title>
    <p>
      Note that in general <m>W_0=\NS(0I-A)=\NS(-A)=\NS(A)</m>.
      (Think about why the last equality is true.)
    </p>
  </subsection>
  <p>
    Thus the eigenvectors of <m>A</m> with eigenvalue 0 are precisely the nonzero vectors of <m>\NS(A)</m>,
    should there be any at all! \end{bsolution}
  </p>
</section>
