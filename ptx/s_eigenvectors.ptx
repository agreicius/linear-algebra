<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="ss_eigenvectors">
  <title>Eigenvectors and eigenvalues</title>
  <introduction>
    <p>
      For the remaining sections of this chapter we will focus our investigation on linear transformations of the form <m>T\colon V\rightarrow V</m>: that is, transformations from a space <m>V</m> into itself. When <m>V</m> is finite-dimensional we can get a computational grip on <m>T</m> by choosing an ordered basis <m>B</m> and considering the matrix representation <m>[T]_B</m>. As was illustrated in <xref ref="eg_matrixreps_proj"/>, different matrix representations <m>[T]_B</m> and <m>[T]_{B'}</m> provide different insights into the nature of <m>T</m>. Furthermore, we see from this example that if the action of <m>T</m> on a chosen basis is simple to describe, then so too is the matrix representation of <m>T</m> with respect to that basis.
    </p>
    <p>
      A particularly agreeable situation arises when the basis <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> satisfies
      <me>
        T(\boldv_i)=c_i\boldv_i, c_i\in \R
      </me>
      for all <m>1\leq i\leq n</m>. Using recipe <xref ref="eq_matrixrep_formula"/> we in this case that the corresponding matrix representation
      <me>
        [T]_B=\begin{bmatrix}
          c_1 \amp 0\amp \dots \amp \amp 0\\
          0   \amp c_2\amp 0\amp \dots \amp 0\\
          0\amp 0\amp \ddots \amp \amp 0\\
          \vdots  \amp \amp \amp \amp \vdots \\
          0\amp 0\amp \dots \amp 0\amp c_n
      \end{bmatrix}
      </me>
      is diagonal! Diagonal matrices are about as simple as they come: they wear all of their properties (rank, nullity, invertibility, <etc />) on their sleeve.  If we hope to find a diagonal matrix representation of <m>T</m>, then we should seek nonzero vectors <m>\boldv</m> satisfying <m>T(\boldv)=c\boldv</m> for some <m>c\in \R</m>: these are called <em>eigenvectors</em> of <m>T</m>.
    </p>
  </introduction>

  <subsection>
    <title>Eigenvectors</title>
    <p>
      We further motivate the notion of an eigenvector with an illustrative example.
    </p>
    <example xml:id="eg_reflection_eigenvectors">
      <statement>
        <p>
          Consider <m>T_A\colon \R^2\rightarrow \R^2</m> where
           <me>
           A=\frac{1}{5}\begin{amatrix}[rr]-3\amp 4\\ 4\amp 3 \end{amatrix}
           </me>.
           It turns out that <m>T=T_A</m> has a simple geometric description, though you would not have guessed this from <m>A</m>. To reveal the geometry at play, we represent <m>T</m> with respect to the orthogonal basis <m>B'=(\boldv_1=(1,2), \boldv_2=(2,-1))</m>. Since
            <md>
              <mrow>T((1,2)) \amp=A\colvec{1 \\ 2 }=\colvec{1 \\ 2}=1(1,2)+0(2,-1) </mrow>
              <mrow>T_A((2,-1)) \amp=A\colvec{2 \\ -1}=-(2,-1)=0(1,2)+(-1)(2,-1) </mrow>
            </md>,
            it follows that
            <me>
              [T]_{B'}=\begin{amatrix}[rr] 1\amp 0\\ 0\amp -1  \end{amatrix}
            </me>.
            The alternative representation given by <m>A'=[T]_{B'}</m> reveals that <m>T</m> is none other than reflection through the line <m>\ell=\Span\{(1,2)\}</m>!  How? Given any vector <m>\boldv\in \R^2</m>, we can write
          <men xml:id="eq_reflection_decomposition">
            \boldv=c_1\boldv_1+c_2\boldv_2
          </men>.
          Note that since <m>\boldv_1</m> and <m>\boldv_2</m> are orthogonal, we have <m>c_1\boldv_1\in \ell</m> and <m>c_2\boldv_2\in \ell^\perp</m>: <ie />, <xref ref="eq_reflection_decomposition"/> is the orthogonal decomposition of <m>\boldv</m> with respect to <m>\ell</m>. Next, the representation <m>A'=[T]_{B'}</m> tells us that <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=-\boldv_2</m>. It follows that <m>T(\boldv)=c_1\boldv_1-c_2\boldv_2</m>.
          This is nothing more than a vector description of reflection through the line <m>\ell</m>, as <xref ref="fig_reflection_eigenvectors"/> makes clear.
          </p>
      </statement>
    </example>

        <figure xml:id="fig_reflection_eigenvectors">
          <title>Reflection through <m>\ell=\Span\{(1,2)\}</m></title>
          <image xml:id="im_reflection_eigenvectors" width="100%" source="images/im_reflection_eigenvectors"/>
          <caption>Reflection through <m>\ell=\Span\{(1,2)\}</m></caption>
        </figure>
    <p>
      The success of our analysis in <xref ref="eg_reflection_eigenvectors"/> depended on finding the vectors <m>\boldv_1</m> and <m>\boldv_2</m> satisfying <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=(-1)\boldv_2</m>. These are examples of eigenvectors, a concept we now officially define. For reasons that will become clear below, it is convenient to give separate definitions for linear transformations and matrices.
    </p>
    <definition xml:id="d_eigenvectors">
      <idx><h>eigenvector</h></idx>
      <idx><h>eigenvalue</h></idx>
      <title>Eigenvectors and eigenvalues</title>
      <statement>
        <case>
         <title>Eigenvectors of linear transformations</title>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation.
          A <em>nonzero</em> vector <m>\boldv\in V</m> satisfying
          <men xml:id="eq_eigenvector_def">
            T(\boldv)=\lambda\boldv
          </men> for some <m>\lambda\in\R</m> is called an <term>eigenvector</term> of <m>T</m> with <term>eigenvalue</term> <m>\lambda</m>.
        </p>
        </case>
        <case>
         <title>Eigenvectors of matrices</title>
        <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. A nonzero <m>\boldx\in \R^n</m> satisfying
        <men xml:id="eq_eigenvector_matrix_def">
          A\boldx=\lambda\boldx
        </men>
        for some <m>\lambda\in \R</m> is called an <term>eigenvector</term> of <m>A</m> with <term>eigenvalue</term> <m>\lambda</m>.
        </p>
        </case>
        In both cases we will call an eigenvector with eigenvalue <m>\lambda</m> a <term><m>\lambda</m>-eigenvector</term> for short.
      </statement>
    </definition>
      <remark xml:id="rm_eigenvector_lambda">
      <statement>
        <p>
          You ask: Why use <m>\lambda</m> instead of <m>c</m> or <m>k</m>? My answer: tradition!
        </p>
      </statement>
    </remark>
        <remark xml:id="rm_eigenvector_nonzero">
      <statement>
        <p>
         Note well the important condition that an eigenvector must be nonzero. This means the zero vector <m>\boldzero</m> by definition is <em>not</em> an eigenvector. If we allowed <m>\boldzero</m> as an eigenvector, then the notion of the eigenvalue <em>of an eigenvector</em> would no longer be well-defined. This is because for any linear transformation we have
         <me>
           T(\boldzero)=\boldzero,
         </me>
         which implies that
         <me>
           T(\boldzero)=\lambda\boldzero
         </me>
         for all <m>\lambda\in \R</m>.
        </p>
      </statement>
    </remark>
        <remark xml:id="rm_eigenvectors_visual">
      <title>Visualizing eigenvectors</title>
      <statement>
        <p>
        Suppose <m>\boldv\ne\boldzero</m> is an eigenvector of the linear transformation <m>T\colon V\rightarrow V</m>. Letting <m>W=\Span\{\boldv\}</m>, this means that <m>T(\boldv)=\lambda\boldv\in W</m>: <ie />, <m>T</m> maps an eigenvector to some other element of the one-dimensional subspace it defines. The special case where <m>V=\R^2</m> is easy to visualize and can help guide your intuition in the general case. (See <xref ref="fig_eigenvector_visual"/>) Here the space <m>\Span\{\boldv\}=\ell</m>
        is a line passing through origin. If <m>\boldv</m> is an eigenvector of a given linear transformation, then it must be mapped to some other vector pointing along <m>\ell</m>: <eg />, <m>\lambda_1\boldv</m> or <m>\lambda_2\boldv_2</m>. It it is not an eigenvector, it gets mapped to a vector <m>\boldw</m> that does not point along <m>\ell</m>.
        </p>
      </statement>
    </remark>
    <figure xml:id="fig_eigenvector_visual">
      <title>Visualizing eigenvectors</title>
      <caption>Visualizing eigenvectors</caption>
      <image xml:id="im_eigenvector_visual" width="100%" source="images/im_eigenvector_visual">
      </image>
    </figure>



    <p>
      Given a linear transformation <m>T\colon V\rightarrow V</m> we wish to (a) determine which values <m>\lambda\in \R</m> are eigenvalues of <m>T</m>, and (b) find all the eigenvectors corresponding to a given eigenvalue <m>\lambda</m>. In the next examples we carry out such an investigation in an <foreign>ad hoc</foreign> manner.
    </p>
    <example xml:id="eg_eigenvectors_zerotransform">
      <title>Zero and identity transformations</title>
      <statement>
        <p>
          Assume <m>V</m> is nonzero. Recall that the zero transformation <m>0_V\colon V\rightarrow V </m> and identity transformation <m>\id_V\colon V\rightarrow V</m> are defined as <m>0_V(\boldv)=\boldzero</m> and <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>. Find all eigenvalues and eigenvectors of <m>0_V</m> and <m>\id_V</m>.
        </p>
      </statement>
      <solution>
        <p>
          Since <m>0_V(\boldv)=\boldzero=0\boldv</m> for all <m>\boldv\in V</m>, we see that <m>0</m> is the only eigenvalue of <m>0_V</m>, and that all nonzero vectors of <m>V</m> are <m>0</m>-eigenvectors.
        </p>
        <p>
          Similarly, since <m>\id_V(\boldv)=\boldv=1\boldv</m> for all <m>\boldv\in V</m>, we see that <m>1</m> is the only eigenvalue of <m>\id_V</m>, and that all nonzero vectors of <m>V</m> are <m>1</m>-eigenvectors.
        </p>
      </solution>
    </example>
    <example xml:id="eg_eigenvector_adhoc_reflection">
      <title>Reflection</title>
      <statement>
        <p>
        Let <m>\ell</m> be a line in <m>\R^2</m> passing through the origin, and define <m>T\colon \R^2\rightarrow \R^2</m> to be reflection through <m>\ell</m>. (See <xref ref="d_reflection"/>.) Find all eigenvectors and eigenvalues of <m>T</m>. Use a geometric argument.
        </p>
      </statement>
      <solution>
        <p>
        Since the reflection operator fixes all elements of the line <m>\ell</m>, we have <m>T(\boldx)=\boldx</m> for any <m>\boldx\in \ell</m>. This shows that any nonzero element of <m>\ell</m> is an eigenvectors of <m>T</m> with eigenvalue <m>1</m>.
      </p>
      <p>
        Similarly, since <m>\ell^\perp</m> is orthogonal to <m>\ell</m>, reflection through <m>\ell</m> takes any element <m>\boldx=(x,y)\in \ell^\perp</m> and maps it to <m>-\boldx=(-x,-y)</m>. Thus any nonzero element <m>\boldx\in \ell^\perp</m> is  an eigenvector of <m>T</m> with eigenvalue <m>-1</m>.
      </p>
      <p>
        We claim that these two cases exhaust all eigenvectors of <m>T</m>. Indeed, in general a nonzero vector <m>\boldv</m> lies in the line <m>\ell'=\Span\{\boldx\}</m>, and its reflection <m>T(\boldx)</m> lies in the line <m>\ell''=\Span\{T(\boldx)\}</m>, which itself is the result of reflecting the line <m>\ell'</m> through <m>\ell</m>. Now assume <m>T(\boldx)=\lambda\boldx</m>. We must have <m>\lambda\ne 0</m>, since <m>T(\boldv)\ne \boldzero</m> if <m>\boldx\ne \boldzero</m>;
        but if <m>\lambda\ne 0</m> it follows that the line <m>\ell=\Span\{\boldx\}</m> and its reflection <m>\ell''=\Span\{T(\boldv)\}</m> are equal. Clearly the only lines that are mapped to themselves by reflection through <m>\ell</m> are <m>\ell</m> and <m>\ell^\perp</m>. Thus if <m>\boldx</m> is an eigenvector of <m>T</m> it must lie in <m>\ell</m> or <m>\ell^\perp</m>.
      </p>
      </solution>
    </example>
    <example xml:id="eg_eigenvector_adhoc_rotation">
      <title>Rotation</title>

      <statement>
        <p>
          Fix <m>\theta\in (0,2\pi)</m> and define <m>T\colon \R^2\rightarrow \R^2</m> to be rotation by <m>\theta</m>. (See <xref ref="d_rotation"/>) Find all eigenvectors and eigenvalues of <m>T</m>. Use a geometric argument. Your answer will depend on the choice of <m>\theta</m>.
        </p>
      </statement>
      <solution>
        <p>
          <case>
           <title>Case: <m>\theta=\pi</m></title>
          <p>
          Rotation by <m>\pi</m> sends every vector <m>\boldx\in \R^2</m> to <m>-\boldx</m>: <ie />, <m>T(\boldx)=-\boldx=(-1)\boldx</m>. It follows that <m>\lambda=-1</m> is the only eigenvalue of <m>T</m> and all <em>nonzero</em> elements of <m>\R^2</m> are eigenvectors with eigenvalue <m>-1</m>.
          </p>
          </case>
          <case>
           <title>Case: <m>\theta\ne \pi</m></title>
          <p>
            A similar argument to the one in <xref ref="eg_eigenvector_adhoc_reflection"/> shows that <m>T</m> has no eigenvalues in this case. In more detail, a nonzero vector <m>\boldx</m> lies in the line <m>\ell=\Span\{\boldx\}</m>, and its rotation <m>T(\boldx)</m> lies in the line <m>\ell'=\Span\{T(\boldx)\}</m>, which is the result of rotating <m>\ell</m> by the angle <m>\theta</m>. Since <m>\theta\ne \pi</m>, it is clear that <m>\ell\ne \ell'</m>, and thus we cannot have <m>T(\boldv)=\lambda\boldv</m> for some <m>\lambda\in \R</m>.
          </p>
          </case>
        </p>

      </solution>
    </example>
    <example xml:id="eg_eigenvector_adhoc_transposition">
      <title>Transposition</title>
      <statement>
        <p>
          Consider the linear transformation
          <md>
            <mrow>S\colon M_{22} \amp\rightarrow M_{22} </mrow>
            <mrow> A \amp\mapsto A^T </mrow>
          </md>.
          Determine all eigenvectors and eigenvalues of <m>S</m>.
        </p>
      </statement>
      <solution>
        <p>
          To be an eigenvector of <m>S</m> a nonzero matrix <m>A</m> must satisfy <m>S(A)=\lambda A</m> for some <m>\lambda\in \R</m>. Using the definition of <m>S</m>, this means
          <men xml:id="eq_eigenvectors_adhoc_transposition">
            A^T=\lambda A
          </men>.
          We ask for which scalars <m>\lambda\in \R</m> does there exist a nonzero matrix <m>A</m> satisfying <xref ref="eq_eigenvectors_adhoc_transposition"/>. Let's consider some specific choices of <m>\lambda</m>.
        </p>
        <case>
         <title>Case: <m>\lambda=1</m></title>
        <p>
        In this case <xref ref="eq_eigenvectors_adhoc_transposition"/> reads <m>A^T=A</m>. Thus the eigenvectors of <m>S</m> with eigenvalue <m>1</m> are precisely the nonzero  <em>symmetric</em> matrices: <ie />,
        <me>
          A=\begin{amatrix}[rr]a\amp b\\ b\amp c  \end{amatrix}
        </me>.
        </p>
        </case>
        <case>
         <title>Case: <m>\lambda= -1</m></title>
        <p>
        For this choice of <m>\lambda</m> we seek nonzero matrices satisfying <m>S(A)=A^T=(-1)A=-A</m>. These are precisely the nonzero <em>skew-symmetric</em> matrices: <ie />,
        <me>
          A=\begin{amatrix}[rr]0\amp a\\ -a \amp 0  \end{amatrix}
        </me>.
        </p>
        </case>
        <case>
         <title>Case: <m>\lambda\ne \pm 1</m></title>
        <p>
        Suppose <m>A=\begin{amatrix}[cc]a\amp b \\ c\amp d  \end{amatrix}</m> satisfies <m>A^T=\lambda A</m>. Equating the entries of these two matrices yields the system
        <md>
          <mrow> a \amp =\lambda a </mrow>
          <mrow> d \amp = \lambda d</mrow>
          <mrow> b \amp =\lambda c </mrow>
          <mrow> c \amp =\lambda b </mrow>
        </md>.
        The first two equations imply <m>a=d=0</m>, using the fact that <m>\lambda\ne 1</m>. The second two equations imply further that <m>b=\lambda^2 b</m> and <m>c=\lambda^2 c</m>. Since <m>\lambda\ne \pm 1</m>, <m>\lambda^2\ne 1</m>. It follows that <m>b=c=0</m>. We conclude that for <m>\lambda\ne \pm 1</m>, if <m>A^T=\lambda A</m>, then <m>A=\boldzero</m>. It follows that <m>\lambda</m> is not an eigenvalue of <m>S</m> in this case.
        </p>
        </case>
        In summation, our analysis shows that the transposition operator <m>S</m> has exactly two eigenvalues, <m>\lambda_1=1</m> and <m>\lambda_2=-1</m>, that the eigenvectors of <m>S</m> with eigenvalue 1 are the nonzero symmetric matrices, and that the eigenvalues of <m>S</m> with eigenvalue <m>-1</m> are the nonzero skew-symmetric matrices.
      </solution>
    </example>
    <example xml:id="eg_eigenvectors_adhoc_derivative">
      <title>Differentiation</title>
      <statement>
        <p>
          Let <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> be defined as <m>T(f)=f'</m>. Find all eigenvalues and eigenvectors of <m>T</m>.
        </p>
      </statement>
      <solution>
        <p>
          An eigenvector of <m>T</m> is a nonzero function <m>f</m> satisfying <m>T(f)=\lambda f</m> for some <m>\lambda</m>. By definition, this means
          <men xml:id="eq_eigenvectors_adhoc_derivative">
            f'=\lambda f
          </men>
          for some <m>\lambda\in\R</m>. Thus <m>\lambda</m> is an eigenvalue of <m>T</m> if and only if the differential equation <xref ref="eq_eigenvectors_adhoc_derivative"/> has a nonzero solution. This is true for all <m>\lambda\in \R</m>! Indeed for any <m>lambda</m> the exponential function <m>f(x)=e^{\lambda x}</m> satisfies <m>f'(x)=\lambda e^{\lambda x}=\lambda f(x)</m> for all <m>x\in \R</m>. Furthermore, any solution to <xref ref="eq_eigenvectors_adhoc_derivative"/> is of the form <m>f(x)=Ce^{\lambda x}</m> for some <m>C\in \R</m>.
          We conclude that (a) every <m>\lambda\in \R</m> is an eigenvalue of <m>T</m>, and (b) for a given <m>\lambda</m>, the <m>\lambda</m>-eigenvectors of <m>T</m> are presicely the functions of the form <m>f(x)=Ce^{\lambda x}</m> for some <m>C\ne 0</m>.
        </p>
      </solution>
    </example>
  </subsection>

  <subsection>
    <title>Finding eigenvalues of <m>T</m> systematically</title>
    <p>
      You can imagine that our <foreign>ad hoc</foreign> approach to finding eigenvalues and eigenvectors will break down once the linear transformation under consideration becomes complicated enough. As such it is vital to have a systematic method of finding all eigenvalues and eigenvectors of a linear transformation <m>T\colon V\rightarrow V</m>. The rest of this section is devoted to describing just such a method in the special case where <m>\dim V=n\lt\infty</m>. The first key observation is that we can answer the eigenvalues/eigenvectors of <m>T</m> by answering the same question about <m>A=[T]_B</m>, where <m>B</m> is an ordered basis of <m>V</m>.
    </p>
    <theorem xml:id="th_eigenvectors_transformation_matrixrep">
      <title>Eigenvectors of a linear transformation</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation, let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> be an ordered basis of <m>V</m>, and let <m>A=[T]_B</m>.
        </p>
        <ol>
          <li>
            <p>
              A vector <m>\boldv\in V</m> is an eigenvector of <m>T</m> with eigenvalue <m>\lambda</m> if and only if <m>\boldx=[\boldv]_B</m> is an eigenvector of <m>A</m> with eigenvalue <m>\lambda</m>.
            </p>
          </li>
          <li>
            <p>
             A value <m>\lambda\in \R</m> is an eigenvalue of <m>T</m> if and only if it is an eigenvalue of <m>A</m>. Thus <m>T</m> and <m>A</m> have the same set of eigenvalues.
            </p>
          </li>
        </ol>
      </statement>
      <proof>
        <p>
          We prove statement (1) as a chain of equivalences:
          <md>
            <mrow>\boldv \text{ is an eigenvector of } T \amp \iff \boldv\ne 0 \text{ and } T\boldv=\lambda \boldv </mrow>
            <mrow> \amp \iff \boldx=[\boldv]_B\ne \boldzero \text{ and } [T\boldv]_B=[\lambda\boldv] \amp (<xref ref="th_coordinates" text="global"/>, (2)) </mrow>
            <mrow>  \amp \iff \boldx=[\boldv]_B\ne \boldzero \text{ and } [T\boldv]_B=\lambda[\boldv]_B \amp (<xref ref="th_coordinates" text="global"/>, (1))</mrow>
            <mrow>  \amp \iff \boldx=[\boldv]\ne \boldzero \text{ and } [T]_B[\boldv]_B=\lambda[\boldv]_B \amp (<xref ref="th_matrixrep" text="global"/>)</mrow>
            <mrow>  \amp \iff \boldx\ne 0 \text{ and } A\boldx=\lambda\boldx</mrow>
            <mrow>  \amp \iff \boldx \text{ is an eigenvector of } A</mrow>
          </md>.
          From (1) it follows directly that if <m>\lambda</m> is an an eigenvalue of <m>T</m>, it is an eigenvalue of <m>A=[T]_B</m>. Conversey, if <m>\lambda</m> is an eigenvalue of <m>A=[T]_B</m>, then there is a nonzero <m>\boldx\in\R^n </m> such that <m>A\boldx=\lambda \boldx</m>. Since <m>[\phantom{\boldv}]_B</m> is surjective (<xref ref="th_coordinates"/>, (3)), there is a vector <m>\boldv\in V</m> such that <m>[\boldv]_B</m>. It follows from (1) that <m>\boldv</m> is a <m>\lambda</m>-eigenvector of <m>T</m>, and thus that <m>\lambda</m> is an eigenvalue of <m>T</m>.
        </p>
      </proof>
    </theorem>
    <p>
      Thanks to <xref ref="th_eigenvectors_transformation_matrixrep"/>, we can boil down the eigenvector/eigenvalue question for linear transformations of finite vector spaces to the analogous question about square matrices. The next theorem is the key result.
    </p>
    <theorem xml:id="th_eigenvectors_matrices">
      <title>Eigenvectors of matrices</title>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> matrix. Fix <m>\lambda\in \R</m>.
          <ol>
            <li>
              <p>
                The eigenvectors of <m>A</m> with eigenvalue <m>\lambda</m> are the nonzero elements of the subspace <m>\NS(\lambda I-A)</m>.
              </p>
            </li>
            <li>
              <p>
                As a consequence, <m>\lambda\in \R</m> is an eigenvalue of <m>A</m> if and only if the matrix <m>\lambda I-A</m> is singular (<ie />, noninvertible).
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          First observe that
          <me>
            A\boldx=\lambda\boldx \iff \lambda\boldx-A\boldx=\boldzero \iff (\lambda I-A)\boldx=\boldzero
          </me>.
          From this equivalence it follows that
          <me>
            \NS(\lambda I-A)=\{\boldx\in \R^n\colon A\boldx=\lambda \boldx
          </me>.
          Since an eigenvector must be nonzero, we conclude that the <m>\lambda</m>-eigenvectors of <m>A</m> are precisely the nonzero elements of <m>\NS(\lambda I-A)</m>. This proves statement (1). As a consequence, <m>A</m> has <m>\lambda</m> as an eigenvalue if and only if <m>\NS (\lambda I-A)</m> contains a nonzero element, if and only if <m>\lambda I-A</m> is singular (<xref ref="th_invertibility_supersized" text="global"/>).
        </p>
      </proof>

    </theorem>
    <p>
      According to <xref ref="th_eigenvectors_matrices"/>,  the eigenvectors of <m>A</m> live in null spaces of matrices of the form <m>\lambda I-A</m>. Accordingly, we call these spaces <em>eigenspaces</em>.
    </p>
<definition xml:id="d_eigenspace">
  <title>Eigenspaces</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix. Given <m>\lambda\in \R</m> the <term><m>\lambda</m>-eigenspace</term> of <m>A</m> is the subspace <m>W_\lambda\subseteq \R^n</m> defined as
      <me>
        W_\lambda=\NS (\lambda I -A)
      </me>.
      Similarly, given a finite-dimensional vector space <m>V</m>, a linear transformation <m>T\colon V\rightarrow V</m>, and <m>\lambda\in \R</m>, the <term><m>\lambda</m>-eigenspace</term> of <m>T</m>
      is the subspace <m>W\subseteq V</m> defined as
      <me>
        W_\lambda=\NS(\lambda \id_V-T)
      </me>.
      In both cases the nonzero elements of <m>W_\lambda</m> are precisely the <m>\lambda</m>-eigenvectors.
    </p>
  </statement>
</definition>
<p>
  We nearly have a complete method for computing the eigenvalues and eigenectors of a square matrix <m>A</m>. The last step is to identify the values of <m>\lambda</m> for which <m>\lambda I-A</m> is not invertible. By <xref ref="th_invertibility_supersized" text="global"/>, the matrix <m>\lambda I-A</m> is not invertible if and only if <m>\det (\lambda I-A)=0</m>. Thus the eigenvalues of <m>A</m> are precisely the <em>zeros</em> of the function <m>p(t)=\det(tI-A)</m>.
</p>
<corollary xml:id="cor_eigenvalues">
  <title>Eigenvalues of a matrix</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix, and let <m>p(t)=\det(tI-A)</m>. The following are equivalent:
    </p>
    <ol>
      <li>
        <p>
          <m>\lambda</m> is an eigenvalue of <m>A</m>;
        </p>
      </li>
      <li>
        <p>
          <m>\lambda I-A</m> is singular;
        </p>
      </li>
      <li>
        <p>
          <m>\det(\lambda I-A)=0</m>;
        </p>
      </li>
      <li>
        <p>
         <m>p(\lambda)=0</m>.
        </p>
      </li>
    </ol>
  </statement>
</corollary>
Clearly the function <m>p(t)=\det(tI-A)</m> deserves a name; we call it the <em>characteristic polynomial</em> of <m>A</m>.
<definition>
  <idx><h>characteristic polynomial</h><h>of a matrix</h></idx>
  <title>Characteristic polynomial of a matrix</title>
  <statement>
    <p>
      Let <m>A</m> be <m>n\times n</m>.
      The <term>characteristic polynomial</term>
      of <m>A</m> is the function
      <me>
      p(t)=\det(tI-A)
      </me>.
    </p>
  </statement>
</definition>
<p>
We will show in due time that <m>p(t)=\det(tI-A)</m> is indeed a polynomial ((<xref ref="th_characteristic_polynomial"/>)), but before then it is high time we saw some examples of systematically computing eigenvalues and eigenvectors of a matrix. First give a complete description of this procedure.
</p>

<algorithm xml:id="proc_eigenspaces_matrix">
  <title>Computing eigenspaces of a matrix</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix. To find all real eigenvalues <m>\lambda</m> of <m>A</m> and compute a basis for the corresponding eigenspace <m>W_\lambda</m>, proceed as follows.
    </p>
    <ol>
      <li>
        <p>
          Compute <m>p(t)=\det(tI-A)</m>. Let
          <m>\lambda_1, \lambda_2, \dots, \lambda_r</m> be the distinct <em>real</em> roots of <m>p(t)</m>. These are the eigenvalues of <m>A</m>.
        </p>
      </li>
      <li>
        <p>
          For each <m>\lambda\in \{\lambda_1, \lambda_2,\dots, \lambda_r\}</m>, the corresponding eigenspace is
          <me>
          W_{\lambda}=\NS(\lambda I-A)
          </me>.
          Use the null space algorithm (<xref ref="proc_fund_spaces" text="global"/>) to compute a basis for <m>W_\lambda</m>.
        </p>
      </li>
    </ol>
  </statement>
</algorithm>
<example>
  <statement>
    <p>
    Let <m>A=\begin{amatrix}[rr]1\amp 2\\ 1\amp 2 \end{amatrix}</m>.
    <ol>
      <li>
        <p>
          Find all eigenvalue of <m>A</m>.
        </p>
      </li>
      <li>
        <p>
          Compute a basis for the eigenspace <m>W_\lambda</m> for each eigenvalue <m>\lambda</m>.
        </p>
      </li>
    </ol>
    </p>
  </statement>
  <solution>
    <p>
      We compute
      <me>p(t)=\det(tI-A)=\det \begin{amatrix}[rr]t-1\amp -2\\ -1\amp t-2 \end{amatrix}=t^2-3t=t(t-3)
      </me>.
      Thus the eigenvalues of <m>A</m> are <m>\lambda=0</m> and <m>\lambda=3</m>.
    </p>
    <case>
     <title>Basis for <m>W_0</m></title>
    <p>
    We have
    <md>
      <mrow>W_0 \amp =\NS(0I-A)</mrow>
      <mrow> \amp = \NS(-A) </mrow>
      <mrow>  \amp =\NS\begin{amatrix}[rr]-1\amp -2\\ -1\amp -2 \end{amatrix}</mrow>
      <mrow>  \amp =\Span\{(2,-1)\}</mrow>
    </md>.
    Thus all <m>0</m>-eigenvectors of <m>A</m> are of the form <m>c(2,-1)</m>, where <m>c\ne 0</m>.

    </p>
    </case>
    <case>
     <title>Basis for <m>W_3</m></title>
    <p>
    We have
    <md>
      <mrow>W_3 \amp =\NS(3I-A)</mrow>
      <mrow>  \amp =\NS\begin{amatrix}[rr]2\amp -2\\ -1\amp 1 \end{amatrix}</mrow>
      <mrow>  \amp =\Span\{(1,1)\}</mrow>
    </md>.
    Thus all <m>3</m>-eigenvectors of <m>A</m> are of the form <m>c(1,1)</m>, where <m>c\ne 0</m>.
    </p>
    </case>
  </solution>
</example>
<p>
The example above brings to light a connection between eigenvalues and invertibility, which we make official below. At the end of the section we will add this result to our invertibility theorem.
</p>
<corollary xml:id="cor_eigenvalue_invertible">
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix.
      <ol>
        <li>
          <p>
            The <m>0</m>-eigenspace of <m>A</m> is equal to the null space of <m>A</m>: <ie />,
            <me>
              W_0=\NS A
            </me>.
          </p>
        </li>
        <li>
          <p>
            Zero is an eigenvalue of <m>A</m> if and only if <m>A</m> is not invertible.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <proof>
      <ol>
        <li>
          <p>
            We have
            <me>
              W_0=\NS(0I-A)=\NS(-A).
            </me>
            Since <m>-A\boldx=\boldzero</m> if and only if <m>A\boldx=\boldzero</m>, we conclude that <m>\NS(-A)=\NS A</m>, and hence <m>W_0=\NS A</m>.
          </p>
        </li>
        <li>
          <p>
            Zero is an eigenvalue of <m>A</m> if and only if <m>W_0</m> is nontrivial, if and only if <m>\NS A</m> is nontrivial (by (1)), if and only if <m>A</m> is not invertible.
          </p>
        </li>
      </ol>
  </proof>

</corollary>
<example>
  <statement>
    <p>
    Let
    <me>
    A=\begin{amatrix}[rrr]2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{amatrix}</me>.
    </p>
    <ol>
      <li>
        <p>
          Find all eigenvalues of <m>A</m>.
        </p>
      </li>
      <li>
        <p>
          Compute a basis for the eigenspace <m>W_\lambda</m> for each eigenvalue <m>\lambda</m>.
        </p>
      </li>
    </ol>
  </statement>
  <solution>
    <p>
      First compute
      <md>
        <mrow>p(t) \amp = \det (tI-A)</mrow>
        <mrow> \amp = \det \begin{amatrix}[rrr]t-2\amp 1\amp 1\\ 1\amp t-2\amp 1\\ 1\amp 1\amp t-2 \end{amatrix} </mrow>
        <mrow>  \amp = t^3-6t^2+9t</mrow>
        <mrow>  \amp = t(t^2-6t+9)</mrow>
        <mrow>  \amp = t(t-3)^2</mrow>
      </md>.
      We see that the eigenvalues of <m>A</m> are <m>\lambda=0</m> and <m>\lambda=3</m>. Now compute bases for their correponding eigenspaces.
     </p>
     <case>
      <title>Basis of <m>W_0</m></title>
     <p>
     We have
     <md>
       <mrow>W_0 \amp = \NS(0I-A)</mrow>
       <mrow> \amp = \NS \begin{amatrix}[rrr]-2\amp 1\amp 1\\ 1\amp -2\amp 1\\ 1\amp 1\amp -2 \end{amatrix}  </mrow>
       <mrow>  \amp = \Span\{(1,1,1)</mrow>
     </md>.
     (We have skipped the Gaussian elimination steps involved in computing a basis for <m>\NS(-A)</m>.)
     </p>
     </case>
     <case>
      <title>Basis of <m>W_3</m></title>
     <p>
     We have
     <md>
       <mrow>W_3 \amp = \NS(3I-A)</mrow>
       <mrow> \amp= \NS \begin{amatrix}[rrr]1\amp 1\amp 1\\ 1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{amatrix} </mrow>
       <mrow>  \amp = \Span\{ (1,-1,0),(1,0,-1)\}</mrow>
     </md>.
     </p>
     </case>
    We conclude that the <m>0</m>-eigenvectors of <m>A</m> are the nonzero scalar multiples of <m>(1,1,1)</m>, and that the <m>3</m>-eigenvectors are all nonzero vectors of the form <m>\boldx=c_1(1,-1,0)+c_2(1,0,-1)</m>.
  </solution>
</example>
<theorem xml:id="th_characteristic_polynomial">
  <title>Characteristic polynomial</title>
  <statement>
    <p>
      Let <m>A</m> be an <m>n\times n</m> matrix.
    </p>
    <ol>
      <li>
        <p>
          The function <m>p(t)=\det (tI-A)</m> is a monic polynomial of degree <m>n</m>: <ie />,
          <men xml:id="eq_char_poly">
          p(t)=t^n+a_{n-1}t^{n-1}+\cdots +a_1t+a_0
        </men>.
        </p>
      </li>
      <li>
        <p>
          Over <m>\C</m> we can factor <m>p(t)</m> completely as
          <me>
            p(t)=(t-\lambda_1)(t-\lambda_2)\cdots (t-\lambda_n),
          </me>
          where the <m>\lambda_i\in \C</m> are the (not necessarily distinct) roots of <m>p(t)</m>. We call the <m>\lambda_i</m> the <term>complex eigenvalues</term> of <m>A</m>.
        </p>
      </li>
      <li>
        <p>
          The (usual) eigenvalues of <m>A</m> are precisely the real roots of <m>p(t)</m>: <ie />, the <m>\lambda_i</m> for which <m>\lambda_i\in \R</m>.
        </p>
      </li>
      <li>
        <p>
          The coefficient <m>a_{n-1}</m> in <xref ref="eq_char_poly"/> satisfies
          <men xml:id="eq_char_poly_trace">
            a_{n-1}=-\tr A=-(\lambda_1+\lambda_2+\cdots +\lambda_n)
          </men>.
        </p>
      </li>
      <li>
        <p>
          The coefficient <m>a_0</m> in <xref ref="eq_char_poly"/> satisfies
        </p>
        <men xml:id="eq_char_poly_det">
          a_0=(-1)^n\det A=(-1)^n\lambda_1\lambda_2\cdots \lambda_n
        </men>.
      </li>
      <li>
        <p>
          We conclude that
          <mdn>
            <mrow xml:id="eq_tr_sum_eigenvalues"> \tr A \amp = \lambda_1+\lambda_2+\cdots +\lambda_n </mrow>
            <mrow xml:id="eq_det_prod_eigenvalues"> \det A\amp = \lambda_1\lambda_2\cdots \lambda_n </mrow>
          </mdn>.
        </p>
      </li>
    </ol>
  </statement>
</theorem>
<corollary xml:id="cor_eigenvalues_atmost_n">
  <statement>
    <p>
      An <m>n\times n</m> matrix <m>A</m> has at most <m>n</m> distinct eigenvalues.
    </p>
  </statement>
</corollary>
</subsection>



  <algorithm xml:id="proc_eigenspaces_transformation">
    <title>Computing eigenspaces of a linear transformation</title>
    <statement>
      <p>
        Let <m>T\colon V\rightarrow V</m> be a linear transformation of a finite-dimensional vector space <m>V</m>. To compute the eigenvalues and eigenspaces of <m>T</m>, proceed as follows.
      </p>
      <ol>
        <li>
          <p>
            Pick an ordered basis <m>B</m> of <m>V</m> and compute <m>A=[T]_B</m>.
          </p>
        </li>
        <li>
          <p>
            Apply <xref ref="proc_eigenspaces_matrix"/> to <m>A</m> to compute bases of <m>W_\lambda</m> for each eigenvalue <m>\lambda</m> of <m>A</m>.
          </p>
        </li>
        <li>
          <p>
            For each eigenvalue <m>\lambda</m>, <q>lift</q> the basis of <m>W_\lambda\subseteq \R^n</m> back up to <m>V</m> using the coordinate transformation <m>[\phantom{\boldv}]_B</m>. The result is the basis for the <m>\lambda</m>-eigenspace of <m>T</m>.
          </p>
        </li>
      </ol>
    </statement>
  </algorithm>
<xi:include href="./s_eigenvectors_ex.ptx"/>
</section>
