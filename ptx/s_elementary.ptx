<section xml:id="ss_elementary">
  <title>Elementary matrices</title>
  <paragraphs>
  <title><xref ref="c_linear_systems"></xref>: <xref ref="ss_elementary"></xref> executive summary</title>
  <paragraphs>
    <title>Definitions</title>
    <p>
      :elementary matrices \alert{Procedures}: finding <m>A^{-1}</m> using row reduction,
      writing <m>A</m> as a product of elementary matrices. \alert{Theorems}: elementary matrices are invertible,
      and their inverses are themselves elementary;
      the invertibility theorem.
    </p>
  </paragraphs>
  </paragraphs>
  <paragraphs>
    <title><xref ref="c_linear_systems"></xref>: <xref ref="ss_elementary"></xref>: computing inverses with row reduction</title>
    <p>
      Surprisingly row reduction provides a tool for both deciding whether a matrix is invertible,
      and computing this inverse if it exists.
    </p>
    <p>
      First we will construe row operations as certain matrix multiplications.
    </p>
    <definition>
      <statement>
        <p>
          A square matrix <m>E_{m\times m}</m> is an
          <term>elementary matrix</term>
          if multiplying any matrix <m>A_{m\times n}</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
        </p>
      </statement>
    </definition>
  </paragraphs>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Scale.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>c\cdot\boldr_i</m> for <m>c\ne 0</m>.
    Then <m>E</m> is the identity matrix except for the <m>i</m>-th row,
    where the 1 is replaced with <m>c</m>.
    <me>
      \underset{cr_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Swap.} Suppose <m>E</m> swaps rows <m>i</m> and <m>j</m> of <m>A</m>.
    Then <m>E</m> is the identity matrix with the <m>i</m>-th row and <m>j</m>-th rows swapped.
    <me>
      \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Row addition.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>\boldr_i+c\boldr_j</m>.
    Then <m>E</m> is the identity matrix except for its <m>i</m>-th row.
    <me>
      \underset{r_i+cr_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <paragraphs>
    <title>Row reduction as matrix multiplication</title>
    <p>
      Suppose we perform a sequence of row operations on a matrix <m>A</m>.
      Denote the <m>i</m>-th row operation <m>\rho_i</m>,
      and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
    </p>
    <p>
      Our sequence of operations <m>\rho_i</m> would produce the following sequence of matrices:
      <me>
        \begin{array}{c} A\\  \rho_1(A)\\  \rho_2(\rho_1(A))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A)))) \end{array}
      </me>
    </p>
    <p>
      Let <m>\rho_i</m> corresponds to the elementary matrix <m>E_i</m>.
      Then we represent this same sequence using matrix multiplication:
      <me>
        \begin{array}{c} A\\  E_1A\\  E_2E_1A\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A \end{array}
      </me>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Elementary matrices are invertible</title>
    <p>
      We will use our row operation notation to denote the different types of elementary matrices:
      <me>
        \underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+cr_j}E
      </me>.
    </p>
    <theorem>
      <title>Elementary matrix theorem</title>
      <statement>
        <p>
          Fix <m>n</m>.
          All elementary matrices are invertible,
          and their inverses are elementary matrices.
          In fact:
          <md>
            <mrow>\underset{cr_i}{E}^{-1}\amp =\amp \underset{\frac{1}{c}r_i}{E}</mrow>
            <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp =\amp \underset{r_i\leftrightarrow r_j}{E}</mrow>
            <mrow>\underset{r_i+cr_j}E^{-1}\amp =\amp \underset{r_i-cr_j}E</mrow>
          </md>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        These are formulas that you can easily check for each type of elementary matrix directly.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Examples</title>
    <p>
      Fix <m>n=3</m>.
      Verify that the pairs below are indeed inverses.
      We have <m>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> and thus <m>\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> We have
      <m>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</m> and thus <m>\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp \frac{1}{2} \end{bmatrix}</m> We have
      <m>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m> and thus <m>\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      We take a moment to make the following simple,
      somewhat overdo observation.
      Namely, we can represent a <em>system of linear equations</em>
      <me>
        \eqsys\tag{\(*\)}
      </me>
      as a <em>single matrix equation</em>
      <me>
        \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix} ,\tag{\(**\)}
      </me>
      or <m>A\boldx=\boldb</m>,where <m>A=[a_{ij}]</m>,
      <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
    </p>
    <p>
      Indeed if you expand out the left-hand side of <m>(**)</m> into an <m>m\times 1</m> column vector,
      using the definition of matrix multiplication,
      and then invoke the definition of matrix equality,
      then you obtain the linear system <m>(*)</m>.
    </p>
    <p>
      By the same token, an <m>n</m>-tuple
      <m>(s_1,s_2,\dots,
      s_n)</m> is a solution to the <em>system of equations</em>
      <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em> <m>A\boldx=\boldb</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      In particular, a homogeneous linear system
      <me>
        \homsys
      </me>
      can be represented as the single matrix equation
      <me>
        A\boldx=\underset{m\times 1}{\boldzero}
      </me>,
      where <m>A=[a_{ij}]</m> and <m>\boldx=[x_i]</m>;
      and <m>(s_1,s_2,\dots, s_n)</m> is a solution to the
      <em>homogenous system</em>
      if and only if its corresponding column vector
      <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em>
      <m>A\boldx=\underset{m\times 1}{\boldzero}</m>.
    </p>
  </paragraphs>
  <p>
    We are now ready to state a major theorem about invertibility.
  </p>
  <theorem>
    <title>Invertibility theorem</title>
    <statement>
      <p>
        Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
        The following statements are equivalent.
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              <m>A\boldx=\boldzero</m> has a unique solution
              (the trivial one).
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is row equivalent to <m>I_n</m>,
              the <m>n\times n</m> identity matrix.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a product of elementary matrices.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Recall that two show two statements <m>P</m> and <m>Q</m> are equivalent,
      we must show two implications:
      <m>P\Rightarrow Q</m>, and <m>Q\Rightarrow P</m>.
    </p>
    <p>
      We must show this for each pair of statements above.
      That would be 6 different pairs and a total of 12 different implications to show!
      We ease our work load by showing the following
      <em>cycle</em> of implications:
      <me>
        (a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
      </me>
      and using the fact that implication is transitive!
      We will complete the proof in class.
    </p>
  </proof>
  <paragraphs>
    <title>Algorithm for inverses</title>
    <p>
      The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether <m>A</m> is invertible,
      and (2) computing <m>A^{-1}</m> if it exists.
    </p>
    <p>
      The algorithm:
      <ol>
        <li>
          <p>
            Row reduce <m>A</m> to row echelon form <m>U</m>
            (keeping track of the elementary matrices you use).
            The theorem tells us that <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading 1's.
          </p>
        </li>
        <li>
          <p>
            If this is so, we can keep row reducing to the identity matrix.
            In terms of matrices we have
            <me>
              E_rE_{r-1}\cdots E_1A=I_n
            </me>.
            This means <m>A^{-1}=E_rE_{r-1}\cdots E_1</m>.
          </p>
        </li>
        <li>
          <p>
            As an added bonus we also have <m>A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}</m>,
            expressing <m>A</m> as a product of elementary matrices.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Inverses with augmented matrix</title>
    <p>
      It can be bothersome to keep track of those <m>E_i</m>,
      and the order of their multiplication.
      Instead, we use an augmented matrix method,
      beginning with the augmented matrix
      <me>
        \begin{bmatrix}A\amp I_n \end{bmatrix}
      </me>,
      and simply applying row reductions until <m>A</m> on the left becomes <m>I_n</m>,
      at which point <m>I_n</m> on the right has become <m>A^{-1}</m>.
    </p>
    <p>
      Why does this work?
      In terms of the <m>E_i</m>, we have the sequence:
      <me>
        \begin{array}{c} \begin{bmatrix}A\amp I_n \end{bmatrix} \begin{bmatrix}E_1A\amp E_1 \end{bmatrix} \begin{bmatrix}E_2E_1A\amp E_2E_1 \end{bmatrix} \\  \vdots \begin{bmatrix}E_rE_{r-1}\cdots E_1A\amp E_rE_{r-1}\cdots E_1 \end{bmatrix} =\begin{bmatrix}I_n\amp A^{-1} \end{bmatrix} \end{array}
      </me>
    </p>
    <p>
      Thus when we are done,
      the RHS of our augmented matrix magically becomes <m>A^{-1}</m>.
    </p>
  </paragraphs>
  <p>
    \alert{Example:} take <m>A=\begin{bmatrix}1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{bmatrix} </m>.
    <md>
      <mrow>\begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}  \amp \xrightarrow{r_2-r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_3+r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\alert{\text{ (3 leading 1's, thus invertible!) } }\amp \xrightarrow{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_2-3r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_1-2r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    </md>
  </p>
  <p>
    We conclude that <m>A^{-1}=\begin{bmatrix}-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{bmatrix}</m>.
    On the next slide I show how <m>A^{-1}</m> and <m>A</m> can be written as products of elementary matrices.
  </p>
  <paragraphs>
    <title>Example (contd):</title>
    <p>
      in terms of elementary matrices we have
    </p>
  </paragraphs>
  <me>
    E_5E_4E_3E_2E_1A=I
  </me>,
  <p>
    where fuck
    <me>\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      -1\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}
    </me>
    <md>
      <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        -1\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 1\amp 0\\
        1\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 0\amp 1\\
        0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        0\amp 1\amp -3\\
        0\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
        0\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}</mrow>
    </md>
  </p>
  <p>
    Thus we have <m>A^{-1}=E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</m>.
    This shows how both of these matrices can be written as products of elementary matrices
    (recall that each <m>E_i^{-1}</m> is also elementary).
  </p>
</section>
