<section xml:id="ss_elementary">
  <title>Elementary matrices</title>
  <paragraphs>
    <title>Invertibility</title>
    <p>
      What allows cancellation of a nonzero <m>a</m> in the reals is the existence of its
      <em>multiplicative inverse</em>,
      the element <m>a^{-1}=\frac{1}{a}</m>,
      which satisfies <m>a^{-1}a=1</m>.
    </p>
    <p>
      Not all nonzero matrices <m>A</m> have a multiplicative inverse.
      The ones that do are called <em>invertible</em>.
      In order to define this properly,
      we need to identify a matrix that acts as the number one does:
      i.e., a <em>multiplicative identity</em>.
    </p>
    <definition>
      <statement>
        <p>
          The <term>identity matrix of size <m>n\times n</m></term>
          is the <m>n\times n</m> matrix <m>I_n</m> with 1's along the diagonal (i.e.,
          <m>a_{ii}=1</m>) and 0's everywhere else (i.e., <m>a_{ij}=0</m> for <m>i\ne j</m>).
          For example:
          <me>
            I_1=\begin{bmatrix}1 \end{bmatrix} , I_2=\begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} , I_3=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix} , \text{ etc. }
          </me>
        </p>
      </statement>
    </definition>
    <theorem>
      <title>Multiplicative identity</title>
      <statement>
        <p>
          For any <m>A_{m\times n}</m> we have
          <md>
            <mrow>I_mA\amp =\amp A</mrow>
            <mrow>AI_n\amp =\amp A</mrow>
          </md>.
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        Use either column or row method to perform the multiplications involved!
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
  <title>Invertibility</title>
  <definition>
    <statement>
      <p>
        A square matrix <m>A_{n\times n}</m> is <term>invertible</term>
        if there is a matrix <m>B_{n\times n}</m> such that
        <md>
          <mrow>AB\amp =\amp I_n, \text{ and }</mrow>
          <mrow>BA\amp =\amp I_n</mrow>
        </md>
      </p>
      <p>
        In this case we call <m>B</m> an
        <term>inverse</term> of <m>A</m>,
        and we say that <m>A</m> and <m>B</m> are
        <term>inverses</term> of one another.
      </p>
    </statement>
  </definition>
  <paragraphs>
    <title>Theorem (uniqueness of inverses):</title>
    <p>
      an invertible matrix <m>A</m> has exactly one inverse.
      That is, if
    </p>
  </paragraphs>
  <me>
    AB=BA=I_n
  </me>
  <p>
    and
    <me>
      AC=CA=I_n
    </me>,
    then <m>B=C</m>.
  </p>
  <p>
    We denote by <m>A^{-1}</m> the inverse of <m>A</m>.
  </p>
  <proof>
    <p>
      Suppose we have such matrices <m>B</m> and <m>C</m>.
    </p>
    <p>
      Then <m>I_n=AB=AC \Rightarrow BAB=BAC \Rightarrow I_nB=I_nC \Rightarrow B=C</m>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
    <title>$2\times 2$ matrices</title>
    <p>
      When is a <m>2\times 2</m> matrix <m>A=\abcdmatrix{a}{b}{c}{d}</m> invertible?
    </p>
    <p>
      If <m>ad-cb\ne 0</m>, then we can easily show that
      <me>
        A^{-1}=\frac{1}{ad-bc}\abcdmatrix{d}{-b}{-c}{a}
      </me>
      is the inverse.
    </p>
    <p>
      Thus
      <me>
        ad-bc\ne 0\Rightarrow  A \text{ invertible }
      </me>.
    </p>
    <p>
      It turns out that the converse is also true, that is
      <me>
        ad-bc\ne 0\Leftrightarrow  A \text{ invertible }
      </me>.
    </p>
    <p>
      We will prove this a littler further down the line.
    </p>
  </paragraphs>
  <theorem>
    <statement>
      <p>
        Let <m>A,B</m> be <m>n\times n</m> matrices.
        <me>
          A \text{ and }  B  \text{ invertible } \Rightarrow AB \text{ invertible }
        </me>,
        and in fact <m>(AB)^{-1}=B^{-1}A^{-1}</m>.
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      In the absence of more theory,
      we can only prove invertibility by providing an inverse.
      The last statement offers up a candidate:
      viz., <m>C=B^{-1}A^{-1}</m>.
      We need only show that <m>C</m> satisfies the relevant properties:
      <md>
        <mrow>C(AB)\amp =\amp (B^{-1}A^{-1})AB=B^{-1}A^{-1}AB=B^{-1}IB=B^{-1}B=I</mrow>
        <mrow>(AB)C\amp =\amp (AB)B^{-1}A^{-1}=ABB^{-1}A^{-1}=AIA^{-1}=AA^{-1}=I</mrow>
      </md>.
    </p>
  </proof>
  <corollary>
    <statement>
      <p>
        More generally,
        <me>
          A_1,A_2,\dots A_r \text{ invertible } \Rightarrow (A_1A_2\cdots A_r)\text{ invertible }
        </me>,
        and in fact <m>(A_1A_2\cdots A_r)^{-1}=A_r^{-1}A_{r-1}^{-1}\cdots A_1^{-1}</m>.
      </p>
    </statement>
  </corollary>
  <p>
    <em>Question:</em> do the implication arrows above go the other way?
  </p>
  <p>
    <em>Answer:</em> we shall see.
  </p>
  <paragraphs>
  <title>Powers of matrices</title>


  <theorem>
    <statement>
      <p>
        Let <m>A_{n\times n}</m> be invertible.
        Then <m>A^T</m> is invertible,
        and <m>\left(A^T\right)^{-1}=\left(A^{-1}\right)^T</m>.
      </p>
    </statement>
    <proof>
      <p>
        Suppose <m>A</m> is invertible,
        and let <m>A^{-1}</m> be its inverse.
        The claim above is that <m>B=\left(A^{-1}\right)^T</m> is the inverse of <m>A^T</m>.
        To prove this claim we need only show that <m>BA^T=A^TB=I_n</m>:
        <md>
          <mrow>BA^T=\left(A^{-1}\right)^TA^T \amp =\amp \left(AA^{-1}\right)^T \ \text{ (since \((CD)^T=D^TC^T\)) }</mrow>
          <mrow>\amp =\amp I_n^T=I_n \ \checkmark</mrow>
        </md>
        <md>
          <mrow>A^TB=A^T\left(A^{-1}\right)^T \amp =\amp \left(A^{-1}A\right)^T \ \text{ (since \((CD)^T=D^TC^T\)) }</mrow>
          <mrow>\amp =\amp I_n^T=I_n \ \checkmark</mrow>
        </md>
      </p>
    </proof>
  </theorem>
    <definition>
      <statement>
        <p>
          A square matrix <m>E_{m\times m}</m> is an
          <term>elementary matrix</term>
          if multiplying any matrix <m>A_{m\times n}</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
        </p>
      </statement>
    </definition>

  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Scale.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>c\cdot\boldr_i</m> for <m>c\ne 0</m>.
    Then <m>E</m> is the identity matrix except for the <m>i</m>-th row,
    where the 1 is replaced with <m>c</m>.
    <me>
      \underset{cr_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Swap.} Suppose <m>E</m> swaps rows <m>i</m> and <m>j</m> of <m>A</m>.
    Then <m>E</m> is the identity matrix with the <m>i</m>-th row and <m>j</m>-th rows swapped.
    <me>
      \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Row addition.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>\boldr_i+c\boldr_j</m>.
    Then <m>E</m> is the identity matrix except for its <m>i</m>-th row.
    <me>
      \underset{r_i+cr_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  </paragraphs>
  <paragraphs>
    <title>Row reduction as matrix multiplication</title>
    <p>
      Suppose we perform a sequence of row operations on a matrix <m>A</m>.
      Denote the <m>i</m>-th row operation <m>\rho_i</m>,
      and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
    </p>
    <p>
      Our sequence of operations <m>\rho_i</m> would produce the following sequence of matrices:
      <me>
        \begin{array}{c} A\\  \rho_1(A)\\  \rho_2(\rho_1(A))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A)))) \end{array}
      </me>
    </p>
    <p>
      Let <m>\rho_i</m> corresponds to the elementary matrix <m>E_i</m>.
      Then we represent this same sequence using matrix multiplication:
      <me>
        \begin{array}{c} A\\  E_1A\\  E_2E_1A\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A \end{array}
      </me>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Elementary matrices are invertible</title>
    <p>
      We will use our row operation notation to denote the different types of elementary matrices:
      <me>
        \underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+cr_j}E
      </me>.
    </p>
    <theorem>
      <title>Elementary matrix theorem</title>
      <statement>
        <p>
          Fix <m>n</m>.
          All elementary matrices are invertible,
          and their inverses are elementary matrices.
          In fact:
          <md>
            <mrow>\underset{cr_i}{E}^{-1}\amp =\amp \underset{\frac{1}{c}r_i}{E}</mrow>
            <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp =\amp \underset{r_i\leftrightarrow r_j}{E}</mrow>
            <mrow>\underset{r_i+cr_j}E^{-1}\amp =\amp \underset{r_i-cr_j}E</mrow>
          </md>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        These are formulas that you can easily check for each type of elementary matrix directly.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Examples</title>
    <p>
      Fix <m>n=3</m>.
      Verify that the pairs below are indeed inverses.
      We have <m>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> and thus <m>\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> We have
      <m>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</m> and thus <m>\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp \frac{1}{2} \end{bmatrix}</m> We have
      <m>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m> and thus <m>\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      We take a moment to make the following simple,
      somewhat overdo observation.
      Namely, we can represent a <em>system of linear equations</em>
      <me>
        \eqsys\tag{\(*\)}
      </me>
      as a <em>single matrix equation</em>
      <me>
        \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix} ,\tag{\(**\)}
      </me>
      or <m>A\boldx=\boldb</m>,where <m>A=[a_{ij}]</m>,
      <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
    </p>
    <p>
      Indeed if you expand out the left-hand side of <m>(**)</m> into an <m>m\times 1</m> column vector,
      using the definition of matrix multiplication,
      and then invoke the definition of matrix equality,
      then you obtain the linear system <m>(*)</m>.
    </p>
    <p>
      By the same token, an <m>n</m>-tuple
      <m>(s_1,s_2,\dots,
      s_n)</m> is a solution to the <em>system of equations</em>
      <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em> <m>A\boldx=\boldb</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      In particular, a homogeneous linear system
      <me>
        \homsys
      </me>
      can be represented as the single matrix equation
      <me>
        A\boldx=\underset{m\times 1}{\boldzero}
      </me>,
      where <m>A=[a_{ij}]</m> and <m>\boldx=[x_i]</m>;
      and <m>(s_1,s_2,\dots, s_n)</m> is a solution to the
      <em>homogenous system</em>
      if and only if its corresponding column vector
      <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em>
      <m>A\boldx=\underset{m\times 1}{\boldzero}</m>.
    </p>
  </paragraphs>
  <p>
    We are now ready to state a major theorem about invertibility.
  </p>
  <theorem>
    <title>Invertibility theorem</title>
    <statement>
      <p>
        Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
        The following statements are equivalent.
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              <m>A\boldx=\boldzero</m> has a unique solution
              (the trivial one).
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is row equivalent to <m>I_n</m>,
              the <m>n\times n</m> identity matrix.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a product of elementary matrices.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Recall that two show two statements <m>P</m> and <m>Q</m> are equivalent,
      we must show two implications:
      <m>P\Rightarrow Q</m>, and <m>Q\Rightarrow P</m>.
    </p>
    <p>
      We must show this for each pair of statements above.
      That would be 6 different pairs and a total of 12 different implications to show!
      We ease our work load by showing the following
      <em>cycle</em> of implications:
      <me>
        (a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
      </me>
      and using the fact that implication is transitive!
      We will complete the proof in class.
    </p>
  </proof>
  <paragraphs>
    <title>Algorithm for inverses</title>
    <p>
      The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether <m>A</m> is invertible,
      and (2) computing <m>A^{-1}</m> if it exists.
    </p>
    <p>
      The algorithm:
      <ol>
        <li>
          <p>
            Row reduce <m>A</m> to row echelon form <m>U</m>
            (keeping track of the elementary matrices you use).
            The theorem tells us that <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading 1's.
          </p>
        </li>
        <li>
          <p>
            If this is so, we can keep row reducing to the identity matrix.
            In terms of matrices we have
            <me>
              E_rE_{r-1}\cdots E_1A=I_n
            </me>.
            This means <m>A^{-1}=E_rE_{r-1}\cdots E_1</m>.
          </p>
        </li>
        <li>
          <p>
            As an added bonus we also have <m>A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}</m>,
            expressing <m>A</m> as a product of elementary matrices.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Inverses with augmented matrix</title>
    <p>
      It can be bothersome to keep track of those <m>E_i</m>,
      and the order of their multiplication.
      Instead, we use an augmented matrix method,
      beginning with the augmented matrix
      <me>
        \begin{bmatrix}A\amp I_n \end{bmatrix}
      </me>,
      and simply applying row reductions until <m>A</m> on the left becomes <m>I_n</m>,
      at which point <m>I_n</m> on the right has become <m>A^{-1}</m>.
    </p>
    <p>
      Why does this work?
      In terms of the <m>E_i</m>, we have the sequence:
      <me>
        \begin{array}{c} \begin{bmatrix}A\amp I_n \end{bmatrix} \begin{bmatrix}E_1A\amp E_1 \end{bmatrix} \begin{bmatrix}E_2E_1A\amp E_2E_1 \end{bmatrix} \\  \vdots \begin{bmatrix}E_rE_{r-1}\cdots E_1A\amp E_rE_{r-1}\cdots E_1 \end{bmatrix} =\begin{bmatrix}I_n\amp A^{-1} \end{bmatrix} \end{array}
      </me>
    </p>
    <p>
      Thus when we are done,
      the RHS of our augmented matrix magically becomes <m>A^{-1}</m>.
    </p>
  </paragraphs>
  <p>
    \alert{Example:} take <m>A=\begin{bmatrix}1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{bmatrix} </m>.
    <md>
      <mrow>\begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}  \amp \xrightarrow{r_2-r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_3+r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\alert{\text{ (3 leading 1's, thus invertible!) } }\amp \xrightarrow{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_2-3r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_1-2r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    </md>
  </p>
  <p>
    We conclude that <m>A^{-1}=\begin{bmatrix}-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{bmatrix}</m>.
    On the next slide I show how <m>A^{-1}</m> and <m>A</m> can be written as products of elementary matrices.
  </p>
  <paragraphs>
    <title>Example (contd):</title>
    <p>
      in terms of elementary matrices we have
    </p>
  </paragraphs>
  <me>
    E_5E_4E_3E_2E_1A=I
  </me>,
  <p>
    where fuck
    <me>\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      -1\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}
    </me>
    <md>
      <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        -1\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 1\amp 0\\
        1\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 0\amp 1\\
        0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        0\amp 1\amp -3\\
        0\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
        0\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}</mrow>
    </md>
  </p>
  <p>
    Thus we have <m>A^{-1}=E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</m>.
    This shows how both of these matrices can be written as products of elementary matrices
    (recall that each <m>E_i^{-1}</m> is also elementary).
  </p>
</section>
