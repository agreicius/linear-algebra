<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_isom">
<title>Isomorphisms</title>
<introduction>
  In this section we utilize bases, dimension, and the <xref ref="th_rank-nullity" text="custom">rank-nullity theorem</xref> to investigate properties of linear transformations. The main focus will be the notion of an <em>isomorphism</em>, which is simply a linear transformation that is invertible when considered as a function. We begin, however, with an enlightening discussion relating linear transformations and bases.
</introduction>
<subsection xml:id="ss_bases_transformations">
  <title>Bases and linear transformations</title>
  <introduction>
   In <xref ref="s_basis_dimension"/> we saw that a vector space <m>V</m> is completely and concisely determined by a basis <m>B</m> in the sense that all elements of <m>V</m> can be expressed in a unique was as a linear combination of elements of <m>B</m>. A similar principle applies to linear transformations, as the next theorem illustrates.
  </introduction>
  <theorem xml:id="th_bases_transformations">
    <title>Bases and linear transformations</title>
    <statement>
      <p>
        Let <m>B</m> be a basis for the vector space <m>V</m>.
      </p>
      <ol label="i">
        <li>
          <p>
            Let <m>T</m> and <m>T'</m> be linear transformations from <m>V</m> to <m>W</m>. If <m>T(\boldu)=T'(\boldu)</m> for all <m>\boldu\in B</m>, then <m>T=T'</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>W</m> be a vector space. Any mapping
            <me>
              \boldu\mapsto \boldw_\boldu
            </me>
            assigning each element of <m>\boldu\in B</m> to a chosen element <m>\boldw_\boldu\in W</m> extends uniquely to a linear transformation <m>T\colon V\rightarrow W</m> satisfying
            <me>
              T(\boldu)=\boldw_\boldu
            </me>
            for all <m>\boldu\in B</m>. In more detail, given any <m>\boldv\in V</m>, if   <m>\boldv=c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r</m>, where <m>\boldu_i\in B</m>  and <m>c_i\ne 0</m> for all <m>1\leq i\leq r</m>, then
            <men xml:id="eq_bases_transformations">
              T(\boldv)=c_1T(\boldu_{1})+c_2T(\boldu_{2})+\cdots +c_rT(\boldu_{r})
            </men>.

          </p>
        </li>
      </ol>
    </statement>
    <proof>
      <proof>
        <title>Proof of (i)</title>
        <p>
          Assume <m>T</m> and <m>T'</m> are linear transformations from <m>V</m> to <m>W</m> satisfying <m>T(\boldu)=T'(\boldu)</m> for all <m>\boldu\in B</m>. Given any <m>\boldv\in V</m> we can write <m>\boldv=c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r</m>. It follows that
          <md>
            <mrow>T(\boldv) \amp = T(c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r)</mrow>
            <mrow> \amp = c_1T(\boldu_1)+c_2T(\boldu_2)+\cdots +c_rT(\boldu_r) \amp (T \text{ is linear})</mrow>
            <mrow>  \amp =c_1T'(\boldu_1)+c_2T'(\boldu_2)+\cdots +c_rT'(\boldu_r)</mrow>
            <mrow>  \amp = T'(c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r) \amp (T' \text{ is linear}) </mrow>
            <mrow>  \amp = T'(\boldv)</mrow>
          </md>.
          Since <m>T(\boldv)=T'(\boldv)</m> for all <m>\boldv\in V</m>, we have <m>T=T'</m>.
        </p>
      </proof>
      <proof>
        <title>Proof of (ii)</title>
        <p>
          That there can me <em>at most</em> one such <m>T\colon V\rightarrow W</m> follows from (i). Thus we need only show that such a <m>T</m> exists.
        </p>
        <p>
          Since any <m>\boldv\in V</m> has a <em>unique</em> expression of the form
          <me>
            \boldv=c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r
          </me>,
          where <m>c_i\ne 0</m> for all <m>1\leq i\leq r</m>,
          the formula in <xref ref="eq_bases_transformations"/> defines a function <m>T\colon V\rightarrow W</m> in a well-defined manner. Note also that the formula still applies even if some of the coefficients are equal to 0: if <m>c_i=0</m>, then <m>c_iT(\boldv_i)=\boldzero</m>, and the right-hand side of <xref ref="eq_bases_transformations"/> is unchanged. We will use this fact below.
        </p>
        <p> We now show that <m>T</m> is linear. Given <m>\boldv, \boldv'\in V</m> we can find a common collection of elements <m>\boldu_1,\boldu_2,\dots, \boldu_r\in B</m> for which
          <md>
            <mrow>\boldv \amp = c_1\boldu_1+c_2\boldu_2+\cdots +c_r\boldu_r</mrow>
            <mrow> \boldv'\amp=d_1\boldu_1+d_2\boldu_2+\cdots +d_r\boldu_r </mrow>
          </md>
          for some <m>c_i, d_i\in \R</m>. We can no longer assume that <m>c_i\ne 0</m> and <m>d_i\ne 0</m> for all <m>1\leq i\leq r</m>, but as observed above we still have
          <md>
          <mrow>T(\boldv) \amp = c_1T(\boldu_1)+c_2T(\boldu_2)+\cdots +c_rT(\boldu_r)</mrow>
          <mrow> T(\boldv')\amp=d_1T(\boldu_1)+d_2T(\boldu_2)+\cdots +d_rT(\boldu_r) </mrow>
          </md>.
          Given any <m>c, d\in \R</m>, we have
          <md>
            <mrow> T(c\boldv+d\boldv')\amp=T(cc_1\boldu_1+cc_2\boldu_2+\cdots +cc_r\boldu_r+dd_1\boldu_1+dd_2\boldu_2+\cdots +dd_r\boldu_r) </mrow>
            <mrow> \amp= T\left((cc_1+dd_1)\boldu_1+(cc_2+dd_2)\boldu_2+\cdots +(cc_r+dd_r)\boldu_r\right) </mrow>
              <mrow> \amp =(cc_1+dd_1)T(\boldu_1)+(cc_2+dd_2)T(\boldu_2)+\cdots +(cc_r+dd_r)T(\boldu_r) \amp (<xref ref="eq_bases_transformations"/>) </mrow>
              <mrow> \amp= c(c_1T(\boldu_1)+c_2T(\boldu_2)+\cdots +c_rT(\boldu_r))+d(d_1T(\boldu_1)+d_2T(\boldu_2)+\cdots +d_rT(\boldu_r)</mrow>
              <mrow>  \amp =cT(\boldv)+dT(\boldv')</mrow>
            </md>.
            Thus <m>T</m> is a linear transformation.
        </p>
      </proof>

    </proof>

  </theorem>
      <remark xml:id="rm_bases_transformations">
        <title>Transformations determined by behavior on basis</title>
    <statement>
      <p>
        Let's paraphrase the two results of <xref ref="th_bases_transformations"/>.
        <ol label="i">
          <li>
            <p>
            A linear transformation <m>T\colon V\rightarrow W</m> is completely determined by its behavior on a basis <m>B\subseteq V</m>. Once we know the images <m>T(\boldu)</m> for all <m>\boldu\in B</m>, the image <m>T(\boldv)</m> for any other <m>\boldv\in V</m> is then completely determined. Put another way, if two linear transformations out of <m>V</m> <em>agree</em> on the elements of a basis <m>B\subseteq V</m>, then they agree for all elements of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              Once we have a basis <m>B\subseteq V</m> on hand, it is easy to construct linear transformations <m>T\colon V\rightarrow W</m>: simply choose images <m>T(\boldu)\in W</m> for all <m>\boldu\in B</m> in any manner you like, and then define <m>T(\boldv)</m> for any element <m>\boldv\in V</m> using <xref ref="eq_bases_transformations"/>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </remark>
<example>
  <title>Composition of reflections</title>
  <statement>
    <p>
      Let <m>r_0\colon \R^2\rightarrow\R^2</m> be reflection across the <m>x</m>-axis, and let <m>r_{\pi/2}\colon \R^2\rightarrow \R^2</m> be reflection across the <m>y</m>-axis. (See <xref ref="ex_transformation_reflection"/>.) Use an argument in the spirit of statement (i) from <xref ref="rm_bases_transformations"/> to show that
      <me>
        r_{\pi/2}\circ r_{0}=\rho_{\pi}
      </me>.
      (Note: this equality can also be shown using our matrix formulas for rotations and reflections. See <xref ref="ex_transformation_composition_rotations_reflections"/>. )
    </p>
  </statement>
  <solution>
    <p>
      Since <m>r_0</m> and <m>r_{\pi/2}</m> are both linear transformations (<xref ref="ex_transformation_reflection"/>), so is the composition <m>T=r_{\pi/2}\circ r_{0}</m>. We wish to show <m>T=\rho_{\pi}</m>. Since <m>\rho_{\pi}</m> is also a linear transformation, it suffices by <xref ref="th_bases_transformations"/> to show that <m>T</m> and <m>\rho_\pi</m> agree on a basis of <m>\R^2</m>. Take the standard basis <m>B=\{(1,0), (0,1)\}</m>. Compute:
      <md>
        <mrow>T(1,0) \amp=r_{\pi}(r_{0}(1,0)) </mrow>
        <mrow> \amp =r_{\pi}(1,0) </mrow>
        <mrow>  \amp =(-1,0)</mrow>
        <mrow>  \amp =\rho_{\pi}(1,0)</mrow>
        <mrow> T(0,1) \amp </mrow>
        <mrow>T(0,1) \amp=r_{\pi}(r_{0}(0,1)) </mrow>
        <mrow> \amp =r_{\pi}(0,-1) </mrow>
        <mrow>  \amp =(0,-1)</mrow>
        <mrow>  \amp =\rho_{\pi}(0,1)</mrow>
        </md>.
        Since <m>T</m> and <m>\rho_\pi</m> agree on the basis <m>B</m>, we have <m>T=\rho_\pi</m>.
    </p>
  </solution>
</example>
<p>
  As a corollary to <xref ref="th_bases_transformations"/> we can at last complete the partial description of linear transformations of the form <m>T\colon \R^n\rightarrow \R^m</m> given in <xref ref="th_matrix_transform_i"/>.
</p>
<corollary xml:id="cor_matrix_transformations">
  <title>Matrix transformations II</title>
  <statement>
    <p>
    Given any linear transformation <m>T\colon \R^n\rightarrow \R^m</m> there is a unique <m>m\times n</m> matrix <m>A</m> such that <m>T=T_A</m>. In fact we have
    <me>
    A=\begin{bmatrix}\vert\amp \vert\amp  \amp \vert \\ T(\bolde_1)\amp  T(\bolde_2)\amp \cdots \amp T(\bolde_n)\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix}
    </me>,
    where <m>B=\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m> is the standard basis of <m>\R^n</m>.
    As a result, in the special case where the domain and codomain are both spaces of tuples, all linear transformations are matrix transformations.
    </p>
  </statement>
  <proof>
    <p>
      Let <m>B=\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m> be the standard basis of <m>\R^n</m>, and let <m>A</m> be the <m>m\times n</m> matrix defined as
      <me>
      A=\begin{bmatrix}\vert\amp \vert\amp  \amp \vert \\ T(\bolde_1)\amp  T(\bolde_2)\amp \cdots \amp T(\bolde_n)\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix}
      </me>.
      In other words, the <m>j</m>-th column of <m>A</m> is <m>T(\bolde_j)</m>, considered as an <m>m\times 1</m> column vector. The corresponding matrix transformation <m>T_A\colon \R^n\rightarrow \R^m</m> is linear by <xref ref="th_matrix_transform_i"/>. Since <m>T</m> is linear by assumption, <xref ref="th_bases_transformations"/> applies: to show <m>T=T_A</m> we need only show that <m>T(\bolde_j)=T_A(\bolde_j)</m> for all <m>1\leq j\leq n</m>. We have
      <md>
        <mrow>T_A(\bolde_j) \amp =A\bolde_j  \amp (<xref ref="d_matrix_transform"/>)</mrow>
        <mrow> \amp=(j\text{-th column of } A) \amp (<xref ref="th_column_method"/>) </mrow>
        <mrow>  \amp = T(\bolde_j) \amp (\text{def. of } A)</mrow>
      </md>.
      Thus <m>T=T_A</m>, as claimed.
    </p>
  </proof>
</corollary>
<p>
  Besides rounding out our theoretical discussion of linear transformations from <m>\R^n</m> to <m>\R^m</m>, <xref ref="cor_matrix_transformations"/>, computationally it provides a recipe for computing a matrix formula for a linear transformation <m>T\colon \R^n\colon \rightarrow \R^m</m>. In other words, it tells us how to build the <m>A</m>, column by column, such that <m>T=T_A</m>. Of course, to use <xref ref="cor_matrix_transformations"/> we first need to know that the function <m>T</m> in question is linear.
</p>
<example>
  <title>Rotation matrices revisited</title>
  <statement>
    <p>
      Fix an angle <m>\alpha</m>. Taking for granted that the rotation operation <m>\rho_\alpha\colon\R^2\rightarrow\R^2</m> is a linear transformation, re-derive the matrix formula for <m>\rho_\alpha\colon \R^2\rightarrow\R^2</m>: i.e., find the matrix <m>A</m> such that <m>\rho_\alpha=T_A</m>.
    </p>
  </statement>
  <solution>
    <p>
      Let <m>B=\{\bolde_1, \bolde_2\}=\{(1,0), (0,1)\}</m>. According to <xref ref="cor_matrix_transformations"/>
      <md>
        <mrow> A \amp=\begin{bmatrix}
          \vert\amp \vert \\
          \rho_\alpha(1,0)\amp \rho_\alpha(0,1) \\
          \vert \amp \vert
        \end{bmatrix} </mrow>
        <mrow> \amp=\begin{amatrix}[rr]
          \cos\alpha \amp -\sin\alpha \\
          \sin\alpha \amp \cos\alpha
        \end{amatrix} </mrow>
      </md>,
      since <m>(1,0)</m> gets rotated by <m>\rho_\alpha</m> to <m>(\cos\alpha, \sin\alpha)</m>, and <m>(0,1)</m> gets rotated to <m>(-\sin\alpha, \cos\alpha)</m>.
    </p>
  </solution>
</example>
</subsection>
<subsection xml:id="ss_isomorphisms">
  <title>Isomorphisms</title>
  <introduction>
    The word <q>isomorphism</q> is derived from the Greek terms <foreign>iso</foreign>, meaning <q>same</q>, and <foreign>morphe</foreign>, meaning <q>form</q>. As we will see, isomorphic vector spaces <m>V</m> and <m>W</m> are essentially the same creature, at least as far as linear algebraic properties are concerned. Furthermore, an isomorphism <m>T\colon V\rightarrow W</m> provides a one-to-one correspondence between them: a dictionary that allows us to translate statements about <m>V</m> to statements about <m>W</m>, and vice versa.
  </introduction>
  <definition xml:id="d_isomorphism">
    <title>Isomorphism</title>

    <idx><h>isomorphism</h></idx>
    <idx><h>invertible</h><h>linear transformation</h></idx><statement>
      <p>
        A linear transformation <m>T\colon V\rightarrow W</m> is an <term>isomorphism</term> if <m>T</m> is an invertible function (<xref ref="d_invertible_function"/>): equivalently (<xref ref="th_invertible_bijective"/>), if <m>T</m> is bijective (<xref ref="d_injective_surjective_bijective"/>).
      </p>
      <p>
        Vector spaces <m>V</m> and <m>W</m> are <term>isomorphic</term> if there is an isomorphism <m>T\colon V\rightarrow W</m>.
      </p>
    </statement>
  </definition>
  <remark>
  <title>Proving <m>T</m> is an isomorphism</title>
  <p>
    According to <xref ref="d_isomorphism"/>, to prove a function <m>T\colon V\rightarrow W</m> is an isomorphism, we must show that
    <ol label="i">
      <li>
        <p>
          <m>T</m> is linear, and
        </p>
      </li>
      <li>
        <p>
          <m>T</m> is invertible.
        </p>
      </li>
    </ol>
    Since being invertible is equivalent to being bijective, there are two main approaches to proving that (ii) holds for a linear transformation <m>T\colon V\rightarrow W</m>:
    <ol label="a">
      <li>
        <p>
          we can show directly that <m>T</m> is invertible by providing an inverse <m>T^{-1}\colon W\rightarrow V</m>;
        </p>
      </li>
      <li>
        <p>
          we can show that <m>T</m> is bijective (i.e., injective and surjective).
        </p>
      </li>
    </ol>
    Which approach, (a) or (b), is more convenient depends on the linear transformation <m>T</m> in question.
  </p>
   </remark>
       <remark xml:id="rm_isomorphism_inverse">
    <title>Inverse of isomorphism is an isomorphism</title>
     <statement>
       <p>
         Let <m>T\colon V\rightarrow W</m> be an isomorphism. Since <m>T</m> is invertible, there is an inverse function <m>T^{-1}\colon W\rightarrow V</m>. Not surprisingly, <m>T^{-1}</m> is itself a linear transformation, though of course this requires proof. (See <xref provisional="ex_isomorphism_inverse"/>.) Since <m>T^{-1}</m> is also invertible (<m>T</m> is its inverse), it follows that <m>T^{-1}</m> is itself an isomorphism.
       </p>
     </statement>
   </remark>
<p>
  We mentioned in the introduction that two isomorphic vector spaces are, for all linear algebraic intents and purposes, essentially the same thing. The next theorem provides some evidence for this claim. It also illustrates how a given isomorphism <m>T\colon V\rightarrow W</m> can translate back and forth between two isomorphic vector spaces. Recall (<xref ref="d_image"/>) that for a subset <m>S\subseteq V</m>, the image <m>T(S)</m> of <m>S</m> under <m>T</m> is the set
  <me>
    T(S)=\{\boldw\in W\colon \boldw=T(\boldv) \text{ for some } \boldv\in S\}=\{T(\boldv)\colon \boldv\in S\}
  </me>.
</p>
<theorem xml:id="th_isomorphism_preserves">
  <title>Properties preserved by isomorphisms</title>
  <statement>
    <p>
      Let <m>T\colon V\rightarrow W</m> be an isomorphism. The following properties hold:
      <ol label="i">
        <li>
          <p>
            <m>S\subseteq V</m> is linearly independent if and only if <m>T(S)\subseteq W</m> is linearly independent;
          </p>
        </li>
        <li>
          <p>
            <m>S\subseteq V</m> spans <m>V</m> if and only if <m>T(S)\subseteq W</m> spans <m>W</m>;
          </p>
        </li>
        <li>
          <p>
            <m>S\subseteq V</m> is a basis of <m>V</m> if and only if <m>T(S)\subseteq W</m> is a basis of <m>W</m>
          </p>
        </li>
        <li>
          <p>
            <m>\dim V=\dim W</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>

</subsection>



</section>
