<section xml:id="s_diagonalization">
  <title>Diagonalization</title>


<introduction>
  <p>
Intro
  </p>
</introduction>


  <subsection xml:id="ss_diagonalizable">
    <title>Diagonalizable transformations</title>
  <definition>
    <statement>
      <p>
        Let <m>V</m> be a finite-dimensional vector space. A linear transformation <m>T\colon V\rightarrow V</m> is
        <term>diagonalizable</term>
        if there exists an ordered basis <m>B</m> of <m>V</m> for which <m>[T]_B</m> is a diagonal matrix.
      </p>
      <p>
        An <m>n\times n</m> matrix <m>A</m> is <term>diagonalizable</term> if the matrix transformation <m>T_A\colon \R^n\rightarrow \R^n</m> is diagonalizable.
      </p>
    </statement>
  </definition>

    <p>
      At last we relate the property of being diagonalizable with the notion of eigenvectors.
      In the process we make clear what we mean when we say <m>T</m> is diagonalizable if and only if it has
      <q>enough</q>
      linearly independent eigenvectors.
    </p>
    <theorem xml:id="th_diagonalizability">
      <title>Diagonalizability conditions</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation of the finite-dimensional space <m>V</m>.
          <ol>
            <li>
              <title>Basis of eigenvectors</title>
              <p>
                Given an ordered basis <m>B</m> of <m>V</m>,
                the matrix <m>[T]_B</m> is diagonal if and only if <m>B</m> consists of eigenvectors of <m>T</m>.
                Thus <m>T</m> is diagonalizable if and only if there is an ordered basis of <m>V</m> consisting of eigenvectors of <m>T</m>.
              </p>
            </li>
            <li>
              <title>Dimension of eigenspaces</title>
              <p>
                Let <m>\lambda_1, \lambda_2, \dots, \lambda_r</m> be the
                distinct eigenvalues of <m>T</m>, and for each <m>1\leq i\leq r</m>, let
                <m>W_{\lambda_i}</m> be the <m>\lambda_i</m>-eigenspace. We have
                <me>T \text{ is diagonalizable } \iff \sum_{i=1}^r\dim W_{\lambda_i}=n
                </me>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
      </proof>
    </theorem>
    <corollary xml:id="cor_diagonalizability">
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation of the <m>n</m>-dimensional space <m>V</m>, let <m>B</m> be an ordered basis of <m>V</m>, and let <m>A=[T]_B</m>. The following are equivalent:
          <ol>
            <li>
              <p>
                <m>T</m> is diagonalizable;
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is diagonalizable;
              </p>
            </li>
            <li>
              <p>
                there is a basis of <m>\R^n</m> consisting of eigenvectors of <m>A</m>;
              </p>
            </li>
            <li>
              <p>
                if <m>\lambda_1,\lambda_2, \dots, \lambda_r</m> are the distinct eigenvalues of <m>A</m>, then
                <me>
                  \sum_{i=1}^r\dim W_{\lambda_i}=n
                </me>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </corollary>
    <algorithm xml:id="proc_diagonalize">
      <title>Deciding whether a linear transformation is diagonalizable</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m>, where <m>V</m> is an <m>n</m>-dimensional vector space. To decide whether <m>T</m> is diagonalizable proceed as follows.
        </p>
        <ol>
          <li>
            <p>
              Pick <em>any</em> ordered basis <m>B</m> of <m>V</m> and compute <m>A=[T]_B</m>. By <xref ref="cor_diagonalizability"/> we know that <m>T</m> is diagonalizable if and only if <m>A</m> is diagonalizable.
            </p>
          </li>
          <li>
            <p>
              Compute the distinct real eigenvalues <m>\lambda_1, \lambda_2, \dots, \lambda_r</m> of <m>A</m> as well as bases for their corresponding eigenspaces <m>W_{\lambda_i}</m>.
            </p>
          </li>
          <li>
            <p>
              We have
              <me>
                A \text{ diagonalizable }\iff \sum_{i=1}^n\dim W_{\lambda_i}=n
              </me>.
            </p>
          </li>
          <li>
            <p>
              For all <m>1\leq i\leq r</m> compute and ordered basis <m>B_i</m> of <m>W_{\lambda_i}</m>, let <m>B'</m> be the ordered basis obtained by concatenating the bases <m>B_1, B_2, \dots, B_r</m>, and let <m>P</m> be the matrix whose <m>j</m>-th column is the <m>j</m>-th element of <m>B'</m>. The basis <m>B'</m> is a basis of eigenvectors of <m>A</m> and we have
              <me>
                [T_A]_{B'}=P^{-1}AP=D
              </me>,
              where <m>D</m> is the diagonal matrix
              <me>
              D=
                \begin{bmatrix}
                \lambda_1 \amp \amp  \amp \amp \amp \amp   \\
                         \amp \ddots \amp \amp \amp \amp     \\
                  \amp   \amp   \lambda_2 \amp  \amp \amp   \\
                \amp  \amp  \amp     \ddots \amp  \amp    \\
                \amp  \amp  \amp    \amp   \lambda_r \amp   \\
                \amp  \amp  \amp    \amp  \amp   \ddots    \\
                \amp  \amp  \amp    \amp  \amp  \amp   \lambda_r
                \end{bmatrix}
              </me>.

            </p>
          </li>
          <li>
            <p>
              <q>Lifting</q> the basis <m>B'</m> back to <m>V</m> via the coordinate transformation <m>[\phantom{\boldv}]_B</m> yields a basis of eigenvectors of <m>T</m>.
            </p>
          </li>
        </ol>
      </statement>
    </algorithm>
  <remark xml:id="rm_diagonalize_matrices">
    Our examaples will focus on the special case where <m>T=T_A</m> is a matrix transformation. In this case deciding whether <m>T</m> is diagonalizable is equivalent to deciding whether <m>A</m> is diagonalizable, and this question is answered by performing steps (2)-(4) of <xref ref="proc_diagonalize"/>.
  </remark>
  <p>
   As a first demonstration of <xref ref="proc_diagonalize"/> we give an example of a matrix that is not diagonalizable.
  </p>
  <example xml:id="eg_diagonalizable_not">
    <statement>
      <p>
        Show that <m>A=\begin{bmatrix}
          1\amp 1\\ 0\amp 1
        \end{bmatrix}</m> is not diagonalizable.
      </p>
    </statement>
    <solution>
      <p>
        Since <m>A</m> is upper triangular, its characteristic polynomial is easily seen to be <m>p(t)=(t-1)^2</m>. Thus <m>\lambda=1</m> is the only eigenvalue of <m>A</m>. We have
        <me>
          W_1=\NS(I-A)=\NS \begin{amatrix}[rr]
          0\amp -1 \\ 0\amp 0
        \end{amatrix}=\Span\{(1,0)\}
        </me>.
        Since <m>\dim W_1=1\lt 2</m>, we conclude that <m>A</m> is not diagonalizable.
      </p>
    </solution>
  </example>
  <p>
    A slightly larger example will better illustrate the various steps of <xref ref="proc_diagonalize"/>.
  </p>
  <example xml:id="eg_diagonalizable_big">
    <statement>
      <p>
        The matrix
        <me>
        A=\begin{amatrix}[rrrr]14 \amp 21 \amp 3 \amp -39 \\ 12 \amp 25 \amp 3 \amp -41 \\ 12 \amp 24 \amp 5 \amp -42 \\ 12 \amp 22 \amp 3 \amp -38 \end{amatrix}
        </me>
        has characteristic polynomial <m>p(t)=t^4 - 6t^3 + 9t^2 + 4t - 12</m>. Decide whether <m>A</m> is diagonalizable. If yes, find an invertible matrix <m>P</m> and diagonal matrix <m>D</m> such that <m>D=P^{-1}AP</m>.
      </p>
    </statement>
    <solution>
      <p>
       To factor <m>p(t)</m>, we first look for integer roots dividing the constant term <m>-12</m>: <ie />, we test whether any of <m>\pm 1, \pm 2, \pm 3, \pm 4, \pm 6, \pm 12</m> are roots. Luckily, we see that <m>-1</m> is a root of <m>p(t)</m>. Doing polynomial division of <m>p(t)</m> by <m>(t+1)</m> yields
       <me>
         p(t)=(t+1)\underset{q(t)}{(t^3-7t^2+16t-12)}
       </me>.
       Repeating this factoring technique on <m>q(t)</m>, we see that <m>q(2)=0</m>, and thus can continue to factor:
       <md>
         <mrow> p(t)\amp=(t+1)(t^3-7t^2+16t-12)</mrow>
         <mrow> \amp=(t+1)(t-2)(t^2-5t+6) </mrow>
         <mrow>  \amp = (t+1)(t-2)^2(t-3)</mrow>
       </md>.
       We conclude that the eigenvalues of <m>A</m> are <m>-1</m>, <m>2</m>, and <m>3</m>.  We now compute bases for the corresponding eigenspaces. The bases below were obtained using <xref ref="proc_fund_spaces"/>. We omit the details of the Gaussian elimination performed in each case. (Check for yourself!)
       <md>
         <mrow>W_{-1} \amp =\NS \begin{amatrix}[rrrr]
         -15\amp -21\amp -3\amp 39\\
         -12\amp -26\amp -3\amp 41\\
         -12\amp -24\amp -6\amp 42\\
         -12\amp -22\amp -3\amp -37
         \end{amatrix}=\Span\{(1,1,1,1)\} </mrow>
         <mrow>W_{2} \amp =\NS \begin{amatrix}[rrrr]
         -12\amp -21\amp -3\amp 39\\
         -12\amp -23\amp -3\amp 41\\
         -12\amp -24\amp -3\amp 42\\
         -12\amp -22\amp -3\amp 40
         \end{amatrix}=\Span\{(3,2,0,2),(1,1,2,1)\}  </mrow>
         <mrow>W_{-1} \amp =\NS \begin{amatrix}[rrrr]
         -11\amp -21\amp -3\amp 39\\
         -12\amp -22\amp -3\amp 41\\
         -12\amp -24\amp -2\amp 42\\
         -12\amp -22\amp -3\amp 41
         \end{amatrix}=\Span\{(3,5,6,4)\}  </mrow>
       </md>.
       We have ski
       Since
       <me>
         \dim W_{-1}+\dim W_{2}+\dim W_{3}=1+2+1=4=\dim \R^4
       </me>,
       we conclude that <m>A</m> is diagonalizable. Furthermore, we have <m>D=P^{-1}AP</m>, where
       <me>
         P=\begin{amatrix}[rrrr]
           1\amp 3\amp 1\amp 3\\
           1\amp 2\amp 1\amp 5\\
           1\amp 0\amp 2\amp 6\\
           1\amp 2\amp 1\amp 4
         \end{amatrix},
         D=\begin{amatrix}[rrrr]
         -1\amp 0 \amp 0 \amp 0 \\
         0 \amp 2\amp 0\amp 0\\
         0\amp 0\amp 2\amp 0\\
         0\amp 0\amp 0\amp 3
       \end{amatrix}
       </me>.
     </p>
    </solution>
  </example>
</subsection>
<subsection xml:id="ss_diagonalizable_matrices">
  <title>Diagonalizable matrices</title>
  <p>
    Recall that two square matrices <m>A</m> and <m>B</m> are similar if <m>B=P^{-1}AP</m> for some invertible matrix <m>P</m> (<xref ref="d_similar" text="global"/>). From our theory above it follows that a matrix <m>A</m> is diagonalizable if and only if it is similar to a diagonal matrix.
  </p>
  <corollary xml:id="cor_diagonalizable_matrix">
    <title>Diagonalizabilty and similarity</title>
    <statement>
      <p>
        An <m>n\times n</m> matrix <m>A</m> is diagonalizable if and only if it is similar to a diagonal matrix: <ie />, if and only if there is an invertible matrix <m>P</m> and a diagonal matrix <m>D</m> such that
        <me>
          D=P^{-1}AP
        </me>.
      </p>
    </statement>
    <proof>
      <p>
        Let <m>B</m> be the standard ordered basis of <m>\R^n</m>. If <m>A</m> is diagonalizable, then there is an ordered basis <m>B'</m> of <m>\R^n</m> such that <m>[T_A]_{B'}=D</m> is diagonal. Setting <m>P=\underset{B'\rightarrow B}{P}</m>, we have
        <md>
          <mrow>D=[T_A]_{B'} \amp= \underset{B\rightarrow B'}{P}[T_A]_B\underset{B'\rightarrow B}{P} \amp (<xref ref="th_change_of_basis_transformations" text="global"/>) </mrow>
          <mrow> \amp= P^{-1}AP \amp (\underset{B'\rightarrow B}{P}=P, [T_A]_B=A) </mrow>
        </md>.
        Conversely, suppose we have <m>D=P^{-1}AP</m>, for some invertible matrix <m>P</m> and diagonal matrix <m>D</m>. Let <m>B'</m> be ordered basis whose <m>j</m>-th element is the <m>j</m>-th column of <m>P</m>. We have <m>P=\underset{B'\rightarrow B}{P}</m> (since <m>B</m> is the standard basis), and hence
        <me>
          [T_A]_{B'}=P^{-1}[T_A]_BP=P^{-1}AP=D
        </me>,
        showing that <m>T_A</m> is diagonalizable, as desired.

      </p>
    </proof>

  </corollary>
  <p>
  Similar matrices share many of the same properties. (See <xref ref="th_similarity"/>.)  As observed in <xref ref="rm_holy_commutative_tent"/>, this is a consequence of the fact that similar matrices can be thought of as two different matrix representations of the same fixed linear transformation.
</p>
<theorem xml:id="th_conjugation">
  <title>Properties of conjugation</title>
  <statement>
    <p>
      Let <m>P</m> be any invertible <m>n\times n</m> matrix.
      <ol>
        <li>
          <p>
            For all <m>A_1,A_2\in M_{nn}</m> and <m>c_1,c_2\in \R</m>, we have <m>P^{-1}(c_1A_1+c_2A_2)P=c_1P^{-1}A_1P+c_2P^{-1}A_2P</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>A\in M_{nn}</m>. For all integers <m>k\geq 0</m>, we have <m>P^{-1}A^kP=(P^{-1}AP)^k</m>.
            If <m>A</m> is invertible,
            this equality holds for <em>all</em>
            integers <m>n</m>.
          </p>
        </li>
        <li>
          <p>
            Recall that given any polynomial <m>f(x)=a_rx^r+a_{r-1}x^{r-1}+\cdots +a_1x+a_0</m> and any
            <m>n\times n</m> matrix <m>A</m> we define <m>f(A)=a_rA^r+a_{r-1}A^{r-1}+\cdots +a_1A+a_0I_n</m>.
            We have <m>f(P^{-1}AP)=P^{-1}f(A)P</m> for any polynomial <m>f(x)</m>. 
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>

<theorem xml:id="th_similarity">
  <title>Properties of similarity</title>
  <statement>
    <p>
      Suppose <m>A</m> is similar to <m>B</m>:
      <ie />, there is an invertible matrix <m>P</m> such that <m>B=P^{-1}AP</m>. The following hold:
      <ol>
        <li>
          <p>
            <m>B</m> is similar to <m>A</m>;
          </p>
        </li>
        <li>
          <p>
            <m>A</m> and <m>B</m> have the same trace and determinant;
          </p>
        </li>
        <li>
          <p>
            <m>A</m> and <m>B</m> have the same rank;
          </p>
        </li>
        <li>
          <p>
            <m>A</m> and <m>B</m> have the same characteristic polynomial;
          </p>
        </li>
        <li>
          <p>
            <m>A</m> and <m>B</m> have the same eigenvalues;
          </p>
        </li>
        <li>
          <p>
            for any <m>\lambda\in \R</m> we have <m>\dim W_{\lambda}=\dim W_{\lambda}'</m>, where <m>W_\lambda, W_\lambda'</m> are the <m>\lambda</m>-eigen spaces of <m>A</m> and <m>B</m>, respectively.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <proof>
    <p>
      By definition we have <m>B=P^{-1}AP</m> for some matrix <m>P</m>.
      We wish to show the characteristic polynomials <m>p_A(t)</m> and <m>p_B(t)</m> of the two matrices are equal.
      Compute:
      <md>
        <mrow>p_B(t)\amp =\amp \det(tI-B)</mrow>
        <mrow>\amp =\amp \det(tI-P^{-1}AP)</mrow>
        <mrow>\amp =\amp \det(tP^{-1}IP-P^{-1}AP) \ \text{ (since \(P^{-1}IP=I\))}</mrow>
        <mrow>\amp =\amp \det(P^{-1}tIP-P^{-1}AP) \ \text{ (\(t\) behaves as scalar) }</mrow>
        <mrow>\amp =\amp \det(P^{-1}(tI-A)P)</mrow>
        <mrow>\amp =\amp \det(P^{-1})\det(tI-A)\det(P)</mrow>
        <mrow>\amp =\amp (\det(P))^{-1}\det(P)\det(tI-A)</mrow>
        <mrow>\amp =\amp \det(tI-A)=p_A(t)</mrow>
      </md>.
    </p>
  </proof>
</theorem>
<p>
  In this spirit, a good way of thinking about a diagonalizable matrix <m>A</m> is that it is <q>as good as diagonal</q>. In practical terms, if <m>A</m> satisfies
  <men xml:id="eq_diagonalizable">
    D=P^{-1}AP
  </men>,
  where <m>D</m> is diagonal,
  then we can answer many computational and theoretical questions about <m>A</m> by first answering them about <m>D</m> and then using the relation <xref ref="eq_diagonalizable"/>. The following examples are nice illustrations of this technique.
</p>
<example xml:id="eg_diagonalizable_matrix_powers">
  <title>Diagoanlizable: matrix powers</title>
  <statement>
    <p>
      Assume <m>D=P^{-1}AP</m>, where <m>D</m> is diagonal.
      To normally difficult matrix power computation <m>A^{n}</m> can be accomplished by computing <m>D^{n}</m> (easy) and then observing that
      <md>
        <mrow>A^n\amp = (PDP^{-1})^n \amp </mrow>
        <mrow> \amp =PD^nP^{-1} \amp (<xref ref="th_conjugation"/>, 2) </mrow>
      </md>.
      For example,
      take <m>A=\begin{bmatrix}1\amp 3\\ 1\amp -1 \end{bmatrix}</m>.
      Using <xref ref="proc_diagonalize"/>, we see that <m>D=P^{-1}AP</m>,
      where <m>D=\begin{bmatrix}2\amp 0\\ 0\amp -2 \end{bmatrix}</m>
      and <m>P=\begin{bmatrix}3\amp 1\\ 1\amp -1 \end{bmatrix}</m>.
      It follows that for any <m>n\in \Z</m> we have
      <md>
        <mrow>A^n \amp=PD^nP^{-1} </mrow>
        <mrow> \amp = P\begin{bmatrix}2^{n}\amp 0\\ 0\amp (-2)^{n} \end{bmatrix} P^{-1}</mrow>
        <mrow>  \amp = \frac{1}{4}\begin{bmatrix}3\cdot2^n+(-2)^n\amp 3\cdot 2^n-3(-2)^{n}\\ 2^{n}-(-2)^n\amp 2^n+3(-2)^{n} \end{bmatrix}</mrow>
      </md>.
    </p>
  </statement>
</example>
<example xml:id="eg_diagonalizable_matrix_polynomials">
  <title>Diagonalizable: matrix polynomials</title>

  <statement>
    <p>
      Assume <m>D=P^{-1}AP</m>, where <m>D</m> is a diagonal <m>n\times n</m> matrix. Let <m>[D]_{ii}=d_{i}</m>. Given any polynomial <m>f(x)=\anpoly</m>, we have
      <md>
        <mrow>f(A) \amp= f(PDP^{-1}) </mrow>
        <mrow> \amp =Pf(D)P^{-1}  \amp (<xref ref="th_conjugation"/>)</mrow>
      </md>.
      Furthemore, since <m>D</m> is diagonal, it follows that <m>f(D)</m> is also diagonal, and in fact its diagonal entries are given by <m>f(d_i)</m>. This gives us an easy method of computing arbitary polynomials of the matrix <m>A</m>.
    </p>
    <p>
      Take the matrix <m>A=\begin{bmatrix}1\amp 3\\ 1\amp -1 \end{bmatrix}</m> from <xref ref="eg_diagonalizable_matrix_powers"/>, which satisfies <m>D=P^{-1}AP</m>,
      where <m>D=\begin{bmatrix}2\amp 0\\ 0\amp -2 \end{bmatrix}</m>
      and <m>P=\begin{bmatrix}3\amp 1\\ 1\amp -1 \end{bmatrix}</m>. Let <m>f(x)=x^2 -4</m>. Since <m>f(2)=f(-2)=0</m>, it follows that <m>f(D)=D^2-4I=\boldzero</m> and hence
      <me>
        f(A)=A^2-4I=Pf(D)P^{-1}=P\boldzero P^{-1}=\boldzero
      </me>,
      as you can easily check.
    </p>
  </statement>
</example>
  <example>
    <statement>
      <p>
        <m>A</m> has a <em>square-root</em>
        (i.e., a matrix <m>B</m> such that <m>B^2=A</m>) iff <m>D</m> has a square-root.
      </p>
      <p>
        Indeed, suppose <m>B^2=A</m>.
        Set <m>C=P^{-1}BP</m>.
        Then <m>C^2=P^{-1}B^2P=P^{-1}AP=D</m>.
        Similarly, if <m>C^2=D</m>, then <m>B^2=A</m>, where <m>B=PCP^{-1}</m>.
      </p>
      <p>
        As an example,
        the matrix <m>A=\begin{bmatrix}0\amp -2\\ 1 \amp 3 \end{bmatrix}</m>,
        satisfies <m>D=P^{-1}AP</m>,
        where <m>D=\begin{bmatrix}1\amp 0\\ 0\amp 2 \end{bmatrix}</m>,
        and <m>P=\begin{bmatrix}2\amp 1\\ -1\amp -1 \end{bmatrix}</m>.
        Since <m>C=\begin{bmatrix}1\amp 0\\ 0\amp \sqrt{2} \end{bmatrix}</m> is a square-root of <m>D</m>,
        <m>B=PCP^{-1}=\begin{bmatrix}2-\sqrt{2}\amp 2-2\sqrt{2}\\ -1+\sqrt{2}\amp -1+2\sqrt{2} \end{bmatrix}</m> is a square-root of <m>A</m>,
        as you can easily check.
      </p>
      <p>
        So when exactly does a diagonal matrix <m>D</m> have a square-root?
        Clearly, it is sufficient that
        <m>d_i\geq 0</m> for all <m>i</m>, as in the example above.
        Interestingly, this is not a necessary condition!
        Indeed, consider the following example:
      </p>
      <p>
        <m>\begin{bmatrix}-1\amp 0\\ 0\amp -1 \end{bmatrix} =\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} ^2</m>.
      </p>
    </statement>
  </example>


</subsection>
  <subsection>
    <title>Geometric and algebraic multiplicity</title>
    <p>
      Take <m>A</m>
      (or <m>T</m>)
      and suppose the characteristic polynomial <m>p(t)</m> factors as
      <me>
        p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}
      </me>,
      where the <m>\lambda_i</m> are the
      <em>distinct</em> eigenvalues of <m>A</m>
      (or <m>T</m>).
      It turns out that the exponent <m>n_i</m>,
      called the <term>algebraic multiplicity</term>
      of the eigenvalue <m>\lambda_i</m>,
      is an upper bound on <m>m_i=\dim W_{\lambda_i}</m>,
      called the <term>geometric multiplicity</term>.
    </p>
    <theorem>
      <title>Algebraic and geometric multiplicity theorem</title>
      <statement>
        <p>
          Let <m>A</m>
          (or <m>T</m>)
          have characteristic polynomial
          <me>
            p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}
          </me>,
          where the <m>\lambda_i</m> are the
          <em>distinct</em> eigenvalues of <m>A</m>
          (or <m>T</m>).
          Then
          <me>
            \dim W_{\lambda_i}\leq n_i:
          </me>
          i.e., the geometric multiplicity is less than or equal to the algebraic multiplicity.
        </p>
      </statement>
    </theorem>
  </subsection>
  <subsection>
    <title>Linear independence and eigenvectors</title>
    <p>
      The following result is used to prove the diagonalizability theorem
      (<xref ref="th_diagonalizability">Theorem</xref>),
      but is also very useful in its own right.
    </p>
    <theorem xml:id="th_independentteigenvectors">
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation,
          and let <m>S=\{\boldv_1,\dots, \boldv_r\}</m> be a set of eigenvectors of <m>T</m> with <m>T\boldv_i=\lambda_i\boldv_i</m>.
        </p>
        <p>
          If the <m>\lambda_i</m> are all distinct,
          then <m>S</m> is linearly independent.
        </p>
      </statement>
    </theorem>

    <corollary>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow V</m> be a linear transformation,
          and suppose <m>\dim V=n</m>.
        </p>
        <p>
          If <m>T</m> has <m>n</m> distinct eigenvalues,
          then <m>T</m> is diagonalizable.
        </p>
      </statement>
      <proof>
        <p>
          Let <m>\boldv_1, \boldv_2,\dots, \boldv_n</m> be eigenvectors corresponding to these <m>n</m> distinct eigenvalues.
          The theorem tells us they form a linearly independent set.
          Since <m>\dim V=n</m>,
          they form a basis for <m>V</m> by the dimension theorem compendium.
          Since <m>T</m> has a basis of eigenvectors, it is diagonalizable.
        </p>
      </proof>
    </corollary>


  <p>
    <xref ref="th_independentteigenvectors">Theorem</xref>
    makes no assumption about the dimension of <m>V</m>.
    It can thus be applied to interesting infinite-dimensional examples.
  </p>
  <example>
    <statement>
      <p>
        Let <m>V=C^\infty(\R)</m>,
        and let <m>T\colon V\rightarrow V</m> be defined as <m>T(f)=f'</m>.
      </p>
      <p>
        Let <m>f_i(x)=e^{k_ix}</m>, where the <m>k_i</m> are all distinct constants.
        I claim <m>S=\{f_1,f_2,\dots , f_r\}</m> is linearly independent.
      </p>
      <p>
        Indeed, each <m>f_i</m> is an eigenvector of <m>T</m>,
        since <m>T(f_i)=(e^{k_ix})'=k_ie^{k_ix}=k_if_i</m>.
      </p>
      <p>
        Since the <m>k_i</m>'s are all distinct,
        it follows that the <m>f_i</m> are eigenvectors with distinct eigenvalues,
        hence linearly independent!
      </p>
      <p>
        Note: try proving that <m>S</m> is linearly independent using the the Wronskian!
        You get a very interesting determinant computation.
      </p>
    </statement>
  </example>



    <!-- <p>
      Hopefully <xref ref="th_conjugation">Theorems</xref>
      and <xref ref="th_similarity"></xref>
      convince you that similar matrices
      (in the linear algebraic sence)
      are truly similar
      (in the usual sense).
    </p>
    <p>
      There is, however, a deeper explanation for this.
      Namely, if <m>A</m> and <m>A'</m> are similar,
      then they are simply two different matrix representations of a common linear transformation!
    </p>
    <p>
      In more detail: suppose we have <m>A'=P^{-1}AP</m>.
      <ul>
        <li>
          <p>
            Let <m>B</m> be the standard basis of <m>\R^n</m>,
            and let <m>B'</m> be the basis of <m>\R^n</m> obtained by taking the columns of the invertible matrix <m>P</m>.
            Finally, let <m>T=T_A</m> be the matrix transformation associated to <m>A</m>.
          </p>
        </li>
        <li>
          <p>
            Then <m>A=[T]_B</m>, <m>P=\underset{B'\rightarrow B}{P}</m>,
            and <m>P^{-1}=\underset{B\rightarrow B'}{P}</m>.
          </p>
        </li>
        <li>
          <p>
            From the change of basis formula it follows that
            <me>
              A'=P^{-1}AP=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=[T]_{B'}
            </me>
          </p>
        </li>
      </ul>
    </p>
    <p>
      In other words to say <m>A</m> and <m>A'</m> are similar is simply to say that they are different matrix representations of the same overlying linear transformation <m>T</m>
      (see Holy Commutative Tent of Linear Algebra on next slide).
      All their shared properties
      (same eigenvalues, same determinant, same trace, etc.)
      are simply the properties they inherit from this one overlying <m>T</m>,
      of which they are but earthly shadows.
    </p>
    <p>
      There is one true <m>T</m>!
    </p> -->
  </subsection>

</section>
