<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_invertibility_theorem">
  <title>The invertibility theorem</title>
  <introduction>
    <p>
    We saw in <xref ref="ex_invertible_matrices"/> that verifying directly whether a matrix is invertible, using only <xref ref="d_invertible_matrix"/>, can be quite an involved task. The goal of this section is to make this less onerous by developing some equivalent methods of testing invertibilty. Our work culminates in <xref ref="th_invertibility"/> and <xref ref="th_invertibility_algorithm"/>, which draw connections between invertiblity, solutions to linear systems, and row echelon forms of a square matrix. Not surprisingly, our old friend Gaussian elimination emerges as the fundamental computational tool.
    </p>
  </introduction>
<paragraphs xml:id="ss_elementary_matrix">
  <title>Elementary matrices</title>
  <p>
  We begin with a treatment of <em>elementary matrices</em>, which serve as the basic building blocks for invertible matrices, and provide a crucial link between row reduction and matrix multiplication.
  </p>
  <definition xml:id="d_elementary_matrix">
    <idx><h>elementary matrix</h></idx>
    <idx><h>matrix</h><h>elementary</h></idx>
    <notation>
      <usage>\underset{cr_i}{E}</usage>
      <description>Scaling elementary matrix</description>
    </notation>
    <notation>
      <usage>\underset{r_i\leftrightarrow r_j}{E}</usage>
      <description>Row swap elementary matrix</description>
    </notation>
    <notation>
      <usage>\underset{r_i\rightarrow r_i+c\,r_j}{E}</usage>
      <description>Row addition elementary matrix</description>
    </notation>
    <statement>
      <p>
        An <m>m\times m</m>  matrix <m>E</m> is
        <term>elementary</term>
        if multiplying any <m>m\times n</m> matrix <m>A</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
      </p>
      <p>
        We have different types of elementary matrices depending on the type of row operation they perform, and we denote these with an elaboration of our earlier row operation notation:
        <ul>
          <li>
            <p>
              A <term>scaling elementary matrix </term><m>\underset{c\,r_i}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{cr_i}{E}</m> scales the <m>i</m>-th row of <m>A</m> by <m>c</m>.
            </p>
          </li>
          <li>
            <p>
              A <term>row swap elementary matrix </term><m>\underset{r_i\leftrightarrow r_j}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{r_i\leftrightarrow r_j}{E}</m> swaps the <m>i</m>-th and <m>j</m>-th rows of <m>A</m>.
            </p>
          </li>
          <li>
            <p>
              A <term>row addition elementary matrix </term><m>\underset{r_i+c\,r_j}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{r_i+c\,r_j}{E}</m> replaces the <m>i</m>-th row of <m>A</m> with <m>r_i+c\,r_j</m>.
            </p>
          </li>
        </ul>
      </p>
    </statement>
  </definition>
<p>
   Naturally, the row method of multiplication is the key to connecting a given row operation with a particular elementary matrix. <xref ref="th_elementary_matrices"/> shows that once you fix the dimension, an elementary matrix is uniquely defined by the row operation it performs.
</p>
<theorem xml:id="th_elementary_matrices">
  <title>Elementary matrix formulas</title>
  <statement>
    <p>
    Fix an integer <m>m\geq 2</m>. The three types of <m>n\times n</m> elementary matrices can be described as follows:
    </p>
    <ul>
      <li>
        <p>
          The <m>n\times n </m> scaling matrix <m>\underset{c\,r_i}{E}</m> is the result of scaling the <m>i</m>-th row of <m>I_m</m> by <m>c</m>: i.e.,
          <me>
            \underset{c\,r_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>

        </p>
      </li>
      <li>
        <p>
          The <m>n\times n </m> row swap matrix <m>\underset{r_i\leftrightarrow r_j}{E}</m> is the result of swapping the <m>i</m>-th and <m>j</m>-th rows of <m>I_m</m>: i.e.,
          <me>
            \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>

        </p>
      </li>
      <li>
        <p>
          The <m>n\times n </m> row addition matrix <m>\underset{r_i+c\,r_j}{E}</m> is the result of swapping the <m>i</m>-th row of <m>I_m</m> with the sum of its <m>i</m>-th row and <m>c</m> times its <m>j</m>-th row: i.e.,
          <me>
            \underset{r_i+c\,r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
      </li>
    </ul>
  </statement>
  <proof>
    <p>
      First we show that if <m>E</m> is one of the <m>m\times m</m> elementary matrices, then it must assume one of the forms described above. Indeed, since multiplying on the left by <m>E</m> performs a certain row operation, and since <m>E=E\,I_m</m>, we see that <m>E</m> itself is the result of performing this particular row operation on the <m>m\times m</m> identity matrix. Thus <m>E</m>  is one of the three types of matrices described above, obtained by performing an elementary row operation on <m>I_m</m>.
    </p>
    <p>
      Next, we must show that any of the <m>m\times m</m> matrices <m>E</m> described above is indeed elementary in the sense of <xref ref="d_elementary_matrix"/>: that is, we must show that multiplying any <m>m\times n</m> matrix <m>A</m> on the left by <m>E</m> performs the relevant row operation on <m>A</m>. This is now a direct consequence of <xref ref="th_row_method"/>.
    </p>
    <p> For example, take <m>E=\underset{c\,r_i}{E}</m>. For <m>j\ne i</m>, the <m>j</m>-th row of <m>E\,A</m> is given by the <m>j</m>-th row of <m>E</m> times <m>A</m>. Since the <m>j</m>-th row of <m>E</m> in this case has a one in the <m>j</m>-th entry and zeros elsewhere, the product of this row and <m>A</m> is just the <m>j</m>-th row of <m>A</m>. Similarly, the <m>i</m>-th row of <m>E\, A</m> in this case is <m>c</m> times the <m>i</m>-th row of <m>A</m>. Thus <m>E</m> leaves all the rows of <m>A</m> except for the <m>i</m>-th one, which is scaled by <m>c</m>.
    </p>

  </proof>
</theorem>
<p>
  Elementary matrices provide us a way of understanding row reduction as a series of matrix multiplications (on the left). Recall that wow operations on linear systems are useful in so far as they preserve the set of solutions, and that this is the result of each operation being in some sense  <q>reversible</q>. (See <xref ref="ex_row_ops_preserve"/>.) In terms of matrix multiplication, this reversible attribute is reflected in the fact that elementary matrices are <em>invertible</em>.
</p>
  <theorem xml:id="th_inverse_elem">
    <title>Inverses of elementary matrices</title>
    <statement>
      <p>
        Fix <m>n</m>.
        All elementary matrices are invertible,
        and their inverses are elementary matrices.
        In fact, we have the following formulas:
        <md>
          <mrow>\underset{cr_i}{E}^{-1}\amp = \underset{\frac{1}{c}r_i}{E}</mrow> [2ex]
          <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp = \underset{r_i\leftrightarrow r_j}{E}</mrow>[2ex]
          <mrow>\underset{r_i+c\,r_j}E^{-1}\amp = \underset{r_i-cr_j}E</mrow>
        </md>
      </p>
    </statement>
    <proof>
      <p>
        These formulas all follow easily from <xref ref="th_row_method"/>, and the fact that the proposed inverse elementary matrix performs the <q>reverse</q>, or inverse, of the row operation corresponding to the given elementary matrix.
      </p>
    </proof>
  </theorem>
<example>
  <statement>
      <p>
        Fix <m>n=3</m>.
        Verify that the following pairs of <m>3\times 3</m> elementary matrices are indeed inverses of one another.
      </p>
        <ul>
          <li>
            <p>
              <me>\underset{2r_3}{E},\  \underset{\frac{1}{2}r_3}{E}</me>
            </p>
          </li>
          <li>
            <p>
              <me>\underset{r_1\leftrightarrow r_3}{E}, \  \underset{r_1\leftrightarrow r_3}{E}</me>
            </p>
          </li>
          <li>
            <p>
              <me>\underset{r_1+3r_3}{E},\  \underset{r_1-3r_3}{E}</me>
            </p>
          </li>
        </ul>
  </statement>
  <solution>
    <ul>
      <li>
        <p>
          We have
          <me>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</me> and <me>\underset{\frac{1}{2}r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 1/2 \end{bmatrix}</me>.
          You can verify for yourself that <me>\underset{2r_3}{E}\ \underset{\frac{1}{2}r_3}{E}=\underset{\frac{1}{2}r_3}{E}\ \underset{2r_3}{E}=I_3</me>.
        </p>
      </li>
      <li>
        <p>
          We have
          <me>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</me>.
          You can verify for yourself that <me>\underset{r_1\leftrightarrow r_3}{E}\,\underset{r_1\leftrightarrow r_3}{E}=I_3</me>.
        </p>
      </li>
      <li>
        <p>
          We have
          <me>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</me> and <me>\underset{r_1-3r_3}{E}=\begin{amatrix}[rrr]1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{amatrix}</me>.
          You can verify for yourself that <me>\underset{r_1+3r_3}{E}\,\underset{r_1-3r_3}{E}=\underset{r_1-3r_3}{E}\,\underset{r_1+3r_3}{E}=I_3</me>.
        </p>
      </li>
    </ul>
  </solution>
</example>

</paragraphs>
<paragraphs xml:id="ss_systems_to_matrix_eqns" >
  <title>Interlude on matrix equations</title>
  <p>
    We take a moment to make the following simple,
    somewhat overdo observation.
    Namely, we can represent a <em>system of linear equations</em>
    <men xml:id="eq_lin_sys">
      \eqsys
    </men>
    as a <em>single matrix equation</em>
    <men xml:id="eq_matrix_eq">
      \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix}
    </men>,
    or <me>A\boldx=\boldb</me>, where <m>A=[a_{ij}]</m>,
    <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
  </p>
  <p>
    Indeed if you expand out the left-hand side of <xref ref="eq_matrix_eq"/> into an <m>m\times 1</m> column vector,
    using the definition of matrix multiplication,
    and then invoke the definition of matrix equality,
    then you obtain the linear system <xref ref="eq_lin_sys"/>.
  </p>
  <p>
    By the same token, an <m>n</m>-tuple
    <m>(s_1,s_2,\dots,
    s_n)</m> is a solution to the <em>system of equations</em>
    <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\bolds}=[s_i]</m> is a solution to the
    <em>matrix equation</em> <m>A\boldx=\boldb</m>.
  </p>
  <p>
  We have thus recast the problem of solving linear systems to the problem of solving a certain matrix equation of the form
  <me>
    A\underset{n\times 1}{\boldx}=\underset{m\times 1}{\boldb}
  </me>
  for the unknown column vector <m>\boldx</m>.
    In particular, a <em>homogeneous</em> linear system
    <me>
      \homsys
    </me>
    can be represented as a matrix equation of the form
    <me>
      A\underset{n\times 1}{\boldx}=\underset{m\times 1}{\boldzero}
    </me>.
  </p>
  <p>
    Lastly, the use of Gaussian elimination to solve a linear system can now be understood in an <em>algebraic</em> way using matrix multiplication.
  </p>
  <p>
    In more detail, suppose our given linear system has augmented matrix <m>\begin{amatrix}[c|c]A\amp \boldb  \end{amatrix}</m> that row reduces to the row echelon matrix <m>\begin{amatrix}[c|c] U \amp \boldb'  \end{amatrix}</m> after performing a sequence of row operations.
    Denote the <m>i</m>-th row operation <m>\rho_i</m>,
    and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
  </p>
  <p>
    Our sequence of operations <m>\rho_i</m> translates to the following sequence of matrix equations:
    <me>
      \begin{array}{c} A\boldx=\boldb\\  \rho_1(A)\boldx=\rho_1(\boldb)\\  \rho_2(\rho_1(A))\boldx=\rho_2(\rho_1(\boldb))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A))))\boldx=\rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(\boldb)))) \\
      U\boldx=\boldb'\end{array}
    </me>.
  </p>
  <p>
    Let <m>\rho_i</m> correspond to the elementary matrix <m>E_i</m>.
    Then we represent this same sequence using matrix multiplication:
    <men xml:id="eq_GE_via_elem_mat">
      \begin{array}{c} A\boldx=\boldb\\  E_1A\boldx=E_1\boldb\\  E_2E_1A\boldx=E_2E_1\boldb\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A\boldx=E_rE_{r-1}\cdots E_2E_1\boldb \\
       U\boldx=\boldb' \end{array}
    </men>.
  This depiction of row reduction in terms of successive left-multiplication by elementary matrices will be useful to us in many ways. In particular, it follows from this discussion that two matrices <m>A</m> and <m>B</m> are row equivalent if and only if we have
  <men xml:id="eq_row_equiv_via_elem_mat">
    B=E_rE_{r-1}\cdots E_2E_1A
  </men>
  for some collection of elementary matrices <m>E_i</m>.
  </p>
  <p>
    We are now in position to prove our first big theorem.
  </p>

</paragraphs>
<theorem xml:id="th_invertibility">
  <title>Invertibility theorem</title>
  <statement>
    <p>
      Let <m>A=[a_{ij}]</m> be an <m>n\times n</m> matrix.
      The following statements are equivalent.
    </p>
      <ol>
        <li>
          <p>
            <m>A</m> is invertible.
          </p>
        </li>

        <li>
          <p>
            The matrix equation <me>A\underset{n\times 1}{\boldx}=\underset{n\times 1}{\boldb}</me> has a <em>unique solution</em> for <em>any</em> column vector  <m>\boldb</m>.
          </p>
        </li>
        <li>
          <p>
            The matrix equation <me>A\underset{n\times 1}{\boldx}=\underset{n\times 1}{\boldzero}</me> has a <em>unique solution</em>: namely, <m>\boldx=\boldzero_{n\times 1}</m>.
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is row equivalent to <m>I_n</m>,
            the <m>n\times n</m> identity matrix.
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is a product of elementary matrices.
          </p>
        </li>
      </ol>

  </statement>
  <proof>
    <p>
      Recall that to show two statements <m>\mathcal{P}</m> and <m>\mathcal{Q}</m> are equivalent,
      we must show two implications:
      <m>\mathcal{P}\implies \mathcal{Q}</m>, and <m>\mathcal{Q}\implies \mathcal{P}</m>. Instead of doing this for each possible pair of sentences above,
      we ease our work load by instead showing the following
      <em>cycle</em> of implications:
      <me>
        (1)\implies(2)\implies(3)\implies(4)\implies(5)\implies (1)
      </me>.
      Since implication is transitive, starting at any point in our cycle and making our way around the chain of implications, we see that any one of the propositions implies any other proposition.
    </p>
    <case>
     <title><m>(1)\implies (2)</m></title>
    <p>
    Suppose <m>A^{-1}</m> exists. Given any column vector <m>\boldb</m>, we have
    <md>
    <mrow>
      A\boldx=\boldb \iff \boldx=A^{-1}\boldb \amp (<xref ref="th_inverse_cancel"/>)
      </mrow>
    </md>,
    which shows that <m>\boldx=A^{-1}\boldb</m> is the unique solution to <m>A\boldx=\boldb</m>.
    </p>
  </case>
  <case>
   <title><m>(2)\implies (3)</m></title>
  <p>
  Clearly, if <m>A\boldx=\boldb</m> has a unique solution for <em>any</em> choice of <m>\boldb</m>, then it has a unique solution for the <em>particular</em> choice <m>\boldb=\boldzero</m>. Since <m>\boldx=\boldzero_{n\times 1}</m> is clearly a solution to the equation, it must be the only solution.
</p>
  </case>
  <case>
   <title><m>(3)\implies (4)</m></title>
  <p>
  Row reduce <m>A</m> to a matrix <m>U</m> in <em>reduced</em> row echelon form using Gauss-Jordan elimination. (See <xref ref="s_ge_th_matrixforms"/>.) Since the set of solutions to <m>A\boldx=\boldzero</m> is identical to the set of solutions to <m>U\boldx=\boldzero</m> (apply <xref ref="s_systems_th_rowops"/> to their corresponding linear systems), we see that <m>\boldzero</m> is the only solution to <m>U\boldx=\boldzero</m>. <xref ref="th_solveSystem"/> now implies <m>U</m> has a leading one in each column. Since <m>U</m> is <m>n\times n</m> and in <em>reduced</em> row echelon form, it follows that <m>U</m> must be the identity matrix. (Convince yourself of this.) Thus <m>A</m> is row equivalent to <m>U=I</m>, the identity matrix.
  </p>
  </case>
  <case>
 <title><m>(4)\implies (5)</m></title>
  <p>
  If <m>A</m> is row equivalent to <m>I</m>, then according to our discussion after <xref ref="eq_GE_via_elem_mat"/>, we have <m>I=E_rE_{r-1}\cdots E_1A</m> for some collection of elementary matrices <m>E_i</m>. Since elementary matrices are invertible we can multiply both sides of this equation by
  <me>(E_rE_{r-1}\cdots E_1)^{-1}=E_1^{-1}E_{2}^{-1}\cdots E_r^{-1}</me> to conclude
  <me>
    A=E_1^{-1}E_{2}^{-1}\cdots E_r^{-1}
  </me>.
  Since inverses of elementary matrices are elementary (<xref ref="th_inverse_elem"/>), we conclude that <m>A</m> is a product of elementary matrices.
  </p>
  </case>
  <case>
   <title><m>(5)\implies (1)</m></title>
  <p>
  If <m>A</m> is a product of elementary matrices, then it is a product of invertible matrices. Since products of invertible matrices are invertible, we conclude that <m>A</m> is invertible.
  </p>
  </case>
  </proof>
</theorem>
    <remark xml:id="rm_inv_solutions">
  <statement>
    <p>
      The <xref ref="th_invertibility" text='custom'> invertibility theorem </xref> has an immediate application to linear systems where the number of equations is <em>equal to</em> the number of unknowns. In this special situation, the system is equivalent to a matrix equation of the form
      <me>
        A\boldx=\boldb
      </me>,
      where <m>A</m> is a <em>square matrix</em>. According to the theorem, if we know <m>A</m> is invertible, then the matrix equation, and hence the linear system, has a unique solution: namely, <m>\boldx=A^{-1}\boldb</m>.
    </p>
    <p>
      What if <m>A</m> is not invertible? Then the theorem only tells us that there is some column vector <m>\boldc</m>, not necessarily the given <m>\boldb</m>, such that the equation
      <me>
        A\boldx=\boldc
      </me>
      does not have a unique solution. In other words, the theorem alone doesn't allow us to conclude whether the given <m>A\boldx=\boldb</m> has a solution, and we must resort to our usual Gaussian elimination procedure to answer this question.
    </p>
  </statement>
</remark>
<p>
  The family of <em>triangular</em> matrices (upper, lower, and diagonal) defined below provides an easy testing ground for our new invertibility theorem.
</p>
<definition>
  <statement>
    <p>
      Let <m>A=[a_{ij}]</m> be <m>n\times n</m>.
      We say
      <ol type="i">
        <li>
          <p>
            <m>A</m> is <term>diagonal</term>
            if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>i\ne j</m>. (
            <q><m>A</m> is zero off the diagonal.</q>
            )
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is <term>upper triangular</term>
            if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>i>j</m>. (
            <q><m>A</m> is zero above the diagonal.</q>
            )
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is <term>lower triangular</term>
            if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>j>i</m>. (
            <q><m>A</m> is zero below the diagonal.</q>
            )
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is <term>triangular</term>
            if <m>A</m> is upper triangular or lower triangular.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</definition>
<theorem xml:id="th_invertible_triangular">
  <title>Invertibility of triangular matrices</title>
  <statement>
    <p>
      Let <m>A=[a_{ij}]</m> be a triangular <m>n\times n</m> matrix.
      Then <m>A</m> is invertible if and only if
      <m>a_{ii}\ne 0</m> for all <m>1\leq i\leq n</m>.
    </p>
    <p>
      In other words,
      <m>A</m> is invertible if and only if the diagonal entries of <m>A</m> are all nonzero.
    </p>
  </statement>
  <proof>
    <p>
      In this proof we will make use of the following easy consequence of <xref ref="th_inverse_trans"/>: namely, <m>A</m> is invertible if and only if <m>A^T</m> is invertible. (The forward implication follows directly from the theorem; the reverse direction follows from the theorem applied to <m>B=A^T</m>.)
    </p>
    <case>
     <title>Case: <m>A</m> is upper triangular</title>
    <p>
    If <m>a_{ii}\ne 0</m> for all <m>i</m>, then it is easy to see that we can row reduce <m>A</m> first to a row echelon matrix with leading ones in every diagonal entry, and then further to the identity matrix. Thus <m>A</m> is row equivalent to <m>I</m> in this case, and we conclude from statement (4) of <xref ref="th_invertibility"/> that <m>A</m> is invertible.
    </p>
    <p>
      For the other implication, we show that if it is not the case that <m>a_{ii}\ne 0</m> for all <m>i</m>, then there is a nonzero solution <m>\boldx\ne \boldzero</m> to the matrix equation <m>A\boldx=\boldzero</m>. If this is the case, then since we have two distinct solutions to <m>A\boldx=\boldzero</m>, <m>A</m> is not invertible by Statement (3) of <xref ref="th_invertibility"/>.
    </p>
    <p> To this end, assume it is not the case that <m>a_{ii}\ne 0</m> for all <m>i</m>. Then we can find a smallest index <m>k</m> such that <m>a_{kk}=0</m> and <m>a_{ii}\ne 0</m> for any <m>i &lt;k</m>. It is easy to see that <m>A</m> is row equivalent to a matrix <m>A'=[a'_{ij}]</m>, satisfying <m>a_{jj}=1</m>  for <m>1\leq j\leq k-1</m> and <m>a_{ij}=0</m> for all <m>1\leq i &lt; j\leq k-1</m>: i.e., <m>A'</m> is <q>diagonal up until the <m>k</m>-th column</q>.
  </p>
    <p>
    We now provide a nonzero solution <m>\boldx=[x_i]_{n\times 1}</m>to <m>A'\boldx=\boldzero</m>: namely, set <m>x_k=1</m>, <m>x_{i}=0</m> for all <m>i>k</m>, and <m>x_{i}=-a'_{ik}</m> for all <m>i &lt; k</m>. (Verify this for yourself, using the description above of <m>a'_{ij}</m> for <m>j\leq k-1</m>.) Since <m>A'</m> is row equivalent to <m>A</m>, the linear systems cooresponding to <m>\begin{amatrix}[cc] A\amp \boldzero\end{amatrix}</m> and <m>\begin{amatrix}[cc] A' \amp \boldzero\end{amatrix}</m> have the same solutions. Hence <m>\boldx</m> is also a nonzero solution to <m>A\boldx=\boldzero</m>. We conclude that <m>A</m> is not invertible by Statement (3) of <xref ref="th_invertibility"/>. This concludes the proof of this implication.
    </p>
    </case>
    <case>
     <title>Case: <m>A</m> is lower triangular</title>
    <p>
    Set <m>B=A^T</m>. Then <m>B</m> is upper triangular, and <m>(B)_{ii}=(A)_{ii}=a_{ii}</m> for all <m>i</m>. Then
    <md>
      <mrow>A \text{ invertible} \amp \iff B=A^T \text{ invertible} \amp \text{(see statement at top )}</mrow>
      <mrow> \amp \iff a_{ii}=0 \text{ for all } i \amp \text{(by previous case)} </mrow>
    </md>.
    </p>
    </case>
  </proof>

</theorem>
<paragraphs xml:id="ss_invertibility_alg">
  <title>Invertibility algorithms</title>
  <p>
    The proof of the implication <m>(4)\implies (5)</m> of <xref ref="th_invertibility"/> can be expanded into an algorithm that (1) decides whether a given matrix <m>A</m> is invertible, and (2) computes <m>A^{-1}</m> if <m>A</m> is invertible.
  </p>
  <theorem xml:id="th_invertibility_algorithm">
    <title>Inverse algorithm</title>
    <statement>
      <p>
      Let <m>A</m> be an <m>n\times n</m> matrix. To test for invertiblity of <m>A</m> proceed as follows.
      </p>
      <dl>
        <li>
          <title>Step 1</title>
          <p>
          Build the <m>n\times 2n</m> augmented matrix <m>\begin{amatrix}[c|c]A\amp I_n\end{amatrix}</m> and use Gaussian elimination to row reduce to the form <m> \begin{amatrix}[c|c]U\amp B\end{amatrix} </m>, where <m>U</m> is in row echelon form.
          </p>
          <p>
            The matrix <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading ones.
          </p>
        </li>
        <li>
          <title>Step 2</title>
          <p>
            If <m>U</m> has <m>n</m> leading ones, row reduce further to a matrix of the form <m>\begin{amatrix}[c|c] I_n\amp C\end{amatrix}</m>. Then <m>A^{-1}=C</m>.
          </p>
        </li>
      </dl>
    </statement>
    <proof>
      <p>
        From the proof of <xref ref="th_invertibility"/> we know <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading ones. The question remains as to why reducing the augmented matrix <m>\begin{amatrix}[c|c]A\amp I\end{amatrix}</m>  to <m>\begin{amatrix}[c|c]I\amp C\end{amatrix}</m> tells us that <m>A^{-1}=C</m>. Let <m>E_1, E_2, \dots, E_r</m> be the elementary matrices representing the row operations involved in this process. Then we have
        <me>
            E_rE_{r-1}\cdots E_1A=I_n
        </me>.
        After a little algebra, we see that
        <me>
          A^{-1}=E_rE_{r-1}\cdots E_1
        </me>.
        Since <m>C</m> is the result of applying same row operations to <m>I_n</m> we have
        <me>
          C=E_rE_{r-1}\cdots E_1I=E_rE_{r-1}\cdots E_1=A^{-1}
        </me>,
        as claimed.
      </p>
    </proof>
  </theorem>
  <p>
    From the proof of <xref ref="th_invertibility"/> we also derive an algorithm for writing an invertible matrix as a product of elementary matrices.
  </p>
  <theorem xml:id="th_elem_matrices_alg">
    <title>Product of elementary matrices algorithm</title>

    <statement>
      <p>
      Let <m>A</m> be an <m>n\times n</m> matrix. To (potentially) write <m>A</m> as a product of elementary matrices, proceed as follows.
      </p>
      <dl>
        <li>
          <title>Step 1</title>
          <p>
            Attempt to row reduce <m>A</m> to the identity, keeping track of your sequence of row operations in the form of elementary matrices.
          </p>
        </li>
        <li>
          <title>Step 2</title>


          <p>
            If you are able to row reduce <m>A</m> to <m>I_n</m> with a sequence of row operatons corresponding to the elementary matrices <m>E_1, E_2, \dots, E_r</m>, then
            <me>
              A=E_1^{-1}E_2^{-1}\cdots E_{r}.
            </me>
            Since the inverse of an elementary matrix is elementary, we have written <m>A</m> as a product of elementary matrices.

          </p>
        </li>
      </dl>
    </statement>
    <proof>
      <p>
        See the proof of the implication <m>(4)\implies (5)</m> in <xref ref="th_invertibility"/>.
      </p>
    </proof>
  </theorem>
</paragraphs>
<paragraphs xml:id="ss_inv_alg_example">
  <title>In-depth example</title>
      <p>
      Let <me>A=\begin{amatrix}[rrr]1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{amatrix} </me>.
      Combining both algorithms we can decide whether <m>A</m> is invertible, and if so, compute <m>A^{-1}</m> and write <m>A</m> as a product of elementary matrices.
    <md>
      <mrow>\begin{amatrix}[rrr|rrr]1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{amatrix}  \amp \xrightarrow{r_2-r_1} \begin{amatrix}[rrr|rrr]1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{amatrix}</mrow>
      <mrow>\amp \xrightarrow{r_3+r_1} \begin{amatrix}[rrr|rrr]1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{amatrix}</mrow>

      <mrow>\text{ (three leading ones, thus invertible) }\amp \xrightarrow{r_2\leftrightarrow r_3} \begin{amatrix}[rrr|rrr]\boxed{1}\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp \boxed{1}\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp \boxed{1}\amp -1\amp 1\amp 0 \end{amatrix}</mrow>

      <mrow>\amp \xrightarrow{r_2-3r_3} \begin{amatrix}[rrr|rrr]1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{amatrix}</mrow>

      <mrow>\amp \xrightarrow{r_1-2r_2} \begin{amatrix}[rrr|rrr]1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{amatrix}</mrow>
      </md>
      According to <xref ref="th_invertibility_algorithm"/>, the computation shows that <m>A</m> is invertible and
      <me>
        A^{-1}=\begin{amatrix}[rrr]-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{amatrix}
      </me>.
      Next, representing our row operations as elementary matrices, we see that
      <me>
        E_5E_4E_3E_2E_1A=I
      </me>,
      where
        <md>
          <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
            -1\amp 1\amp 0\\
            0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
            0\amp 1\amp 0\\
            1\amp 0\amp 1\end{amatrix}
          </mrow>
          <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
            0\amp 0\amp 1\\
            0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
            0\amp 1\amp -3\\
            0\amp 0\amp 1\end{amatrix}
          </mrow>
          <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
            0\amp 1\amp 0\\
            0\amp 0\amp 1\end{amatrix}</mrow>
        </md>.
        We conclude that
        <md>
          <mrow>A \amp =E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</mrow>
          <mrow> \amp= \begin{amatrix}[rrr] 1\amp 0\amp 0\\
            1\amp 1\amp 0\\
            0\amp 0\amp 1\end{amatrix}
            \begin{amatrix}[rrr]1\amp 0\amp 0\\
            0\amp 1\amp 0\\
            -1\amp 0\amp 1\end{amatrix}
            \begin{amatrix}[rrr]1\amp 0\amp 0\\
              0\amp 0\amp 1\\
              0\amp 1\amp 0 \end{amatrix}
              \begin{amatrix}[rrr] 1\amp 0\amp 0\\
              0\amp 1\amp 3\\
              0\amp 0\amp 1\end{amatrix}
              \begin{amatrix}[rrr]1\amp 2\amp 0\\
                0\amp 1\amp 0\\
                0\amp 0\amp 1\end{amatrix}
            </mrow>
        </md>.
    </p>
</paragraphs>
<paragraphs xml:id="ss_invertible_loose_ends">
  <title>Some theoretical loose ends</title>


<p>
The two inveribility algorithms above are nice examples of how a <em>theoretical</em> result like our <xref ref="th_invertibility" text='custom'> invertibility theorem </xref> can pay some serious <em>computational</em> dividends. Namely, thanks to the theory we have discovered a method of computing the inverse of a matrix that essentially boils down to row reduction.
</p>
<p>
   We finish this section with two further theoretical implications that tie up some loose ends: namely, we complete our understanding of the invertiblility of products of matrices, and we show that to test whether <m>B</m> is an inverse of <m>A</m> it suffices to show <em>just one</em> of the equalities <m>AB=I</m> and <m>BA=I</m>
</p>
<corollary xml:id="cor_left-right_inverse">
  <title>Left-inverse if and only if right-inverse</title>

  <statement>
    <p>
      Let <m>A</m> and <m>B</m> be <m>n\times n</m>. Then
      <ol>
        <li>
          <p>
            <m>BA=I_n\implies AB=I_n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>AB=I_n\implies BA=I_n</m>.
          </p>
        </li>
      </ol>
      In plain English: a matrix <m>B</m> is a <em>left-inverse</em> of <m>A</m> if and only if it is a <em>right-inverse</em> of <m>A</m>.
    </p>
  </statement>
  <proof>
    <p>
      It is enough to prove the first implication:
      the second then follows by exchanging the roles of <m>A</m> and <m>B</m>.
    </p>
    <p>
      Suppose <m>BA=I_n</m>.
      We first show that <m>A</m> is invertible.
      We have
      <md>
        <mrow> A\boldx=\boldzero\amp\implies BA\boldx=\boldzero </mrow>
        <mrow> \amp\implies I\boldx=\boldzero </mrow>
        <mrow>  \amp \implies \boldx=\boldzero </mrow>
      </md>.
      By Statement (3) of <xref ref="th_invertibility"/>, we conclude that <m>A</m> is invertible.
    </p>
    <p>
      Now that we know <m>A^{-1}</m> exists we have
      <md>
        <mrow>BA=I_n\amp \implies  BAA^{-1}=IA^{-1} </mrow>
        <mrow>  \amp \implies B=A^{-1}</mrow>
        <mrow>  \amp \implies AB=AA^{-1}=I_n </mrow>
      </md>.
    </p>
  </proof>
</corollary>

  <corollary xml:id="cor_inv_prod_eq">
    <title>Invertibility of product equivalence</title>

    <statement>
      <p>
        Let <m>A</m> and <m>B</m> be <m>n\times n</m>.
        Then <m>AB</m> is invertible if and only if <m>A</m> and <m>B</m> are both invertible.
      </p>
    </statement>
    <proof>
      <p>
        We know from <xref ref="th_invertible_prod"/>  that if <m>A</m> and <m>B</m> are invertible, then so is <m>AB</m>.
      </p>
      <p>
        For the forward implication,
        assume <m>AB</m> is invertible and let <m>C</m> be its inverse,
        so that <m>C(AB)=(AB)C=I_n</m>.
      </p>
      <p>
        We first prove <m>B</m> is invertible. We have
        <md>
          <mrow>B\boldx=\boldzero\amp \implies  AB\boldx=\boldzero </mrow>
          <mrow>\amp \implies  \boldx=\boldzero</mrow>
        </md>.
       The last implication uses Statement (3) of <xref ref="th_invertibility"/> and the fact that <m>AB</m> is invertible. We have shown that
       <me>
         B\boldx=\boldzero\implies \boldx=\boldzero
       </me>,
       and hence that <m>B</m> is invertible, using once again Statement (3) of <xref ref="th_invertibility"/>.
      </p>
      <p>
        Next we prove directly that <m>A</m> is invertible. Namely, we claim that <m>A^{-1}=BC</m>.
        Indeed, since <m>C</m> is the inverse of <m>AB</m>, we have <m>A(BC)=(AB)C=I</m>. Thus <m>BC</m> is a right-inverse of <m>A</m>. <xref ref="cor_left-right_inverse"/> now implies <m>(BC)A=I</m>, and hence that <m>A^{-1}=BC</m>, as claimed.
      </p>
      <p>
        This completes the proof that if <m>AB</m> is invertible, then <m>A</m> and <m>B</m> are invertible.
      </p>
    </proof>
  </corollary>


</paragraphs>
<xi:include href="./s_invertibility_theorem_ex.ptx"/>

</section>
