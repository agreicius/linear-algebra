<section xml:id="s_invertibility_theorem">
  <title>The invertibility theorem</title>
  <introduction>
    <p>
    We saw in <xref ref="ex_invertible_matrices"/> that verifying directly whether a matrix is invertible, using only <xref ref="d_invertible_matrix"/>, can be quite an involved task. The goal of this section is to make this less onerous by developing some equivalent methods of testing invertibilty. Our work culminates in <xref ref="th_invertibility"/> and <xref ref="th_invertibility_algorithm"/>, which draw connections between invertiblity, solutions to linear systems, and row echelon forms of a square matrix. Not surprisingly, our old friend Gaussian elimination emerges as the fundamental computational tool.
    </p>
  </introduction>
<paragraphs xml:id="ss_elementary_matrix">
  <title>Elementary matrices</title>
  <p>
  We begin with a treatment of <em>elementary matrices</em>, which serve as the basic building blocks for invertible matrices, and provide a crucial link between row reduction and matrix multiplication.
  </p>
  <definition xml:id="d_elementary_matrix">
    <idx><h>elementary matrix</h></idx>
    <idx><h>matrix</h><h>elementary</h></idx>
    <notation>
      <usage>\underset{cr_i}{E}</usage>
      <description>Scaling elementary matrix</description>
    </notation>
    <notation>
      <usage>\underset{r_i\leftrightarrow r_j}{E}</usage>
      <description>Row swap elementary matrix</description>
    </notation>
    <notation>
      <usage>\underset{r_i\rightarow r_i+c\,r_j}{E}</usage>
      <description>Row addition elementary matrix</description>
    </notation>
    <statement>
      <p>
        An <m>m\times m</m>  matrix <m>E</m> is
        <term>elementary</term>
        if multiplying any <m>m\times n</m> matrix <m>A</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
      </p>
      <p>
        We have different types of elementary matrices depending on the type of row operation they perform, and we denote these with an elaboration of our earlier row operation notation:
        <ul>
          <li>
            <p>
              A <term>scaling elementary matrix </term><m>\underset{c\,r_i}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{cr_i}{E}</m> scales the <m>i</m>-th row of <m>A</m> by <m>c</m>.
            </p>
          </li>
          <li>
            <p>
              A <term>row swap elementary matrix </term><m>\underset{r_i\leftrightarrow r_j}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{r_i\leftrightarrow r_j}{E}</m> swaps the <m>i</m>-th and <m>j</m>-th rows of <m>A</m>.
            </p>
          </li>
          <li>
            <p>
              A <term>row addition elementary matrix </term><m>\underset{r_i+c\,r_j}{E}</m> is a matrix such that multiplying a matrix <m>A</m> on the left by <m>\underset{r_i+c\,r_j}{E}</m> replaces the <m>i</m>-th row of <m>A</m> with <m>r_i+c\,r_j</m>.
            </p>
          </li>
        </ul>
      </p>
    </statement>
  </definition>
<p>
   Naturally, the row method of multiplication is the key to connecting a given row operation with a particular elementary matrix. <xref ref="th_elementary_matrices"/> shows that once you fix the dimension, an elementary matrix is uniquely defined by the row operation it performs.
</p>
<theorem xml:id="th_elementary_matrices">
  <title>Elementary matrix formulas</title>
  <statement>
    <p>
    Fix an integer <m>m\geq 2</m>. The three types of <m>n\times n</m> elementary matrices can be described as follows:
    </p>
    <ul>
      <li>
        <p>
          The <m>n\times n </m> scaling matrix <m>\underset{c\,r_i}{E}</m> is the result of scaling the <m>i</m>-th row of <m>I_m</m> by <m>c</m>: i.e.,
          <me>
            \underset{c\,r_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>

        </p>
      </li>
      <li>
        <p>
          The <m>n\times n </m> row swap matrix <m>\underset{r_i\leftrightarrow r_j}{E}</m> is the result of swapping the <m>i</m>-th and <m>j</m>-th rows of <m>I_m</m>: i.e.,
          <me>
            \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>

        </p>
      </li>
      <li>
        <p>
          The <m>n\times n </m> row addition matrix <m>\underset{r_i+c\,r_j}{E}</m> is the result of swapping the <m>i</m>-th row of <m>I_m</m> with the sum of its <m>i</m>-th row and <m>c</m> times its <m>j</m>-th row: i.e.,
          <me>
            \underset{r_i+c\,r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
      </li>
    </ul>
  </statement>
  <proof>
    <p>
      First we show that if <m>E</m> is one of the <m>m\times m</m> elementary matrices, then it must assume one of the forms described above. Indeed, since multiplying on the left by <m>E</m> performs a certain row operation, and since <m>E=E\,I_m</m>, we see that <m>E</m> itself is the result of performing this particular row operation on the <m>m\times m</m> identity matrix. Thus <m>E</m>  is one of the three types of matrices described above, obtained by performing an elementary row operation on <m>I_m</m>.
    </p>
    <p>
      Next, we must show that any of the <m>m\times m</m> matrices <m>E</m> described above is indeed elementary in the sense of <xref ref="d_elementary_matrix"/>: that is, we must show that multiplying any <m>m\times n</m> matrix <m>A</m> on the left by <m>E</m> performs the relevant row operation on <m>A</m>. This is now a direct consequence of <xref ref="th_row_method"/>.
    </p>
    <p> For example, take <m>E=\underset{c\,r_i}{E}</m>. For <m>j\ne i</m>, the <m>j</m>-th row of <m>E\,A</m> is given by the <m>j</m>-th row of <m>E</m> times <m>A</m>. Since the <m>j</m>-th row of <m>E</m> in this case has a one in the <m>j</m>-th entry and zeros elsewhere, the product of this row and <m>A</m> is just the <m>j</m>-th row of <m>A</m>. Similarly, the <m>i</m>-th row of <m>E\, A</m> in this case is <m>c</m> times the <m>i</m>-th row of <m>A</m>. Thus <m>E</m> leaves all the rows of <m>A</m> except for the <m>i</m>-th one, which is scaled by <m>c</m>.
    </p>

  </proof>
</theorem>
</paragraphs>
<paragraphs>
  <title>Row reduction as matrix multiplication</title>
  <p>
    Suppose we perform a sequence of row operations on a matrix <m>A</m>.
    Denote the <m>i</m>-th row operation <m>\rho_i</m>,
    and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
  </p>
  <p>
    Our sequence of operations <m>\rho_i</m> would produce the following sequence of matrices:
    <me>
      \begin{array}{c} A\\  \rho_1(A)\\  \rho_2(\rho_1(A))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A)))) \end{array}
    </me>
  </p>
  <p>
    Let <m>\rho_i</m> corresponds to the elementary matrix <m>E_i</m>.
    Then we represent this same sequence using matrix multiplication:
    <me>
      \begin{array}{c} A\\  E_1A\\  E_2E_1A\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A \end{array}
    </me>
  </p>
</paragraphs>
<paragraphs>
  <title>Elementary matrices are invertible</title>
  <p>
    We will use our row operation notation to denote the different types of elementary matrices:
    <me>
      \underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+c\,r_j}E
    </me>.
  </p>
  <theorem>
    <title>Elementary matrix theorem</title>
    <statement>
      <p>
        Fix <m>n</m>.
        All elementary matrices are invertible,
        and their inverses are elementary matrices.
        In fact:
        <md>
          <mrow>\underset{cr_i}{E}^{-1}\amp =\amp \underset{\frac{1}{c}r_i}{E}</mrow>
          <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp =\amp \underset{r_i\leftrightarrow r_j}{E}</mrow>
          <mrow>\underset{r_i+c\,r_j}E^{-1}\amp =\amp \underset{r_i-cr_j}E</mrow>
        </md>
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      These are formulas that you can easily check for each type of elementary matrix directly.
    </p>
  </proof>
</paragraphs>
<paragraphs>
  <title>Examples</title>
  <p>
    Fix <m>n=3</m>.
    Verify that the pairs below are indeed inverses.
    We have <m>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> and thus <m>\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> We have
    <m>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</m> and thus <m>\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp \frac{1}{2} \end{bmatrix}</m> We have
    <m>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m> and thus <m>\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m>
  </p>
</paragraphs>
<paragraphs>
  <title>Interlude on matrix equations</title>
  <p>
    We take a moment to make the following simple,
    somewhat overdo observation.
    Namely, we can represent a <em>system of linear equations</em>
    <me>
      \eqsys\tag{\(*\)}
    </me>
    as a <em>single matrix equation</em>
    <me>
      \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix} ,\tag{\(**\)}
    </me>
    or <m>A\boldx=\boldb</m>,where <m>A=[a_{ij}]</m>,
    <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
  </p>
  <p>
    Indeed if you expand out the left-hand side of <m>(**)</m> into an <m>m\times 1</m> column vector,
    using the definition of matrix multiplication,
    and then invoke the definition of matrix equality,
    then you obtain the linear system <m>(*)</m>.
  </p>
  <p>
    By the same token, an <m>n</m>-tuple
    <m>(s_1,s_2,\dots,
    s_n)</m> is a solution to the <em>system of equations</em>
    <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
    <em>matrix equation</em> <m>A\boldx=\boldb</m>.
  </p>
</paragraphs>
<paragraphs>
  <title>Interlude on matrix equations</title>
  <p>
    In particular, a homogeneous linear system
    <me>
      \homsys
    </me>
    can be represented as the single matrix equation
    <me>
      A\boldx=\underset{m\times 1}{\boldzero}
    </me>,
    where <m>A=[a_{ij}]</m> and <m>\boldx=[x_i]</m>;
    and <m>(s_1,s_2,\dots, s_n)</m> is a solution to the
    <em>homogenous system</em>
    if and only if its corresponding column vector
    <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
    <em>matrix equation</em>
    <m>A\boldx=\underset{m\times 1}{\boldzero}</m>.
  </p>
</paragraphs>
<p>
  We are now ready to state a major theorem about invertibility.
</p>
<theorem xml:id="th_invertibility">
  <title>Invertibility theorem</title>
  <statement>
    <p>
      Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
      The following statements are equivalent.
      <ol>
        <li>
          <p>
            <m>A</m> is invertible.
          </p>
        </li>
        <li>
          <p>
            <m>A\boldx=\boldzero</m> has a unique solution
            (the trivial one).
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is row equivalent to <m>I_n</m>,
            the <m>n\times n</m> identity matrix.
          </p>
        </li>
        <li>
          <p>
            <m>A</m> is a product of elementary matrices.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>
<proof>
  <p>
    Recall that two show two statements <m>P</m> and <m>Q</m> are equivalent,
    we must show two implications:
    <m>P\Rightarrow Q</m>, and <m>Q\Rightarrow P</m>.
  </p>
  <p>
    We must show this for each pair of statements above.
    That would be 6 different pairs and a total of 12 different implications to show!
    We ease our work load by showing the following
    <em>cycle</em> of implications:
    <me>
      (a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
    </me>
    and using the fact that implication is transitive!
    We will complete the proof in class.
  </p>
</proof>
<paragraphs>
  <title>Algorithm for inverses</title>
  <p>
    The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether <m>A</m> is invertible,
    and (2) computing <m>A^{-1}</m> if it exists.
  </p>
  <p>
    The algorithm:
    <ol>
      <li>
        <p>
          Row reduce <m>A</m> to row echelon form <m>U</m>
          (keeping track of the elementary matrices you use).
          The theorem tells us that <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading 1's.
        </p>
      </li>
      <li>
        <p>
          If this is so, we can keep row reducing to the identity matrix.
          In terms of matrices we have
          <me>
            E_rE_{r-1}\cdots E_1A=I_n
          </me>.
          This means <m>A^{-1}=E_rE_{r-1}\cdots E_1</m>.
        </p>
      </li>
      <li>
        <p>
          As an added bonus we also have <m>A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}</m>,
          expressing <m>A</m> as a product of elementary matrices.
        </p>
      </li>
    </ol>
  </p>
</paragraphs>
<paragraphs>
  <title>Inverses with augmented matrix</title>
  <p>
    It can be bothersome to keep track of those <m>E_i</m>,
    and the order of their multiplication.
    Instead, we use an augmented matrix method,
    beginning with the augmented matrix
    <me>
      \begin{bmatrix}A\amp I_n \end{bmatrix}
    </me>,
    and simply applying row reductions until <m>A</m> on the left becomes <m>I_n</m>,
    at which point <m>I_n</m> on the right has become <m>A^{-1}</m>.
  </p>
  <p>
    Why does this work?
    In terms of the <m>E_i</m>, we have the sequence:
    <me>
      \begin{array}{c} \begin{bmatrix}A\amp I_n \end{bmatrix} \begin{bmatrix}E_1A\amp E_1 \end{bmatrix} \begin{bmatrix}E_2E_1A\amp E_2E_1 \end{bmatrix} \\  \vdots \begin{bmatrix}E_rE_{r-1}\cdots E_1A\amp E_rE_{r-1}\cdots E_1 \end{bmatrix} =\begin{bmatrix}I_n\amp A^{-1} \end{bmatrix} \end{array}
    </me>
  </p>
  <p>
    Thus when we are done,
    the RHS of our augmented matrix magically becomes <m>A^{-1}</m>.
  </p>
</paragraphs>
<p>
  \alert{Example:} take <m>A=\begin{bmatrix}1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{bmatrix} </m>.
  <md>
    <mrow>\begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}  \amp \xrightarrow{r_2-r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
    <mrow>\amp \xrightarrow{r_3+r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{bmatrix}</mrow>
    <mrow>\alert{\text{ (3 leading 1's, thus invertible!) } }\amp \xrightarrow{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    <mrow>\amp \xrightarrow{r_2-3r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    <mrow>\amp \xrightarrow{r_1-2r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
  </md>
</p>
<p>
  We conclude that <m>A^{-1}=\begin{bmatrix}-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{bmatrix}</m>.
  On the next slide I show how <m>A^{-1}</m> and <m>A</m> can be written as products of elementary matrices.
</p>
<paragraphs>
  <title>Example (contd):</title>
  <p>
    in terms of elementary matrices we have
<me>
  E_5E_4E_3E_2E_1A=I
</me>,
  <me>\begin{amatrix}[rrr] 1\amp 0\amp 0\\
    -1\amp 1\amp 0\\
    0\amp 0\amp 1\end{amatrix}
  </me>
  <md>
    <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      -1\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
      0\amp 1\amp 0\\
      1\amp 0\amp 1\end{amatrix}
    </mrow>
    <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
      0\amp 0\amp 1\\
      0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      0\amp 1\amp -3\\
      0\amp 0\amp 1\end{amatrix}
    </mrow>
    <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
      0\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}</mrow>
  </md>
</p>
<p>
  Thus we have <m>A^{-1}=E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</m>.
  This shows how both of these matrices can be written as products of elementary matrices
  (recall that each <m>E_i^{-1}</m> is also elementary).
</p>
</paragraphs>
  <paragraphs>
    <title><xref ref="c_linear_systems"></xref>: <xref ref="ss_invertible"></xref>: expanded invertibility theorem</title>
    <p>
      We add two more equivalent statements of invertibility to our invertibility theorem (IT).
    </p>
    <theorem>
      <title>Expanded invertibility theorem</title>
      <statement>
        <p>
          Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
          The following statements are equivalent.
          <ol>
            <li>
              <p>
                <m>A</m> is invertible.
              </p>
            </li>
            <li>
              <p>
                <m>A\boldx=\boldzero</m> has a unique solution
                (the trivial one).
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is row equivalent to <m>I_n</m>,
                the <m>n\times n</m> identity matrix.
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is a product of elementary matrices.
              </p>
            </li>
            <li>
              <p>
                <m>A\boldx=\boldb</m> has a <em>unique</em>
                solution for every <m>n\times 1</m> column vector <m>\boldb</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A\boldx=\boldb</m> has a solution for every
                <m>n\times 1</m> column vector <m>\boldb</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        We already know statements (a)-(d) are equivalent.
        Adding (e) and (f) to this list is left as an exercise.
      </p>
    </proof>
  </paragraphs>
  <corollary xml:id="cor_inv_prod_eq">
    <title>Invertibility of product equivalence</title>

    <statement>
      <p>
        Let <m>A</m> and <m>B</m> be <m>n\times n</m>.
        Then <m>AB</m> is invertible if and only if <m>A</m> and <m>B</m> are both invertible.
      </p>
    </statement>
  </corollary>
  <proof>
    <p>
      We have already proved the
      <m>\Leftarrow</m> direction of this equivalnce.
    </p>
    <p>
      For the <m>\Rightarrow</m> direction,
      assume <m>AB</m> is invertible and let <m>C</m> be its inverse,
      so that <m>C(AB)=(AB)C=I_n</m>.
    </p>
    <p>
      We first prove <m>B</m> is invertible,
      using equivalent statement (b) of IT; that is we will prove the implication <m>B\boldx=\boldzero\Rightarrow \boldx=\boldzero</m>.
      <md>
        <mrow>B\boldx=\boldzero\amp \Rightarrow \amp  AB\boldx=\boldzero</mrow>
        <mrow>\amp \Rightarrow\amp  \boldx=\boldzero \ \text{ (since \(AB\) invertible) } </mrow>
      </md>.
    </p>
    <p>
      This proves <m>B</m> is invertible,
      and hence that <m>B^{-1}</m> exists.
    </p>
    <p>
      Next we prove <m>A</m> is invertible,
      by explicitly exhibiting an inverse:
      namely, <m>A^{-1}=BC</m>.
    </p>
    <p>
      Indeed we have <m>A(BC)=(AB)C=I_n</m>, from above.
      For the other direction we have
      <md>
        <mrow>C(AB)=I\amp \Leftrightarrow\amp  CA=B^{-1} \ \text{ (mult. both sides on right by \(B^{-1}\))}</mrow>
        <mrow>\amp \Leftrightarrow\amp  (BC)A=I_n \text{ (mult. both sides on left by \(B\)) } </mrow>
      </md>.
    </p>
    <p>
      This proves <m>A</m> and <m>B</m> are both invertible,
      and completes our proof.
    </p>
  </proof>
  <corollary>
    <statement>
      <p>
        Let <m>A</m> and <m>B</m> be <m>n\times n</m>.
        <ol>
          <li>
            <p>
              <m>BA=I_n\Rightarrow AB=I_n</m>.
            </p>
          </li>
          <li>
            <p>
              <m>AB=I_n\Rightarrow BA=I_n</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </corollary>
  <p>
    In other words,
    to show <m>B</m> is the inverse of <m>A</m> it is enough to show either that it is a left-inverse (<m>BA=I_n</m>),
    or a right-inverse
    (<m>AB=I_n</m>).
  </p>
  <proof>
    <p>
      It is enough to prove the first implication:
      the second then follows by exchanging the roles of <m>A</m> and <m>B</m>!
    </p>
    <p>
      Suppose <m>BA=I_n</m>.
      I will first show that <m>A</m> is invertible.
      Indeed, suppose <m>A\boldx=\boldzero</m>.
      Then <m>BA\boldx=\boldzero</m>.
      But <m>BA=I_n</m>,
      and <m>I\boldx=\boldzero</m> implies <m>\boldx=\boldzero</m>.
      It follows from statement (b) of IT that <m>A</m> is invertible.
    </p>
    <p>
      Now that we know <m>A^{-1}</m> exists we have
      <md>
        <mrow>BA=I_n\amp \Rightarrow \amp  B=A^{-1} \ \text{ (mult. both sides on right by \(A^{-1}\))}</mrow>
        <mrow>\amp \Rightarrow \amp AB=I_n \ \text{ (mult. both sides on left by \(A\)) }</mrow>
      </md>
    </p>
  </proof>
  <paragraphs>
  <title>When is a linear system consistent</title>
  <p>
    Consider a general linear equation
    <me>
      \scriptsize \eqsys
    </me>
    which we represent as a matrix equation
    <me>
      \underset{m\times n}{A}\cdot \underset{{n\times 1}}{\boldx}=\underset{m\times 1}{\boldb}
    </me>.
  </p>
  <paragraphs>
    <title>Question:</title>
    <p>
      what conditions on <m>A</m> and <m>\boldb</m> guarantee the system is consistent;
      equivalently, when we can solve the matrix equation above?
    </p>
  </paragraphs>
  <paragraphs>
    <title>Partial answer:</title>
    <p>
      the preceding discussion gives us a partial answer.
      If <m>A</m> happens to be an <em>invertible matrix</em>
      (in particular, it must be square!),
      then the equation is guaranteed to have a (unique) solution.
      We just solve for <m>\boldx</m> directly in this case:
      <m>\boldx=A^{-1}\boldb</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Otherwise:</title>
    <p>
      if <m>A</m> is not invertible,
      then we resort to the theory of Gaussian elimination to answer the question.
      Take the augmented matrix <m>[A \vert \boldb]</m>,
      row reduce to row echelon form
      <m>[U\vert \boldb']</m> and reason from there.
      The example that follows illustrates this.
    </p>
  </paragraphs>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      Find all <m>b_1,b_2,b_3</m> for which the following system is consistent:
      <md>
        \begin{linsys}{3} x_1\amp +\amp x_2\amp +\amp 2x_3\amp =\amp  b_1\\ x_1\amp  \amp  \amp +\amp x_3\amp =\amp b_2\\ 2x_1\amp +\amp x_2\amp +\amp 3x_3\amp =\amp b_3 \end{linsys}
      </md>
    </p>
    <p>
      \begin{bsolution} We consider the augmented matrix <m>[A \vert \boldb]</m> and row reduce
      <me>
        \begin{bmatrix}1\amp 1\amp 2\amp b_1\\ 1\amp 0\amp 1\amp b_2\\ 2\amp 1\amp 3\amp b_3 \end{bmatrix} \xrightarrow{\text{ row red. } } \begin{bmatrix}1\amp 1\amp 2\amp b_1\\ 0\amp 1\amp 1\amp b_2-b_1\\ 0\amp 0\amp 0\amp b_3-b_2-b_1 \end{bmatrix}
      </me>
    </p>
    <p>
      Note: since the <m>3\times 3</m> matrix on the left only 2 leading 1's, we know <m>A</m> is not invertible.
    </p>
    <p>
      But of course that doesn't mean that the given system is necessarily inconsistent!
    </p>
    <p>
      Indeed, the theory of Gaussian elimination tells us that the system will be consistent if and only if <m>b_3-b_2-b_1=0</m>.
    </p>
    <p>
      We conclude the original system is consistent if and only if <m>b_3=b_2+b_1</m>:
      equivalently, iff
      <me>
        \boldb=\begin{bmatrix}b_1\\b_2\\b_1+b_2 \end{bmatrix}
      </me>.
    </p>
    <p>
      \end{bsolution}
    </p>
  </paragraphs>
  <paragraphs>
    <title>Diagonal, triangular and symmetric matrices</title>
    <p>
      Lastly, we officially introduce some special families of square matrices,
      as well as a corresponding invertibility theorem.
      The proof of the latter will be done in class.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>A=[a_{ij}]</m> be <m>n\times n</m>.
          We say
          <ol type="i">
            <li>
              <p>
                <m>A</m> is <term>diagonal</term>
                if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>i\ne j</m>. (
                <q><m>A</m> is zero off the diagonal.</q>
                )
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is <term>upper triangular</term>
                if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>j>i</m>. (
                <q><m>A</m> is zero below the diagonal.</q>
                )
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is <term>lower triangular</term>
                if <m>a_{ij}=0</m> for all <m>(i,j)</m> with <m>i>j</m>. (
                <q><m>A</m> is zero above the diagonal.</q>
                )
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is <term>triangular</term>
                if <m>A</m> is upper triangular or lower triangular.
              </p>
            </li>
            <li>
              <p>
                <m>A</m> is <term>symmetric</term> if <m>A^T=A</m>.
                (Equivalently,
                <m>a_{ij}=a_{ji}</m> for all <m>1\leq i,j\leq n</m>.)
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </definition>
    <theorem>
      <title>Invertibility of triangular matrices</title>
      <statement>
        <p>
          Let <m>A=[a_{ij}]</m> be a triangular <m>n\times n</m> matrix.
          Then <m>A</m> is invertible if and only if
          <m>a_{ii}\ne 0</m> for all <m>1\leq i\leq n</m>.
        </p>
        <p>
          In other words,
          <m>A</m> is invertible if and only if the diagonal entries of <m>A</m> are all nonzero.
        </p>
      </statement>
    </theorem>
  </paragraphs>
</section>
