<section xml:id="s_span_independence">
  <title>Span and linear independence</title>
  <introduction>
    <p>
      There are many situations in mathematics where we want to describe an infinite set in a concise manner. We saw this at work already in <xref ref="s_solving"/>, where infinite sets of solutions to linear systems were neatly described with parametric expressions.
    </p>
    <p>
      A similar issue arises when describing vector spaces and their subspaces. As we know, any vector space is either the zero space or infinite (<xref ref="ex_vs_zero_or_infinite"/>). If we happen to be dealing with a subspace of <m>\R^n</m>, then there is the possibility of giving a parametric description; but how do we proceed when working in one of our more exotic vector spaces like <m>C^1(\R)</m>?
    </p>
    <p>
    As we will see in <xref ref="s_basis_dimension"/> the relevant linear algebraic tool for this purpose is the concept of a <em>basis</em>. Loosely speaking, a basis for a vector space <m>V</m> is a set of vectors that is large enough to <em>generate</em> the entire space, and small enough to contain <em>no redundancies</em>. What exactly we mean by <q>generate</q> is captured by the rigorous notion of <em>span</em>; and what we mean by <q>no redundancies</q> is captured by <em>linear independence</em>.
    </p>
  </introduction>

  <paragraphs>
    <title>Span</title>
    <p>
      Recall that a linear combination
      in a vector space <m>V</m> is a vector of the form
      <me>
        \boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_r\boldv_r
      </me>,
      where <m>c_i\in \R</m> are scalars.
      We use this notion to define the
      <em>span</em> of a set of vectors.
    </p>
    <definition>
      <title>Span</title>
      <idx><h>span</h><h>of a set of vectors</h></idx>
      <notation>
        <usage>\Span S</usage>
        <description>the span of <m>S</m></description>
      </notation>
        <statement>
          <p>
            Let <m>V</m> be a vector space, and let <m>S\subseteq V</m> be any subset of <m>V</m>. The <term>span of <m>S</m></term>, denoted <m>\Span S</m>, is the subset of <m>V</m> defined as follows:
            <ul>
              <li>
                <p>
                  If <m>S=\emptyset</m>, then <m>\Span S=\{\boldzero_V\}</m>.
                </p>
              </li>
              <li>
                <p>
                  Otherwise we define <m>\Span S</m> to be the set of all linear combinations of elements of <m>S</m>: <ie />,
                  <me>
                    \Span S=\{\boldv\in V\colon \boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_r\boldv_r \text{ for some } \boldv_i\in S \text{ and } c_i\in \R\}
                  </me>.
                </p>
              </li>
            </ul>
          </p>
        </statement>
      </definition>
          <remark xml:id="rm_span">
        <statement>
          <p>
          Let <m>S</m> be a subset of <m>V</m>. Some simple observations:
          <ol>
            <li>
              <p>
                The zero vector is always an element of <m>\Span S</m>.  Indeed, if <m>S=\emptyset</m>, then <m>\Span S=\{\boldzero\}</m> by definition. Otherwise, given any <m>\boldv\in S</m>, the linear combination <m>0\boldv=\boldzero</m> is an element of <m>\Span S</m>.
              </p>
            </li>
            <li>
                <p>
                  We have <m>S\subseteq \Span S</m>: <ie />, <m>\Span S</m> includes <m>S</m> itself. Indeed, given any <m>\boldv\in S</m>, the linear combination <m>1\boldv=\boldv</m> is an element of <m>\Span S</m>.
                </p>
            </li>

            <li>
              <p>
                If <m>S=\{\boldv\}</m> contains exactly one element, then <m>\Span S=\{c\boldv\colon c\in \R\}</m> is simply the set of all scalar multiples of <m>\boldv</m>.
              </p>
              <p>
                If <m>\boldv\ne \boldzero</m>, then we know that this set is infinite (<xref ref="ex_vs_zero_or_infinite"/>). Thus even when <m>S</m> is <em>finite</em>, <m>\Span S</m> will be <em>infinite</em>, as long as <m>S</m> contains nonzero vectors.
              </p>
            </li>
          </ol>
          </p>
        </statement>
      </remark>
    <example>
      <statement>
        <p>
        Let <m>V=\R^2</m>. For each <m>S</m>, identify <m>\Span S</m> as a familiar geometric object.
        </p>
        <ol>
          <li>
            <p>
              <m>S=\{ \}</m>.
            </p>
          </li>
          <li>
            <p>
              <m>S=\{(0,0)\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>S=\{\boldv\}</m>, <m>\boldv=(a,b)\ne (0,0)</m>
            </p>
          </li>
          <li>
            <p>
              <m>S=\{ (1,0), (0,1)\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>S=\{ (1,1), (2,2)\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>S=\{(1,1),(1,2)\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>S=\R^2</m>
            </p>
          </li>
        </ol>
      </statement>
      <solution>
        <p>
        <ol>
          <li>
            <p>
              <m>\Span S=\{\boldzero\}</m>, the set containing just the origin, by definition.
            </p>
          </li>
          <li>
            <p>
              <m>\Span S</m> is the set of all scalar multiples of <m>(0,0)</m>. Thus <m>\Span S=\{\boldzero\}</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\Span S</m> is the set of all scalar multiples of the nonzero vector <m>(a,b)</m>. Geometrically, this is the line that passes through the the origin and the point <m>(a,b)</m>.
            </p>
          </li>
          <li>
            <p>
              By definition
              <me>
                S=\{a(1,0)+b(0,1)\colon a,b\in \R\}=\{(a,b)\colon a,b\in\R\}
              </me>.
              Thus <m>S=\R^2</m>, the entire <m>xy</m>-plane.
            </p>
          </li>
          <li>
            <p>
              By definition
              <me>
                S=\{a(1,1)+b(2,2)\colon a,b\in \R\}=\{(a+2b,a+2b)\colon a,b\in\R\}
              </me>.
              It is easy to see that <m>S=\{(c,c)\colon t\in \R\}</m>, the line with equation <m>y=x</m>. Note that in this case we have
              <me>
                S=\Span\{(1,1), (2,2)\}=\Span \{(1,1)\}
              </me>,
              and thus that the vector <m>(2,2)</m> is in some sense redundant.
            </p>
          </li>
          <li>
            <p>
              By definition
              <me>
                S=\{a(1,1)+b(1,2)\colon a,b\in \R\}=\{(a+b,a+2b)\colon a,b\in\R\}
              </me>.
              Claim: <m>\Span S=\R^2</m>. Proving the claim amounts to showing that for all <m>(c,d)\in \R^2</m> there exist <m>a,b\in \R</m> such that
              <me>
                \begin{array}[ccccc]
                  a \amp +\amp b \amp =\amp c\\
                  a \amp +\amp 2b \amp =\amp d
                \end{array}[ccccc]
              </me>.
                Equivalently, we must show that the system corresponding to
                <me>
                  \begin{amatrix}[rr|r] 1\amp 1 \amp c\\ 1\amp 2 \amp d \end{amatrix}
                </me>
                has a solution for <em>any</em> choice of <m>c,d\in \R</m>. The usual Gaussian elimination procedure can be applied to this end.
              </p>
            </li>
            <li>
              <p>
                By <xref ref="rm_span"/>, we have <m>S\subseteq \Span S</m>. Thus <m>\R^2\subseteq \Span \R^2</m>. Since <m>\Span \R^2\subseteq \R^2</m> by definition, we conclude that <m>\Span S=\R^2</m>.
              </p>
            </li>
        </ol>
      </p>
      </solution>
    </example>
    <p>
      You may have noticed that each span computation in the previous example produced a subspace of <m>\R^2</m>. This is no accident!
    </p>
    <theorem>
      <statement>
        <p>
          Let <m>S</m> be a subset of the vector space <m>V</m>.
          <ol>
            <li>
              <p>
               The set <m>\Span S</m> is a subspace of <m>V</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>W</m> is any subspace containing <m>S</m>, then <m>\Span S\subseteq W</m>.
              </p>
            </li>
          </ol>
          Taken together, (1) and (2) imply that <m>\Span S</m> is the <em>smallest subspace of <m>V</m> containing <m>S</m></em>.
        </p>
      </statement>
      <proof>
        <p>
          We prove each statement separately.
        </p>
        <case>
         <title>Statement (1)</title>
        <p>
        To show <m>\Span S</m> is a subspace, we use the two-step technique.
        <ol>
          <li>
            <p>
              By <xref ref="rm_span"/> we know that <m>\boldzero\in \Span S </m>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>\boldv, \boldw\in S</m>. By definition we have
              <md>
                <mrow>\boldv \amp =c_1\boldv_1+c_2\boldv_2+\cdots +c_r\boldv_r \amp \boldw \amp = c_{r+1}\boldv_{r+1}+c_{r+2}\boldv_{r+2}+\cdots +c_{r+s}\boldv_{r+s}</mrow>
              </md>
              for some vectors <m>\boldv_1, \boldv_2, \dots, \boldv_{r+s}\in S</m> and scalars <m>c_1,c_2,\dots, c_{r+s}</m>. Then for any <m>c,d\in \R</m> we have
              <me>
              c\boldv+d\boldw=cc_1\boldv_1+cc_2\boldv_2+\cdots +cc_r\boldv_r+dc_{r+1}\boldv_{r+1}+dc_{r+2}\boldv_{r+2}+\cdots +dc_{r+s}\boldv_{r+s}
              </me>,
              which is clearly a linear combination of elements of <m>S</m>. Thus <m>c\boldv+d\boldw\in \Span S</m>, as desired.
            </p>
          </li>
        </ol>
        </p>
        </case>
        <case>
         <title>Statement (2)</title>
        <p>
        Let <m>W\subseteq V</m> be a subspace that contains all elements of <m>S</m>. Since <m>W</m> is closed under arbitrary linear combinations, it must contain any linear combination of elements of <m>S</m>, and thus <m>\Span S\subseteq W</m>.
        </p>
        </case>
      </proof>

    </theorem>
    </paragraphs>

    <paragraphs>
      <title>More terminology:</title>
      <p>
        in the spirit of the last theorem,
        given a set of vectors <m>S=\{\boldv_1,\dots, \boldv_r\}</m>,
        we call <m>W=\Span(S)</m> the subspace of <m>V</m> <term>generated by</term>
        the vectors <m>\boldv_i</m>.
        Similarly, given a subspace <m>W</m>,
        a set <m>S=\{\boldv_1,\dots, \boldv_r\}</m> for which <m>W=\Span(S)</m> is called a
        <term>spanning set</term> of <m>W</m>.
      </p>
    </paragraphs>
    <paragraphs>
    <title>Examples</title>
    <p>
      \alert{<m>M_{mn}</m>}. Define <m>E_{ij}</m> to be the matrix whose <m>(i,j)</m>-th entry is 1, and whose every other entry is 0.
      Then the set
      <me>
        \{ E_{ij}\colon 1\leq i\leq m, 1\leq j\leq n\}
      </me>
      is a spanning set for <m>M_{mn}</m>.
    </p>
    <paragraphs>
      <title><m>\R^n</m></title>
      <p>
        In a similar vein,
        define <m>\bolde_i</m> to be the <m>n</m>-tuple whose <m>i</m>-th entry is 1, and whose every other entry is 0.
      </p>
    </paragraphs>
    <p>
      Then <m>\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m> is a spanning set for <m>\R^n</m>.
    </p>
    <paragraphs>
      <title><m>P</m> and <m>P_n</m></title>
      <p>
        The set <m>\{1, x, x^2, \dots, \}</m> is a spanning set for <m>P</m>.
        The set <m>\{1, x, x^2, \dots,
        x^n\}</m> is a spanning set for <m>P_n</m>.
      </p>
    </paragraphs>
    <paragraphs>
      <title><m>\R^\infty</m></title>
      <p>
        As above we can define <m>\bolde_i\in \R^\infty</m> to be the infinite sequence whose <m>i</m>-th entry is 1, and whose every other entry is <m>0</m>.
      </p>
    </paragraphs>
    <p>
      Note, however,
      that the set <m>\{\bolde_1, \bolde_2, \bolde_3, \dots\}</m> is <em>not</em>
      a spanning set for <m>\R^\infty</m>.
    </p>
    <p>
      Indeed, the sequence <m>(1,1,1,\dots)</m> is not a (finite!) linear combination of the <m>\bolde_i</m>.
    </p>
    <p>
      \alert{<m>\R_{>0}</m>} Any
      <m>a\ne 1\in \R_{>0}</m> forms a spanning set for <m>\R_{>0}</m> as a vector space.
      This is because scalar multiplication by <m>r</m> in <m>\R_{>0}</m> is defined as exponentiation.
      Thus <m>\Span(\{a\})=\{a^r\colon r\in\R\}=\R_{>0}</m>.
      The last equality holds since the exponential function
      <m>f(x)=a^x</m> has image all positive reals for any base <m>a\ne 1</m>.
    </p>
    </paragraphs>
    <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=P_2</m> and let <m>S=\{p_1, p_2\}</m>,
      where <m>p_1(x)=x^2-1</m> and <m>p_2(x)=x^2-x</m>.
      Show that <m>W=\Span(S)</m> is the subspace of all polynomials
      <m>p(x)=a_2x^2+a_1x+a_0</m> for which <m>p(1)=0</m>.
      That is:
      <me>
        \Span(S)=\{p(x)\in P_2\colon p(1)=0\}
      </me>
    </p>
    <proof>
      <p>
        We wish to prove a <em>set equality</em>.
        We do so by showing the <m>\subseteq</m> and
        <m>\supseteq</m> relations separately. (See my proof technique guide!)
      </p>
      <paragraphs>
        <title><m>\subseteq</m></title>
        <p>
          Note first that <m>p_1(1)=p_2(1)=0</m>.
          Given an element <m>q(x)\in \Span\left(\{p_1(x), p_2(x)\}\right)</m>,
          we have <m>q(x)=ap_1(x)+bp_2(x)</m> for some <m>a, b\in \R</m>.
          But then <m>q(1)=ap_1(1)+bp_2(1)=0+0=0</m>.
          Thus <m>q(x)\in \{p(x)\colon p(1)=0\}</m>.
        </p>
      </paragraphs>
      <paragraphs>
        <title><m>\supseteq</m></title>
        <p>
          Now take <m>p(x)=a_2x^2+a_1x+a_0\in \{p(x)\in P_2\colon p(1)=0\}</m>.
          We must find <m>a, b\in \R</m> such that <m>p(x)=ap_1+bp_2</m>.
        </p>
      </paragraphs>
      <p>
        Since <m>p(1)=0</m>, we have <m>a_2+a_1+a_0=0</m>.
        I claim <m>p(x)=(-a_0)p_1+(-a_1)p_2</m>.
        Indeed we have
        <me>
          -a_0p_1-a_1p_2=(-a_0-a_1)x^2+a_1x+a_0=a_2x^2+a_1x+a_0
        </me>,
        since <m>a_2+a_1+a_0=0</m>.
        This shows that <m>p(x)\in\Span\left(\{p_1, p_2\}\right)</m>,
        as desired.
      </p>
    </proof>
    </paragraphs>
    <paragraphs>
      <title>Example</title>
      <p>
        Let <m>V=P_2</m> and let <m>S=\{p_1, p_2, p_3\}</m> where
        <me>
          p_1(x)=x^2+x+1, p_2(x)=x^2+x, p_3(x)=x^2+1
        </me>.
      </p>
      <p>
        Show that <m>\Span(S)=P_2</m>.
      </p>
      <proof>
        <p>
          Again, we are tasked with showing a <em>set equality</em>.
          It is clear that <m>\Span(S)=\{rp_1+sp_2+tp_3\colon r,s,t\in\R\}\subseteq P_2</m>.
          The harder direction is showing <m>P_2\subseteq \Span(S)</m>:
          i.e., given <em>any</em> <m>p(x)=a_2x^2+a_1x+a_0</m> we must show there are
          <m>r,s,t\in\R</m> such that <m>p(x)=rp_1+sp_2+tp_3</m>.
        </p>
        <p>
          We do so by setting up a system of equations.
          Combining like terms and equating coefficients in the polynomial expression
          <m>p(x)=rp_1+sp_2+tp_3</m> yields the linear system
          <md>
            \begin{linsys}{3} r\amp +\amp s\amp +\amp t\amp =\amp a_2\\ r\amp +\amp s\amp \amp \amp =\amp a_1\\ r\amp \amp \amp +\amp t\amp =\amp a_0 \end{linsys}
          </md>
        </p>
        <p>
          GE shows that the system has a solution for <em>any</em>
          choice of <m>a_2, a_1, a_0</m>:
          namely, <m>r=-a_2+a_1+a_0</m>,
          <m>s=a_2-a_0</m>, <m>t=a_2-a_1</m>.
          Thus given any <m>p(x)=a_2x^2+a_1x+a_0</m>,
          we can find <m>r,s,t</m> such that <m>p=rp_1+sp_2+tp_3</m>,
          showing <m>P_2\subseteq \Span(S)</m>.
        </p>
      </proof>
    </paragraphs>
  <paragraphs>
    <title><xref ref="c_vectorspace"></xref>. <xref ref="ss_independence"></xref>: linear independence</title>
    <definition>
      <statement>
        <p>
          Let <m>V</m> be a vector space.
          A set <m>S=\{\boldv_1,\dots ,\boldv_r\}</m> of elements of <m>V</m> is
          <term>linear independent</term> if
          <me>
            k_1\boldv_1+k_2\boldv_2+\cdots k_r\boldv_r=\boldzero \Rightarrow k_i=0 \text{ for all \(i\) }
          </me>.
        </p>
        <p>
          In other words, the only linear combination of the
          <m>\boldv_i</m> yielding <m>\boldzero</m> is the
          <term>trivial linear combination</term>
          we get by setting <m>k_i=0</m> for all <m>i</m>.
        </p>
        <p>
          The set <m>S</m> is <term>linearly dependent</term>
          if it is not linearly independent;
          i.e., if we can find a nontrivial linear combination of the
          <m>\boldv_i</m> that yields <m>\boldzero</m>.
        </p>
      </statement>
    </definition>
    <theorem>
      <statement>
        <p>
          The set <m>S</m> is linearly independent if and only if no element
          <m>\boldv_i</m> can be written as a linear combination of the other <m>\boldv_j</m>.
          Similarly, <m>S</m> is linearly dependent if and only if one of the elements
          <m>\boldv_i</m> can be written as a linear combination of the remaining <m>\boldv_j</m>.
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <paragraphs>
  <title>Example in $P_n$</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\{x^2+x-2, 2x^2+1, x^2-x\}\subset P_2</m>.
      Decide whether <m>S</m> is linearly independent.
    </p>
  </paragraphs>
  <proof>
    <p>
      First observe that <m>\boldzero=0x^2+0x+0</m>, the zero polynomial.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em> combination
      <md>
        <mrow>a(x^2+x-2)+b(2x^2+1)+c(x^2-x)\amp =\amp 0x^2+0x+0</mrow>
        <mrow>(a+2b+c)x^2+(a-c)x+(-2a+b)\amp =\amp 0x^2+0x+0</mrow>
      </md>
    </p>
    <p>
      Equating like terms gives us the linear system
      <md>
        \begin{linsys}{3} a\amp +\amp 2b\amp +\amp c\amp =\amp 0\\ a\amp \amp \amp -\amp c\amp =\amp 0\\ -2a\amp +\amp b \amp \amp  \amp =\amp 0 \end{linsys}
      </md>
    </p>
    <p>
      Row reduction shows this system only has the trivial solution <m>a=b=c=0</m>.
      Thus <m>S</m> is <em>linearly independent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Example in $M_{mn}$</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\left\{ A_1=\begin{bmatrix}3\amp 1\\ 2\amp -3 \end{bmatrix} , A_2= \begin{bmatrix}0\amp 4\\ 2\amp 0 \end{bmatrix} , A_3=\begin{bmatrix}-2\amp -2\\ -2\amp 2 \end{bmatrix} \right\}\subset M_{22}</m>.
      Decide whether <m>S</m> is linearly independent.
    </p>
  </paragraphs>
  <proof>
    <p>
      First observe that <m>\boldzero=\begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix}</m>,
      the zero matrix.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em> combination
      <md>
        <mrow>a\begin{bmatrix}3\amp 1\\ 2\amp -3 \end{bmatrix} +b\begin{bmatrix}0\amp 4\\ 2\amp 0 \end{bmatrix} +c\begin{bmatrix}-2\amp -2\\ -2\amp 2 \end{bmatrix} \amp =\amp \begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix} \begin{bmatrix}3a-2c\amp a+4b-2c\\ 2a+2b-2c\amp -3a+2c \end{bmatrix}  \amp =\amp \begin{bmatrix}0\amp 0\\0\amp 0 \end{bmatrix}</mrow>
      </md>
    </p>
    <p>
      Equating like terms gives us the linear system
      <md>
        \begin{linsys}{3} 3a\amp \amp \amp -\amp 2c\amp =\amp 0\\ a\amp +\amp 4b\amp -\amp 2c\amp =\amp 0\\ 2a\amp +\amp 2b \amp -\amp 2c \amp =\amp 0\\ -3a\amp \amp \amp +\amp 2c\amp =\amp 0 \end{linsys}
      </md>
    </p>
    <p>
      Row reduction shows this system has a free variable,
      and hence a nontrivial solution<ndash/>in fact infinitely many!
      One example is <m>a=2</m>, <m>b=-1</m>, <m>c=3</m>.
      Thus <m>S</m> is <em>linearly dependent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Example in function space</title>
  <paragraphs>
    <title>General procedure:</title>
    <p>
      all questions of linear dependence can be boiled down to deciding whether a certain linear system can be solved or not!
      Let <m>S=\{f(x)=x, g(x)=\cos(x),
      h(x)=\sin(x)\}\subseteq C^\infty(\R)</m>.
    </p>
  </paragraphs>
  <p>
    Decide whether <m>S</m> is linearly independent.
  </p>
  <proof>
    <p>
      First observe that <m>\boldzero</m> is the zero function:
      the function that assigns 0 to all inputs <m>x</m>.
    </p>
    <p>
      We ask whether there is a <em>nontrivial</em>
      combination <m>af+bg+ch=\boldzero</m>.
    </p>
    <p>
      <em>Key observation:</em> the equality above is an equality of <em>functions</em>.
      Thus this is true if and only if
      <m>af(x)+bg(x)+ch(x)=0</m> \alert{for all <m>x</m>}.
    </p>
    <p>
      To get a linear system,
      we evaluate the above at a few clever choices of <m>x</m>:
      <md>
        <mrow>x=0\amp :\amp  a(0)+b\cos(0)+c\sin(0)=0\Rightarrow 0+b+0=0\Rightarrow b=0</mrow>
        <mrow>x=\pi\amp :\amp  a(\pi)+c\sin(\pi)=0\Rightarrow \pi a+0=0\Rightarrow a=0</mrow>
      </md>
    </p>
    <p>
      Having shown <m>a=b=0</m>,
      we are left with the equation <m>c\sin(x)=0</m> for all <m>x</m>,
      which is true iff <m>c=0</m>.
    </p>
    <p>
      Thus the only linear combination yielding <m>\boldzero</m> is <m>a=b=c=0</m>,
      the trivial one, showing <m>S</m> is
      <em>linearly independent</em>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Linear independence in function spaces</title>
  <p>
    As the last example illustrates,
    a set of functions <m>S=\{f_1,f_2, \dots ,f_r\}</m> is linearly independent if
  </p>
  <p>
    <m>k_1f_1+k_2f_2+\cdots k_rf_r=\boldzero</m> implies <m>k_i=0</m> for all <m>i</m>.
  </p>
  <p>
    Recall <m>\boldzero</m> here stands for the <em>zero function</em>.
    Thus we must treat such a linear combination as a function identity!
    In other words to say
    <me>
      k_1f_1+k_2f_2+\cdots k_rf_r=\boldzero
    </me>
    is simply to say that
    <me>
      k_1f_1(x)+k_2f_2(x)+\cdots k_rf_r(x)=0
    </me>
  </p>
  <paragraphs>
    <title>for all <m>x</m></title>
    <p>
      in the given domain.
    </p>
  </paragraphs>
  <p>
    The beauty of this
    <q>for all <m>x</m></q>
    is that by picking say <m>r</m> actual examples of <m>x</m> and evaluating above,
    we generate <m>r</m> linear equations in the unknowns <m>k_i</m>.
  </p>
  <p>
    If this system has no nontrivial solutions,
    then we know the functions are independent.
  </p>
  <p>
    However, if this system DOES have a nontrivial solution we CANNOT conclude the functions are linearly dependent.
    Why?
    We would have shown the identity above holds only for these <m>r</m> choices of <m>x</m>,
    but not necessarily <em>all</em> <m>x</m>!
  </p>
  </paragraphs>
  <paragraphs>
  <title>$C^\infty(a,b)$ and the Wronskian</title>
  <p>
    Let's consider this observation in the special example of
    <em>differentiable</em> functions.
  </p>
  <definition>
    <statement>
      <p>
        Suppose <m>f_1,f_2,\dots f_n</m> are each <m>(n-1)</m>-differentiable functions on <m>(a,b)</m>.
        We define the Wronskian of the <m>f_i</m> as the function
        <md>
          W(x)=\begin{vmatrix}f_1(x)\amp f_2(x)\amp \cdots \amp f_n(x)\\ f_1'(x)\amp f_2'(x)\amp \cdots \amp f_n'(x)\\ \vdots\amp \cdots \amp  \amp \vdots \\ f_1^{(n-1)}(x)\amp f_2^{(n-1)}(x)\amp \cdots \amp f_n^{(n-1)}(x) \end{vmatrix}.
        </md>
      </p>
    </statement>
  </definition>
  <theorem>
    <title>Wronskian</title>
    <statement>
      <p>
        Let <m>V=C^{\infty}(X)</m>, where <m>X</m> is a fixed interval
        (usually <m>X=\R</m>),
        and let <m>S=\{f_1,f_2,\dots ,f_n\}</m> be a set of elements of <m>V</m>.
        Let <m>W(x)</m> be the Wronskian of the <m>f_i</m>.
        Then
        <me>
          W\ne\boldzero\Rightarrow \text{ \(S\) is linearly indendent }
        </me>.
      </p>
    </statement>
  </theorem>
  <paragraphs>
    <title>Comments</title>
  </paragraphs>
  <p>
    (1) Again <m>\boldzero</m> here is the <em>zero function</em>.
    So <m>W\ne\boldzero</m> means there is an <m>x</m> in <m>(a,b)</m> such that <m>W(x)\ne 0</m>.
  </p>
  <p>
    (2) This implication only goes one way!! In other words,
    <m>W(x)=0</m> for all <m>x</m> does not imply <m>S</m> is dependent!!
  </p>
  </paragraphs>
</section>
