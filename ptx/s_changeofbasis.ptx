<section xml:id="ss_changeofbasis">
  <title>Change of basis</title>
  <paragraphs>
    <title>Change of basis</title>
    <p>
      We have seen how the coordinate vector map
      <m>[\hspace{10pt}]_B</m> and matrix representations
      <m>[T]_B^{B'}</m> are two invaluable computational tools for dealing with abstract vector spaces.
    </p>
    <p>
      As the notation indicates,
      both of these operations depend essentially on your
      <em>choice of basis or bases</em>.
      This gives rise to the following questions:
      <ol>
        <li>
          <p>
            Given <m>V</m> and two choices of basis, <m>B</m> and <m>B'</m>,
            what is the relation between
            <m>[\boldv]_B</m> and <m>[\boldv]_{B'}</m>?
          </p>
        </li>
        <li>
          <p>
            GIven <m>T\colon V\rightarrow W</m> and two choices of pairs of bases,
            <m>(B, B')</m> and <m>(B'', B''')</m>,
            what is the relation between
            <m>[T]_{B}^{B'}</m> and <m>[T]_{B''}^{B'''}</m>?
          </p>
        </li>
      </ol>
    </p>
    <p>
      We will tackle both questions in turn.
      Both answers rely on something called a
      <em>change of basis matrix</em>
      <m>\underset{B\rightarrow B'}{P}</m>.
    </p>
    <p>
      For question (2), we will look at the slightly less general situation where <m>T\colon V\rightarrow V</m>:
      i.e., the situation where <m>W=V</m>,
      and we choose the same basis for the domain and codomain.
      I leave the full general case as an exercise.
    </p>
  </paragraphs>
  <paragraphs>
  <title>Change of basis matrix</title>
  <paragraphs>
    <title>Theorem/Definition</title>
    <p>
      Given a vector space <m>V</m> of dimension <m>n</m>,
      and two different bases <m>B</m> and <m>B'</m>,
      there is a <em>unique</em> <m>n\times n</m> matrix
      <m>\underset{B\rightarrow B'}{P}</m> satisfying the following defining property:
      <me>
        \underset{B\rightarrow B'}{P}[\boldv]_B=[\boldv]_{B'}
      </me>.
    </p>
    <p>
      The matrix <m>\underset{B\rightarrow B'}{P}</m> is called the
      <term>change of basis matrix
      (or transition matrix)
      from <m>B</m> to <m>B'</m></term>.
    </p>
  </paragraphs>
  <proof>
    <p>
      Let <m>B=\{\boldv_1,\dots, \boldv_n\}</m> and let <m>B'=\{\boldw_1,\dots,\boldw_n\}</m>.
    </p>
    <p>
      We build <m>\underset{B\rightarrow B'}{P}</m> \alert{column by column}: specifically,
      <m>\underset{B\rightarrow B'}{P}</m> is the matrix whose <m>j</m>-th column is
      <me>
        \boldp_j=[\boldv_j]_{B'};
      </me>
      i.e., to compute the <m>j</m>-th column of
      <m>\underset{B\rightarrow B'}{P}</m> you must find the <m>B'</m>-coordinates of the <m>j</m>-th basis element of <m>B</m>.
    </p>
    <p>
      Thus defined,
      one can show using the column method that <m>\underset{B\rightarrow B'}{P}[\boldv_j]_{B}=[\boldv_j]_{B'}</m>.
      One then argues that if the condition is true for each basis element <m>\boldv_j</m>,
      then it must be true for all <m>\boldv</m>. (Ask your professor for details!)
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      For any vector space <m>V</m> and basis <m>B</m>,
      we have <m>\underset{B\rightarrow B}{P}=I</m>.
    </p>
  </paragraphs>
  <p>
    Indeed for any vector <m>\boldv</m>,
    we have <m>I[\boldv]_B=[\boldv]_B</m>.
    Thus <m>I</m> satisfies the defining property of <m>\underset{B\rightarrow B}{P}</m>.
    By uniqueness,
    it follows that <m>I=\underset{B\rightarrow B}{P}</m> !!
  </p>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=\R^2</m>.
      Compute <m>\underset{B\rightarrow B'}{P}</m> where
      <m>B=\{\boldv_1=(1,1),\boldv_2=(1,-1)\}</m> and <m>B'=\{(1,2),(2,1)\}</m>.
    </p>
  </paragraphs>
  <p>
    Test that the matrix converts correctly using the vector
    <m>\boldv=1(1,1)+3(1,-1)=(4,-2)</m>. \begin{bsolution} The recipe tells us that
    <md>
      <mrow>\underset{B\rightarrow B'}{P}\amp =\amp \begin{bmatrix}\vert \amp \vert \\ \hspace{7pt}[\boldv_1]_{B'} \amp \hspace{7pt}[\boldv_2]_{B'}\\ \vert \amp \vert \end{bmatrix}</mrow>
      <mrow>\amp =\amp \begin{bmatrix}1/3\amp -1\\ 1/3\amp 1 \end{bmatrix}  \hspace{8pt}\text{ (after some computation) }</mrow>
    </md>
  </p>
  <p>
    For <m>\boldv=1(1,1)+3(1,-1)=(4,-2)</m>,
    we have <m>(\boldv)_B=(1,3)</m>.
    Thus we should have
    <me>
      [\boldv]_{B'}=\underset{B\rightarrow B'}{P}[\boldv]_B=\begin{bmatrix}1/3\amp -1\\ 1/3\amp 1 \end{bmatrix} \begin{bmatrix}1\\ 3 \end{bmatrix} =\begin{bmatrix}-8/3\\ 10/3 \end{bmatrix}
    </me>.
  </p>
  <p>
    Indeed, one easily verifies that <m>(4,-2)=-8/3(1,2)+10/3(2,1)</m>. \end{bsolution}
  </p>
  <paragraphs>
  <title>Example</title>
  <p>
    Take <m>V=P_2</m>,
    <m>B=\{1,x,x^2\}</m> and <m>B'=\{1,(x-2), (x-2)^2\}</m>.
    Compute <m>\underset{B\rightarrow B'}{P}</m>.
  </p>
  <p>
    Follow the recipe:
    let <m>\boldp_j</m> be the <m>j</m>-th column of <m>\underset{B\rightarrow B'}{P}</m>.
    We have (after some computation)
    <me>
      \boldp_1=[1]_{B'}=(1,0,0), \ \boldp_2=[x]_{B'}=(2,1,0), \ \boldp_3=[x^2]_{B'}=(4,4,1)
    </me>.
  </p>
  <p>
    Thus <m>\underset{B\rightarrow B'}{P}=\begin{bmatrix}1\amp 2\amp 4\\ 0\amp 1\amp 4\\ 0\amp 0\amp 1 \end{bmatrix}</m>
  </p>
  <p>
    Let's check with the test vector <m>p(x)=1+x+x^2</m>.
    We have <m>(p)_B=(1,1,1)</m>.
    Thus we should have <m>[p]_{B'}=\underset{B\rightarrow B'}{P}[p]_B=\begin{bmatrix}1\amp 2\amp 4\\ 0\amp 1\amp 4\\ 0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1 \\ 1 \\ 1 \end{bmatrix} =\begin{bmatrix}7\\ 5\\ 1 \end{bmatrix}</m>.
    Equivalently, this means that
    <m>p(x)=7+5(x-2)+(x-2)^2</m>, as one easily verifies.
  </p>
  <p>
    \framebreak
  </p>
  <paragraphs>
    <title>Cool fact</title>
    <p>
      We could have derived the last equality using the theory of Taylor series.
      Namely any polynomial can be
      <q>expanded around <m>x=a</m></q>
      as <m>p(x)=\sum_{i=0}^n\frac{p^{(i)}(a)}{a!}(x-a)^i</m>.
    </p>
  </paragraphs>
  <p>
    More generally, this means
    <me>
      (p(x))_{B'}=(p(a), p'(a), p''(a)/2, \dots , p^{(n)}(a)/n!)
    </me>
    where <m>B'=\{1,x-a, (x-a)^2,\dots , (x-a)^n\}</m>.
  </p>
  </paragraphs>
  <paragraphs>
    <title>Theorem</title>
    <p>
      Let <m>V</m> be a vector space of dimension <m>n</m>,
      and let <m>B</m> and <m>B'</m> be two different bases of <m>V</m>.
    </p>
  </paragraphs>
  <p>
    The change of basis matrix <m>\underset{B\rightarrow B'}{P}</m> is invertible.
    In fact we have
    <me>
      (\underset{B\rightarrow B'}{P})^{-1}=\underset{B'\rightarrow B}{P}
    </me>
  </p>
  <paragraphs>
    <title>Proof</title>
    <p>
      I will show that <m>\underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}=I_n</m>,
      proving that both matrices are invertible,
      and are in fact the inverses of one another.
      My proof will only make use of the defining property of change of basis matrices,
      and their uniqueness.
    </p>
  </paragraphs>
  <p>
    For any <m>\boldv</m> we compute:
    <md>
      <mrow>\underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}[\boldv]_B\amp =\amp \underset{B'\rightarrow B}{P}(\underset{B\rightarrow B'}{P}[\boldv]_B)</mrow>
      <mrow>\amp =\amp \underset{B'\rightarrow B}{P}[\boldv]_{B'} \hspace{5pt} \text{(prop. of \(\underset{B\rightarrow B'}{P}\))}</mrow>
      <mrow>\amp =\amp [\boldv]_{B} \hspace{25pt} \text{(prop. of \(\underset{B'\rightarrow B}{P}\))}</mrow>
    </md>
  </p>
  <p>
    Thus <m>\underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}[\boldv]_B=[\boldv]_B</m> for all <m>\boldv\in V</m>.
    But this means <m>\underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}</m> satisfies the defining property of <m>\underset{B\rightarrow B}{P}</m>.
  </p>
  <p>
    By <em>uniqueness</em> of the change of basis matrix, we must have
    <me>
      \underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}=\underset{B\rightarrow B}{P}
    </me>
  </p>
  <p>
    Lastly, in a previous example we showed that <m>P_{B\rightarrow B}=I_n</m>.
    Thus,
    <me>
      \underset{B'\rightarrow B}{P}\cdot\underset{B\rightarrow B'}{P}=\underset{B\rightarrow B}{P}=I_n
    </me>,
    as claimed.
  </p>
  <paragraphs>
  <title>Example: $V=\R^n$ and $B$ is standard</title>
  <p>
    Consider the simple example where <m>V=\R^n</m>,
    <m>B</m> is the standard basis,
    and <m>B'=\{\boldv_1,\dots,\boldv_n\}</m> is some nonstandard basis.
    I claim the matrix <m>P=\begin{bmatrix}\vert\amp \dots \amp \vert \\ \boldv_1\amp \cdots\amp \boldv_n\\ \vert\amp \dots \amp \vert \end{bmatrix}</m> whose columns are the elements of <m>B'</m> is the change of basis matrix <m>\underset{B'\rightarrow B} P</m>.
    This follows from our recipe since <m>\boldv_j=[\boldv_j]_B</m>. (Recall:
    when <m>B</m> is the standard basis
    <m>[(a_1,a_2,\dots, a_n)]_B=(a_1,a_2,\dots,
    a_n)</m> for all <m>(a_1,a_2,\dots, a_n)\in\R^n</m>. )
  </p>
  <p>
    Since <m>\underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}</m>,
    we see that in this special case we can compute
    <m>\underset{B'\rightarrow B}{P}</m> by placing the elements of <m>B'</m> as columns of a matrix,
    and then compute <m>\underset{B\rightarrow B'}{P}</m> by taking the inverse of this matrix!
  </p>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=\R^2</m>, <m>B</m> the standard basis for <m>\R^2</m>,
      and <m>B'=\{(1,\sqrt{3}),(-\sqrt{3},1)\}</m>.
    </p>
  </paragraphs>
  <p>
    Find <m>P_{B\rightarrow B'}</m>. \begin{bsolution} The recipe above tells us that <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix}</m> and hence that
    <me>
      \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\left(\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix} \right)^{-1}=\frac{1}{4}\begin{bmatrix}1\amp \sqrt{3}\\ -\sqrt{3}\amp 1 \end{bmatrix}
    </me>.
  </p>
  <p>
    \end{bsolution}
  </p>
  </paragraphs>
  <paragraphs>
  <title>Alternative technique when $V=\R^n$</title>
  <p>
    The following is another technique for computing <m>\underset{B\rightarrow B'}{P}</m> in the
    <em>special case</em> where <m>V=\R^n</m>.
  </p>
  <paragraphs>
    <title>1.</title>
    <p>
      Let <m>\widetilde{B}</m> and
      <m>\widetilde{B'}</m> be the matrices obtained by arranging the elements of <m>B</m>,
      respectively <m>B'</m>, as columns of a matirx.
      Build the augmented matrix
    </p>
  </paragraphs>
  <me>
    \begin{bmatrix}\widetilde{B'}\amp  \widetilde{B}
    \end{bmatrix}
  </me>
  <paragraphs>
    <title>2.</title>
    <p>
      Row reduce the augmented matrix until the left hand side is <m>I_n</m>:
    </p>
  </paragraphs>
  <me>
    \begin{bmatrix}I_n\amp  Q
    \end{bmatrix}
  </me>
  <p>
    Then <m>Q=\underset{B\rightarrow B'}{P}</m>.
  </p>
  <p>
    Why does this work?
  </p>
  <p>
    The row reduction process amounts to multiplying both matrices on the left by <m>(\widetilde{B'})^{-1}</m>:
    that is, the matrix <m>Q</m> is <m>(\widetilde{B'})^{-1}\widetilde{B}</m>.
    As we saw in the last example, however,
    letting <m>S</m> be the <em>standard basis</em> of <m>\R^n</m>, we have
    <me>
      (\widetilde{B'})^{-1}=\underset{S\rightarrow B'}{P} \text{ and }  \widetilde{B}=\underset{B\rightarrow S}{P}
    </me>
  </p>
  <p>
    Thus <m>Q=\underset{S \rightarrow B'}{P}\cdot \underset{B\rightarrow S}{P}=\underset{B\rightarrow B'}{P}</m>.
    (This last equality follows from one of the exercises.)
  </p>
  </paragraphs>
  <paragraphs>
    <title>Change of basis for transformations</title>
    <p>
      We now investigate how our choice of basis affects matrix representations of linear transformations.
      As mentioned above, we only consider the special case where
      <m>T\colon V\rightarrow V</m> and we are comparing matrix representations <m>[T]_B</m> and
      <m>[T]_{B'}</m> for two choices of basis for <m>V</m>.
    </p>
    <theorem xml:id="Th_changeofbasis">
      <title>Change of basis for transformations</title>
      <statement>
        <p>
          Let <m>V</m> be finite-dimensional,
          let <m>T\colon V\rightarrow V</m> be linear,
          and let <m>B</m> and <m>B'</m> be two bases for <m>V</m>.
        </p>
        <p>
          Then
          <md>
            <mrow>_{B'}\amp =\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}</mrow>
            <mrow>\amp =\left( \underset{B'\rightarrow B}{P}\right)^{-1}[T]_B\underset{B'\rightarrow B}{P}</mrow>
          </md>
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <paragraphs>
    <title>Pro-tip</title>
    <p>
      It is easy to get the various details of the change of basis formula wrong.
      Here is how I organize things in my mind.
      <ol>
        <li>
          <p>
            We wish to relate <m>[T]_{B'}</m> and <m>[T]_B</m> with an equation of the form <m>[T]_{B'}=*[T]_B*</m>,
            where the asterisks are to be replaced with change of basis matrices or their inverses.
            Think of the three matrices on the RHS as a sequence of three things done to coordinate vectors,
            reading from right to left.
          </p>
        </li>
        <li>
          <p>
            <m>[T]_{B'}</m> takes as inputs <m>B'</m>-coordinates of vectors,
            and outputs <m>B'</m>-coordinates.
            Thus the same should be true for <m>*[T]_B*</m>.
          </p>
        </li>
        <li>
          <p>
            Since <m>[T]_B</m> takes as inputs <m>B</m>-coordinates,
            we must <em>first</em> convert from <m>B'</m>-coordinates to <m>B</m>-coordinates.
            So we should have <m>[T]_{B'}=*[T]_B\underset{B'\rightarrow B}{P}</m>.
          </p>
        </li>
        <li>
          <p>
            Since <m>[T]_B</m> outputs <m>B</m>-coordinates,
            we need to then convert back to <m>B'</m>-coordinates.
            Thus <m>[T]_{B'}=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}</m>.
          </p>
        </li>
        <li>
          <p>
            If desired you may replace
            <m>\underset{B\rightarrow B'}{P}</m> with <m>\left( \underset{B'\rightarrow B}{P}\right)^{-1}</m>.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Proof of change of basis theorem</title>
    <p>
      { \alert{Theorem recap}. Let <m>V</m> be finite-dimensional,
      let <m>T\colon V\rightarrow V</m> be linear,
      let <m>B</m> and <m>B'</m> be two bases for <m>V</m>,
      and let <m>A=[T]_B</m>, <m>A'=[T]_{B'}</m>.
      Then <m>A=(\underset{B\rightarrow B'}{P})^{-1}A'\underset{B\rightarrow B'}{P}</m> and <m>A'=(\underset{B'\rightarrow B}{P})^{-1}A\underset{B'\rightarrow B}{P}</m> }
    </p>
    <p>
      First recall that <m>\underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^{-1}</m>.
      Using this fact,
      it is easy to see that the second equality follows from the first by multiplying on the left and right by an appropriate matrix and its inverse.
    </p>
    <p>
      We prove the first equality using the
      <em>uniqueness property</em> of <m>A=[T]_B</m>.
    </p>
    <p>
      That is, set <m>C=(\underset{B\rightarrow B'}{P})^{-1}A'\underset{B\rightarrow B'}{P}</m>.
    </p>
    <p>
      To show <m>C=A</m>, we need only show
      <m>[T(\boldv)]_B=C[\boldv]_B</m> for all <m>\boldv\in V</m>.
      Here goes:
      <md>
        <mrow>C[\boldv]_B\amp =\amp (\underset{B\rightarrow B'}{P})^{-1}A'\underset{B\rightarrow B'}{P}[\boldv]_B</mrow>
        <mrow>\amp =\amp \underset{B'\rightarrow B}{P}A'\underset{B\rightarrow B'}{P}[\boldv]_B\hspace{10pt} (\text{ since } \underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^{-1})</mrow>
        <mrow>\amp =\amp \underset{B'\rightarrow B}{P}A'[\boldv]_{B'} \hspace{10pt} \text{(prop. of \(\underset{B\rightarrow B'}{P}\))}</mrow>
        <mrow>\amp =\amp \underset{B'\rightarrow B}{P}[T(\boldv)]_{B'} \hspace{10pt} \text{(prop. of \(A'=[T]_{B'}\))}</mrow>
        <mrow>\amp =\amp [T(\boldv)]_{B}</mrow>
      </md>
    </p>
    <p>
      Amazing!
      We didn't even have to get our hands dirty.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>T\colon P_2\rightarrow P_2</m> be defined as <m>T(p(x))=p(x)+2p'(x)+xp''(x)</m>.
      <ol>
        <li>
          <p>
            Let <m>B=\{1, x, x^2\}</m>.
            Compute <m>[T]_B</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>B'=\{1+x+x^2, 1+x, 1+x^2\}</m>.
            Use the change of basis formula to compute <m>[T]_{B'}</m>.
          </p>
        </li>
      </ol>
    </p>
    <p>
      We easily compute <m>[T]_B=\begin{bmatrix}1\amp 2\amp 0\\ 0\amp 1\amp 4\\ 0\amp 0\amp 1 \end{bmatrix}</m> using our usual recipe.
    </p>
    <p>
      We can also easily compute <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 0\\ 1\amp 0\amp 1 \end{bmatrix}</m>,
      essentially by inspection.
    </p>
    <p>
      (In general it is easy to compute the change of basis matrix from a nonstandard basis to the standard basis.)
    </p>
    <p>
      It follows that
      <md>
        <mrow>_B'\amp =\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=\left( \underset{B'\rightarrow B}{P}\right)^{-1}[T]_B\underset{B'\rightarrow B}{P}</mrow>
        <mrow>\amp =\left( \begin{bmatrix} 1\amp 1\amp 1</mrow>
        <mrow>1\amp 1\amp 0</mrow>
        <mrow>1\amp 0\amp 1 \end{bmatrix}\right)^{-1}\begin{bmatrix} 1\amp 2\amp 0</mrow>
        <mrow>0\amp 1\amp 4</mrow>
        <mrow>0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix} 1\amp 1\amp 1</mrow>
        <mrow>1\amp 1\amp 0</mrow>
        <mrow>1\amp 0\amp 1 \end{bmatrix}= \begin{amatrix}[rrr] 3\amp -2\amp 4</mrow>
        <mrow>2\amp 3\amp 0</mrow>
        <mrow>-2\amp 2\amp -3 \end{amatrix}</mrow>
      </md>
    </p>
  </paragraphs>
  <p>
    The change of basis formula is often used in the following type of situation.
    <ol>
      <li>
        <p>
          I am interested in a linear transformation <m>T\colon V\rightarrow V</m>,
          where <m>V</m> is finite-dimensional.
        </p>
      </li>
      <li>
        <p>
          I would like to compute <m>A=[T]_B</m> with respect to a specific basis <m>B</m>.
          (Maybe <m>B</m> is a standard basis for <m>V</m> that I prefer to use.)
        </p>
      </li>
      <li>
        <p>
          For whatever reason, it is easier to compute
          <m>A'=[T]_{B'}</m> with respect to some other basis <m>B'</m>
        </p>
      </li>
      <li>
        <p>
          Thus we first compute <m>[T]_{B'}</m>,
          then compute <m>[T]_B</m> via the formula
          <me>
            [T]_B=\underset{B'\rightarrow B}{P}[T]_B'\underset{B\rightarrow B'}{P}
          </me>.
        </p>
      </li>
    </ol>
  </p>
  <p>
    The next example nicely illustrates this.
  </p>
  <paragraphs>
    <title>Example: orthogonal projection revisited</title>
    <p>
      Let <m>T\colon \R^3\rightarrow\R^3</m> be orthogonal projection onto the plane <m>\mathcal{P}: x+y+z=0</m>,
      as defined earlier.
      We would like to derive a formula for <m>T</m>,
      which amounts to finding the <m>A</m> such that <m>T=T_A</m>.
    </p>
    <p>
      As previously observed we have <m>A=[T]_B</m>,
      where <m>B</m> is the <em>standard basis</em> for <m>\R^3</m>.
      We can compute <m>[T]_B</m> by first computing
      <m>[T]_{B'}</m> for a cleverly chosen
      <em>nonstandard</em> basis <m>B'</m>,
      and then using the change of basis formula.
    </p>
    <p>
      As done previously, we let <m>B'=\{(1,-1,0), (1,0,-1), (1,1,1)</m>.
      Since <m>T</m> maps the first two vectors to themselves,
      and the third vector to <m>(0,0,0)</m>,
      we have <m>[T]_{B'}=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}</m>.
      (Go back to original example for details.)
    </p>
    <p>
      Then
      <md>
        <mrow>A\amp =[T]_B=\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}</mrow>
        <mrow>\amp =\begin{amatrix}[rrr] 1\amp 1\amp 1</mrow>
        <mrow>-1\amp 0\amp 1</mrow>
        <mrow>0\amp -1\amp 1 \end{amatrix} \begin{bmatrix} 1\amp 0\amp 0</mrow>
        <mrow>0\amp 1\amp 0</mrow>
        <mrow>0\amp 0\amp 0 \end{bmatrix} \left( \begin{amatrix}[rrr] 1\amp 1\amp 1</mrow>
        <mrow>-1\amp 0\amp 1</mrow>
        <mrow>0\amp -1\amp 1 \end{amatrix}\right)^{-1} =\frac{1}{3}\begin{amatrix}[rrr] 2\amp -1\amp -1</mrow>
        <mrow>-1\amp 2\amp -1</mrow>
        <mrow>-1\amp -1\amp 2 \end{amatrix}</mrow>
      </md>
    </p>
    <p>
      Lo and behold,
      we have rediscovered our matrix formula for orthogonal projection onto <m>\mathcal{P}</m>!!
    </p>
    <p>
      (Note: since <m>B</m> is the standard basis in this case,
      <m>\underset{B'\rightarrow B}{P}</m> was easy to compute. )
    </p>
  </paragraphs>
  <paragraphs>
    <title>Similar matrices</title>
    <p>
      Let <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m> be two different representations of <m>T\colon V\rightarrow V</m>,
      and set <m>P=\underset{B'\rightarrow B}{P}</m>.
      Then we have <m>A'=P^{-1}AP</m>,
      according to the change of basis formula for transformations.
      We say <m>A</m> and <m>A'</m> are
      <em>similar matrices</em> in this case.
    </p>
    <definition>
      <statement>
        <p>
          Matrices <m>A, A'\in M_{nn}</m> are <term>similar</term>
          if there is an invertible matrix <m>P</m> such that <m>A'=P^{-1}AP</m>.
        </p>
      </statement>
    </definition>
    <p>
      This notion of similarity is a technical one, but the name is fitting:
      <m>A</m> and <m>A'</m> really are similar in the sense that they are just two different matrix representations of the same linear transformation! (See Holy Commutative Tent of Linear Algebra on next slide.)
    </p>
    <p>
      As we will see in coming sections,
      matrices that are similar in this technical sense do indeed share many of the same properties.
      We now have the theoretical foundation to understand why this is so:
      they simply inherit these common properties from the overlying linear transformation <m>T</m>,
      of which they are but earthly shadows.
    </p>
    <p>
      There is but one true <m>T</m>!
    </p>
  </paragraphs>
  <p>
    <image width="75%" source="images/HolyCommutativeTent.png"/>
  </p>
</section>
