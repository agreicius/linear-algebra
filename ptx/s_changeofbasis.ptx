<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_changeofbasis">
  <title>Change of basis</title>
  <introduction>
    <p>
      Coordinate vectors and matrix reprsentations work in tandem to model vectors in an abstract vector space <m>V</m> as column vectors in <m>\R^n</m>, and linear transformations <m>T\colon V\rightarrow W</m> as <m>m\times n</m> matrices.
      In both cases the model depends on our choice of basis. In this section we investigate how different basis choices affect these various models. Specifically, we consider the two questions below.
      <ol>
        <li>
          <p>
            Given <m>V</m> and two ordered bases <m>B</m> and <m>B'</m>,
            what is the algebraic relation between
            <m>[\boldv]_B</m> and <m>[\boldv]_{B'}</m>?
          </p>
        </li>
        <li>
          <p>
            Given <m>T\colon V\rightarrow V</m> and two ordered bases <m>B</m> and <m>B'</m>,
            what is the relation between
            <m>[T]_{B}</m> and <m>[T]_{B'}</m>?
          </p>
        </li>
      </ol>
    </p>
    <p>
      We will tackle each question in turn.
      Both answers rely on something called a
      <em>change of basis matrix</em>
      <m>\underset{B\rightarrow B'}{P}</m>.
    </p>

  </introduction>
<subsection xml:id="ss_change_of_basis" >
  <title>Change of basis matrix</title>
  <p>
    We define change of basis matrices via a column-by-column formula and motivate the definition retroactively with <xref ref="th_change_of_basis_coordinates"/>.
  </p>
  <definition xml:id="d_change_of_basis">
    <title>Change of basis matrix</title>
    <idx><h>change of basis matrix</h></idx>
    <notation>
      <usage><m>\underset{B\rightarrow B'}{P}</m></usage>
      <description>change of basis matrix</description>
    </notation>
    <statement>
      <p>
        Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> and <m>B'</m> be two ordered bases for the vector space <m>V</m>. The <term>change of basis from <m>B</m> to <m>B'</m></term> is the <m>n\times n</m> matrix <m>\underset{B\rightarrow B'}{P}</m> defined as
        <me>
        \underset{B\rightarrow B'}{P}=
        \begin{bmatrix}
          \vert \amp \vert \amp \amp \vert \\
          \phantom{v}[\boldv_1]_{B'} \amp \phantom{v}[\boldv_2]_{B'}\amp \dots \amp \phantom{v}[\boldv_n]_{B}\\
          \vert \amp \vert \amp \amp \vert
        \end{bmatrix}
        </me>.
        In other words, the <m>j</m>-th column of <m>\underset{B\rightarrow B'}{P}</m> is obtained by computing the coordinate vector of the <m>j</m>-th element of the <em>original</em> basis <m>B</m> with respect to the <em>new</em> basis <m>B'</m>.
      </p>
    </statement>
  </definition>
  <theorem xml:id="th_change_of_basis_coordinates">
    <title>Change of basis for coordinate vectors</title>
    <statement>
      <p>
        Let <m>B</m> and <m>B'</m> be two ordered bases of the <m>n</m>-dimensional vector space <m>V</m>.
        <ol>
          <li>
            <p>
              Recall that <m>\id_V\colon V\rightarrow V</m> is the identity transformation (<xref ref="d_transform_zero_identity"/>), defined as <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>. We have
              <me>
                \underset{B\rightarrow B'}{P}=[\id_V]_{B}^{B'}
              </me>.
            </p>
          </li>
          <li>
            <p>
              For all <m>\boldv\in V</m> we have
              <men xml:id="eq_changebasis_matrix_prop">
              \underset{B\rightarrow B'}{P}[\boldv]_B=[\boldv]_{B'}
            </men>.
              In other words, to convert the <m>B</m>-coordinates of a vector <m>\boldv\in V</m> to <m>B'</m>-coordinates, simply multiply on the left by the matrix  <m>\underset{B\rightarrow B'}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Property <xref ref="eq_changebasis_matrix_prop"/> defines <m>\underset{B\rightarrow B'}{P}</m> uniquely: <ie />, if <m>A</m> satisfies <m>A[\boldv]_B=[\boldv]_{B'}</m> for all <m>\boldv\in \R^n</m>, then <m>A=\underset{B\rightarrow B'}{P}</m>.
            </p>
          </li>
        </ol>

      </p>
    </statement>
    <proof>
      <ol>
        <li>
          <p>
            Let <m>I_V\colon V\rightarrow V</m> be the identity transformation: <ie />, <m>I_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>. By <xref ref="th_matrixrep"/> the matrix <m>[I_V]_B^{B'}</m> is the unique matrix satisfying
            <me>
            [I_V]_B^{B'}[\boldv]_B=[I_V(\boldv)]_{B'}=[\boldv]_{B'}
            </me>.
            Comparing the formulas for <m>[I_V]_{B}^{B'}</m> and <m></m> we see directly that
            <me>
            [I_V]_B^{B'} = \underset{B\rightarrow B'}{P}
            </me>.
          </p>
        </li>
        <li>
          <p>
            This follows from (1) and <xref ref="th_matrixrep"/>:
            <md>
              <mrow> \underset{B\rightarrow B'}{P}[\boldv]_B \amp = [\id_V]_{B}^{B'}[\boldv]_B </mrow>
              <mrow>  \amp = [\id_V(\boldv)]_{B'} \amp (<xref ref="th_matrixrep"/>)</mrow>
              <mrow> \amp =[\boldv]_{B'} </mrow>
            </md>.
          </p>
        </li>
        <li>
          <p>
            By (2) of <xref ref="th_matrixrep"/> (the uniqueness claim), if <m>A</m> satisfies <m>A[\boldv]_B=[\boldv]_{B'}</m> for all <m>\boldv\in \R^n</m>, then <m>A=[\id_V]_{B}^{B'}</m>. Since <m>[\id_V]_{B}^{B'}=\underset{B\rightarrow B'}{P}</m>, we conclude <m>A=\underset{B\rightarrow B'}{P}</m>.
          </p>
        </li>
      </ol>

    </proof>

  </theorem>
<example>
  <statement>
    <p>
      Let <m>V=\R^2</m>, <m>B=(\boldv_1=(1,1),\boldv_2=(1,-1))</m>, <m>B'=(\boldw_1=(1,2), \boldw_2=(2,-1))</m>. Observe that <m>B</m> and <m>B'</m> are both orthogonal with resepct to the dot product. This will simplify your computations below.
      <ol>
        <li>
          <p>
            Compute <m>\underset{B\rightarrow B'}{P}</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>\boldx=(4,-2)</m>. Compute <m>[\boldx]_{B'}</m> using <xref ref="eq_changebasis_matrix_prop"/>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          Using <xref ref="d_change_of_basis"/>, we have
          <md>
          <mrow>\underset{B\rightarrow B'}{P}\amp = \begin{bmatrix}\vert \amp \vert \\ \hspace{7pt}[\boldv_1]_{B'} \amp \hspace{7pt}[\boldv_2]_{B'}\\ \vert \amp \vert \end{bmatrix}</mrow>
          <mrow>\amp = \begin{bmatrix}\frac{3}{5}\amp -\frac{1}{5}\\ \frac{1}{5}\amp \frac{3}{5} \end{bmatrix}  </mrow>
          </md>.
          Here the two coordinate vector computations <m>[(1,1)]_{B'}=(3/5, 1/5)</m> and <m>[(1,-1)]_{B'}=(-1/5,3/5)</m> were done using <xref ref="th_coordinates_orthogonal"/>.
        </p>
      </li>
      <li>
        <p>
          First we easily compute <m>[\boldx]_{B}=(1, 3)</m>, again using <xref ref="th_coordinates_orthogonal"/>. Now use <xref ref="eq_changebasis_matrix_prop"/>:
          <md>
            <mrow> [\boldx]_{B'} \amp =\underset{B\rightarrow B'}{P}[\boldx]_B </mrow>
            <mrow> \amp = \begin{bmatrix}\frac{3}{5}\amp -\frac{1}{5}\\ \frac{1}{5}\amp \frac{3}{5} \end{bmatrix} \begin{amatrix}[c]1 \\ 3  \end{amatrix}</mrow>
            <mrow>  \amp = \begin{amatrix}[r]0\\ 2   \end{amatrix}</mrow>
          </md>.
          This should come as now surprise since
          <me>\boldx=(4,-2)=2(2,-1)=0\boldw_1+2\boldw_2
          </me>.
        </p>
      </li>
    </ol>
  </solution>
</example>
<example>
  <statement>
    <p>
      Let <m>V=P_2</m>, <m>B=(x^2,x,1)</m>, <m>B'=(p_1(x)=(x-2)^2, p_2(x)=x-2, p_3(x)=1)</m>.
    </p>
    <ol>
      <li>
        <p>
          Compute <m>\underset{B\rightarrow B'}{P}</m>.
        </p>
      </li>
      <li>
        <p>
          Compute <m>[x^2+x+1]_{B'}</m> using <xref ref="eq_changebasis_matrix_prop"/>.
        </p>
      </li>
    </ol>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          We have
          <md>
            <mrow>\underset{B\rightarrow B'}{P} \amp = \begin{bmatrix}
              \vert \amp \vert \amp \vert \\
              [x^2]_{B'}\amp [x]_{B'}\amp [1]_{B'} \\
              \vert \amp \vert \amp \vert
            \end{bmatrix} </mrow>
            <mrow> \amp = \begin{bmatrix}
              1\amp 0\amp 0 \\
              4\amp 1\amp 0\\
              4\amp 2\amp 1
            \end{bmatrix} </mrow>
          </md>.
          The first two coordinate vector computations are nontrivial; you can verify for yourself that <m>x^2=1(x-2)^2+4(x-2)+4</m> and  <m>x=0(x-2)^2+1(x-2)+2</m>. Alternatively, see  <xref ref="rm_change_of_basis_taylors"/>) for a neat trick for computing these coordinate vectors.
        </p>
      </li>
      <li>
        <p>
          Since <m>B</m> is the standard basis, we see easily that <m>[x^2+x+1]_{B}=(1,1,1)</m>. Using <xref ref="eq_changebasis_matrix_prop"/> we have
          <md>
            <mrow>[x^2+x+1]_{B'} \amp = \underset{B\rightarrow B'}{P}\begin{bmatrix}
              1\\ 1\\ 1
            \end{bmatrix}</mrow>
            <mrow> \amp =\begin{bmatrix}
              1\\ 5\\ 7
            \end{bmatrix} </mrow>
          </md>.
          Verify for yourself that we do indeed have
          <me>
            x^2+x+1=1(x-2)^2+5(x-2)+7
          </me>.
        </p>
      </li>
    </ol>
  </solution>
</example>
    <remark xml:id="rm_change_of_basis_taylors">
  <title>Taylor's formula and change of basis</title>
  <statement>
    <p>
      Let <m>B=(x^n, x^{n-1}, \dots, x, 1)</m> be the standard basis of <m>P_n</m>. Fix any constant <m>a\in \R</m>, and let <m>B'=((x-a)^n, (x-a)^{n-1}, \dots, (x-a), 1)</m>. It is easy to see that <m>B'</m> is also an ordered basis: a simple degree argument shows that the polynomials <m>p_k(x)=(x-a)^k</m> are linearly independent. It follows from Taylor's theorem (from single-variable calculus) that given any polynomial <m>p\in P_n</m> we have
      <me>
        p(x)=p(a)+p'(a)(x-a)+\frac{p''(x)}{2}(x-a)^2+\cdots \frac{p^{(n)}(a)}{n!}(x-a)^n
      </me>.
      We call this expression the expansion of <m>p(x)</m> about <m>x=a</m>. In terms of coordinate vectors, this means that
      <men xml:id="eq_taylors">
        [p]_{B'}=\left(\frac{p^{(n)}(a)}{n!}, \frac{p^{(n-1)}(a)}{(n-1)!}, \dots, p'(a), p(a)\right)
      </men>.
      In other words, Taylor's theorem provides a simple derivative formula for computing coordinate vectors with respect to the basis <m>B'</m>.
    </p>
  </statement>
</remark>
<p>
  The following properties are often useful when computing various change of basis matrices.
</p>
  <theorem xml:id="th_change_of_basis_properties">
    <title>Change of basis matrix properties</title>
    <statement>
      <p>
        Let <m>B, B', B''</m> be ordered bases for the <m>n</m>-dimensional vector space <m>V</m>.
      </p>
      <ol>
        <li>
          <p>
            We have
            <me>
            \underset{B\rightarrow B}{P}=I
            </me>.
          </p>
        </li>
        <li>
          <p>
            The matrix <m>\underset{B\rightarrow B'}{P}</m> is invertible. In fact, we have
            <me>
            \left(\underset{B\rightarrow B'}{P}\right)^{-1}=\underset{B'\rightarrow B}{P}
            </me>
          </p>
        </li>
        <li>
          <p>
            We have
            <me>
            \underset{B\rightarrow B''}{P}=\underset{B'\rightarrow B''}{P}\, \underset{B\rightarrow B'}{P}
            </me>.
          </p>
        </li>

      </ol>
    </statement>
    <proof>
      <ol>
        <li>
          <p>
          Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m>. By definition, the <m>j</m>-th column of <m>\underset{B\rightarrow B'}{P}</m> is <m>[\boldv_j]_B</m>. Since <m>\boldv_j=0\boldv_1+0\boldv_2+\cdots +1\boldv_j+\cdots</m>, we see that <m>[\boldv_j]_B=\bolde_j</m>, and hence that <m>\underset{B\rightarrow B'}{P}=I</m>, as claimed.
          </p>
        </li>
        <li>
          <p>
            Let <m>A=\underset{B\rightarrow B'}{P}</m> and <m>B=\underset{B'\rightarrow B"}{P}</m>. For any <m>\boldv\in V</m> we have
            <md>
              <mrow>BA[\boldv]_B \amp = \underset{B'\rightarrow B}{P}\, \underset{B\rightarrow B'}{P}[\boldv]</mrow>
              <mrow> \amp =\underset{B'\rightarrow B}{P}[\boldv]_{B'} </mrow>
              <mrow>  \amp = [\boldv]_{B}</mrow>
            </md>.
            It follows from (3) of <xref ref="th_change_of_basis_coordinates"/> that <m>BA=\underset{B\rightarrow B}{P}</m>. Since <m>\underset{B\rightarrow B}{P}=I</m> by (1), we conclude that <m>BA=I</m>, and hence <m>B=A^{-1}</m>, as desired.
          </p>
        </li>
        <li>
          <p>
            See <xref ref="ex_changebasis_three_bases"/>.
          </p>
        </li>
      </ol>
    </proof>

  </theorem>

<example xml:id="eg_changebasis_standard">
  <title><m>V=\R^n</m>, <m>B</m> standard basis</title>
  <statement>
    <p>
      Consider the special situation where <m>V=\R^n</m>,
      <m>B</m> is the standard basis,
      and <m>B'=\{\boldv_1,\dots,\boldv_n\}</m> is some nonstandard basis.
      In this case we have
    <md>
      <mrow> \underset{B'\rightarrow B}{P}\amp =\begin{bmatrix}\vert\amp\vert\amp  \amp \vert \\ [\boldv_1]_B\amp [\boldv_2]_B \amp \cdots\amp [\boldv_n]_B\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix}
      </mrow>
      <mrow>  \amp = \begin{bmatrix}\vert\amp\vert\amp  \amp \vert \\ \boldv_1\amp \boldv_2\amp\cdots\amp \boldv_n\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix} \amp (B \text{ standard basis})</mrow>
    </md>.
    In other words, <m>\underset{B'\rightarrow B}{P}</m> is the matrix whose <m>j</m>-th column is just the <m>j</m>-th element of <m>B'</m>. Thus, in this situation we can compute <m>\underset{B'\rightarrow B}{P}</m> by placing the elements of <m>B'</m> as columns of a matrix, and then use (2) of <xref ref="th_change_of_basis_properties"/> to compute <m>\underset{B\rightarrow B'}{P}=\left(\underset{B'\rightarrow B}{P}\right)^{-1}</m>.
    </p>
  </statement>
</example>

<example>
  <statement>
    <p>
      Let <m>V=\R^2</m>, <m>B=((1,0),(0,1))</m>, <m>B'=\{(1,\sqrt{3}),(-\sqrt{3},1)\}</m>. Compute <m>\underset{B\rightarrow B'}{P}</m> and <m>\underset{B'\rightarrow B}{P}</m>.
    </p>
  </statement>
  <solution>
    <p>
      According to <xref ref="eg_changebasis_standard"/> we have
      <me>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix}</me>. We then compute
      <me>
      \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\left(\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix} \right)^{-1}=\frac{1}{4}\begin{bmatrix}1\amp \sqrt{3}\\ -\sqrt{3}\amp 1 \end{bmatrix}
      </me>.
    </p>
  </solution>
</example>
<remark xml:id="rm_changebasis_standard">
<title><m>B</m> standard basis of <m>V</m></title>
    <p>
      The observation from <xref ref="eg_changebasis_standard"/> applies more generally when <m>B</m> is the standard basis of the given vector space <m>V</m> and <m>B'=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is nonstandard. In this case computing <m>\underset{B'\rightarrow B}{P}</m> will be easy as the coordinate vectors <m>[\boldv_j]_{B}</m> can be produced by inspection. See <xref ref="eg_changebasis_standard_mat"/>.
    </p>
</remark>
<example xml:id="eg_changebasis_standard_mat">
  <statement>
    <p>
      Let <m>V=M_{22}</m>, <m>B=(E_{11}, E_{12}, E_{21}, E_{22})</m> (standard basis) and <m>B'=(A_1,A_2,A_3,A_4)</m>, where
      <me>
        A_1=\begin{amatrix}[rr]
          1\amp 1\\ 1\amp 1
      \end{amatrix}, A_2=\begin{amatrix}[rr]
        1\amp -1\\ 1\amp -1
    \end{amatrix}, A_3=\begin{amatrix}[rr]
      1\amp 1\\ -1\amp -1
  \end{amatrix}, A_4=\begin{amatrix}[rr]
    -1\amp 1\\ 1\amp -1
\end{amatrix}
  </me>.
    Compute <m>\underset{B'\rightarrow B}{P}</m>.
    </p>
  </statement>
  <solution>
    <p>
      We have
      <md>
        <mrow> \underset{B'\rightarrow B}{P}\amp = \begin{bmatrix}
          \vert\amp \vert\amp \vert\amp \vert\\
          [A_1]_{B}\amp [A_2]_B\amp [A_3]_B\amp [A_4]_B\\
          \vert\amp \vert\amp \vert\amp \vert
        \end{bmatrix}</mrow>
        <mrow> \amp = \begin{amatrix}[rrrr]
          1\amp 1\amp 1\amp -1\\
          1\amp -1\amp 1\amp 1\\
          1\amp 1\amp -1\amp 1\\
          1\amp -1\amp -1\amp -1
        \end{amatrix} </mrow>
      </md>.
      Here the coordinate vectors <m>[A_i]_B</m> are easily computed by inspection since <m>B</m> is the standard basis.
    </p>
    <p>
      It turns out that <m>\underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}</m> is not so difficult to compute in this case since the columns <m>\boldc_j</m> of <m>\underset{B'\rightarrow B}{P}</m> satisfy
      <me>
        \boldc_i\cdot\boldc_j=\begin{cases} 4\amp \text{if } i=j\\ 0\amp \text{if } i\ne j
      \end{cases}
      </me>.
      From this observation and <xref ref="th_dotproduct_method"/> it is easy to see that
      <me>
        \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\frac{1}{4}
        \begin{amatrix}[rrrr]
          1\amp 1\amp 1\amp 1\\
          1\amp -1\amp 1\amp -1\\
          1\amp 1\amp -1\amp -1\\
          -1\amp -1\amp 1\amp -1
        \end{amatrix}
      </me>.
    </p>
  </solution>
</example>


</subsection>

<subsection xml:id="ss_changebasis_orthonormal">
  <title>Working with orthonormal bases</title>
  <p>
    Let <m>B</m> and <m>B'</m> be ordered bases of an <m>n</m>-dimensional inner product space <m>(V,\langle\, , \rangle)</m>. Not surprisingly, if one or more of <m>B</m> and <m>B'</m> are orthornormal, our change of basis computations are simplified significantly. For example, if <m>B'</m> is orthonormal (or even orthogonal), the coordinate vector computations necessary to compute <m>\underset{B\rightarrow B'}{P}</m> can be done using the inner product formula of <xref ref="th_coordinates_orthogonal"/>. Furthermore, as we see below, if both <m>B</m> and <m>B'</m> are orthonormal, then the columns of <m>\underset{B\rightarrow B'}{P}</m> form an orthonormal basis of <m>\R^n</m>, in which case we can compute <m>\underset{B'\rightarrow B}{P}</m> as
    <me>
      \underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}=\left(\underset{B\rightarrow B'}{P}\right)^T
    </me>.
    In other words, when <m>B</m> and <m>B'</m> are orthonormal bases, the change of basis matrix is what we call an <em>orthogonal matrix</em>.
  </p>
  <definition xml:id="d_orthogonal_matrix">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
      An invertible <m>n\times n</m> matrix <m>A</m> is <term>orthogonal</term> if <m>A^{-1}=A^T</m>.
      </p>
    </statement>
  </definition>
      <remark xml:id="rm_orthogonal_matrices">
    <statement>
      <p>
        Since for an invertible matrix <m>A</m> we have <m>(A^T)^{-1}=(A^{-1})^T</m> it follows immediately from <xref ref="d_orthogonal_matrix"/> that
        <me>
          A \text{ is orthogonal}\iff A^T \text{ is orthogonal} \iff A^{-1} \text{ is orthogonal}
        </me>.
      </p>
    </statement>
  </remark>
<example>
  <statement>
    <p>
      The matrix
      <me>
      A=\begin{amatrix}[rr]\frac{\sqrt{2}}{2}\amp -\frac{\sqrt{2}}{2}\\ \frac{\sqrt{2}}{2} \amp \frac{\sqrt{2}}{2} \end{amatrix}
      </me> is orthogonal, as one easily checks by computing <m>A^TA</m>. Observe that the columns of <m>A</m> form an orthonormal set with respect to the dot product, as do the rows. This is not a coincidence!
    </p>
  </statement>
</example>
  <theorem xml:id="th_orthogonal_matrices">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. The following statements are equivalent.
      </p>
      <ol>
        <li>
          <p>
            The matrix <m>A</m> is orthogonal.
          </p>
        </li>
        <li>
          <p>
            The columns of <m>A</m> form an orthonormal basis of <m>\R^n</m> with respect to the dot product.
          </p>
        </li>
        <li>
          <p>
            The rows of <m>A</m> form an orthonormal basis of <m>\R^n</m> with respect to the dot product.
          </p>
        </li>
      </ol>
    </statement>
    <proof>
      <p>
         Let <m>\boldr_i</m> and <m>\boldc_i</m> be the <m>i</m>-th row and column of <m>A</m>, respectively, for each <m>1\leq i\leq n</m>. From <xref ref="th_dotproduct_method"/> we see that
        <mdn>
          <mrow xml:id="eq_orthogonal_matrixleft">A^TA \amp=[\boldc_i\cdot \boldc_j]_{1\leq i,j\leq n} </mrow>
          <mrow xml:id="eq_orthogonal_matrixright">AA^T \amp=[\boldr_i\cdot \boldr_j]_{1\leq i,j\leq n} </mrow>
        </mdn>.
        We use here that rows of <m>A^T</m> are the columns of <m>A</m>, and the columns of <m>A^T</m> are the rows of <m>A</m>. From <xref first="eq_orthogonal_matrixleft" last="eq_orthogonal_matrixright"/> it follows easily that
        <md>
          <mrow>A^{-1}=A^T \amp \iff A^TA=I </mrow>
          <mrow> \amp \iff \boldc_i\cdot\boldc_j=\begin{cases} 1\amp \text{if } i=j\\ 0\amp \text{if } i\ne j\end{cases} </mrow>
          <mrow>  \amp \iff \{\boldc_1,\boldc_2,\dots, \boldc_n\} \text{ is orthonormal}</mrow>
          <mrow>  \amp \iff \{\boldc_1,\boldc_2,\dots, \boldc_n\} \text{ is an orthonormal basis}\amp (n=\dim\R^n)</mrow>
        </md>,
        and
        <md>
          <mrow>A^{-1}=A^T \amp \iff AA^T=I </mrow>
          <mrow> \amp \iff \boldr_i\cdot\boldr_j=\begin{cases} 1\amp \text{if } i=j\\ 0\amp \text{if } i\ne j\end{cases} </mrow>
          <mrow>  \amp \iff \{\boldr_1,\boldr_2,\dots, \boldr_n\} \text{ is orthonormal}</mrow>
          <mrow>  \amp \iff \{\boldr_1,\boldr_2,\dots, \boldr_n\} \text{ is an orthonormal basis} \amp (n=\dim\R^n)</mrow>
        </md>.
        This proves <m>(1)\iff (2)</m> and <m>(1)\iff (3)</m>. The result follows.
      </p>
    </proof>
  </theorem>
      <remark xml:id="rm_orthogonal_matrices_misnomer">
    <statement>
      <p>
        It is somewhat unfortunate that the property of being an <em>orthogonal</em> matrix is equivalent to your rows or columns forming an <em>orthonormal</em> basis. You ask: Why not simply call such matrices <em>orthonormal</em> matrices? My answer: tradition!
      </p>
    </statement>
  </remark>

<theorem xml:id="th_changebasis_orthonormal">
  <title>Orthonormal change of basis</title>
  <statement>
    <p>
      Let <m>(V,\langle\, , \rangle)</m> be a finite dimensional inner product space, and suppose <m>B</m> and <m>B'</m> are orthonormal bases of <m>V</m>.
    </p>
    <ol>
      <li>
        <p>
          The matrices <m>\underset{B\rightarrow B'}{P}</m> and <m>\underset{B'\rightarrow B}{P}</m> are orthogonal.
        </p>
      </li>
      <li>
        <p>
          We have
          <me>
            \underset{B'\rightarrow B}{P}=\left( \underset{B\rightarrow B'}{P} \right)^T
          </me>.
        </p>
      </li>
    </ol>
  </statement>
  <proof>
    <ol>
      <li>
        <p>
          Let <m>B=(\boldv_1, \boldv_2,\dots, \boldv_n)</m>. By definition, the columns of <m>\underset{B\rightarrow B'}{P}</m> are the coordinate vectors <m>[\boldv_i]_{B'}</m>, <m>1\leq i\leq n</m>. By <xref ref="ex_coordinates_orthonormal"/>, these coordinate vectors form an orthonormal subset of <m>\R^n</m>; since there are <m>n=\dim\R^n</m> of them, they form an orthonormal basis. From <xref ref="th_orthogonal_matrices"/> it follows that <m>\underset{B\rightarrow B'}{P}</m> is orthogonal.
          Lastly, from <xref ref="rm_orthogonal_matrices"/> it follows that <m>\underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}</m> is also orthogonal.
        </p>
      </li>
      <li>
        <p>
          Since <m>\underset{B\rightarrow B'}{P}</m> is orthogonal, we have
          <me>
            \underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}=\left(\underset{B\rightarrow B'}{P}\right)^T
          </me>.
        </p>
      </li>
    </ol>
  </proof>

</theorem>
<example>
  <statement>
    <p>
      Consider the vector space <m>P_1</m> with inner product <m>\langle p(x), q(x)\rangle=p(-1)q(-1)+p(1)q(1)</m>. The ordered bases
      <me>B=\left(p_1(x)=\frac{1}{\sqrt{2}}x,p_2(x)=\frac{1}{\sqrt{2}}\right), B'=\left(q_1(x)=\frac{1}{2}(x-1), q_2(x)=\frac{1}{2}(x+1)\right)
      </me>
      are both orthonormal with respect to this inner product. Compute <m>\underset{B\rightarrow B'}{P}</m> and <m>\underset{B'\rightarrow B}{P}</m>.
    </p>
  </statement>
  <solution>
    <p>
      Since <m>B'</m> is orthonormal, we use  <xref ref="th_coordinates_orthogonal"/> to compute
      <md>
        <mrow> [p_1(x)]_{B'} \amp = \left(\langle p_1(x),q_1(x)\rangle, \langle p_1(x),q_2(x)\rangle\right)</mrow>
        <mrow> \amp =(p_1(-1)q_1(-1)+p_1(1)q_2(1),p_1(1)q_1(1)+p_1(1)q_2(1))=\frac{1}{\sqrt{2}}(1,1)</mrow>
        <mrow> [p_2(x)]_{B'} \amp = \left(\langle p_2(x),q_1(x)\rangle, \langle p_1(x),q_2(x)\rangle\right)</mrow>
        <mrow> \amp =(p_2(-1)q_1(-1)+p_2(1)q_2(1),p_2(1)q_1(1)+p_2(1)q_2(1))=\frac{1}{\sqrt{2}}(-1,1)</mrow>
      </md>.
      Thus
      <me>
      \underset{B\rightarrow B'}{P}=\frac{1}{\sqrt{2}}\begin{amatrix}[rr]
      1\amp -1\\
      1\amp 1
      \end{amatrix}
      </me>
      and by <xref ref="th_changebasis_orthonormal"/>
      <me>
        \underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^T=\frac{1}{\sqrt{2}}\begin{amatrix}[rr]
        1\amp 1\\
        -1\amp 1
        \end{amatrix}
      </me>.
    </p>
  </solution>
</example>
<p>
  Before connecting change of basis matrices with matrix representations of linear transformations, it is worth gathering some the different techniques for computing change of basis matrices we have discussed so far.
</p>
<algorithm xml:id="proc_changebasis_tips">
<title>Change of basis computational tips</title>
  <statement>
    <p>
      Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> and <m>B'</m> be ordered bases of the vector space <m>V</m>. Below you find a variety of techniques for computing <m>\underset{B\rightarrow B'}{P}</m> and <m>\underset{B'\rightarrow B}{P}</m>.
    </p>
    <ol>
      <li>
        <p>
          To compute <m>\underset{B\rightarrow B'}{P}</m> directly, we must compute <m>[\boldv_j]_{B'}</m> for each <m>1\leq j\leq n</m>. This typically involves setting up and solving a linear system.
        </p>
      </li>
      <li>
        <p>
          We have <m>\underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^{-1}</m>. This observation is useful in situations where (a) one change of basis matrix is easier to compute than the other and (b) computing inverse matrices is not too onerous.
        </p>
      </li>
      <li>
        <p>
          If <m>B</m> is the standard basis of <m>V</m>, then <m>\underset{B'\rightarrow B}{P}</m> is easy to compute. (See <xref ref="rm_changebasis_standard"/>.)
        </p>
      </li>
      <li>
        <p>
          If <m>B'</m> is orthogonal with respect to some inner product on <m>V</m>, then we can easily compute <m>[\boldv_j]_{B'}</m> for each <m>1\leq j\leq n</m> using <xref ref="th_coordinates_orthogonal"/>.
        </p>
      </li>
      <li>
        <p>
          If <m>B</m> and <m>B'</m> are both orthonormal bases of <m>V</m> with respect to a common inner product, then both change of basis matrices are orthogonal and we have <m>\underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^T</m>.
        </p>
      </li>
    </ol>
  </statement>
</algorithm>
</subsection>
  <subsection>
    <title>Change of basis for transformations</title>
    <p>
      We now investigate how our choice of basis affects matrix representations of linear transformations.
      We will only consider the special case where
      <m>T\colon V\rightarrow V</m> and we are comparing matrix representations <m>[T]_B</m> and
      <m>[T]_{B'}</m> for two different ordered bases of <m>V</m>.
    </p>
    <theorem xml:id="th_change_of_basis_transformations">
      <title>Change of basis for transformations</title>
      <statement>
        <p>
          Let <m>V</m> be finite-dimensional,
          let <m>T\colon V\rightarrow V</m> be linear,
          and let <m>B</m> and <m>B'</m> be two ordered bases for <m>V</m>. We have
          <men xml:id="eq_changebasis_transform">
            [T]_{B'}=\underset{B\rightarrow B'}{P}\, [T]_B\, \underset{B'\rightarrow B}{P}
          </men>,
           or equivalently
           <men xml:id="eq_changebasis_transform_inverse">
             [T]_{B'}=\underset{B'\rightarrow B}{P}^{-1}\, [T]_B\, \underset{B'\rightarrow B}{P}
           </men>.
        </p>
      </statement>
      <proof>
        <p>
          First observe that <xref ref="eq_changebasis_transform_inverse"/> follows from <xref ref="eq_changebasis_transform"/> and (2) of <xref ref="th_change_of_basis_properties"/>. Next, to prove <xref ref="eq_changebasis_transform"/>, it suffices by (2) of <xref ref="th_matrixrep"/> to show that the matrix <m>A=\underset{B\rightarrow B'}{P}\, [T]_B\, \underset{B'\rightarrow B}{P}</m> satisfies
          <me>
            A[\boldv]_{B'}=[T(\boldv)]_{B'}
          </me>
          for all <m>\boldv\in V</m>. To this end, given any <m>\boldv\in V</m>, we have
          <md>
            <mrow>A[\boldv]_{B'}=\underset{B\rightarrow B'}{P}\, [T]_B\, \underset{B'\rightarrow B}{P}[\boldv]_{B'} \amp= \underset{B\rightarrow B'}{P}\, [T]_B [\boldv]_B \amp (<xref ref="th_change_of_basis_coordinates"/>)</mrow>
            <mrow> \amp= \underset{B\rightarrow B'}{P}[T(\boldv)]_{B} \amp (<xref ref="th_matrixrep"/>, (1)) </mrow>
            <mrow>  \amp = [\boldv]_{B'} \amp (<xref ref="th_change_of_basis_coordinates"/>)</mrow>
          </md>.
        </p>
      </proof>

    </theorem>

  <remark xml:id="rm_change_of_basis_transformations">
    <title>Getting change of basis formulas correct</title>
    <statement>
      <p>
        It is easy to get the various details of the change of basis formula wrong.
        Here is a potential way to keep things organized in your mind.
        <ol>
          <li>
            <p>
              We wish to relate <m>[T]_{B'}</m> and <m>[T]_B</m> with an equation of the form <m>[T]_{B'}=*[T]_B*</m>,
              where the asterisks are to be replaced with change of basis matrices or their inverses.
              Think of the three matrices on the right-hand side of this equation  as a sequence of three things done to coordinate vectors,
              reading from right to left.
            </p>
          </li>
          <li>
            <p>
              <m>[T]_{B'}</m> takes as inputs <m>B'</m>-coordinates of vectors,
              and outputs <m>B'</m>-coordinates.
              Thus the same should be true for <m>*[T]_B*</m>.
            </p>
          </li>
          <li>
            <p>
              Since <m>[T]_B</m> takes as inputs <m>B</m>-coordinates,
              we must <em>first</em> convert from <m>B'</m>-coordinates to <m>B</m>-coordinates.
              So we should have <m>[T]_{B'}=*[T]_B\underset{B'\rightarrow B}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Since <m>[T]_B</m> outputs <m>B</m>-coordinates,
              we need to then convert back to <m>B'</m>-coordinates.
              Thus <m>[T]_{B'}=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              If desired you may replace
              <m>\underset{B\rightarrow B'}{P}</m> with <m> \underset{B'\rightarrow B}{P}^{-1}</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </remark>
<example>
  <statement>
    <p>
      Let <m>T\colon P_2\rightarrow P_2</m> be defined as <m>T(p(x))=p(x)+2p'(x)+xp''(x)</m>.
      <ol>
        <li>
          <p>
            Let <m>B=(x^2, x, 1)</m>.
            Compute <m>[T]_B</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>B'=(x^2+x+1, x^2+1, x+1)</m>.
            Use the change of basis formula to compute <m>[T]_{B'}</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
            We easily compute <m>[T]_B=\begin{bmatrix}1\amp 0\amp 0\\ 6\amp 1\amp 0\\ 0\amp 2\amp 1 \end{bmatrix}</m> using our usual recipe.
        </p>
      </li>
      <li>
        <p>
          We need to compute both change of basis matrices. Since <m>B</m> is standard we compute
          <me>
            \underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp 1\amp 0\\ 1\amp 0\amp 1\\ 1\amp 1\amp 1 \end{bmatrix}
          </me>
          essentially by inspection. It follows that
          <me>
            \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\begin{amatrix}[rrr]
            1\amp 1\amp -1\\ 0\amp -1\amp 1\\ -1\amp 0\amp 1
            \end{amatrix}
          </me>.
          Lastly, using <xref ref="eq_changebasis_transform"/> we have
          <md>
            <mrow> [T]_{B'}\amp =\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P} </mrow>
            <mrow>  \amp = \begin{amatrix}[rrr]
            1\amp 1\amp -1\\ 0\amp -1\amp 1\\ -1\amp 0\amp 1
            \end{amatrix}
            \begin{bmatrix}1\amp 0\amp 0\\ 6\amp 1\amp 0\\ 0\amp 2\amp 1 \end{bmatrix}
              \begin{bmatrix}1\amp 1\amp 0\\ 1\amp 0\amp 1\\ 1\amp 1\amp 1 \end{bmatrix}
            </mrow>
            <mrow>  \amp = \begin{amatrix}[rrr]
            5\amp 6\amp -2\\ -4\amp -5\amp 2\\ 2\amp 0\amp 3
            \end{amatrix}
            </mrow>
          </md>.
        </p>
      </li>
    </ol>
  </solution>
</example>
<remark xml:id="rm_changebasis_matrix_formulas">
<title>Computing matrix formulas with <xref ref="th_change_of_basis_transformations"/></title>
<p>
  Consider the special case where <m>T\colon \R^n\rightarrow \R^n</m>: that is, when <m>V=\R^n</m> is a space of <m>n</m>-tuples. We know from <xref ref="cor_matrix_transformations"/> that <m>T=T_A</m> for a unique <m>n\times n</m> matrix <m>A</m>; and furthermore we have <m>A=[T]_B</m>, where <m>B=(\bolde_1, \bolde_2, \dots, \bolde_n)</m> is the <em>standard</em> ordered basis of <m>\R^n</m>. (See <xref ref="eg_matrixreps_matrixtransforms"/>.)
</p>
<p>
  To compute <m>A=[T]_B</m> directly, we must compute <m>T(\bolde_j)</m> for each of the standard basis elements <m>\bolde_j</m>. However, for many naturally occurring transformations <m>T</m>, computing <m>A=[T]_B</m> directly is often not as convenient as computing <m>A'=[T]_{B'}</m> for some nonstandard basis <m>B'</m>. When this is the case <xref ref="th_change_of_basis_transformations"/> allows to us derive the desired matrix <m>A</m> from the more conveniently computed <m>A'</m>: namely, we have
  <me>
    A=P^{-1}A'P
  </me>,
  where <m>P=\underset{B\rightarrow B'}{P}</m>.
  This gives us a powerful technique for computing matrix formulas for many interesting geometric linear transformations of <m>\R^n</m> whose very definitions involve an implicit choice nonstandard basis. Rotations, reflections and orthogonal projections are all examples of such transformations.
</p>
</remark>
<example>
  <title>Orthogonal projection (again)</title>
  <statement>
    <p>
      Consider <m>V=\R^3</m> together with the dot product. Let's derive (once again) a matrix formula for orthogonal projection  <m>\operatorname{proj}_W\colon \R^3\rightarrow \R^3</m>, where <m>W=\{(x,y,z)\colon x+y+z=0\}</m>. (See <xref ref="eg_projection_plane"/> and <xref ref="eg_matrixreps_proj"/>) In other words we want to compute <m>A=[\operatorname{proj}_W]_B</m>, where <m>B=((1,0,0), (0,1,0), (0,0,1))</m> is the standard basis. We will do so <em>indirectly</em> by first computing <m>[\operatorname{proj}_W]_{B'}</m>
      with respect to a more convenient basis: namely, <m>B'=((1,-1,0),(1,1,-2), (1,1,1))</m>. This is the same basis from <xref ref="eg_matrixreps_proj"/>, and was selected deliberateley so that the first two vectors form a basis of <m>W</m>, and the third vector spans the normal line to <m>W</m>. As in <xref ref="eg_matrixreps_proj"/> we then easily compute
      <me>
        [\operatorname{proj}_W]_{B'}=\begin{bmatrix}
          1\amp 0\amp 0\\
          0\amp 1\amp 0\\
          0\amp 0\amp 0
      \end{bmatrix}
      </me>.
      Now use <xref ref="eq_changebasis_transform_inverse"/> to compute
      <md>
        <mrow> A=[\operatorname{proj}_W]_{B}\amp = \underset{B'\rightarrow B}{P}[\operatorname{proj}_W]_{B'}\underset{B\rightarrow B'}{P} </mrow>
        <mrow> \amp= \begin{amatrix}[rrr]
        1\amp 1\amp 1\\
        -1\amp 1\amp 1\\
        0\amp -2\amp 1
        \end{amatrix}
         \begin{bmatrix}
          1\amp 0\amp 0\\
          0\amp 1\amp 0\\
          0\amp 0\amp 0
      \end{bmatrix}
      \begin{amatrix}[rrr]
      \frac{1}{2}\amp -\frac{1}{2}\amp 0\\
      \frac{1}{6}\amp \frac{1}{6}\amp -\frac{1}{3}\\
      \frac{1}{3}\amp \frac{1}{3}\amp \frac{1}{3}
      \end{amatrix}
      </mrow>
      <mrow>  \amp = \frac{1}{3}\begin{amatrix}[rrr]
      2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2
      \end{amatrix}
      </mrow>
      </md>.
    Lo and behold, we've discovered our matrix formula for projection onto <m>W</m> once again!
    </p>
  </statement>
</example>
</subsection>
<subsection xml:id="ss_changebasis_similarity">
<title>Similarity and the holy commutative tent of linear algebra</title>
<p>
  <xref ref="th_change_of_basis_transformations"/> supplies an algebraic answer to the question: What is the relation between two matrix representations <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m>? Letting <m>P=\underset{B'\rightarrow B}{P}</m>, equation <xref ref="eq_changebasis_transform_inverse"/> becomes <m>A'=P^{-1}AP</m>. Matrices satisfying such a relation are said to be <em>similar</em>.
</p>

  <definition xml:id="d_similar">
    <idx><h>similar matrices</h></idx><statement>
      <p>
        Matrices <m>A, A'\in M_{nn}</m> are <term>similar</term>
        if there is an invertible matrix <m>P</m> such that <m>A'=P^{-1}AP</m>.
      </p>
    </statement>
  </definition>
  <p>
    So any two matrix representations of a linear transformation <m>T\colon V\rightarrow V</m> are similar in the technical sense of <xref ref="d_similar"/>. In fact, a converse of sorts is also true, as articulated in the theorem below.
  </p>
    <theorem xml:id="th_similarity_matrixreps">
      <title>Similarity and matrix representations</title>

      <statement>
        <p>
          Two <m>n\times n</m> matrices <m>A</m> and <m>A'</m> are similar if and only if there is a linear transformation <m>T\colon \R^n\rightarrow \R^n</m> and bases <m>B, B'</m> of <m>\R^n</m> satisfying <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m>.
        </p>
      </statement>
      <proof>
        <p>
          The discussion above shows that if <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m>, then <m>A'=P^{-1}AP</m>, where <m>P=\underset{B'\rightarrow B}{P}</m>; thus <m>A</m> and <m>A'</m> are similar in this case.
        </p>
        <p>
          Now assume that <m>A</m> and <m>A'</m> are similar. Then there is an invertible matrix <m>P</m> such that <m>A'=P^{-1}AP</m>. Letting <m>T=T_A</m>, we have <m>A=[T]_B</m> where <m>B</m> is the standard basis of <m>\R^n</m> (<xref ref="eg_matrixreps_matrixtransforms"/>). Next, if we let <m>B'</m> be the ordered basis whose <m>j</m>-th element is the <m>j</m>-th column of <m>P</m>, then we have <m>P=\underset{B'\rightarrow B}{P}</m> (<xref ref="eg_changebasis_standard"/>), and hence
          <me>
            A'=P^{-1}AP=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=[T]_{B'}
          </me>,
          as desired.
        </p>
      </proof>
    </theorem>
      <p>
      We will see in coming sections that similar matrices are indeed similar algebraically speaking: <ie />, they share many of the same properties. <xref ref="th_similarity_matrixreps"/> provides the theoretical foundation to understand why this should be so: if <m>A</m> and <m>A'</m> are similar, then they are two matrix representations of a common linear transformation <m>T</m>; their many shared properties are simply inherited from the single overlying linear transformation that they both represent! This circle of ideas is neatly encompassed by <xref ref="fig_comm_tent"/>.
    </p>
    <figure xml:id="fig_comm_tent">
      <title>Holy commutative tent of linear algebra: <m>P=\underset{B\rightarrow B'}{P}</m>, <m>B=P^{-1}AP</m></title>
      <caption>Holy commutative tent of linear algebra: <m>P=\underset{B\rightarrow B'}{P}</m>, <m>B=P^{-1}AP</m>
    </caption>
      <image xml:id="im_holycomm" width="100%" source="images/im_holycomm"/>
      <!-- <image xml:id="im_holycomm" width="100%">
        <latex-image>
          \begin{tikzcd}
          \ampV \arrow[rrr, "T"] \arrow[ddl, leftrightarrow, "{[\hspace{5pt}]_B}"'] \arrow[drr, leftrightarrow,"{[\hspace{5pt}]_{B'}}"]  \amp \amp \amp V \arrow[ddl, leftrightarrow,"{[\hspace{5pt}]_B}", pos=11/20] \arrow[drr, leftrightarrow, "{[\hspace{5pt}]_{B'}}"]\\
          \amp  \amp   \amp\R^n \arrow[rrr, "A' "] \arrow[dlll, "P^{-1}"', pos=.4] \amp \amp \amp \R^n \\
          \R^n \arrow[rrr, "A"']\amp   \amp \amp \R^n  \arrow[urrr, "P"']\amp
          \end{tikzcd}
        </latex-image>
      </image> -->
    </figure>
    <p>
    Perhaps a little exegesis is in order here. Think of the map <m>T\colon V\rightarrow V</m> as a linear transformation up in abstract heaven. The two slanted sides of the tent are both commutative diagrams (<xref ref="fig_comm_diag"/>), and give us two models of <m>T</m> as matrices <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m>. Think of these as earthly shadows of <m>T</m>. We translate back and forth between these worldly <m>\R^n</m> models and the linear transformation <m>T</m> up in abstract heaven via the vertical coordinate vector transformations <m>[\phantom{\boldv}]_B</m> and <m>[\phantom{\boldv}]_{B'}</m>. On the earthly plane the two models <m>A</m> and <m>A'</m> are related via the equation <m>A'=P^{-1}AP</m>,
    where <m>P=\underset{B\rightarrow B'}{P}</m>. This can be interpreted as saying that the base of our tent is also a commutative diagram. It follows that all faces of our tent are commutative diagrams (including the triangular end faces); hence we have a commutative tent. 
    </p>

  </subsection>
<xi:include href="./s_changeofbasis_ex.ptx"/>
</section>
