<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_changeofbasis">
  <title>Change of basis</title>
  <introduction>
    <p>
      Coordinate vectors and matrix reprsentations work in tandem to model vectors in an abstract vector space <m>V</m> as column vectors in <m>\R^n</m>, and linear transformations <m>T\colon V\rightarrow W</m> as <m>m\times n</m> matrices.
      In both cases the model depends on our choice of basis. In this section we investigate how different basis choices affect these various models. Specifically, we consider the two questions below.
      <ol>
        <li>
          <p>
            Given <m>V</m> and two ordered bases <m>B</m> and <m>B'</m>,
            what is the algebraic relation between
            <m>[\boldv]_B</m> and <m>[\boldv]_{B'}</m>?
          </p>
        </li>
        <li>
          <p>
            Given <m>T\colon V\rightarrow V</m> and two ordered bases <m>B</m> and <m>B'</m>,
            what is the relation between
            <m>[T]_{B}</m> and <m>[T]_{B'}</m>?
          </p>
        </li>
      </ol>
    </p>
    <p>
      We will tackle each question in turn.
      Both answers rely on something called a
      <em>change of basis matrix</em>
      <m>\underset{B\rightarrow B'}{P}</m>.
    </p>

  </introduction>
<subsection xml:id="ss_change_of_basis" >
  <title>Change of basis matrix</title>
  <p>
    We define change of basis matrices via a column-by-column formula and motivate the definition retroactively with <xref ref="th_change_of_basis_coordinates"/>.
  </p>
  <definition xml:id="d_change_of_basis">
    <title>Change of basis matrix</title>
    <idx><h>change of basis matrix</h></idx>
    <notation>
      <usage><m>\underset{B\rightarrow B'}{P}</m></usage>
      <description>change of basis matrix</description>
    </notation>
    <statement>
      <p>
        Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> and <m>B'</m> be two ordered bases for the vector space <m>V</m>. The <term>change of basis from <m>B</m> to <m>B'</m></term> is the <m>n\times n</m> matrix <m>\underset{B\rightarrow B'}{P}</m> defined as
        <me>
        \underset{B\rightarrow B'}{P}=
        \begin{bmatrix}
          \vert \amp \vert \amp \amp \vert \\
          \phantom{v}[\boldv_1]_{B'} \amp \phantom{v}[\boldv_2]_{B'}\amp \dots \amp \phantom{v}[\boldv_n]_{B}\\
          \vert \amp \vert \amp \amp \vert
        \end{bmatrix}
        </me>.
        In other words, the <m>j</m>-th column of <m>\underset{B\rightarrow B'}{P}</m> is obtained by computing the coordinate vector of the <m>j</m>-th element of the <em>original</em> basis <m>B</m> with respect to the <em>new</em> basis <m>B'</m>.
      </p>
    </statement>
  </definition>
  <theorem xml:id="th_change_of_basis_coordinates">
    <title>Change of basis for coordinate vectors</title>
    <statement>
      <p>
        Let <m>B</m> and <m>B'</m> be two ordered bases of the <m>n</m>-dimensional vector space <m>V</m>.
        <ol>
          <li>
            <p>
              Recall that <m>\id_V\colon V\rightarrow V</m> is the identity transformation (<xref ref="d_transform_zero_identity"/>), defined as <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>. We have
              <me>
                \underset{B\rightarrow B'}{P}=[\id_V]_{B}^{B'}
              </me>.
            </p>
          </li>
          <li>
            <p>
              For all <m>\boldv\in V</m> we have
              <men xml:id="eq_changebasis_matrix_prop">
              \underset{B\rightarrow B'}{P}[\boldv]_B=[\boldv]_{B'}
            </men>.
              In other words, to convert the <m>B</m>-coordinates of a vector <m>\boldv\in V</m> to <m>B'</m>-coordinates, simply multiply on the left by the matrix  <m>\underset{B\rightarrow B'}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Property <xref ref="eq_changebasis_matrix_prop"/> defines <m>\underset{B\rightarrow B'}{P}</m> uniquely: <ie />, if <m>A</m> satisfies <m>A[\boldv]_B=[\boldv]_{B'}</m> for all <m>\boldv\in \R^n</m>, then <m>A=\underset{B\rightarrow B'}{P}</m>.
            </p>
          </li>
        </ol>

      </p>
    </statement>
    <proof>
      <ol>
        <li>
          <p>
            Let <m>I_V\colon V\rightarrow V</m> be the identity transformation: <ie />, <m>I_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>. By <xref ref="th_matrixrep"/> the matrix <m>[I_V]_B^{B'}</m> is the unique matrix satisfying
            <me>
            [I_V]_B^{B'}[\boldv]_B=[I_V(\boldv)]_{B'}=[\boldv]_{B'}
            </me>.
            Comparing the formulas for <m>[I_V]_{B}^{B'}</m> and <m></m> we see directly that
            <me>
            [I_V]_B^{B'} = \underset{B\rightarrow B'}{P}
            </me>.
          </p>
        </li>
        <li>
          <p>
            This follows from (1) and <xref ref="th_matrixrep"/>:
            <md>
              <mrow> \underset{B\rightarrow B'}{P}[\boldv]_B \amp = [\id_V]_{B}^{B'}[\boldv]_B </mrow>
              <mrow>  \amp = [\id_V(\boldv)]_{B'} \amp (<xref ref="th_matrixrep"/>)</mrow>
              <mrow> \amp =[\boldv]_{B'} </mrow>
            </md>.
          </p>
        </li>
        <li>
          <p>
            By (2) of <xref ref="th_matrixrep"/> (the uniqueness claim), if <m>A</m> satisfies <m>A[\boldv]_B=[\boldv]_{B'}</m> for all <m>\boldv\in \R^n</m>, then <m>A=[\id_V]_{B}^{B'}</m>. Since <m>[\id_V]_{B}^{B'}=\underset{B\rightarrow B'}{P}</m>, we conclude <m>A=\underset{B\rightarrow B'}{P}</m>.
          </p>
        </li>
      </ol>

    </proof>

  </theorem>
<example>
  <statement>
    <p>
      Let <m>V=\R^2</m>, <m>B=(\boldv_1=(1,1),\boldv_2=(1,-1))</m>, <m>B'=(\boldw_1=(1,2), \boldw_2=(2,-1))</m>. Observe that <m>B</m> and <m>B'</m> are both orthogonal with resepct to the dot product. This will simplify your computations below.
      <ol>
        <li>
          <p>
            Compute <m>\underset{B\rightarrow B'}{P}</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>\boldx=(4,-2)</m>. Compute <m>[\boldx]_{B'}</m> using <xref ref="eq_changebasis_matrix_prop"/>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          Using <xref ref="d_change_of_basis"/>, we have
          <md>
          <mrow>\underset{B\rightarrow B'}{P}\amp = \begin{bmatrix}\vert \amp \vert \\ \hspace{7pt}[\boldv_1]_{B'} \amp \hspace{7pt}[\boldv_2]_{B'}\\ \vert \amp \vert \end{bmatrix}</mrow>
          <mrow>\amp = \begin{bmatrix}\frac{3}{5}\amp -\frac{1}{5}\\ \frac{1}{5}\amp \frac{3}{5} \end{bmatrix}  </mrow>
          </md>.
          Here the two coordinate vector computations <m>[(1,1)]_{B'}=(3/5, 1/5)</m> and <m>[(1,-1)]_{B'}=(-1/5,3/5)</m> were done using <xref ref="th_coordinates_orthogonal"/>.
        </p>
      </li>
      <li>
        <p>
          First we easily compute <m>[\boldx]_{B}=(1, 3)</m>, again using <xref ref="th_coordinates_orthogonal"/>. Now use <xref ref="eq_changebasis_matrix_prop"/>:
          <md>
            <mrow> [\boldx]_{B'} \amp =\underset{B\rightarrow B'}{P}[\boldx]_B </mrow>
            <mrow> \amp = \begin{bmatrix}\frac{3}{5}\amp -\frac{1}{5}\\ \frac{1}{5}\amp \frac{3}{5} \end{bmatrix} \begin{amatrix}[c]1 \\ 3  \end{amatrix}</mrow>
            <mrow>  \amp = \begin{amatrix}[r]0\\ 2   \end{amatrix}</mrow>
          </md>.
          This should come as now surprise since
          <me>\boldx=(4,-2)=2(2,-1)=0\boldw_1+2\boldw_2
          </me>.
        </p>
      </li>
    </ol>
  </solution>
</example>
<example>
  <statement>
    <p>
      Let <m>V=P_2</m>, <m>B=(x^2,x,1)</m>, <m>B'=(p_1=(x-2)^2, p_2(x)=(x-2), p_3(x)=1)</m>.
    </p>
    <ol>
      <li>
        <p>
          Compute <m>\underset{B\rightarrow B'}{P}</m>.
        </p>
      </li>
      <li>
        <p>
          Compute <m>[x^2+x+1]_{B'}</m> using <xref ref="eq_changebasis_matrix_prop"/>.
        </p>
      </li>
    </ol>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          We have
          <md>
            <mrow>\underset{B\rightarrow B'}{P} \amp = \begin{bmatrix}
              \vert \amp \vert \amp \vert \\
              [x^2]_{B'}\amp [x]_{B'}\amp [1]_{B'} \\
              \vert \amp \vert \amp \vert
            \end{bmatrix} </mrow>
            <mrow> \amp = \begin{bmatrix}
              1\amp 0\amp 0 \\
              4\amp 1\amp 0\\
              4\amp 2\amp 1
            \end{bmatrix} </mrow>
          </md>.
          The first two coordinate vector computations are nontrivial; you can verify for yourself that <m>x^2=1(x-2)^2+4(x-2)+4</m> and  <m>x=0(x-2)^2+1(x-2)+2</m>. Alternatively, see  <xref ref="rm_change_of_basis_taylors"/>) for a neat trick for computing these coordinate vectors.
        </p>
      </li>
      <li>
        <p>
          Since <m>B</m> is the standard basis, we see easily that <m>[x^2+x+1]_{B}=(1,1,1)</m>. Using <xref ref="eq_changebasis_matrix_prop"/> we have
          <md>
            <mrow>[x^2+x+1]_{B'} \amp = \underset{B\rightarrow B'}{P}\begin{bmatrix}
              1\\ 1\\ 1
            \end{bmatrix}</mrow>
            <mrow> \amp =\begin{bmatrix}
              1\\ 5\\ 7
            \end{bmatrix} </mrow>
          </md>.
          Verify for yourself that we do indeed have
          <me>
            x^2+x+1=1(x-2)^2+5(x-2)+7
          </me>.
        </p>
      </li>
    </ol>
  </solution>
</example>
    <remark xml:id="rm_change_of_basis_taylors">
  <title>Taylor's formula and change of basis</title>
  <statement>
    <p>
      Let <m>B=(x^n, x^{n-1}, \dots, x, 1)</m> be the standard basis of <m>P_n</m>. Fix any constant <m>a\in \R</m>, and let <m>B'=((x-a)^n, (x-a)^{n-1}, \dots, (x-a), 1)</m>. It is easy to see that <m>B'</m> is also an ordered basis: a simple degree argument shows that the polynomials <m>p_k(x)=(x-a)^k</m> are linearly independent. It follows from Taylor's theorem (from single-variable calculus) that given any polynomial <m>p\in P_n</m> we have
      <me>
        p(x)=p(a)+p'(a)(x-a)+\frac{p''(x)}{2}(x-a)^2+\cdots \frac{p^{(n)}(a)}{n!}(x-a)^n
      </me>.
      We call this expression the expansion of <m>p(x)</m> about <m>x=a</m>. In terms of coordinate vectors, this means that
      <men xml:id="eq_taylors">
        [p]_{B'}=(\frac{p^{(n)}(a)}{n!}, \frac{p^{(n-1)}(a)}{(n-1)!}, \dots, p'(a), p(a))
      </men>.
      In other words, Taylor's theorem provides a simple derivative formula for computing coordinate vectors with respect to the basis <m>B'</m>.
    </p>
  </statement>
</remark>
<p>
  The following properties are often useful when computing various change of basis matrices.
</p>
  <theorem xml:id="th_change_of_basis_properties">
    <title>Change of basis matrix properties</title>
    <statement>
      <p>
        Let <m>B, B', B''</m> be ordered bases for the <m>n</m>-dimensional vector space <m>V</m>.
      </p>
      <ol>
        <li>
          <p>
            We have
            <me>
            \underset{B\rightarrow B}{P}=I
            </me>.
          </p>
        </li>
        <li>
          <p>
            The matrix <m>\underset{B\rightarrow B'}{P}</m> is invertible. In fact, we have
            <me>
            \left(\underset{B\rightarrow B'}{P}\right)^{-1}=\underset{B'\rightarrow B}{P}
            </me>
          </p>
        </li>
        <li>
          <p>
            We have
            <me>
            \underset{B\rightarrow B''}{P}=\underset{B'\rightarrow B''}{P}\, \underset{B\rightarrow B'}{P}
            </me>.
          </p>
        </li>

      </ol>
    </statement>
    <proof>
      <ol>
        <li>
          <p>
          Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m>. By definition, the <m>j</m>-th column of <m>\underset{B\rightarrow B'}{P}</m> is <m>[\boldv_j]_B</m>. Since <m>\boldv_j=0\boldv_1+0\boldv_2+\cdots +1\boldv_j+\cdots</m>, we see that <m>[\boldv_j]_B=\bolde_j</m>, and hence that <m>\underset{B\rightarrow B'}{P}=I</m>, as claimed.
          </p>
        </li>
        <li>
          <p>
            Let <m>A=\underset{B\rightarrow B'}{P}</m> and <m>B=\underset{B'\rightarrow B"}{P}</m>. For any <m>\boldv\in V</m> we have
            <md>
              <mrow>BA[\boldv]_B \amp = \underset{B'\rightarrow B}{P}\, \underset{B\rightarrow B'}{P}[\boldv]</mrow>
              <mrow> \amp =\underset{B'\rightarrow B}{P}[\boldv]_{B'} </mrow>
              <mrow>  \amp = [\boldv]_{B}</mrow>
            </md>.
            It follows from (3) of <xref ref="th_change_of_basis_coordinates"/> that <m>BA=\underset{B\rightarrow B}{P}</m>. Since <m>\underset{B\rightarrow B}{P}=I</m> by (1), we conclude that <m>BA=I</m>, and hence <m>B=A^{-1}</m>, as desired.
          </p>
        </li>
        <li>
          <p>
            See <xref ref="ex_changebasis_three_bases"/>.
          </p>
        </li>
      </ol>
    </proof>

  </theorem>

<example xml:id="eg_changebasis_standard">
  <title><m>V=\R^n</m>, <m>B</m> standard basis</title>
  <statement>
    <p>
      Consider the special situation where <m>V=\R^n</m>,
      <m>B</m> is the standard basis,
      and <m>B'=\{\boldv_1,\dots,\boldv_n\}</m> is some nonstandard basis.
      In this case we have
    <md>
      <mrow> \underset{B'\rightarrow B}{P}\amp =\begin{bmatrix}\vert\amp\vert\amp  \amp \vert \\ [\boldv_1]_B\amp [\boldv_2]_B \cdots\amp [\boldv_n]_B\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix}
      </mrow>
      <mrow>  \amp = \begin{bmatrix}\vert\amp\vert\amp  \amp \vert \\ \boldv_1\amp \boldv_2\amp\cdots\amp \boldv_n\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix} \amp (B \text{ standard basis})</mrow>
    </md>.
    In other words, <m>\underset{B'\rightarrow B}{P}</m> is the matrix whose <m>j</m>-th column is just the <m>j</m>-th element of <m>B'</m>. Thus, in this situation we can compute <m>\underset{B'\rightarrow B}{P}</m> by placing the elements of <m>B'</m> as columns of a matrix, and then use (2) of <xref ref="th_change_of_basis_properties"/> to compute <m>\underset{B\rightarrow B'}{P}=\left(\underset{B'\rightarrow B}{P}\right)^{-1}</m>.
    </p>
  </statement>
</example>
<example>
  <statement>
    <p>
      Let <m>V=\R^2</m>, <m>B=((1,0),(0,1))</m>, <m>B'=\{(1,\sqrt{3}),(-\sqrt{3},1)\}</m>. Compute <m>P_{B\rightarrow B'}</m> and <m>\underset{B'\rightarrow B}{P}</m>.
    </p>
  </statement>
  <solution>
    <p>
      According to <xref ref="eg_changebasis_standard"/> we have
      <me>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix}</me>. We then compute
      <me>
      \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\left(\begin{bmatrix}1\amp -\sqrt{3}\\ \sqrt{3}\amp 1 \end{bmatrix} \right)^{-1}=\frac{1}{4}\begin{bmatrix}1\amp \sqrt{3}\\ -\sqrt{3}\amp 1 \end{bmatrix}
      </me>.
    </p>
  </solution>
</example>

</subsection>

<subsection xml:id="ss_changebasis_orthonormal">
  <title>Working with orthonormal bases</title>
  <p>
    Let <m>B</m> and <m>B'</m> be ordered bases of an <m>n</m>-dimensional inner product space <m>(V,\langle\, , \rangle)</m>. Not surprisingly, if one or more of <m>B</m> and <m>B'</m> are orthornormal, our change of basis computations are simplified significantly. For example, if <m>B'</m> is orthonormal (or even orthogonal), the coordinate vector computations necessary to compute <m>\underset{B\rightarrow B'}{P}</m> can be done using the inner product formula of <xref ref="th_coordinates_orthogonal"/>. Furthermore, as we see below, if both <m>B</m> and <m>B'</m> are orthonormal, then the columns of <m>\underset{B\rightarrow B'}{P}</m> form an orthonormal basis of <m>\R^n</m>, in which case we can compute <m>\underset{B'\rightarrow B}{P}</m> as
    <me>
      \underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}=\left(\underset{B\rightarrow B'}{P}\right)^T
    </me>.
    In other words, when <m>B</m> and <m>B'</m> are orthonormal bases, the change of basis matrix is what we call an <em>orthogonal matrix</em>.
  </p>
  <definition xml:id="d_orthogonal_matrix">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
      An invertible <m>n\times n</m> matrix <m>A</m> is <term>orthogonal</term> if <m>A^{-1}=A^T</m>.
      </p>
    </statement>
  </definition>
      <remark xml:id="rm_orthogonal_matrices">
    <statement>
      <p>
        Since for an invertible matrix <m>A</m> we have <m>(A^T)^{-1}=(A^{-1})^T</m> it follows immediately from <xref ref="d_orthogonal_matrix"/> that
        <me>
          A \text{ is orthogonal}\iff A^T \text{ is orthogonal} \iff A^{-1} \text{ is orthogonal}
        </me>.
      </p>
    </statement>
  </remark>
<example>
  <statement>
    <p>
      The matrix
      <me>
      A=\begin{amatrix}[rr]\frac{\sqrt{2}}{2}\amp -\frac{\sqrt{2}}{2}\\ \frac{\sqrt{2}}{2} \amp \frac{\sqrt{2}}{2} \end{amatrix}
      </me> is orthogonal, as one easily checks by computing <m>A^TA</m>. Observe that the columns of <m>A</m> form an orthonormal set with respect to the dot product, as do the rows. This is not a coincidence!
    </p>
  </statement>
</example>
  <theorem xml:id="th_orthogonal_matrices">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. The following statements are equivalent.
      </p>
      <ol>
        <li>
          <p>
            The matrix <m>A</m> is orthogonal.
          </p>
        </li>
        <li>
          <p>
            The columns of <m>A</m> form an orthonormal basis of <m>\R^n</m> with respect to the dot product.
          </p>
        </li>
        <li>
          <p>
            The rows of <m>A</m> form an orthonormal basis of <m>\R^n</m> with respect to the dot product.
          </p>
        </li>
      </ol>
    </statement>
    <proof>
      <p>
         Let <m>\boldr_i</m> and <m>\boldc_i</m> be the <m>i</m>-th row and column of <m>A</m>, respectively, for each <m>1\leq i\leq n</m>. From <xref ref="th_dotproduct_method"/> we see that
        <mdn>
          <mrow xml:id="eq_orthogonal_matrixleft">A^TA \amp=[\boldc_i\cdot \boldc_j]_{1\leq i,j\leq n} </mrow>
          <mrow xml:id="eq_orthogonal_matrixright">AA^T \amp=[\boldr_i\cdot \boldr_j]_{1\leq i,j\leq n} </mrow>
        </mdn>.
        We use here that rows of <m>A^T</m> are the columns of <m>A</m>, and the columns of <m>A^T</m> are the rows of <m>A</m>. From <xref first="eq_orthogonal_matrixleft" last="eq_orthogonal_matrixright"/> it follows easily that
        <md>
          <mrow>A^{-1}=A^T \amp \iff A^TA=I </mrow>
          <mrow> \amp \iff \boldc_i\cdot\boldc_j=\begin{cases} 1\amp \text{if } i=j\\ 0\amp \text{if } i\ne j\end{cases} </mrow>
          <mrow>  \amp \iff \{\boldc_1,\boldc_2,\dots, \boldc_n\} \text{ is orthonormal}</mrow>
          <mrow>  \amp \iff \{\boldc_1,\boldc_2,\dots, \boldc_n\} \text{ is an orthonormal basis}\amp (n=\dim\R^n)</mrow>
        </md>,
        and
        <md>
          <mrow>A^{-1}=A^T \amp \iff AA^T=I </mrow>
          <mrow> \amp \iff \boldr_i\cdot\boldr_j=\begin{cases} 1\amp \text{if } i=j\\ 0\amp \text{if } i\ne j\end{cases} </mrow>
          <mrow>  \amp \iff \{\boldr_1,\boldr_2,\dots, \boldr_n\} \text{ is orthonormal}</mrow>
          <mrow>  \amp \iff \{\boldr_1,\boldr_2,\dots, \boldr_n\} \text{ is an orthonormal basis} \amp (n=\dim\R^n)</mrow>
        </md>.
        This proves <m>(1)\iff (2)</m> and <m>(1)\iff (3)</m>. The result follows.
      </p>
    </proof>
  </theorem>
      <remark xml:id="rm_orthogonal_matrices_misnomer">
    <statement>
      <p>
        It is somewhat unfortunate that the property of being an <em>orthogonal</em> matrix is equivalent to your rows or columns forming an <em>orthonormal</em> basis. You ask: Why not simply call such matrices <em>orthonormal</em> matrices? My answer: tradition!
      </p>
    </statement>
  </remark>

<theorem xml:id="th_changebasis_orthonormal">
  <title>Orthonormal change of basis</title>
  <statement>
    <p>
      Let <m>(V,\langle\, , \rangle)</m> be a finite dimensional inner product space, and suppose <m>B</m> and <m>B'</m> are orthonormal bases of <m>V</m>.
    </p>
    <ol>
      <li>
        <p>
          The matrices <m>\underset{B\rightarrow B'}{P}</m> and <m>\underset{B'\rightarrow B}{P}</m> are orthogonal.
        </p>
      </li>
      <li>
        <p>
          We have
          <me>
            \underset{B'\rightarrow B}{P}=\left( \underset{B\rightarrow B'}{P} \right)^T
          </me>.
        </p>
      </li>
    </ol>
  </statement>
  <proof>
    <ol>
      <li>
        <p>
          Let <m>B=(\boldv_1, \boldv_2,\dots, \boldv_n)</m>. By definition, the columns of <m>\underset{B\rightarrow B'}{P}</m> are the coordinate vectors <m>[\boldv_i]_{B'}</m>, <m>1\leq i\leq n</m>. By <xref ref="ex_coordinates_orthonormal"/>, these coordinate vectors form an orthonormal subset of <m>\R^n</m>; since there are <m>n=\dim\R^n</m> of them, they form an orthonormal basis. From <xref ref="th_orthogonal_matrices"/> it follows that <m>\underset{B\rightarrow B'}{P}</m> is orthogonal.
          Lastly, from <xref ref="rm_orthogonal_matrices"/> it follows that <m>\underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}</m> is also orthogonal.
        </p>
      </li>
      <li>
        <p>
          Since <m>\underset{B\rightarrow B'}{P}</m> is orthogonal, we have
          <me>
            \underset{B'\rightarrow B}{P}=\left(\underset{B\rightarrow B'}{P}\right)^{-1}=\left(\underset{B\rightarrow B'}{P}\right)^T
          </me>.
        </p>
      </li>
    </ol>
  </proof>

</theorem>

</subsection>
  <subsection>
    <title>Change of basis for transformations</title>
    <p>
      We now investigate how our choice of basis affects matrix representations of linear transformations.
      We will only consider the special case where
      <m>T\colon V\rightarrow V</m> and we are comparing matrix representations <m>[T]_B</m> and
      <m>[T]_{B'}</m> for two different ordered bases of <m>V</m>.
    </p>
    <theorem xml:id="th_change_of_basis_transformations">
      <title>Change of basis for transformations</title>
      <statement>
        <p>
          Let <m>V</m> be finite-dimensional,
          let <m>T\colon V\rightarrow V</m> be linear,
          and let <m>B</m> and <m>B'</m> be two bases for <m>V</m>. The matrices <m>[T]_B</m> and <m>[T]_{B'}</m> representing <m>T</m> with respect to <m>B</m> and <m>B'</m>, respectively,  are related as follows:
          <md>
          <mrow>[T]_{B'}\amp =\underset{B\rightarrow B'}{P}\, [T]_B\, \underset{B'\rightarrow B}{P}</mrow>[2ex]
          <mrow>\amp =\underset{B'\rightarrow B}{P}^{-1}\, [T]_B\, \underset{B'\rightarrow B}{P}</mrow>
          </md>.
        </p>
      </statement>
    </theorem>
  </subsection>
  <remark xml:id="rm_change_of_basis_transformations">
    <title>Getting change of basis formulas correct</title>
    <statement>
      <p>
        It is easy to get the various details of the change of basis formula wrong.
        Here is a potential way to keep things organized in your mind.
        <ol>
          <li>
            <p>
              We wish to relate <m>[T]_{B'}</m> and <m>[T]_B</m> with an equation of the form <m>[T]_{B'}=*[T]_B*</m>,
              where the asterisks are to be replaced with change of basis matrices or their inverses.
              Think of the three matrices on the right-hand side of this equation  as a sequence of three things done to coordinate vectors,
              reading from right to left.
            </p>
          </li>
          <li>
            <p>
              <m>[T]_{B'}</m> takes as inputs <m>B'</m>-coordinates of vectors,
              and outputs <m>B'</m>-coordinates.
              Thus the same should be true for <m>*[T]_B*</m>.
            </p>
          </li>
          <li>
            <p>
              Since <m>[T]_B</m> takes as inputs <m>B</m>-coordinates,
              we must <em>first</em> convert from <m>B'</m>-coordinates to <m>B</m>-coordinates.
              So we should have <m>[T]_{B'}=*[T]_B\underset{B'\rightarrow B}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Since <m>[T]_B</m> outputs <m>B</m>-coordinates,
              we need to then convert back to <m>B'</m>-coordinates.
              Thus <m>[T]_{B'}=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              If desired you may replace
              <m>\underset{B\rightarrow B'}{P}</m> with <m> \underset{B'\rightarrow B}{P}^{-1}</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </remark>

  <subsection>
    <title>Example</title>
    <p>
      Let <m>T\colon P_2\rightarrow P_2</m> be defined as <m>T(p(x))=p(x)+2p'(x)+xp''(x)</m>.
      <ol>
        <li>
          <p>
            Let <m>B=\{1, x, x^2\}</m>.
            Compute <m>[T]_B</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>B'=\{1+x+x^2, 1+x, 1+x^2\}</m>.
            Use the change of basis formula to compute <m>[T]_{B'}</m>.
          </p>
        </li>
      </ol>
    </p>
    <p>
      We easily compute <m>[T]_B=\begin{bmatrix}1\amp 2\amp 0\\ 0\amp 1\amp 4\\ 0\amp 0\amp 1 \end{bmatrix}</m> using our usual recipe.
    </p>
    <p>
      We can also easily compute <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 0\\ 1\amp 0\amp 1 \end{bmatrix}</m>,
      essentially by inspection.
    </p>
    <p>
      (In general it is easy to compute the change of basis matrix from a nonstandard basis to the standard basis.)
    </p>
    <p>
      It follows that
      <md>
      <mrow>_B'\amp =\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=\left( \underset{B'\rightarrow B}{P}\right)^{-1}[T]_B\underset{B'\rightarrow B}{P}</mrow>
      <mrow>\amp =\left( \begin{bmatrix} 1\amp 1\amp 1</mrow>
      <mrow>1\amp 1\amp 0</mrow>
      <mrow>1\amp 0\amp 1 \end{bmatrix}\right)^{-1}\begin{bmatrix} 1\amp 2\amp 0</mrow>
      <mrow>0\amp 1\amp 4</mrow>
      <mrow>0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix} 1\amp 1\amp 1</mrow>
      <mrow>1\amp 1\amp 0</mrow>
      <mrow>1\amp 0\amp 1 \end{bmatrix}= \begin{amatrix}[rrr] 3\amp -2\amp 4</mrow>
      <mrow>2\amp 3\amp 0</mrow>
      <mrow>-2\amp 2\amp -3 \end{amatrix}</mrow>
      </md>
    </p>
  </subsection>
  <definition xml:id="d_similar">
    <idx><h>similar matrices</h></idx><statement>
      <p>
        Matrices <m>A, A'\in M_{nn}</m> are <term>similar</term>
        if there is an invertible matrix <m>P</m> such that <m>A'=P^{-1}AP</m>.
      </p>
    </statement>
  </definition>
  <remark xml:id="rm_holy_commutative_tent">
    <p>
      As we will see in coming sections, matrices that are similar in the techinical sense given by <xref ref="d_similar"/> are indeed similar algebraically speaking: <ie />, similar matrices share many of the same properties.
      We now have the theoretical foundation to understand why this is so:
      they simply inherit these common properties from the overlying linear transformation <m>T</m>,
      of which they are but earthly shadows. (See <xref ref="fig_comm_tent"/>.) There is but one true <m>T</m>!
    </p>
  </remark>

  <!-- <figure xml:id="fig_comm_tent">
    <caption>Holy commutative tent of linear algebra: <m>P=\underset{B\rightarrow B'}{P}</m>, <m>B=P^{-1}AP</m></caption>
    <image xml:id="im_holycomm">
      <latex-image>
        \begin{tikzcd}
        \amp V \arrow[rrr, "T"] \arrow[ddl, leftrightarrow, "{[\hspace{5pt}]_B}"'] \arrow[drr, leftrightarrow,"{[\hspace{5pt}]_{B'}}"]  \amp \amp \amp V \arrow[ddl, leftrightarrow,"{[\hspace{5pt}]_B}", pos=11/20] \arrow[drr, leftrightarrow, "{[\hspace{5pt}]_{B'}}"]\\
        \amp  \amp   \amp\R^n \arrow[rrr, "T_B"] \arrow[dlll, "P^{-1}"', pos=.4] \amp \amp \amp \R^n \\
        \R^n \arrow[rrr, "T_A"']\amp   \amp \amp \R^n  \arrow[urrr, "P"']\amp
        \end{tikzcd}
      </latex-image>
    </image>

  </figure> -->
  <figure xml:id="fig_comm_tent">
    <caption>Holy commutative tent of linear algebra: <m>P=\underset{B\rightarrow B'}{P}</m>, <m>B=P^{-1}AP</m>
  </caption>
    <image xml:id="im_holycomm" source="./images/im_holycomm.svg"/>

  </figure>

  <subsection>
    <title>Change of basis for <m>V=\R^n</m></title>

    <p>
      Let's consider the special case where <m>T\colon \R^n\rightarrow \R^n</m>: that is, when <m>V=\R^n</m> is a space of <m>n</m>-tuples. We know from <xref ref="cor_matrix_transformations"/> that <m>T=T_A</m> for a unique <m>n\times n</m> matrix <m>A</m>; in fact we know from the proof that <m>A=[T]_B</m>, where <m>B=(\bolde_1, \bolde_2, \dots, \bolde_n)</m> is the <em>standard</em> ordered basis of <m>\R^n</m>.
    </p>
    <p>
      To compute <m>A=[T]_B</m> directly, we must compute <m>T(\bolde_j)</m> for each of the standard basis elements <m>\bolde_j</m>. However, for many naturally occurring transformations <m>T</m>, computing with respect to the standard basis is often not as convenient as computing with respect to some other basis <m>B'</m>: <ie />, it is often easier to compute <m>A'=[T]_{B'}</m> for some nonstandard basis <m>B'</m>. When this is the case <xref ref="th_change_of_basis_transformations"/> allows to us derive the desired matrix <m>A</m> from the more conveniently computed <m>A'</m>: namely, we have
      <me>
        A=P^{-1}A'P
      </me>,
      where <m>P=\underset{B\rightarrow B'}{P}</m>.
    </p>
    <p>
      This gives us a powerful technique for computing matrix formulas for many interesting geometric linear transformations of <m>\R^n</m> whose very definitions involve an implicit choice nonstandard basis. Rotations, reflections and orthogonal projections are all examples of such transformations.
    </p>


  </subsection>

  <subsection>
    <title>Example: orthogonal projection revisited</title>
    <p>
      Let <m>T\colon \R^3\rightarrow\R^3</m> be orthogonal projection onto the plane <m>\mathcal{P}: x+y+z=0</m>,
      as defined earlier.
      We would like to derive a formula for <m>T</m>,
      which amounts to finding the <m>A</m> such that <m>T=T_A</m>.
    </p>
    <p>
      As previously observed we have <m>A=[T]_B</m>,
      where <m>B</m> is the <em>standard basis</em> for <m>\R^3</m>.
      We can compute <m>[T]_B</m> by first computing
      <m>[T]_{B'}</m> for a cleverly chosen
      <em>nonstandard</em> basis <m>B'</m>,
      and then using the change of basis formula.
    </p>
    <p>
      As done previously, we let <m>B'=\{(1,-1,0), (1,0,-1), (1,1,1)</m>.
      Since <m>T</m> maps the first two vectors to themselves,
      and the third vector to <m>(0,0,0)</m>,
      we have <m>[T]_{B'}=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}</m>.
      (Go back to original example for details.)
    </p>
    <p>
      Then
      <md>
        <mrow>A\amp =[T]_B=\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}</mrow>
        <mrow>\amp =\begin{amatrix}[rrr] 1\amp 1\amp 1</mrow>
        <mrow>-1\amp 0\amp 1</mrow>
        <mrow>0\amp -1\amp 1 \end{amatrix} \begin{bmatrix} 1\amp 0\amp 0</mrow>
        <mrow>0\amp 1\amp 0</mrow>
        <mrow>0\amp 0\amp 0 \end{bmatrix} \left( \begin{amatrix}[rrr] 1\amp 1\amp 1</mrow>
        <mrow>-1\amp 0\amp 1</mrow>
        <mrow>0\amp -1\amp 1 \end{amatrix}\right)^{-1} =\frac{1}{3}\begin{amatrix}[rrr] 2\amp -1\amp -1</mrow>
        <mrow>-1\amp 2\amp -1</mrow>
        <mrow>-1\amp -1\amp 2 \end{amatrix}</mrow>
      </md>
    </p>
    <p>
      Lo and behold,
      we have rediscovered our matrix formula for orthogonal projection onto <m>\mathcal{P}</m>!!
    </p>
    <p>
      (Note: since <m>B</m> is the standard basis in this case,
      <m>\underset{B'\rightarrow B}{P}</m> was easy to compute. )
    </p>
  </subsection>
<xi:include href="./s_changeofbasis_ex.ptx"/>
</section>
