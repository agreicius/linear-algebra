<section xml:id="ss_orthogonality">
  <title>Orthogonality, Gram-Schmidt, orthogonal projection</title>
  <paragraphs>
    <title>Orthogonal sets</title>
    <definition>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space.
        </p>
        <p>
          A set <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> of
          <term>nonzero vectors</term> is <term>orthogonal</term>
          if <m>\langle\boldv_i,\boldv_j\rangle=0</m> for all <m>i\ne j</m>:
          i.e., the elements are pairwise orthogonal.
        </p>
        <p>
          An orthogonal set that further satisfies
          <m>\norm{\boldv_i}=1</m> for all <m>i</m> is called <term>orthonormal</term>.
        </p>
      </statement>
    </definition>
    <theorem>
      <statement>
        <p>
          Let <m>(V,\langle\ , \rangle)</m> be an inner product space.
          If <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> is orthogonal,
          then <m>S</m> is linearly independent.
        </p>
      </statement>
    </theorem>
    <proof>
      <md>
        <mrow>a_1\boldv_1 +a_2\boldv_2+\cdots +a_r\boldv_r=\boldzero\amp \Rightarrow\amp  \langle a_1\boldv_1 +a_2\boldv_2 +\cdots +a_r\boldv_r,\boldv_i\rangle=\langle\boldzero,\boldv_i\rangle</mrow>
        <mrow>\amp \Rightarrow\amp  a_1\langle\boldv_1,\boldv_i\rangle +a_2\langle \boldv_2,\boldv_i\rangle +\cdots +a_r\langle\boldv_r,\boldv_i\rangle=0</mrow>
        <mrow>\amp \Rightarrow\amp  a_i\langle \boldv_i,\boldv_i\rangle=0 \ \text{ (since \(\langle\boldv_j,\boldv_i\rangle= 0\) for \(j\ne i\)) }</mrow>
        <mrow>\amp \Rightarrow\amp  a_i=0  \text{ (since \(\langle\boldv_i,\boldv_i\rangle\ne 0\)) }</mrow>
      </md>
      <p>
        We have shown that if <m>a_1\boldv_1+a_2\boldv_2+\cdots +a_r\boldv_r=\boldzero</m>,
        then <m>a_i=0</m> for all <m>i</m>,
        proving that <m>S</m> is linearly independent.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=C([0,2\pi])</m> with standard inner product <m>\langle f, g\rangle=\int_0^{2\pi} f(x)g(x) \, dx</m>.
    </p>
    <p>
      Let
      <me>
        S=\{\cos(x),\sin(x),\cos(2x),\sin(2x), \dots\}=\{\cos(nx)\colon n\in\Z_{>0}\}\cup\{\sin(mx)\colon m\in\Z_{>0}\}
      </me>.
    </p>
    <p>
      Then <m>S</m> is orthogonal, hence linearly independent.
    </p>
    <proof>
      <p>
        Using some trig identities, one can show the following:
        <md>
          <mrow>\langle \cos(nx),\cos(mx)\rangle=\int_0^{2\pi}\cos(nx)\cos(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \sin(nx),\sin(mx)\rangle=\int_0^{2\pi}\sin(nx)\sin(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \cos(nx),\sin(mx)\rangle=\int_0^{2\pi}\cos(nx)\sin(mx)\, dx\amp =0 \text{ for any \(n,m\) }</mrow>
        </md>
      </p>
    </proof>
    <p>
      Orthogonality holds more generally if we replace the interval
      <m>[0,2\pi]</m> with any interval of length <m>L</m>, and replace <m>S</m> with
      <me>
        \scriptsize \left\{\cos\left(\frac{2\pi x}{L}\right), \sin\left(\frac{2\pi x}{L}\right), \cos\left(2\cdot\frac{2\pi x}{L}\right),\sin\left(2\cdot\frac{2\pi x}{L}\right),\dots\right\}
      </me>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Orthogonal basis</title>
    <definition>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space,
          <m>\dim V=n</m>.
        </p>
        <p>
          An <term>orthogonal basis</term> is a basis
          <m>B=\{\boldv_1,\dots,\boldv_n\}</m> that is an orthogonal set;
          an orthogonal basis <m>B</m> is <term>orthonormal</term>
          if <m>B</m> is orthonormal.
        </p>
      </statement>
    </definition>
    <theorem>
      <title>Existence/extension of orthonormal bases</title>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be a finite-dimensional inner product vector space.
        </p>
        <p>
          Then <em>(1)</em> <m>V</m> has an ortho(gonal/normal) basis.
          (Proof is the <em>Gram-Schmidt</em> process.
          See two slides on.)
        </p>
        <p>
          Furthermore,
          <em>(2)</em> any ortho(gonal/normal) set can be <em>extended</em>
          to an ortho(gonal/normal) basis of <m>V</m>.
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <theorem>
    <title>Calculating with ortho(gonal/normal) bases</title>
    <statement>
      <p>
        Let <m>B=\{\boldv_1,\dots,\boldv_n\}</m> be an
        <em>orthogonal basis</em> for <m>V</m>.
      </p>
      <p>
        Given any <m>\boldv\in V</m> we have
        <m>\boldv=a_1\boldv_1+a_2\boldb_2+\cdots +a_n\boldv_n</m> where <m>a_i=\frac{\langle \boldv,\boldv_i\rangle}{\langle\boldv_i,\boldv_i\rangle}</m>.
      </p>
      <p>
        Equivalently,
        <me>
          [\boldv]_B=\begin{bmatrix}\frac{\langle \boldv,\boldv_1\rangle}{\langle\boldv_1,\boldv_1\rangle}\\ \frac{\langle \boldv,\boldv_2\rangle}{\langle\boldv_2,\boldv_2\rangle}\\ \vdots \\ \frac{\langle \boldv,\boldv_n\rangle}{\langle\boldv_n,\boldv_n\rangle} \end{bmatrix}
        </me>
      </p>
      <p>
        If <m>B</m> is ortho\alert{normal} then these formulas simplify to
        <me>
          a_i=\langle\boldv,\boldv_i\rangle \text{ for all \(i\) }
        </me>.
      </p>
    </statement>
  </theorem>
  <paragraphs>
  <title>Example</title>
  <p>
    Let <m>V=\R^2</m> with the standard inner produce
    (aka the dot product).
  </p>
  <p>
    (a) Verify that <m>B'=\{\boldv_1=(\sqrt{3}/2,1/2), \boldv_2=(-1/2,\sqrt{3}/2)\}</m> is an orthonormal basis.
  </p>
  <p>
    (b) Compute <m>[\boldv]_{B'}</m> for <m>\boldv=(4,2)</m>.
  </p>
  <p>
    (c) Compute <m>\underset{B\rightarrow B'}{P}</m>,
    where <m>B</m> is the standard basis. \begin{bsolution}
  </p>
  <p>
    (a) Easily seen to be true.
  </p>
  <p>
    (b) Since <m>B'</m> is orthonormal,
    <m>\boldv=a_1\boldv_1+a_2\boldv_2</m> where
    <m>a_1=\boldv\cdot\boldv_1=2\sqrt{3}+1</m> and <m>a_2=\boldv\cdot\boldv_2=\sqrt{3}-2</m>.
    Thus <m>[\boldv]_{B'}=\begin{bmatrix}2\sqrt{3}+1\\ \sqrt{3}-2 \end{bmatrix}</m>
  </p>
  <p>
    (c) As we have seen before,
    <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}\sqrt{3}/2\amp -1/2\\1/2\amp \sqrt{3}/2 \end{bmatrix}</m>
    (put elements of <m>B'</m> in as columns).
    Hence <m>\underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\begin{bmatrix}\sqrt{3}/2\amp 1/2\\-1/2\amp \sqrt{3}/2 \end{bmatrix}</m> \end{bsolution}
  </p>
  <paragraphs>
    <title>Useful fact</title>
    <p>
      If the columns of an <m>n\times n</m> matrix <m>P</m> are orthonormal,
      then <m>P</m> is invertible, and <m>P^{-1}=P^T</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Proof</title>
    <p>
      Let <m>\boldp_j</m> be the <m>j</m>-th column of <m>P</m>.
      By the <em>dot product method</em>
      of matrix multiplication,
      we have <m>P^TP=[\boldp_i\cdot\boldp_j]=I_n</m>,
      since the <m>\boldp_i</m> are orthonormal.
      This proves <m>P^T=P^{-1}</m>.
    </p>
  </paragraphs>
  </paragraphs>
  <paragraphs>
    <title>Gram-Schmidt Process</title>
    <p>
      The proof that every finite-dimensional vector space has an orthogonal basis is actually a procedure,
      called the <em>Gram-Schmidt process</em>,
      for converting an arbitrary basis for an inner product space to an orthogonal basis.
    </p>
    <theorem>
      <title>Gram-Schmidt process</title>
      <statement>
        <p>
          Let <m>(V, \langle \ , \ \rangle)</m> be an inner product space,
          and let <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> be a basis for <m>V</m>.
          We can convert <m>B</m> into an orthogonal basis
          <m>B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}</m> using the following recursive procedure:
          <ol>
            <li>
              <p>
                Set <m>\boldw_1=\boldv_1</m>.
              </p>
            </li>
            <li>
              <p>
                For <m>2\leq r\leq n</m> replace <m>\boldv_r</m> with
                <me>
                  \boldw_r:=\boldv_r-\frac{\angvec{\boldv_r, \boldw_{r-1}}}{\angvec{\boldw_{r-1},\boldw_{r-1}}}\boldw_{r-1}-\frac{\angvec{\boldv_r, \boldw_{r-2}}}{\angvec{\boldw_{r-2},\boldw_{r-2}}}\boldw_{r-2}-\cdots -\frac{\angvec{\boldv_r, \boldw_{1}}}{\angvec{\boldw_{1},\boldw_{1}}}\boldw_1
                </me>
              </p>
            </li>
          </ol>
        </p>
        <p>
          Lastly, to further transform to an orthonormal basis,
          replace each <m>\boldw_i</m> with <m>\ds \boldu_i=\frac{\boldw_i}{\norm{\boldw_i}}</m>.
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <paragraphs>
  <title>Orthogonal complement</title>
  <definition>
    <statement>
      <p>.
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
        The <term>orthogonal complement of <m>W</m></term> is defined as
        <me>
          W^\perp:=\{\boldv\in V\colon \langle \boldv, \boldw\rangle=0 \text{ for all } \boldw\in W\}
        </me>.
      </p>
      <p>
        In English: <m>W^\perp</m> is the set of vectors that are orthogonal to
        <term>all</term> vectors in <m>W</m>.
      </p>
    </statement>
  </definition>
  <theorem>
    <title>Orthogonal complement theorem</title>
    <statement>
      <p>
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a subspace.
        <ol>
          <li>
            <p>
              <m>W^\perp</m> is a subspace of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              <m>W\cap W^\perp=\{\boldzero\}</m>
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=\R^3</m> equipped with the dot product,
      and let <m>W=\Span\{(1,1,1)\}\subset \R^3</m>.
      This is the line defined by the vector <m>(1,1,1)</m>.
      Then <m>W^\perp</m> is the set of vectors orthogonal to <m>(1,1,1)</m>:
      i.e., the plane perpendicular to <m>(1,1,1)</m>.
    </p>
  </paragraphs>
  </paragraphs>
  <paragraphs>
    <title>Geometry of fundamental spaces</title>
    <p>
      The notion of orthogonal complement gives us a new way of understanding the relationship between the various fundamental spaces of a matrix.
    </p>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be <m>m\times n</m>,
          and consider <m>\R^n</m> and <m>\R^m</m> as inner product spaces with respect to the dot product.
          Then:
          <ol>
            <li>
              <p>
                <m>\NS(A)=\left(\RS(A)\right)^\perp</m>,
                and thus <m>\RS(A)=\left(\NS(A)\right)^\perp</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\NS(A^T)=\left(\CS(A)\right)^\perp</m>,
                and thus <m>\CS(A)=\left(\NS(A^T)\right)^\perp</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        (i) Using the dot product method of matrix multiplication,
        we see that a vector <m>\boldv\in\NS(A)</m> if and only if
        <m>\boldv\cdot\boldr_i=0</m> for each row <m>\boldr_i</m> of <m>A</m>.
        Since the <m>\boldr_i</m> span <m>\RS(A)</m>,
        the linear properties of the dot product imply that <m>\boldv\cdot\boldr_i=0</m> for each row
        <m>\boldr_i</m> of <m>A</m> if and only if <m>\boldv\cdot\boldw=0</m> for <em>all</em>
        <m>\boldw\in\RS(A)</m> if and only if <m>\boldv\in \RS(A)^\perp</m>.
      </p>
      <p>
        (ii) This follows from (i) and the fact that <m>\CS(A)=\RS(A^T)</m>.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Visualizing the rank-nullity theorem</title>
    <me>
      <image width="65%" source="images/RankNullity.png"/>
    </me>
  </paragraphs>
  <paragraphs>
    <title>Example</title>
    <p>
      Understanding the orthogonal relationship between <m>\NS(A)</m> and <m>\RS(A)</m> allows us in many cases to quickly determine/visualize the one from the other.
      Consider the example <m>A=\begin{bmatrix}1\amp -1\amp 1\\ 1\amp -1\amp -1 \end{bmatrix}</m>.
    </p>
    <p>
      Looking at the columns, we see easily that <m>\rank(A)=2</m>,
      which implies that <m>\nullity(A)=3-2=1</m>.
      Since <m>(1,-1,0)</m> is an element of <m>\NS(A)</m> and <m>\dim(\NS(A))=1</m>,
      we must have <m>\NS(A)=\Span\{(1,-1,0)\}</m>, a line.
    </p>
    <p>
      By orthogonality, we conclude that
      <me>
        \RS(A)=\NS(A)^\perp=\text{ (plane perpendicular to \((1,-1,0)\)) }
      </me>.
      <me>
        <image width="37%" source="images/NullRow.png"/>
      </me>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Orthogonal Projection</title>
    <theorem>
      <title>Orthogonal projection theorem</title>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space,
          and let <m>W\subseteq V</m> be a
          <em>finite-dimensional</em> subspace.
          <ol>
            <li>
              <p>
                (Orthogonal decomposition).
                For all <m>\boldv\in V</m> there is a unique choice of vectors <m>\boldw\in W</m> and
                <m>\boldw^\perp\in W^\perp</m> such that <m>\boldv=\boldw+\boldw^\perp</m>.
                We call this vector expression an
                <term>orthogonal decomposition</term> of <m>\boldv</m>,
                and denote <m>\boldw=\proj{\boldv}{W}</m> and <m>\boldw^\perp=\proj{\boldv}{W^\perp}</m>,
                the <term>orthogonal projections</term>
                of <m>\boldv</m> onto <m>W</m> and <m>W^\perp</m>, respectively.
              </p>
            </li>
            <li>
              <p>
                The orthogonal projection <m>\boldw=\proj{\boldv}{W}</m> is the element of <m>W</m> \alert{closest} to <m>\boldv</m>.
                This means <m>\norm{\boldv-\proj{\boldv}{W}}\leq\norm{\boldv-\boldw'}</m> for any other <m>\boldw'\in W</m>.
                We thus define the distance from <m>\boldv</m> to <m>W</m> as <m>d(\boldv, W)=\norm{\boldv-\proj{\boldv}{W}}=\norm{\boldw^\perp}</m>.
              </p>
            </li>
            <li>
              <p>
                (Orthogonal projection formula).
                Pick an <em>orthogonal</em> basis <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> of <m>W</m>.
                Then <m>\ds \proj{\boldv}{W}=\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
  </paragraphs>
  <paragraphs>
  <title>Proof of orthogonal projection theorem</title>
  <p>
    Pick an <em>orthogonal</em> basis
    <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> of <m>W</m> and set <m>\boldw=\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i</m>.
    This is clearly an element of <m>W</m>.
    Next we set <m>\boldw^\perp=\boldv-\boldw=\boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i</m>.
  </p>
  <p>
    To complete the proof, we must show the following: (A)
    <m>\boldw^\perp\in W^\perp</m>, (B) this choice of <m>\boldw</m> and <m>\boldw^\perp</m> is unique,
    and (C) <m>\boldw</m> is the closest element of <m>W</m> to <m>\boldv</m>.
  </p>
  <paragraphs>
    <title>(A)</title>
    <p>
      For all <m>i</m> we have
    </p>
  </paragraphs>
  <md>
    <mrow>\langle\boldw^\perp,\boldv_i\rangle\amp =\amp \langle \boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i, \boldv_i\rangle</mrow>
    <mrow>\amp =\amp \langle \boldv, \boldv_i\rangle-\langle \sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i ,\boldv_i\rangle \hspace{9pt} \text{ (distr.) }</mrow>
    <mrow>\amp =\amp \langle \boldv, \boldv_i\rangle-\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i,\boldv_i}}\langle\boldv_i,\boldv_i\rangle \hspace{9pt} \text{ (by orthogonality) }</mrow>
    <mrow>\amp =\amp 0</mrow>
  </md>
  <paragraphs>
    <title>(B)+(C)</title>
    <p>
      Recall: <m>\boldw</m> satisfies <m>\boldv=\boldw+\boldw^\perp</m>,
      where <m>\boldw^\perp\in W^\perp</m>.
      Now take any other <m>\boldw'\in W</m>.
      Then
    </p>
  </paragraphs>
  <md>
    <mrow>\norm{\boldv-\boldw'}^2\amp =\amp \norm{\boldw^\perp+(\boldw-\boldw')}^2
    =\norm{\boldw^\perp}^2+\norm{\boldw-\boldw'}^2 \hspace{9pt} \text{ (Pythag. theorem) }</mrow>
    <mrow>\amp \geq\amp  \norm{\boldw^\perp}^2=\norm{\boldv-\boldw}^2</mrow>
  </md>.
  <p>
    Taking square-roots now proves the desired inequality.
    Furthermore, we have <em>equality</em>
    iff the last inequality is an equality iff
    <m>\norm{\boldw''}=\norm{\boldw-\boldw'}=0</m> iff <m>\boldw=\boldw'</m>.
    This proves our choice of <m>\boldw</m> is the <em>unique</em>
    element of <m>W</m> minimizing the distance to <m>\boldv</m>!
  </p>
  </paragraphs>
  <corollary>
    <statement>
      <p>
        Let <m>(V,\angvec{\ , \ })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
        Then <m>(W^\perp)^\perp=W</m>.
      </p>
    </statement>
  </corollary>
  <proof>
    <p>
      Clearly <m>W\subseteq (W^\perp)^\perp</m>.
      For the other direction, take <m>\boldv\in (W^\perp)^\perp</m>.
      Using the <em>orthogonal projection theorem</em>,
      we can write <m>\boldv=\boldw+\boldw^\perp</m> with
      <m>\boldw\in W</m> and <m>\boldw^\perp\in W^\perp</m>.
      We will show <m>\boldw^\perp=\boldzero</m>.
    </p>
    <p>
      Since <m>\boldv\in (W^\perp)^\perp</m> we have <m>\angvec{\boldv,\boldw^\perp}=0</m>.
      Then we have
      <md>
        <mrow>0\amp =\angvec{\boldv,\boldw^\perp}</mrow>
        <mrow>\amp =\angvec{\boldw+\boldw^\perp,\boldw^\perp}</mrow>
        <mrow>\amp =\angvec{\boldw,\boldw^\perp}+\angvec{\boldw^\perp,\boldw^\perp} \amp \text{ (since \(W\perp W^\perp\)) }</mrow>
        <mrow>\amp =0+\angvec{\boldw^\perp,\boldw^\perp}</mrow>
      </md>
    </p>
    <p>
      Thus <m>\angvec{\boldw^\perp,\boldw^\perp}=0</m>.
      It follows that <m>\boldw^\perp=\boldzero</m>,
      and hence <m>\boldv=\boldw+\boldzero=\boldw\in W</m>.
    </p>
  </proof>
  <corollary>
    <statement>
      <p>
        Let <m>(V,\angvec{\ , \ })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
      </p>
      <p>
        Define <m>T\colon V\rightarrow V</m> as <m>T(\boldv)=\proj{\boldv}{W}</m>.
        Then <m>T</m> is a linear transformation.
      </p>
      <p>
        In other words,
        orthogonal projection onto <m>W</m> defines a linear transformation of <m>V</m>.
      </p>
    </statement>
  </corollary>
  <proof>
    <p>
      We must show that <m>T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)</m> for all
      <m>c,d\in\R</m> and <m>\boldv_1,\boldv_2\in V</m>.
      This is easily shown by picking an orthonormal basis
      <m>B=\{\boldv_1,\boldv_2, \dots, \boldv_r\}</m> of <m>W</m> and using the formula from the orthogonal projection theorem.
    </p>
  </proof>
  <paragraphs>
    <title>Projection onto lines and planes in $\R^3$</title>
    <p>
      Let's revisit orthogonal projection onto lines and planes in <m>\R^3</m> passing through the origin.
      Here the relevant inner product is dot product.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Projection onto a line $\ell$</title>
    <p>
      Any line in <m>\R^3</m> passing through the origin can be described as <m>\ell=\Span\{\boldv_0\}</m>,
      for some <m>\boldv_0=(a,b,c)\ne 0</m>.
      Since this is an orthogonal basis of <m>\ell</m>,
      by the orthogonal projection theorem we have,
      for any <m>\boldv=(x,y,z)</m>
      <me>
        \proj{\boldv}{\ell}=\frac{\boldv\cdot \boldv_0}{\boldv_0\cdot\boldv_0}\boldv_0=\frac{ax+by+cz}{a^2+b^2+c^2}(a,b,c)=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} \begin{bmatrix}x\\ y\\ z \end{bmatrix}
      </me>.
    </p>
    <p>
      We have re-derived the matrix formula for orthogonal projection onto <m>\ell</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Projection onto lines and planes in $\R^3$</title>
    <p>
      Let's revisit orthogonal projection onto lines and planes in <m>\R^3</m> passing through the origin.
      Here the relevant inner product is dot product.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Projection onto a plane</title>
    <p>
      Any plane in <m>\R^3</m> passing through the origin can be described with the equation
      <m>\mathcal{P}\colon ax+by+cz=0</m> for some <m>\boldn=(a,b,c)\ne 0</m>.
      This says precisely that <m>\mathcal{P}</m> is the orthogonal complement of the line <m>\ell=\Span\{(a,b,c)\}</m>:
      i.e., <m>\mathcal{P}=\ell^\perp</m>.
    </p>
    <p>
      From the orthogonal projection theorem, we know that
      <me>
        \boldv=\proj{\boldv}{\ell}+\proj{\boldv}{\ell^\perp}=\proj{\boldv}{\ell}+\proj{\boldv}{\mathcal{P}}
      </me>.
    </p>
    <p>
      But then
      <me>
        \proj{\boldv}{\mathcal{P}}=\boldv-\proj{\boldv}{\ell}=I \ \boldv-\proj{\boldv}{\ell}=(I-A)\boldv
      </me>,
      where <m>A</m> is the matrix formula for
      <m>\proj{\boldv}{\ell}</m> from the previous example.
      We conclude that the matrix defining <m>\proj{\boldv}{\mathcal{P}}</m> is
      <me>
        I-\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} = \frac{1}{a^2+b^2+c^2}\begin{bmatrix}b^2+c^2\amp -ab\amp -ac\\ -ab\amp a^2+c^2\amp -bc\\ -ac\amp -bc\amp a^2+b^2 \end{bmatrix}
      </me>
    </p>
  </paragraphs>
  <p>
    We can express this in terms of matrix multiplication as
  </p>
  <p>
    \item Translate the whole picture by <m>-Q=(-q_1,-q_2, -q_3)</m>,
    which means we replace <m>P=(x,y,z)</m> with
    <m>P-Q=(x-q_1,y-q_2,z-q_3)</m>. \item Apply our formulas from before,
    replacing <m>(x,y,z)</m> with
    <m>(x-q_1,y-q_2,z-q_3)</m> \item Translate back by adding <m>Q</m> to your answer.
  </p>
  <paragraphs>
    <title>Example: sine/cosine series</title>
    <p>
      Let <m>V=C[0,2\pi]</m> with inner product <m>\langle f, g\rangle=\int_0^{2\pi}f(x)g(x) \, dx</m>.
    </p>
    <p>
      We have seen that the set
      <me>
        B=\{1, \cos(x),\sin(x),\cos(2x),\sin(2x), \dots , \cos(nx),\sin(nx)\}
      </me>
      is orthogonal.
      Thus <m>B</m> is an orthogonal basis of <m>W=\Span(B)</m>,
      which we might describe as the space of
      <term>trigonometric polynomials of degree at most <m>n</m></term>.
    </p>
    <p>
      Given an arbitrary function <m>f(x)\in C[0,2\pi]</m>,
      its orthogonal projection onto <m>W</m> is the function
      <me>
        \hat{f}(x)=a_0+a_1\cos(x)+b_1\sin(x)+a_2\cos(2x)+b_2\sin(2x)+\cdots +a_n\cos(nx)+b_n\sin(nx)
      </me>,
      where
      <me>
        a_0=\frac{1}{2\pi}\int_0^{2\pi} f(x) \ dx, \ a_j=\frac{1}{\pi}\int_0^{2\pi}f(x)\cos(jx)\, dx, \ b_k=\frac{1}{\pi}\int_0^{2\pi}f(x)\sin(kx)\, dx
      </me>.
    </p>
    <p>
      The projection theorem tells us that <m>\hat{f}</m> is the
      <q>best</q>
      trigonometric polynomial approximation of <m>f(x)</m>
      (of degree at most <m>n</m>),
      in the sense that for any other sinusoidal <m>g\in W</m>,
      <m>\left\vert\left\vert f-\hat{f}\right\vert\right\vert\leq \norm{f-g}</m>.
    </p>
    <p>
      This means in turn
      <me>
        \int_0^{2\pi} (f-\hat{f})^2\, dx\leq \int_0^{2\pi} (f-g)^2 \, dx
      </me>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Example: least-squares solution to $A\boldx=\boldy$</title>
    <p>
      Often in applications we have an
      <m>m\times n</m> matrix <m>A</m> and vector
      <m>\boldy\in\R^m</m> for which the matrix equation
      <me>
        A\boldx=\boldy
      </me>
      has no solution.
      In terms of fundamental spaces,
      this means simply that <m>\boldy\notin \CS(A)</m>.
      Set <m>W=\CS(A)</m>.
    </p>
    <p>
      In such situations we speak of a <em>least-squares</em>
      solution to the matrix equation.
      This is a vector <m>\hat{\boldx}</m> such that <m>A\hat{\boldx}=\hat{\boldy}</m>,
      where <m>\hat{\boldy}=\proj{\boldy}{W}</m>.
      Here the inner product is taken to be the dot product.
    </p>
    <p>
      Note: the equation <m>A\hat{\boldx}=\hat{\boldy}</m> is guaranteed to have a solution since
      <m>\hat{\boldy}=\proj{\boldy}{W}</m> lies in <m>\CS(A)</m>.
    </p>
    <p>
      The vector <m>\hat{\boldx}</m> is called a least-square solutions because its image
      <m>\hat{\boldy}</m> is the element of <m>\CS(A)</m> that is
      <q>closest</q>
      to <m>\boldy</m> in terms of the dot product.
      Writing <m>\boldy=(y_1,y_2,\dots,y_n)</m> and <m>\hat{\boldy}=(y_1',y_2',\dots,
      y_n')</m>,
      this means that <m>\hat{\boldy}</m> minimizes the distance
      <me>
        \norm{\boldy-\hat{\boldy}}=\sqrt{(y_1-y_1')^2+(y_2-y_2')^2+\cdots +(y_n-y_n')^2}
      </me>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Least-squares example (curve fitting)</title>
    <p>
      Suppose we wish to find an equation of a line <m>y=mx+b</m> that best fits
      (in the least-square's sense)
      the following <m>(x,y)</m> data points:
      <m>P_1=(-3,1), P_2=(1,2), P_3=(2,3)</m>.
    </p>
    <p>
      Then we seek <m>m</m> and <m>b</m> such that
      <md>
        <mrow>1\amp =m(-3)+b</mrow>
        <mrow>2\amp =m(1)+b</mrow>
        <mrow>3\amp =m(2)+b</mrow>
      </md>,
      or equivalently,
      we wish to solve <m>\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} \begin{bmatrix}m \\ b \end{bmatrix} =\begin{bmatrix}1\\ 2\\ 3 \end{bmatrix}</m>.
    </p>
    <p>
      This equation has no solution as
      <m>\boldy=(1,2,3)</m> does no lie in <m>W=\CS(A)=\Span(\{(-3,1,2),(1,1,1)\}</m>.
      So instead we compute <m>\hat{\boldy}=\proj{\boldy}{W}=(13/14,33/14,38/14)</m>. (This was not hard to compute as conveniently the given basis of <m>W</m> was already orthogonal!)
    </p>
    <p>
      Finally we solve <m>A\begin{bmatrix}m\\ b \end{bmatrix} =\hat{\boldy}</m>,
      getting <m>m=5/14</m>, <m>b=28/14=2</m>.
      Thus <m>y=\frac{5}{14}x+2</m> is the line best fitting the data in the least-squares sense.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Least-squares example contd.</title>
    <p>
      In what sense does <m>y=\frac{5}{14}x+2</m>
      <q>best</q>
      fit the data?
      <me>
        <image width="37%" source="images/LeastSquares.png"/>
      </me>
    </p>
    <p>
      Let <m>\boldy=(1,2,3)=(y_1,y_2,y_3)</m> be the given <m>y</m>-values of the points,
      and <m>\hat{\boldy}=(y_1',y_2',y_3')</m> be the projection we computed before.
      In the graph the values <m>\epsilon_i</m> denote the vertical difference
      <m>\epsilon_i=y_i-y_i'</m> between the data points, and our fitting line.
    </p>
    <p>
      The projection <m>\hat{\boldy}</m> makes the error
      <m>\norm{\boldy-\hat{\boldy}}=\sqrt{ \epsilon_1^2+\epsilon_2^2+\epsilon_3^2}</m> as small as possible.
    </p>
    <p>
      This means if I draw <em>any other line</em>
      and compute the corresponding differences
      <m>\epsilon_i'</m> at the <m>x</m>-values -3, 1 and 2, then we have
      <me>
        \epsilon_1^2+\epsilon_2^2+\epsilon_3^2\leq (\epsilon_1')^2+(\epsilon_2')^2+(\epsilon_3')^2
      </me>
    </p>
  </paragraphs>
  <paragraphs>
  <title>Finding least squares solutions</title>
  <p>
    As the last example illustrated,
    one method of finding a least-squares solution <m>\boldx</m> to
    <m>A\boldx=\boldy</m> is to first produce an orthogonal basis for <m>\CS(A)</m>,
    then compute <m>\hat{\boldy}=\proj{\boldy}{\CS(A)}</m>,
    and then use GE to solve <m>A\boldx=\hat{\boldy}</m>.
  </p>
  <p>
    Alternatively, it turns out
    (through a little trickery)
    that <m>\hat{\boldy}=A\boldx</m>,
    where <m>\boldx</m> is a solution to the equation
    <me>
      A^TA\boldx=A^T\boldy
    </me>.
  </p>
  <p>
    This solves us the hassle of computing an orthogonal basis for <m>\CS(A)</m>;
    to find a least-squares solution <m>\boldx</m> for <m>A\boldx=\boldy</m>,
    we simply use GE to solve the boxed equation. (Some more trickery shows a solution is guaranteed to exist!)
  </p>
  <paragraphs>
    <title>Example</title>
    <p>
      In the previous example we were seeking a least-squares solution
      <m>\boldx=\colvec{m\\ b}</m> to <m>A\boldx=\boldy</m>,
      where <m>A=\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} , \boldy=\colvec{1\\2\\3}</m>.
    </p>
  </paragraphs>
  <p>
    The equation <m>A^TA\boldx=A^T\boldy</m> is thus
    <me>
      \begin{bmatrix}14\amp 0\\ 0\amp 3 \end{bmatrix} \boldx= \colvec{5\\ 6}
    </me>
  </p>
  <p>
    As you can see,
    <m>\boldx=\colvec{m\\ b}=\colvec{5/14\\ 2}</m> is a least-squares solution,
    just as before
  </p>
  </paragraphs>
</section>