<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_orthogonality">
  <title>Orthogonal bases</title>
  <subsection xml:id="ss_orthogonal">
    <title>Orthogonal vectors and sets</title>
    <definition>
      <title>Orthogonality</title>
      <idx><h>orthogonal</h><h>vectors</h></idx>
      <idx><h>orthogonal</h><h>sets</h></idx>
      <idx><h>orthonormal</h><h>basis</h></idx>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space. Vectors <m>\boldv, \boldw\in V</m> are <term>orthogonal</term> if <m>\langle \boldv, \boldw\rangle =0</m>.
        </p>
        <p>
          Let <m>S\subseteq V</m> be a set of <em>nonzero</em> vectors.
          <ul>
            <li>
              <p>
                The set <m>S</m> is <term>orthogonal</term>
                if <m>\langle\boldv,\boldw \rangle=0</m> for all <m>\boldv\ne\boldw\in S</m>. We say that the elements of <m>S</m>
                are <term>pairwise orthogonal</term> in this case.
              </p>
            </li>
            <li>
              <p>
                The set <m>S</m> is <term>orthonormal</term> if it is both orthogonal and satisfies
                <m>\norm{\boldv}=1</m> for all <m>\boldv\in S</m>: <ie />, <m>S</m> consists of pairwise orthogonal unit vectors.
              </p>
            </li>
          </ul>
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_orthogonal">
      <title>Orthogonal implies linearly independent</title>
      <statement>
        <p>
          Let <m>(V,\langle\ , \rangle)</m> be an inner product space.
          If <m>S</m> is orthogonal,
          then <m>S</m> is linearly independent.
        </p>
      </statement>
      <proof>
        <p>
          Given any distinct elements <m>\boldv_1, \boldv_2, \dots, \boldv_r\in S</m>, we have
        <md>
          <mrow>c_1\boldv_1 +c_2\boldv_2+\cdots +c_r\boldv_r=\boldzero\amp \Rightarrow\amp  \langle c_1\boldv_1 +c_2\boldv_2 +\cdots +c_r\boldv_r,\boldv_i\rangle=\langle\boldzero,\boldv_i\rangle</mrow>
          <mrow>\amp \Rightarrow\amp  c_1\langle\boldv_1,\boldv_i\rangle +c_2\langle \boldv_2,\boldv_i\rangle +\cdots +c_r\langle\boldv_r,\boldv_i\rangle=0</mrow>
          <mrow>\amp \Rightarrow\amp  c_i\langle \boldv_i,\boldv_i\rangle=0 \ \text{ (since \(\langle\boldv_j,\boldv_i\rangle= 0\) for \(j\ne i\)) }</mrow>
          <mrow>\amp \Rightarrow\amp  c_i=0  \text{ (since \(\langle\boldv_i,\boldv_i\rangle\ne 0\)) }</mrow>
        </md>.
        This proves that <m>S</m> is linearly independent.
        </p>
      </proof>
    </theorem>

    <example>
      <statement>
        <p>
          Show that the set <m>S=\{\boldx_1=(1,1,1),\boldx_2=(1,2,-3), \boldx_3=(-5,4,1)\}</m> is orthogonal with respect to the dot product. Explain why it follows that <m>S</m> is a basis of <m>\R^3</m>.
        </p>
      </statement>
      <solution>
        <p>
          A simple computation shows <m>\boldx_i\cdot \boldx_j=0</m> for all <m>1\leq i\ne j\leq 3</m>, showing that <m>S</m> is orthogonal. <xref ref="th_orthogonal"/> implies <m>S</m> is linearly independent. Since <m>\val{S}=\dim \R^3=3</m>, it follows from <xref ref="cor_street_smarts"/> that <m>S</m> is a basis.
        </p>
      </solution>
    </example>
  <example xml:id="eg_orthogonal_functions">
    <statement>
      <p>
        Let <m>V=C([0,2\pi])</m> with integral inner product <m>\langle f, g\rangle=\int_0^{2\pi} f(x)g(x) \, dx</m>, and let
        <me>
          S=\{\cos(x),\sin(x),\cos(2x),\sin(2x), \dots\}=\{\cos(nx)\colon n\in\Z_{>0}\}\cup\{\sin(mx)\colon m\in\Z_{>0}\}
        </me>.
      Show that <m>S</m> is orthogonal and hence linearly independent.
      </p>
    </statement>
    <solution>
      <p>
        Using the trigonometric identities
        <md>
          <mrow>\cos\alpha\cos\beta \amp =\frac{1}{2}(\cos(\alpha-\beta)+\cos(\alpha+\beta))</mrow>
          <mrow> \sin\alpha\sin\beta  \amp=\frac{1}{2}(\cos(\alpha-\beta)-\cos(\alpha+\beta)) </mrow>
          <mrow> \sin\alpha\cos\beta \amp =\frac{1}{2}(\sin(\alpha-\beta)+\sin(\alpha+\beta)) </mrow>
        </md>
        we easily compute
        <md>
          <mrow>\langle \cos(nx),\cos(mx)\rangle=\int_0^{2\pi}\cos(nx)\cos(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \sin(nx),\sin(mx)\rangle=\int_0^{2\pi}\sin(nx)\sin(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \cos(nx),\sin(mx)\rangle=\int_0^{2\pi}\cos(nx)\sin(mx)\, dx\amp =0 \text{ for any \(n,m\) }</mrow>
        </md>
      </p>
    </solution>
  </example>
</subsection>
  <subsection>
    <title>Orthogonal bases</title>
    <definition xml:id="d_orthogonal_basis">
      <title>Orthogonal and orthonormal bases</title>
      <idx><h>orthogonal</h><h>basis</h></idx>
      <idx><h>orthonormal</h><h>basis</h></idx>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space. An <term>orthogonal basis</term> (resp., <term>orthonormal basis</term>) of <m>V</m> is a basis <m>B</m> that is orthogonal (resp., orthonormal) as a set.
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_orthonormal_existence">
      <title>Existence of orthonormal bases</title>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space of dimension <m>n</m>.
        </p>
        <ol>
          <li>
            <p>
            There is an orthonormal basis for <m>V</m>. In fact, any basis of <m>V</m> can be converted to an orthonormal basis using the <xref ref="proc_gram-schmidt" text="custom">Gram-Schmidt procedure</xref>.
            </p>
          </li>
          <li>
            <p>
              If <m>S\subseteq V</m> is an orthogonal set, then there is an orthogonal basis <m>B</m> containing <m>S</m>: <ie />, any orthogonal set can be extended to an orthogonal basis.
            </p>
          </li>
        </ol>
      </statement>
      <proof>
        <ol>
          <li>
            <p>
              See <xref ref="proc_gram-schmidt"/> and its proof.
            </p>
          </li>
          <li>
            <p>
              The orthogonal set <m>S</m> is linearly independent by <xref ref="th_orthogonal"/>. Let <m>S=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> be the distinct elements of <m>S</m>. (We must have <m>r\leq n</m> since <m>S</m> is linearly independent.) By <xref ref="th_basis_contract_expand"/> we can extend <m>S</m> to a basis <m>B=\{\boldv_1,\dots, \boldv_r, \boldv_{r+1}, \dots, \boldv_n\}</m>. It is easy to see that when we apply the Gram-Schmidt procedure to <m>B</m>, the first <m>r</m> vectors are left unchanged, as they are already pairwise orthogonal. Thus Gram-Schmidt returns an orthogonal basis of the form <m>B'=\{\boldv_1,\dots, \boldv_r, \boldw_{r+1}, \dots, \boldw_n\}</m>, as desired.
            </p>
          </li>
        </ol>
      </proof>

    </theorem>
    <p>
      The proof that every finite-dimensional vector space has an orthogonal basis is actually a procedure,
      called the <em>Gram-Schmidt procedure</em>,
      for converting an arbitrary basis for an inner product space to an orthogonal basis.
    </p>
    <algorithm xml:id="proc_gram-schmidt">
      <title>Gram-Schmidt procedure</title>
      <idx><h>Gram-Schmidt procedure</h></idx>
      <statement>
        <p>
          Let <m>(V, \langle \ , \ \rangle)</m> be an inner product space,
          and let <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> be a basis for <m>V</m>.
          We can convert <m>B</m> into an orthogonal basis
          <m>B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}</m>, and further to an orthonormal basis <m>B''=\{\boldu_1, \boldu_2, \dots, \boldu_n\}</m>, as follows:
          <ol>
            <li>
              <p>
                Set <m>\boldw_1=\boldv_1</m>.
              </p>
            </li>
            <li>
              <title>Orthogonalize</title>
              <p>
                Proceeding in succession for each <m>2\leq r\leq n</m>, replace <m>\boldv_r</m> with
                <me>
                  \boldw_r:=\boldv_r-\frac{\angvec{\boldv_r, \boldw_{r-1}}}{\angvec{\boldw_{r-1},\boldw_{r-1}}}\boldw_{r-1}-\frac{\angvec{\boldv_r, \boldw_{r-2}}}{\angvec{\boldw_{r-2},\boldw_{r-2}}}\boldw_{r-2}-\cdots -\frac{\angvec{\boldv_r, \boldw_{1}}}{\angvec{\boldw_{1},\boldw_{1}}}\boldw_1
                </me>.
                The resulting set <m>B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}</m> is an orthogonal basis of <m>V</m>.
              </p>
            </li>
            <li>
              <title>Normalize</title>
              <p>
                For each <m>1\leq i\leq n</m> let
                <me>
                  \boldu_i=\frac{1}{\norm{\boldw_i}}\,\boldw_i
                </me>.
                The set <m>B''=\{\boldu_1, \boldu_2, \dots, \boldu_n\}</m> is an orthonormal basis of <m>V</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </algorithm>
  </subsection>
  <theorem xml:id="th_orthogonal_basis_formula">
    <title>Calculating with orthogonal bases</title>
    <statement>
      <p>
        Let <m>(V, \langle , \rangle )</m> be an inner product space.  and let
      </p>
      <ol>
        <li>
          <p>
            Let <m>B=\{\boldv_1,\dots,\boldv_n\}\subseteq V</m> be an
            orthogonal basis of <m>V</m>. For any <m>\boldv\in V</m> we have
            <me>
            \boldv=c_1\boldv_1+c_2\boldb_2+\cdots +c_n\boldv_n
            </me>, where
            <me>
            c_i=\frac{\langle \boldv,\boldv_i\rangle}{\langle\boldv_i,\boldv_i\rangle}</me>
            for all <m>1\leq i\leq n</m>.
            If <m>B</m> is <em>orthonormal</em>, so that <m>\langle \boldv_i, \boldv_i\rangle =1</m> for all <m>1\leq i\leq n</m>, then <m>c_i=\langle \boldv, \boldv_i\rangle</m>.
          </p>
        </li>
        <li>
          <title>Generalized Pythagorean theorem</title>
          <p>
            Let <m>B=\{\boldv_1,\dots,\boldv_n\}\subseteq V</m> be an
            orthonormal basis of <m>V</m>. Given <m>
            \boldv=c_1\boldv_1+c_2\boldb_2+\cdots +c_n\boldv_n
            </m>, we have
            <me>
              \norm{\boldv}=\sqrt{c_1^2+c_2^2+\cdots c_n^2}
            </me>.

          </p>
        </li>
      </ol>
    </statement>
  </theorem>
<example>
  <statement>
    <p>
      Consider the inner product space <m>V=\R^2</m> with the dot product.
      <ol>
        <li>
          <p>
            Verify that <m>B'=\{\boldv_1=(\sqrt{3}/2,1/2), \boldv_2=(-1/2,\sqrt{3}/2)\}</m> is an orthonormal basis of <m>\R^2</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>\boldv=(4,2)</m>. Find the scalars <m>c_1, c_2\in \R</m> such that <m>\boldv=c_1\boldv_1+c_2\boldv_2</m>.
          </p>
        </li>
        <li>
          <p>
            Verify that <m>\norm{\boldv}=\sqrt{c_1^2+c_2^2}</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          Easily verified.
        </p>
      </li>
      <li>
        <p>
          Using <xref ref="th_orthogonal_basis_formula"/> we compute
          <md>
            <mrow>c_1 \amp =\boldv\cdot \boldv_1=2\sqrt{3}+1  </mrow>
            <mrow> c_2\amp= \boldv\cdot \boldv_2=\sqrt{3}-2 </mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          Computing  directly yields <m>\norm{\boldv}=\sqrt{20}=2\sqrt{5}</m>. Using the generalized Pythagorean theorem we have
          <md>
            <mrow>\norm{\boldv} \amp= \sqrt{(2\sqrt{3}+1)^2+(\sqrt{3}-2)^2} </mrow>
            <mrow> \amp=\sqrt{(12+4\sqrt{3}+1)+(3-4\sqrt{3}+4)} </mrow>
            <mrow>  \amp = \sqrt{20}=2\sqrt{5}</mrow>
          </md>,
          as desired.
        </p>
      </li>
    </ol>
  </solution>
</example>

  <theorem xml:id="th_orthogonal_matrices">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. The following statements are equivalent.
      </p>
      <ol>
        <li>
          <p>
            <m>A</m> is invertible and <m>A^{-1}=A^T</m>.
          </p>
        </li>
        <li>
          <p>
            The columns of <m>A</m> are orthonormal.
          </p>
        </li>
        <li>
          <p>
            The rows of <m>A</m> are orthonormal.
          </p>
        </li>
        <li>
          <p>
            The columns (resp., rows) of <m>A</m> form an orthonormal basis of <m>\R^n</m>.
          </p>
        </li>
      </ol>
    </statement>
  </theorem>
  <definition xml:id="d_orthogonal_matrix">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
      An <m>n\times n</m> matrix <m>A</m> is <term>orthogonal</term> if it is invertible and <m>A^{-1}=A^T</m>. Equivalently, <m>A</m> is orthogonal if its columns (or rows) are orthonormal.
      </p>
    </statement>
  </definition>
  <subsection>
  <title>Orthogonal complement</title>
  <definition xml:id="d_orthogonal_complement">
    <title>Orthogonal complement</title>
    <idx><h>orthogonal</h><h>complement (of a subspace)</h></idx>
    <notation>
      <usage><m>W^\perp</m></usage>
      <description>the orthogonal complement of <m>W</m></description>
    </notation>
    <statement>
      <p>.
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a subspace.
        The <term>orthogonal complement of <m>W</m></term>, denoted <m>W^\perp</m>, is defined as
        <me>
          W^\perp=\{\boldv\in V\colon \langle \boldv, \boldw\rangle=0 \text{ for all } \boldw\in W\}
        </me>.
        In other words <m>W^\perp</m> is the set of vectors that are orthogonal to
        <term>all</term> elements of <m>W</m>.
      </p>
    </statement>
  </definition>
  <theorem xml:id="th_orthogonal_complement">
    <title>Orthogonal complement</title>
    <statement>
      <p>
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a subspace.
        <ol>
          <li>
            <p>
              The orthogonal complement <m>W^\perp</m> is a subspace of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              We have <m>W\cap W^\perp=\{\boldzero\}</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\dim V=n\lt \infty</m> then
              <me>
                \dim W+\dim W^\perp=n
              </me>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <example>
    <statement>
      <p>
        Consider the inner product space <m>\R^3</m> with the dot product. Let <m>W=\Span\{(1,1,1)\}\subset \R^3</m>, the line
        passing through the origin with direction vector <m>(1,1,1)</m>.
        The orthogonal complement <m>W^\perp</m> is the set of vectors orthogonal to <m>(1,1,1)</m>. Using the definition of dot product, this is the set of solutions <m>(x,y,z)</m> to the equation
        <me>
          x+y+z=0
        </me>,
          which we recognize as the plane passing through the origin with normal vector <m>(1,1,1)</m>. Note that we have
          <me>
            \dim W+\dim W^\perp=1+2=3,
          </me>
          as predicted in <xref ref="th_orthogonal_complement"/>.

      </p>
    </statement>
  </example>
    <p>
      The notion of orthogonal complement gives us a more conceptual way of understanding the relationship between the various fundamental spaces of a matrix.
    </p>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be <m>m\times n</m>,
          and consider <m>\R^n</m> and <m>\R^m</m> as inner product spaces with respect to the dot product.
          Then:
          <ol>
            <li>
              <p>
                <m>\NS(A)=\left(\RS(A)\right)^\perp</m>,
                and thus <m>\RS(A)=\left(\NS(A)\right)^\perp</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\NS(A^T)=\left(\CS(A)\right)^\perp</m>,
                and thus <m>\CS(A)=\left(\NS(A^T)\right)^\perp</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <ol>
          <li>
            <p>
              Using the dot product method of matrix multiplication,
              we see that a vector <m>\boldv\in\NS(A)</m> if and only if
              <m>\boldv\cdot\boldr_i=0</m> for each row <m>\boldr_i</m> of <m>A</m>.
              Since the <m>\boldr_i</m> span <m>\RS(A)</m>,
              the linear properties of the dot product imply that <m>\boldv\cdot\boldr_i=0</m> for each row
              <m>\boldr_i</m> of <m>A</m> if and only if <m>\boldv\cdot\boldw=0</m> for <em>all</em>
              <m>\boldw\in\RS(A)</m> if and only if <m>\boldv\in \RS(A)^\perp</m>.
            </p>
          </li>
          <li>
            <p>
              This follows from (1) and the fact that <m>\CS(A)=\RS(A^T)</m>.
            </p>
          </li>
        </ol>
      </proof>
    </theorem>

    <example>
      <statement>
        <p>
          Understanding the orthogonal relationship between <m>\NS(A)</m> and <m>\RS(A)</m> allows us in many cases to quickly determine/visualize the one from the other. As an example, consider <m>A=\begin{bmatrix}1\amp -1\amp 1\\ 1\amp -1\amp -1 \end{bmatrix}</m>. Looking at the columns, we see easily that <m>\rank(A)=2</m>,
          which implies that <m>\nullity(A)=3-2=1</m>.
          Since <m>(1,-1,0)</m> is an element of <m>\NS(A)</m> and <m>\dim(\NS(A))=1</m>,
          we must have <m>\NS(A)=\Span\{(1,-1,0)\}</m>, a line. By orthogonality, we conclude that
          <me>
            \RS(A)=\NS(A)^\perp=\text{ (plane perpendicular to \((1,-1,0)\)) }
          </me>.
        </p>
      </statement>
    </example>
  </subsection>

  <xi:include href="./s_orthogonal_bases_ex.ptx"/>
</section>
