<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_orthogonality">
  <title>Orthogonal bases</title>
  <subsection xml:id="ss_orthogonal">
    <title>Orthogonal vectors and sets</title>
    <definition>
      <title>Orthogonality</title>
      <idx><h>orthogonal</h><h>vectors</h></idx>
      <idx><h>orthogonal</h><h>sets</h></idx>
      <idx><h>orthonormal</h><h>basis</h></idx>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space. Vectors <m>\boldv, \boldw\in V</m> are <term>orthogonal</term> if <m>\langle \boldv, \boldw\rangle =0</m>.
        </p>
        <p>
          Let <m>S\subseteq V</m> be a set of <em>nonzero</em> vectors.
          <ul>
            <li>
              <p>
                The set <m>S</m> is <term>orthogonal</term>
                if <m>\langle\boldv,\boldw \rangle=0</m> for all <m>\boldv\ne\boldw\in S</m>. We say that the elements of <m>S</m>
                are <term>pairwise orthogonal</term> in this case.
              </p>
            </li>
            <li>
              <p>
                The set <m>S</m> is <term>orthonormal</term> if it is both orthogonal and satisfies
                <m>\norm{\boldv}=1</m> for all <m>\boldv\in S</m>: <ie />, <m>S</m> consists of pairwise orthogonal unit vectors.
              </p>
            </li>
          </ul>
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_orthogonal">
      <title>Orthogonal implies linearly independent</title>
      <statement>
        <p>
          Let <m>(V,\langle\ , \rangle)</m> be an inner product space.
          If <m>S</m> is orthogonal,
          then <m>S</m> is linearly independent.
        </p>
      </statement>
      <proof>
        <p>
          Given any distinct elements <m>\boldv_1, \boldv_2, \dots, \boldv_r\in S</m>, we have
        <md>
          <mrow>c_1\boldv_1 +c_2\boldv_2+\cdots +c_r\boldv_r=\boldzero\amp \Rightarrow\amp  \langle c_1\boldv_1 +c_2\boldv_2 +\cdots +c_r\boldv_r,\boldv_i\rangle=\langle\boldzero,\boldv_i\rangle</mrow>
          <mrow>\amp \Rightarrow\amp  c_1\langle\boldv_1,\boldv_i\rangle +c_2\langle \boldv_2,\boldv_i\rangle +\cdots +c_r\langle\boldv_r,\boldv_i\rangle=0</mrow>
          <mrow>\amp \Rightarrow\amp  c_i\langle \boldv_i,\boldv_i\rangle=0 \ \text{ (since \(\langle\boldv_j,\boldv_i\rangle= 0\) for \(j\ne i\)) }</mrow>
          <mrow>\amp \Rightarrow\amp  c_i=0  \text{ (since \(\langle\boldv_i,\boldv_i\rangle\ne 0\)) }</mrow>
        </md>.
        This proves that <m>S</m> is linearly independent.
        </p>
      </proof>
    </theorem>

    <example>
      <statement>
        <p>
          Show that the set <m>S=\{\boldx_1=(1,1,1),\boldx_2=(1,2,-3), \boldx_3=(-5,4,1)\}</m> is orthogonal with respect to the dot product. Explain why it follows that <m>S</m> is a basis of <m>\R^3</m>.
        </p>
      </statement>
      <solution>
        <p>
          A simple computation shows <m>\boldx_i\cdot \boldx_j=0</m> for all <m>1\leq i\ne j\leq 3</m>, showing that <m>S</m> is orthogonal. <xref ref="th_orthogonal"/> implies <m>S</m> is linearly independent. Since <m>\val{S}=\dim \R^3=3</m>, it follows from <xref ref="cor_street_smarts"/> that <m>S</m> is a basis.
        </p>
      </solution>
    </example>
  <example xml:id="eg_orthogonal_functions">
    <statement>
      <p>
        Let <m>V=C([0,2\pi])</m> with integral inner product <m>\langle f, g\rangle=\int_0^{2\pi} f(x)g(x) \, dx</m>, and let
        <me>
          S=\{\cos(x),\sin(x),\cos(2x),\sin(2x), \dots\}=\{\cos(nx)\colon n\in\Z_{>0}\}\cup\{\sin(mx)\colon m\in\Z_{>0}\}
        </me>.
      Show that <m>S</m> is orthogonal and hence linearly independent.
      </p>
    </statement>
    <solution>
      <p>
        Using the trigonometric identities
        <md>
          <mrow>\cos\alpha\cos\beta \amp =\frac{1}{2}(\cos(\alpha-\beta)+\cos(\alpha+\beta))</mrow>
          <mrow> \sin\alpha\sin\beta  \amp=\frac{1}{2}(\cos(\alpha-\beta)-\cos(\alpha+\beta)) </mrow>
          <mrow> \sin\alpha\cos\beta \amp =\frac{1}{2}(\sin(\alpha-\beta)+\sin(\alpha+\beta)) </mrow>
        </md>
        we easily compute
        <md>
          <mrow>\langle \cos(nx),\cos(mx)\rangle=\int_0^{2\pi}\cos(nx)\cos(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \sin(nx),\sin(mx)\rangle=\int_0^{2\pi}\sin(nx)\sin(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }</mrow>
          <mrow>\pi\amp  \text{ if \(n=m\) }  \end{cases}</mrow>
          <mrow>\langle \cos(nx),\sin(mx)\rangle=\int_0^{2\pi}\cos(nx)\sin(mx)\, dx\amp =0 \text{ for any \(n,m\) }</mrow>
        </md>
      </p>
    </solution>
  </example>
</subsection>
  <subsection>
    <title>Orthogonal bases</title>
    <definition xml:id="d_orthogonal_basis">
      <title>Orthogonal and orthonormal bases</title>
      <idx><h>orthogonal</h><h>basis</h></idx>
      <idx><h>orthonormal</h><h>basis</h></idx>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space. An <term>orthogonal basis</term> (resp., <term>orthonormal basis</term>) of <m>V</m> is a basis <m>B</m> that is orthogonal (resp., orthonormal) as a set.
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_orthogonal_basis_formula">
      <title>Calculating with orthogonal bases</title>
      <statement>
        <p>
          Let <m>(V, \langle , \rangle )</m> be an inner product space.  and let
        </p>
        <ol>
          <li>
            <p>
              Let <m>B=\{\boldv_1,\dots,\boldv_n\}\subseteq V</m> be an
              orthogonal basis of <m>V</m>. For any <m>\boldv\in V</m> we have
              <me>
              \boldv=c_1\boldv_1+c_2\boldb_2+\cdots +c_n\boldv_n
              </me>, where
              <me>
              c_i=\frac{\langle \boldv,\boldv_i\rangle}{\langle\boldv_i,\boldv_i\rangle}</me>
              for all <m>1\leq i\leq n</m>.
              If <m>B</m> is <em>orthonormal</em>, so that <m>\langle \boldv_i, \boldv_i\rangle =1</m> for all <m>1\leq i\leq n</m>, then <m>c_i=\langle \boldv, \boldv_i\rangle</m>.
            </p>
          </li>
          <li>
            <title>Generalized Pythagorean theorem</title>
            <p>
              Let <m>B=\{\boldv_1,\dots,\boldv_n\}\subseteq V</m> be an
              orthonormal basis of <m>V</m>. Given <m>
              \boldv=c_1\boldv_1+c_2\boldb_2+\cdots +c_n\boldv_n
              </m>, we have
              <me>
                \norm{\boldv}=\sqrt{c_1^2+c_2^2+\cdots c_n^2}
              </me>.

            </p>
          </li>
        </ol>
      </statement>
    </theorem>
  <example>
    <statement>
      <p>
        Consider the inner product space <m>V=\R^2</m> with the dot product.
        <ol>
          <li>
            <p>
              Verify that <m>B'=\{\boldv_1=(\sqrt{3}/2,1/2), \boldv_2=(-1/2,\sqrt{3}/2)\}</m> is an orthonormal basis of <m>\R^2</m>.
            </p>
          </li>
          <li>
            <p>
              Let <m>\boldv=(4,2)</m>. Find the scalars <m>c_1, c_2\in \R</m> such that <m>\boldv=c_1\boldv_1+c_2\boldv_2</m>.
            </p>
          </li>
          <li>
            <p>
              Verify that <m>\norm{\boldv}=\sqrt{c_1^2+c_2^2}</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <solution>
      <ol>
        <li>
          <p>
            Easily verified.
          </p>
        </li>
        <li>
          <p>
            Using <xref ref="th_orthogonal_basis_formula"/> we compute
            <md>
              <mrow>c_1 \amp =\boldv\cdot \boldv_1=2\sqrt{3}+1  </mrow>
              <mrow> c_2\amp= \boldv\cdot \boldv_2=\sqrt{3}-2 </mrow>
            </md>.
          </p>
        </li>
        <li>
          <p>
            Computing  directly yields <m>\norm{\boldv}=\sqrt{20}=2\sqrt{5}</m>. Using the generalized Pythagorean theorem we have
            <md>
              <mrow>\norm{\boldv} \amp= \sqrt{(2\sqrt{3}+1)^2+(\sqrt{3}-2)^2} </mrow>
              <mrow> \amp=\sqrt{(12+4\sqrt{3}+1)+(3-4\sqrt{3}+4)} </mrow>
              <mrow>  \amp = \sqrt{20}=2\sqrt{5}</mrow>
            </md>,
            as desired.
          </p>
        </li>
      </ol>
    </solution>
  </example>
  <p>
    <xref ref="th_orthogonal_basis_formula"/> gives a first glimpse how working with orthogonal and orthonormal bases can make life easier. The question remains, however: Can we always find an orthonormal basis? The <em>Gram-Schmidt procedure</em> gives an affirmative answer to this question, at least in the finite-dimensional case, as it provides a method of converting an arbitrary basis into an orthogonal one.
  </p>
  <algorithm xml:id="proc_gram-schmidt">
    <title>Gram-Schmidt procedure</title>
    <idx><h>Gram-Schmidt procedure</h></idx>
    <statement>
      <p>
        Let <m>(V, \langle \ , \ \rangle)</m> be an inner product space,
        and let <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> be a basis for <m>V</m>.
        We can convert <m>B</m> into an orthogonal basis
        <m>B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}</m>, and further to an orthonormal basis <m>B''=\{\boldu_1, \boldu_2, \dots, \boldu_n\}</m>, as follows:
        <ol>
          <li>
            <p>
              Set <m>\boldw_1=\boldv_1</m>.
            </p>
          </li>
          <li>
            <title>Orthogonalize</title>
            <p>
              Proceeding in succession for each <m>2\leq r\leq n</m>, replace <m>\boldv_r</m> with
              <me>
                \boldw_r:=\boldv_r-\frac{\angvec{\boldv_r, \boldw_{r-1}}}{\angvec{\boldw_{r-1},\boldw_{r-1}}}\boldw_{r-1}-\frac{\angvec{\boldv_r, \boldw_{r-2}}}{\angvec{\boldw_{r-2},\boldw_{r-2}}}\boldw_{r-2}-\cdots -\frac{\angvec{\boldv_r, \boldw_{1}}}{\angvec{\boldw_{1},\boldw_{1}}}\boldw_1
              </me>.
              The resulting set <m>B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}</m> is an orthogonal basis of <m>V</m>.
            </p>
          </li>
          <li>
            <title>Normalize</title>
            <p>
              For each <m>1\leq i\leq n</m> let
              <me>
                \boldu_i=\frac{1}{\norm{\boldw_i}}\,\boldw_i
              </me>.
              The set <m>B''=\{\boldu_1, \boldu_2, \dots, \boldu_n\}</m> is an orthonormal basis of <m>V</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </algorithm>
    <corollary xml:id="cor_orthonormal_existence">
      <title>Existence of orthonormal bases</title>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space of dimension <m>n</m>.
        </p>
        <ol>
          <li>
            <p>
            There is an orthonormal basis for <m>V</m>. In fact, any basis of <m>V</m> can be converted to an orthonormal basis using the <xref ref="proc_gram-schmidt" text="custom">Gram-Schmidt procedure</xref>.
            </p>
          </li>
          <li>
            <p>
              If <m>S\subseteq V</m> is an orthogonal set, then there is an orthogonal basis <m>B</m> containing <m>S</m>: <ie />, any orthogonal set can be extended to an orthogonal basis.
            </p>
          </li>
        </ol>
      </statement>
      <proof>
        <ol>
          <li>
            <p>
              See <xref ref="proc_gram-schmidt"/> and its proof.
            </p>
          </li>
          <li>
            <p>
              The orthogonal set <m>S</m> is linearly independent by <xref ref="th_orthogonal"/>. Let <m>S=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> be the distinct elements of <m>S</m>. (We must have <m>r\leq n</m> since <m>S</m> is linearly independent.) By <xref ref="th_basis_contract_expand"/> we can extend <m>S</m> to a basis <m>B=\{\boldv_1,\dots, \boldv_r, \boldv_{r+1}, \dots, \boldv_n\}</m>. It is easy to see that when we apply the Gram-Schmidt procedure to <m>B</m>, the first <m>r</m> vectors are left unchanged, as they are already pairwise orthogonal. Thus Gram-Schmidt returns an orthogonal basis of the form <m>B'=\{\boldv_1,\dots, \boldv_r, \boldw_{r+1}, \dots, \boldw_n\}</m>, as desired.
            </p>
          </li>
        </ol>
      </proof>
    </corollary>



  <theorem xml:id="th_orthogonal_matrices">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix. The following statements are equivalent.
      </p>
      <ol>
        <li>
          <p>
            <m>A</m> is invertible and <m>A^{-1}=A^T</m>.
          </p>
        </li>
        <li>
          <p>
            The columns of <m>A</m> are orthonormal.
          </p>
        </li>
        <li>
          <p>
            The rows of <m>A</m> are orthonormal.
          </p>
        </li>
        <li>
          <p>
            The columns (resp., rows) of <m>A</m> form an orthonormal basis of <m>\R^n</m>.
          </p>
        </li>
      </ol>
    </statement>
  </theorem>
  <definition xml:id="d_orthogonal_matrix">
    <title>Orthogonal matrices</title>
    <statement>
      <p>
      An <m>n\times n</m> matrix <m>A</m> is <term>orthogonal</term> if it is invertible and <m>A^{-1}=A^T</m>. Equivalently, <m>A</m> is orthogonal if its columns (or rows) are orthonormal.
      </p>
    </statement>
  </definition>
  </subsection>
  

  <xi:include href="./s_orthogonal_bases_ex.ptx"/>
</section>
