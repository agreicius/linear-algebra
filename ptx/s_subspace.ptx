<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_subspace">
  <title>Subspaces</title>
  <introduction>
    <p>
      The definition of a <em>subspace</em> of a vector space <m>V</m> is very much in the same spirit as our definition of linear transformations. It is a subset of <m>V</m> that in some sense respects the vector space structure: in the language of <xref ref="d_subspace"/>, it is a subset that is <em>closed under addition</em> and <em>closed under scalar multiplication</em>.
    </p>
    <p>
      In fact the connection between linear transformations and subspaces goes deeper than this. As we will see in <xref ref="d_nullspace_image"/>, a linear transformation <m>T\colon V\rightarrow W</m> naturally gives rise to two important subspaces: the <em>null space of <m>T</m></em> and the <em>image of <m>T</m></em>.
    </p>
  </introduction>
    <definition xml:id="d_subspace">
    <title>Subspace</title>
    <idx><h>subspace</h></idx>
    <idx><h>vector space</h><h>subspace</h></idx>
      <statement>
        <p>
          Let <m>V</m> be a vector space.
          A subset <m>W\subseteq V</m> is a
          <term>subspace</term> of <m>V</m> if the following conditions hold:
        </p>
          <ol label="i">
            <li>
              <title><m>W</m> contains the zero vector</title>
              <p>
                We have <m>\boldzero\in W</m>.
              </p>
            </li>
            <li>
              <title><m>W</m> is closed under addition</title>
              <p> For all <m>\boldv_1,\boldv_2\in V</m>, if <m>\boldv_1,\boldv_2\in W</m>, then <m>\boldv_1+\boldv_2\in W</m>. Using logical notation:
              <me>
                \boldv_1,\boldv_2\in W\implies \boldv_1+\boldv_2\in W
              </me>.
              </p>
            </li>
            <li>
              <title><m>W</m> is closed under scalar multiplication</title>
              <p> For all <m>c\in \R</m> and <m>\boldv\in V</m>, if <m>\boldv\in W</m>, then <m>c\boldv\in W</m>. In logical notation:
                <me>\boldv\in W\Rightarrow c\boldv\in W</me>.
              </p>
            </li>
          </ol>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>V=\R^2</m> and let
          <me>W=\{(t,t)\in\R^2 \colon t\in\R\}
          </me>.
          Prove that <m>W</m> is a subspace.
        </p>
      </statement>
      <solution>
        <p>
          We must show properties (i)-(iii) hold for <m>W</m>.
        </p>
        <ol label="i">
          <li>
            <p>
              The zero element of <m>V</m> is <m>\boldzero=(0,0)</m>,
              which is certainly of the form <m>(t,t)</m>.
              Thus <m>\boldzero\in W</m>.
            </p>
          </li>
          <li>
            <p>
              We must prove the implication <m>\boldv_1, \boldv_2\in W\Rightarrow \boldv_1+\boldv_2\in W</m>.
              <md>
                <mrow>\boldv_1,\boldv_2\in W\amp \Rightarrow\amp  \boldv_1=(t,t), \boldv_2=(s,s) \text{ for some \(t,s\in\R\) }</mrow>
                <mrow>\amp \Rightarrow\amp \boldv_1+\boldv_2=(t+s,t+s)</mrow>
                <mrow>\amp \Rightarrow\amp \boldv_1+\boldv_2\in W</mrow>
              </md>.
            </p>
          </li>
          <li>
            <p>
              We must prove the implication <m>\boldv\in W\Rightarrow c\boldv\in W</m>, for any <m>c\in \R</m>. We have
              <md>
                <mrow>\boldv\in W\amp \Rightarrow\amp  \boldv=(t,t)</mrow>
                <mrow>\amp \Rightarrow\amp  c\boldv=(ct,ct)</mrow>
                <mrow>\amp \Rightarrow\amp  c\boldv\in W</mrow>
              </md>
            </p>
          </li>
        </ol>
      </solution>
    </example>
    <example>
      <statement>
        <p>
          Let <m>V=\R^n</m> and let
          <me>
            W=\{(x,y)\in \R^2\colon x, y\geq 0\}
          </me>.
          Is <m>W</m> a vector space? Decide which of the of properties (i)-(iii) in <xref ref="d_subspace"/> (if any) are satisfied by <m>W</m>.
        </p>
      </statement>
      <solution>
        <ol label="i">
          <li>
            <p>
              Clearly <m>\boldzero=(0,0)\in W</m>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>\boldv_1=(x_1,y_1), \boldv_2=(x_2,y_2)\in W</m>. Then <m>x_1, x_2, y_1, y_2\geq 0</m>, in which case <m>x_1+x_2, y_1+y_2\geq 0</m>, and hence <m>\boldv_1+\boldv_2\in W</m>. Thus <m>W</m> is closed under addition.
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> is <em>not</em> closed under scalar multiplication. Indeed, let <m>\boldv=(1,1)\in W</m>. Then <m>(-2)\boldv=(-2,-2)\notin W</m>.
            </p>
          </li>
        </ol>
      </solution>
    </example>

        <remark xml:id="rm_twostep_proof">
          <title>Two-step proof for subspaces</title>
      <statement>
        <p>
        As with proofs regarding linearity of functions, we can merge conditions (ii)-(iii) of <xref ref="d_subspace"/> into a single statement about linear combinations, deriving the following two-step method for proving a set <m>W</m> is a subspace of a vector space <m>V</m>:
        </p>
        <ol label="i">
          <li>
            <p>
            Show <m>\boldzero_V\in W</m>
            </p>
          </li>
          <li>
            <p>
              Show that
              <me>
                \boldv_1, \boldv_2\in W\implies c\boldv_1+d\boldv_2\in W
              </me>,
              for all <m>c,d\in\R</m>.
            </p>
          </li>
        </ol>
      </statement>
    </remark>

        <remark xml:id="rm_subspace_is_vectorspace">
          <title>Subspaces are vector spaces</title>
      <statement>
        <p>
          If <m>W</m> is a subspace of a vector space <m>V</m>, then it <em>inherits</em> a vector space structure from <m>V</m> by simply  <em>restricting</em> the vector operations defined on <m>V</m> to the subset <m>W</m>.
        </p>
        <p>
           It is important to understand how conditions (ii)-(iii) of <xref ref="d_subspace"/> come into play here. Without them we would not be able to say that restricting the vector operations of <m>V</m> to elements of <m>W</m> actually gives rise to well-defined operations on <m>W</m>. To be well-defined the operations must output elements that lie not just in <m>V</m>, but in <m>W</m> itself. This is precisely what being closed under addition and scalar multiplication guarantees.
        </p>
        <p>
        Once we know restriction gives rise to well-defined operations on <m>W</m>, verifying the axioms of  <xref ref="d_vector_space"/> mostly amounts to observing that if a condition is true for all <m>\boldv</m> in <m>V</m>, it is certainly true for all <m>\boldv</m> in the subset <m>W</m>.
        </p>
        <p>
          The <q>existential axioms</q> (iii) and (iv) of <xref ref="d_vector_space"/>, however, require special consideration. By definition, a subspace <m>W</m> contains the zero vector of <m>V</m>, and clearly this still acts as the zero vector when we restrict the vector operations to <m>W</m>. What about vector inverses? We know that for any <m>\boldv\in W</m> there is a vector inverse <m>-\boldv</m> lying somewhere in <m>V</m>. We must show that in fact <m>-\boldv</m> lies in <m>W</m>: <ie /> we need to show that the operation of taking the vector inverse is well-defined on <m>W</m>. We prove this as follows:
          <md>
            <mrow>\boldv\in W \amp\implies (-1)\boldv\in W \amp (<xref ref="d_subspace"/>, \text{(iii) } )</mrow>
            <mrow> \amp\implies -\boldv\in W \amp (<xref ref="th_vectorspace_props"/>, (iii)) </mrow>
          </md>.
        </p>
      </statement>
    </remark>
<paragraphs xml:id="ss_nullspace_image">
  <title>Null space and image of a linear transformation</title>
  <p>
    Before looking at more examples, we first show how a linear transformation <m>T\colon V\rightarrow W</m> gives rise to two different subspaces: one insides <m>V</m>, and the other inside <m>W</m>. These are called the <em>null space</em> and <em>image</em> of <m>T</m>, respectively.
  </p>
  <definition xml:id="d_nullspace_image">
    <title>Null space and image</title>
    <idx><h>linear transformation</h><h>null space</h></idx>
    <idx><h>linear transformation</h><h>image</h></idx>
    <idx><h>null space</h></idx>
    <idx><h>image</h></idx>

    <statement>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation.
      </p>
      <ol>
        <li>
          <title>Null space</title>
          <p>
            The <term>null space</term> of <m>T</m>, denoted <m>\NS T</m>, is defined as
            <me>
              \NS T=\{\boldv\in V\colon T(\boldv)=\boldzero_W\}
            </me>.
          </p>
        </li>
        <li>
          <title>Image</title>
          <p>
            The <term>image</term> (or <term>range</term>) of <m>T</m>, denoted <m>\im T</m>, is defined as
            <me>
              \im T=\{\boldw\in W\colon \boldw=T(\boldv) \text{ for some } \boldv\in V\}
            </me>.
          </p>
        </li>
      </ol>
    </statement>
  </definition>
    <remark xml:id="rm_nullspace_image">
  <statement>
    <p>
    A few remarks:
    </p>
    <ol>
      <li>
        <p>
          Let <m>T\colon V\rightarrow W</m>. It is useful to keep in mind where <m>\NS T</m> and <m>\im T</m> <q>live</q> in this picture: we have <m>\NS T\subseteq V</m> and <m>\im T\subseteq W</m>. In other words, the null space is a subset of the domain, and the image is a subset of the codomain.
        </p>
      </li>
      <li>
        <p>
          More generally we can define the image (or range) <m>\im f</m>, for any function of sets <m>f\colon A\rightarrow B</m>: namely,
          <me>
            \im f=\{b\in B\colon b=f(a) \text{ for some } a\in A\}
          </me>.
          To decide whether or not an element <m>b\in B</m> is an element of <m>\im f</m>, we must determine whether there is an element <m>a\in A</m> such that <m>f(a)=b</m>. That is somewhat of a mouthful, and as such we often say simply <q><m>b</m> is hit by <m>f</m></q> (or not) for this condition.
        </p>
      </li>
      <li>
        <p>
          The notion of a null space is analogous to the set of zeros (or roots) of a real-valued function <m>f\colon X\rightarrow \R</m>,
          <me>
            \{x\in X\colon f(x)=0\}
          </me>,
          and <q>the zeros of <m>T</m></q> is a useful English shorthand for <m>\NS T</m>.
           However, there is an important difference between the null space of a linear transformation and the zeros of an arbitrary real-valued function: the null space of a linear transformation comes with the added structure of a vector space (<xref ref="th_nullspace_image"/>), whereas the zeros of an arbitrary function in general do not.
        </p>
        <p>
          The same observation can be made about the image of a linear transformation (<xref ref="th_nullspace_image"/>), in comparison to the image of an arbitrary function.
        </p>
      </li>
    </ol>
  </statement>
</remark>
<theorem xml:id="th_nullspace_image">
  <title>Null space and image</title>

  <statement>
    <p>
    If <m>T\colon V\rightarrow W</m> is a linear transformation, then <m>\NS T</m> is a subspace of <m>V</m>, and <m>\im T</m> is a subspace of <m>W</m>.
    </p>
  </statement>
  <proof>
      <case>
       <title>Null space of <m>T</m></title>
      <p>
      We use the two-step technique to prove <m>\NS T</m> is a subspace.
      </p>
      <ol>
        <li>
          <p>
            Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_V\in \NS T</m>.
          </p>
        </li>
        <li>
          <p>
            Suppose <m>\boldv_1, \boldv_2\in \NS T</m>. Given any <m>c,d\in \R</m>, we have
            <md>
              <mrow>T(c\boldv_1+d\boldv_2) \amp=cT(\boldv_1)+dT(\boldv_2) \amp (T \text{ is linear, } <xref ref="th_trans_props"/>)</mrow>
              <mrow> \amp=c\boldzero_W+d\boldzero_W \amp (\boldv_1, \boldv_2\in \NS T) </mrow>
              <mrow>  \amp = \boldzero_W</mrow>
            </md>.
            This shows that <m>c\boldv_1+d\boldv_2\in \NS T</m>, completing our proof.
          </p>
        </li>
      </ol>
      </case>
      <case>
       <title>Image of <m>T</m></title>
      <p>
        The proof proceeds in a similar manner.
      </p>
      <ol>
        <li>
          <p>
            Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_W</m> is <q>hit</q> by <m>T</m>, and hence is a member of <m>\im T</m>.
          </p>
        </li>
      </ol>
      </case>
  </proof>
</theorem>
<p>
  <xref ref="th_nullspace_image"/> provides us a with a useful indirect way of proving a subset <m>W\subseteq V</m> is a subspace:
  namely, find a linear transformation $T$ for which <m>W=\NS T</m>. (You could also identify <m>W=\im T</m> for some linear transformation <m>T</m>, but this approach tends to be not as convenient.
</p>
<example>
  <statement>
    <p>
      Prove that the set <m>W\subseteq\R^3</m> of all vectors <m>(x,y,z)</m> satisfying
      <m>x+2y+3z=x-y-z=0</m> is a subspace of <m>\R^3</m>.
    </p>
  </statement>
  <solution>
    <p>
        Let <me>A=\begin{bmatrix}1\amp 2\amp 3\\ 1\amp -1\amp -1 \end{bmatrix}</me>, and let
        <md>
          <mrow>T_A\colon \R^3 \amp\rightarrow \R^2 </mrow>
          <mrow> (x,y,z)\amp\mapsto (x+2y+3z, x-y-z) </mrow>
        </md>
        be its associated linear transformation. Then
        <md>
          <mrow> \NS T_A \amp =\{(x,y,z)\in\R^3\colon T_A(x,y,z)=(0,0)\} </mrow>
          <mrow> \amp = \{(x,y,z)\in\R^3\colon (x+2y+3z,x-y-z)=(0,0)\} </mrow>
          <mrow>  \amp = \{(x,y,z)\in\R^3\colon x+2y+3z=x-y-z=0\}</mrow>
          <mrow>  \amp = W</mrow>
        </md>.
        This shows that <m>W</m> can be identified as the null space of a linear transformation, and hence is a subspace of <m>\R^3</m>.
    </p>
  </solution>
</example>
<definition xml:id="d_symmetric_skewsymmetric">
  <title>Symmetric and skew-symmetric matrices</title>
  <statement>
    <p>
    A matrix <m>A\in M_{nn}</m> is <term>symmetric</term> if <m>A^T=A</m>. It is <term>skew-symmetric</term> if <m>A^T=-A</m>.
    </p>
  </statement>
</definition>
<example xml:id="eg_symmetric_skesymmetric_subspace">
  <title>Vector spaces of symmetric/skew-symmetric matrices</title>

  <statement>
    <p>
      Let <m>V=M_{nn}</m>, let <m>W_1</m> be the set of all symmetric <m>n\times n</m> matrices, and let <m>W_2</m> be the set of all skew-symmetric <m>n\times n</m> matrices. Prove that <m>W_1</m> and <m>W_2</m> are subspaces of <m>V</m>.
    </p>

  </statement>
  <solution>
    <p>
      Define functions <m>T_i\colon M_{nn}\rightarrow M_{nn}</m>, <m>i=1,2</m>, as follows:
      <md>
        <mrow>T_1(A) \amp = A^T-A </mrow>
        <mrow> T_2(A)\amp = A^T+A</mrow>
      </md>.
      Firstly, it is easy to see, using <xref ref="th_trans_props"/> that both <m>T_1</m> and <m>T_2</m> are both linear transformations. For example, for <m>T_1</m> we have
      <md>
        <mrow>T_1(cA+dB) \amp=(cA+dB)^T-(cA+dB) </mrow>
        <mrow> \amp=cA^T+dB^T-cA-cB </mrow>
        <mrow>  \amp = c(A^T-A)+d(B^T-B)</mrow>
        <mrow>  \amp = cT_1(A)+dT_1(B)</mrow>
      </md>.
      As similar proof applies to <m>T_2</m>.
    </p>
    <p>
      Next, it is easy to see that <m>W_1=\NS T_1</m> and <m>W_2=\NS T_2</m>. Taking <m>W_1</m> for example, we have
      <md>
        <mrow>A \text{ symmetric} \amp\iff A^T=A </mrow>
        <mrow> \amp\iff A^T-A=\boldzero_{n\times n} </mrow>
        <mrow>  \amp \iff T_1(A)=\boldzero_{n\times n}</mrow>
        <mrow>  \amp \iff A\in \NS T_1</mrow>
      </md>.
      Having identified <m>W_1</m> and <m>W_2</m> as null spaces of linear transformations, we conclude that they are subspaces of <m>M_{nn}</m>.
    </p>
  </solution>
</example>
<p>
  The next two examples illustrate how to compute the image of a given linear transformation <m>T\colon V\rightarrow W</m>. The examples make clear that computing images (i.e., determining the set of elements of <m>W</m> that are hit by <m>T</m>) is often more onerous than computing null spaces.
</p>
<example xml:id="ex_subspace_image">
  <title>Image computation</title>
  <statement>
    <p>
      Let <m>T=T_A\colon \R^2\rightarrow\R^3</m>,
      where <m>A=\begin{bmatrix}1\amp 1\\ 2\amp 1\\ 3\amp 5 \end{bmatrix}</m>.
      According to <xref ref="th_nullspace_image"/>, <m>\im T_A</m> is a subspace of <m>\R^3</m>.
      Identify this subspace as a familiar geometric object.
    </p>
  </statement>
  <solution>
    <p>
      By definition <m>\im T_A</m> is the set
      <me>
        \{\boldy\in\R^3\colon \boldy=T_A(\boldx) \text{ for some \(\boldx\in \R^3\) } \}=\left\{\boldy\colon \boldy=A\boldx \text{ for some \(\boldx\in\R^2\) } \right\}
      </me>.
    </p>
    <p>
      Thus to compute <m>\im T_A</m> we must determine which choice of
      <m>\boldy=(a,b,c)</m> makes the system <m>A\boldx=\boldy</m> consistent.
      We answer this using our old friend Gaussian elimination!
      <md>
        <mrow> \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 2\amp 1\amp b\\ 3\amp 5\amp c \end{amatrix} \amp \xrightarrow[r_3-3r_1]{r_2-2r_1} \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 0\amp 1\amp 2a-b\\ 0\amp 0\amp -7a+2b+c \end{amatrix}</mrow>
      </md>.
      To be consistent we need <m>-7a+2b+c=0</m>.
      We conclude that <m>\im T</m> is the set of all <m>(a,b,c)</m> satisfying <m>-7a+2b+c=0</m>.
      Geometrically this is the plane passing through <m>(0,0,0)</m> with normal vector <m>\boldn=(-7,2,1)</m>.
    </p>
  </solution>
</example>
<example>
  <statement>
    <p>
      Consider again the linear transformation <m>T_1\colon M_{nn}\rightarrow M_{nn}</m>,
      <m>T_1(A)=A^T-A</m>, from <xref ref="eg_symmetric_skesymmetric_subspace"/>.
      We saw that <m>\NS T_1</m> was the space of symmetric matrices.
      Identify <m>\im T_1</m> as a familiar subspace of <m>M_{nn}</m>.
    </p>
  </statement>
  <solution>
    <p>
      Take <m>B\in \im T_1</m>.
      By definition this means <m>B=T(A)=A^T-A</m> for some <m>A</m>.
      So one, somewhat unsatisfying way of describing
      <m>\im T_1</m> is as the set of all matrices of the form <m>A^T-A</m>.
    </p>
    <p>
      Let's investigate further.
      Notice that if <m>B=A^T-A</m>,
      then <m>B^T=(A^T-A)^T=(A^T)^T-A^T=A-A^T=-B</m>.
      Thus every element <m>B\in \im T</m> satisfies <m>B^T=-B</m>: i.e., all elements of the image are skew-symmetric!
    </p>
    <p>
      We claim further that in fact <m>\im T_1</m> is the the set of
      <em>all</em> skew-symmetric matrices.
      To prove this, we need to show that given a skew-symmetric matrix <m>B</m>, we have <m>B\in \im T_1</m>: <ie />, that
      there is a matrix <m>A</m> such that <m>T_1(A)=B</m>.
    </p>
    <p>
      Suppose <m>B</m> satisfies <m>B^T=-B</m>.
      Let <m>A=-\frac{1}{2}B</m>.
      Then
      <me>
        T_1(A)=T_1(-\frac{1}{2}B)=-\frac{1}{2}(B^T-B)=-\frac{1}{2}(-B-B)=B
      </me>.
    </p>
    <p>
      This shows that <m>B\in \im T_1</m>,
      and concludes the proof that
      <m>\im T_1</m> is the set of all skew-symmetric matrices.
    </p>
  </solution>
</example>

    <remark xml:id="rm_lines_planes">
    <title>Lines and planes</title>
  <statement>
    <p>
      Recall that a line <m>\ell</m> in <m>\R^2</m> that <em>passes through the origin </em> can be expressed as the set of solutions
      <m>(x_1,x_2)\in\R^2</m> to an equation of the form
      <me>
      \ell\colon  ax_1+bx_2=0
      </me>.
      Similarly, a plane <m>\mathcal{P}</m> in <m>\R^3</m> that <em>passes through the origin</em> can be expressed as the the set of solutions
      <m>(x_1,x_2,x_3)</m> to an equation of the form
      <me>
        \mathcal{P}\colon ax_1+bx_2+cx_3=0
      </me>.
    Using the language of linear algebra, we see that the line <m>\ell</m> is precisely <m>\NS T_A</m>, where <m>\underset{1\times 2}{A}=\begin{bmatrix} a \amp b\end{bmatrix}</m>; and the plane <m>\mathcal{P}</m> is precisely <m>\NS T_B</m>, where <m>\underset{1\times 3}{B}=\begin{bmatrix}a\amp b\amp c\end{bmatrix}</m>. We conclude from <xref ref="th_nullspace_image"/>
      that lines in <m>\R^2</m>, and planes in <m>\R^3</m>, are subspaces, <em>as long as they pass through the origin</em>.
    </p>
    <p>
      On the other hand, a line or plane that does <em>not</em>
      pass through the origin is not a subspace,
      since it does not contain the zero vector.
    </p>
    <p>
      The question arises: Can we describe <em>all</em> the subspaces of <m>\R^2</m> or <m>\R^3</m>? The answer is yes, as we will see in <xref ref="s_dimension"/>
    </p>
  </statement>
</remark>
</paragraphs>
<paragraphs xml:id="ss_subspace_functions">
  <title>Important subspaces of <m>F(X,\R)</m></title>
 <p>
   Let <m>X</m> be an interval of <m>\R</m>. Recall that <m>F(X,\R)</m> is the set of <em>all</em> functions from <m>X</m> to <m>\R</m>. This is a pretty unwieldy set, containing some pathological characters, and when studying functions on an interval we will often restrict our attention to certain more well-behaved subsets: e.g., continuous, differentiable, or infinitely differentiable functions. Not surprisingly, these subsets turn out to be subspaces of <m>F(X,\R)</m>.
 </p>
  <definition xml:id="d_subspace_functions">
    <idx><h>function space</h><h>continuous</h></idx>
    <idx><h>function space</h><h><m>C^n</m></h></idx>
    <idx><h>function space</h><h>infinitely differentiable</h></idx>
    <idx><h>function space</h><h>polynomial</h></idx>
    <idx><h>polynomial</h></idx>
    <statement>
      <p>
      Let <m>X\subseteq \R</m> be an interval.
      </p>
      <ol>
        <li>
          <p>
           We denote by <m>C(X)</m> the set of all continuous functions on <m>X</m>: <ie />,
           <me>
             C(X)=\{f\in F(X,\R)\colon f \text{ is continuous on } X\}
           </me>.
          </p>
        </li>
        <li>
          <p>
            Fix <m>n\geq 1</m>. A function <m>f\in F(X,\R)</m> is <term><m>C^n</m> on X</term> if <m>f</m> is <m>n</m>-times differentiable on <m>X</m> and its <m>n</m>-th derivative <m>f^{(n)}(x)</m> is continuous. The set of all <m>C^n</m> functions on <m>X</m> is denoted <m>C^n(X)</m>.
          </p>
        </li>
        <li>
          <p>
            A function <m>f\in F(X,\R)</m> is <term><m>C^\infty</m> on X</term> if <m>f</m> is infinitely differentiable on <m>X</m>. The set of all <m>C^\infty</m> functions on <m>X</m> is denoted <m>C^\infty(X)</m>.
          </p>
        </li>
        <li>
          <p>
            A <term>polynomial on <m>X</m></term> is a function <m>f\colon X\rightarrow \R</m> of the form <m>f(x)=\anpoly</m>, where <m>a_i\in \R</m> and <m>a_n\ne 0</m>. We call <m>n</m> the <term>degree of f</term>, denoted <m>\deg f=n</m>; and we call <m>a_n</m> the <term>leading term of <m>f</m></term>.
          </p>
          <p>
            The set of polynomials of degree at most <m>n</m> on <m>X</m> is denoted <m>P_n(X)</m>; the set of all polynomials on <m>X</m> is denoted <m>P(X)</m>. When <m>X=\R</m>, we shorten the notation to <m>P_n</m> and <m>P</m>.
          </p>
        </li>
      </ol>
    </statement>
  </definition>
  <theorem xml:id="th_subspace_functions">
    <statement>
      <p>
        Let <m>X\subseteq \R</m> be an interval. The sets <m>C(X), C^n(X), C^\infty(X), P_n(X), P(X)</m> are all subspaces of <m>F(X,\R)</m>. Thus we have the following chain of subspaces:
        <me>
          P_n(X)\subseteq P(X)\subseteq C^\infty(X)\subseteq C^n(X)\subseteq C(X)\subseteq F(X,\R)
        </me>.
      </p>
    </statement>
    <proof>
      <p>
        The proof amounts to the following observations:
        <ul>
          <li>
            <p>
              The zero function <m>0_X\colon X\rightarrow \R</m> is an element of all of these sets: <ie /> the zero function is continuous, <m>C^n</m>, <m>C^\infty</m>, a polynomial, <etc />.
            </p>
          </li>
          <li>
            <p>
              If <m>f</m> and <m>g</m> both satisfy one of these properties (continuous, <m>C^n</m>, <m>C^\infty</m>, polynomial, <etc />), then so does <m>cf+dg</m> for any <m>c,d\in \R</m>.
            </p>
          </li>
        </ul>
      The second, <q>closed under linear combinations</q> observation is easily seen for <m>P(X)</m> and <m>P_n(X)</m> (<ie />, the sum of two polynomials of degree at most <m>n</m> is clearly a polynomial of degree at most <m>n</m>); for the other spaces, this is a result of calculus properties to the effect that adding and scaling functions preserves continuity and differentiability.
      </p>
      <p>
        Lastly, that each subset relation holds in the given chain follows from similar observations: polynomials are infinitely differentiable, differentiable functions are continuous, <etc />.
      </p>
    </proof>
  </theorem>
</paragraphs>

        <remark xml:id="rm_polynomial_equality">
      <title>Polynomial equality</title>
      <statement>
        <p>
          An important fact that we will make frequent use of is that for two polynomials <m>f(x)=\anpoly</m> and <m>g(x)=\bmpoly</m> of degree <m>n</m> and <m>m</m>,respectively,  we have <m>f(x)=g(x)</m> on an interval <m>X</m> if and only if (1) <m>n=m</m>, and (2) <m>a_i=b_i</m> for all <m>0\leq i\leq n</m>.
        </p>
        <p>
          In particular, we have <m>f(x)=\boldzero</m> (the zero function) on <m>X</m> if and only if <m>a_i=0</m> for all <m>1\leq i\leq n</m>.
        </p>
      </statement>
    </remark>
    <p>
      Our new wealth of function spaces allows us at last to introduce an important family of linear transformations: <em>differential operators</em>.
    </p>
    <remark xml:id="rm_subspaces_derivatives">
    <title>Differential operators</title>
    <statement>
      <p>
        Let <m>X\subseteq \R</m> be an interval. Define <m>T_1\colon C^1(X)\rightarrow C(X)</m> as <m>T_1(f)=f'</m>: <ie />, <m>T_1</m> takes as input a <m>C^1</m> function on the interval <m>X</m>, and returns its (first) derivative. Note that the definition of <m>C^1</m> ensures that <m>f'</m> exists and is continuous on <m>X</m>: hence <m>f'
        \in C(X)</m>, as claimed.
      </p>
      <p>
        The operator <m>T_1</m> is a linear transformation. Indeed, given <m>c,d\in \R</m> and <m>f,g\in C^1(X)</m>, we have
        <md>
          <mrow>T_1(cf+dg)   \amp = (cf+dg)' \amp \text{(by def.)} </mrow>
          <mrow> \amp =(cf)'+(dg)' \amp \text{(derivative prop.)} </mrow>
          <mrow>  \amp =cf'+dg' \amp \text{(derivative prop.)}</mrow>
          <mrow>  \amp = cT_1(f)+dT_1(g)</mrow>
        </md>.
      </p>
      <p>
        Since taking <m>n</m>-th derivatives amounts to <em>composing</em> the derivative operator <m>T_1</m> with itself <m>n</m> times, it follows from <xref ref="th_transform_composition"/> that for any <m>n\geq 1</m> the  map
        <md>
          <mrow>T_n\colon C^n(X) \amp \rightarrow C(X) </mrow>
          <mrow> f \amp\mapsto f^{(n)} </mrow>
        </md>,
        which takes a function <m>f</m> to its <m>n</m>-th derivative, is also linear. (Note that we are careful to pick the domain <m>C^n(X)</m> to guarantee this operation is well-defined!)
      </p>
      <p>
        Lastly, by <xref ref="ex_transformation_add_scale"/>, we can add and scale these various operators to obtain more general linear transformations of the form
        <me>
          T(f)=c_nf^{(n)}+c_{n-1}f^{(n-1)}+\cdots c_1f'+c_0f
        </me>.
        We call such a function a <em>linear differential operator</em>. Understanding the linear algebraic properties of these operators is crucial to the theory of linear differential equations, as the following basic example illustrates.
      </p>

    </statement>
  </remark>
  <example xml:id="ex_diff_eq_ex">
    <title>A differential equation</title>
    <statement>
      <p>
      Fix an interval <m>X\subseteq \R</m>. Let <m>S</m> be the set of functions of <m>C^1(X)</m> satisfying the differential equation
      <men tag='star' xml:id="eq_diff_eq_ex">
        f'=f
      </men>.
      Then
      <me>
        S=\{f\in C^1(X)\colon f'-f=\boldzero\}=\NS T
      </me>,
      where <m>T\colon C^1(X)\rightarrow C(X)</m> is the linear transformation <m>T(f)=f'-f</m>.
      </p>
      <p>
        Having identified the set of solution to <xref ref="eq_diff_eq_ex"/> with the subspace <m>\NS T</m>, we can now bring some linear algebra to the table. For example, since <m>\NS T</m> is closed under vector addition and scalar multiplication, we know that if <m>f</m> and <m>g</m> are solutions to <xref ref="eq_diff_eq_ex"/>, then so is <m>cf+dg</m> for any <m>c,d\in\R</m>.
      </p>
      <p>
        It is an interesting exercise to try and understand what <m>\im T</m> means in terms of differential equations. By definition, <m>\im T</m> is the set of <m>g\in C(X)</m> such that <m>g=T(f)=f'-f</m>, for some <m>f\in C^1(X)</m>. In the language of differential equations, this is the set of all continuous functions on <m>X</m> for which the differential equation
        <me>
          f'-f=g
        </me>
        <em>can be solved</em> for <m>f</m>.
      </p>
    </statement>
  </example>

    <paragraphs xml:id="ss_subspace_intersection_union">
    <title>Intersections and unions</title>
    <p>
      We end this section with some simple observations about how the property of being a subspace relates to basic set operations like intersection and union.
    </p>
    <theorem xml:id="th_subspace_intersection">
      <title>Intersection of subspaces</title>

      <statement>
        <p>
          Let <m>V</m> be a vector space,
          and suppose <m>W_1,W_2,\dots, W_r</m> are each subspaces of <m>V</m>.
          Then the intersection
          <me> W=W_1\cap W_2\cdots \cap W_r</me>.
        </p>
      </statement>
      <proof>
        <p>
          Exercise.
        </p>
      </proof>
    </theorem>
        <remark xml:id="rm_subspace_union">
          <title>Unions of subspaces</title>
      <statement>
        <p>
        While the intersection of subspaces is again a subspace, the same is not true for unions of subspaces.
        </p>
        <p>
          For example, take <m>V=\R^2</m>,
          <m>W_1=\{(t,t)\colon t\in\R\}</m> and <m>W_2=\{(t,-t)\colon t\in\R\}</m>.
          Then each <m>W_i</m> is a subspace,
          but their union <m>W_1\cup W_2</m> is not.
        </p>
        <p>
          Indeed, observe that <m>\boldw_1=(1,1)\in W_1\subset W_1\cup W_2</m> and <m>\boldw_2=(1,-1)\in W_2\subset W_1\cup W_2</m>,
          but <m>\boldw_1+\boldw_2=(2,0)\notin W_1\cup W_2</m>. Thus <m>W_1\cup W_2</m> is not closed under addition. (Interestingly, it is closed under scalar multiplication.)
        </p>
      </statement>
    </remark>
  </paragraphs>
<xi:include href="./s_subspace_ex.ptx"/>

</section>
