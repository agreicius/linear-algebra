<section xml:id="s_subspace">
  <title>Subspaces</title>
  <introduction>
    <p>
      The definition of a <em>subspace</em> of a vector space <m>V</m> is very much in the same spirit as our definition of linear transformations. It is a subset of <m>V</m> that in some sense respects the vector space structure: in the language of <xref ref="d_subspace"/>, it is a subset that is <em>closed under addition</em> and <em>closed under scalar multiplication</em>.
    </p>
    <p>
      In fact the connection between linear transformations and subspaces goes deeper than this. As we will see in <xref first="th_nullspace" last="th_image"/>, a linear transformation <m>T\colon V\rightarrow W</m> naturally gives rise to two important subs
      paces: the <em>null space of <m>T</m></em> and the <em>image of <m>T</m></em>.
    </p>
  </introduction>
    <definition xml:id="d_subspace">
    <title>Subspace</title>
    <idx><h>subspace</h></idx>
    <idx><h>vector space</h><h>subspace</h></idx>
      <statement>
        <p>
          Let <m>V</m> be a vector space.
          A subset <m>W\subseteq V</m> is a
          <term>subspace</term> of <m>V</m> if the following conditions hold:
        </p>
          <ol label="i">
            <li>
              <title><m>W</m> contains the zero vector</title>
              <p>
                We have <m>\boldzero\in W</m>.
              </p>
            </li>
            <li>
              <title><m>W</m> is closed under addition</title>
              <p> For all <m>\boldv_1,\boldv_2\in V</m>, if <m>\boldv_1,\boldv_2\in W</m>, then <m>\boldv_1+\boldv_2\in W</m>. Using logical notation:
              <me>
                \boldv_1,\boldv_2\in W\implies \boldv_1+\boldv_2\in W
              </me>.
              </p>
            </li>
            <li>
              <title><m>W</m> is closed under scalar multiplication</title>
              <p> For all <m>c\in \R</m> and <m>\boldv\in V</m>, if <m>\boldv\in W</m>, then <m>c\boldv\in W</m>. In logical notation:
                <me>\boldv\in W\Rightarrow c\boldv\in W</me>.
              </p>
            </li>
          </ol>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>V=\R^2</m> and let
          <me>W=\{(t,t)\in\R^2 \colon t\in\R\}
          </me>.
          Prove that <m>W</m> is a subspace.
        </p>
      </statement>
      <solution>
        <p>
          We must show properties (i)-(iii) hold for <m>W</m>.
        </p>
        <ol label="i">
          <li>
            <p>
              The zero element of <m>V</m> is <m>\boldzero=(0,0)</m>,
              which is certainly of the form <m>(t,t)</m>.
              Thus <m>\boldzero\in W</m>.
            </p>
          </li>
          <li>
            <p>
              We must prove the implication <m>\boldv_1, \boldv_2\in W\Rightarrow \boldv_1+\boldv_2\in W</m>.
              <md>
                <mrow>\boldv_1,\boldv_2\in W\amp \Rightarrow\amp  \boldv_1=(t,t), \boldv_2=(s,s) \text{ for some \(t,s\in\R\) }</mrow>
                <mrow>\amp \Rightarrow\amp \boldv_1+\boldv_2=(t+s,t+s)</mrow>
                <mrow>\amp \Rightarrow\amp \boldv_1+\boldv_2\in W</mrow>
              </md>.
            </p>
          </li>
          <li>
            <p>
              We must prove the implication <m>\boldv\in W\Rightarrow c\boldv\in W</m>, for any <m>c\in \R</m>. We have
              <md>
                <mrow>\boldv\in W\amp \Rightarrow\amp  \boldv=(t,t)</mrow>
                <mrow>\amp \Rightarrow\amp  c\boldv=(ct,ct)</mrow>
                <mrow>\amp \Rightarrow\amp  c\boldv\in W</mrow>
              </md>
            </p>
          </li>
        </ol>
      </solution>
    </example>
    <example>
      <statement>
        <p>
          Let <m>V=\R^n</m> and let
          <me>
            W=\{(x,y)\in \R^2\colon x, y\geq 0\}
          </me>.
          Is <m>W</m> a vector space? Decide which of the of properties (i)-(iii) in <xref ref="d_subspace"/> (if any) are satisfied by <m>W</m>.
        </p>
      </statement>
      <solution>
        <ol label="i">
          <li>
            <p>
              Clearly <m>\boldzero=(0,0)\in W</m>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>\boldv_1=(x_1,y_1), \boldv_2=(x_2,y_2)\in W</m>. Then <m>x_1, x_2, y_1, y_2\geq 0</m>, in which case <m>x_1+x_2, y_1+y_2\geq 0</m>, and hence <m>\boldv_1+\boldv_2\in W</m>. Thus <m>W</m> is closed under addition.
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> is <em>not</em> closed under scalar multiplication. Indeed, let <m>\boldv=(1,1)\in W</m>. Then <m>(-2)\boldv=(-2,-2)\notin W</m>.
            </p>
          </li>
        </ol>
      </solution>
    </example>

        <remark xml:id="rm_twostep_proof">
          <title>Two-step proof for subspaces</title>
      <statement>
        <p>
        As with proofs regarding linearity of functions, we can merge conditions (ii)-(iii) of <xref ref="d_subspace"/> into a single statement about linear combinations, deriving the following two-step method for proving a set <m>W</m> is a subspace of a vector space <m>V</m>:
        </p>
        <ol label="i">
          <li>
            <p>
            Show <m>\boldzero_V\in W</m>
            </p>
          </li>
          <li>
            <p>
              Show that
              <me>
                \boldv_1, \boldv_2\in W\implies c\boldv_1+d\boldv_2\in W
              </me>,
              for all <m>c,d\in\R</m>.
            </p>
          </li>
        </ol>
      </statement>
    </remark>

        <remark xml:id="rm_subspace_is_vectorspace">
          <title>Subspaces are vector spaces</title>
      <statement>
        <p>
          If <m>W</m> is a subspace of a vector space <m>V</m>, then it <em>inherits</em> a vector space structure from <m>V</m> by simply  <em>restricting</em> the vector operations defined on <m>V</m> to the subset <m>W</m>.
        </p>
        <p>
           It is important to understand how conditions (ii)-(iii) of <xref ref="d_subspace"/> come into play here. Without them we would not be able to say that restricting the vector operations of <m>V</m> to elements of <m>W</m> actually gives rise to well-defined operations on <m>W</m>. To be well-defined the operations must output elements that lie not just in <m>V</m>, but in <m>W</m> itself. This is precisely what being closed under addition and scalar multiplication guarantees.
        </p>
        <p>
        Once we know restriction gives rise to well-defined operations on <m>W</m>, verifying the axioms of  <xref ref="d_vector_space"/> mostly amounts to observing that if a condition is true for all <m>\boldv</m> in <m>V</m>, it is certainly true for all <m>\boldv</m> in the subset <m>W</m>.
        </p>
        <p>
          The <q>existential axioms</q> (iii) and (iv) of <xref ref="d_vector_space"/>, however, require special consideration. By definition, a subspace <m>W</m> contains the zero vector of <m>V</m>, and clearly this still acts as the zero vector when we restrict the vector operations to <m>W</m>. What about vector inverses? We know that for any <m>\boldv\in W</m> there is a vector inverse <m>-\boldv</m> lying somewhere in <m>V</m>. We must show that in fact <m>-\boldv</m> lies in <m>W</m>: <ie /> we need to show that the operation of taking the vector inverse is well-defined on <m>W</m>. We prove this as follows:
          <md>
            <mrow>\boldv\in W \amp\implies (-1)\boldv\in W \amp (<xref ref="d_subspace"/>, \text{(iii) } )</mrow>
            <mrow> \amp\implies -\boldv\in W \amp (<xref ref="th_vectorspace_props"/>, (iii)) </mrow>
          </md>.
        </p>
      </statement>
    </remark>
<paragraphs xml:id="ss_nullspace_image">
  <title>Null space and image of a linear transformation</title>
  <p>
    Before looking at more examples, we first show how a linear transformation <m>T\colon V\rightarrow W</m> gives rise to two different subspaces: one insides <m>V</m>, and the other inside <m>W</m>. These are called the <em>null space</em> and <em>image</em> of <m>T</m>, respectively.
  </p>
  <definition xml:id="d_nullspace_image">
    <title>Null space and image</title>
    <idx><h>linear transformation</h><h>null space</h></idx>
    <idx><h>linear transformation</h><h>image</h></idx>
    <idx><h>null space</h></idx>
    <idx><h>image</h></idx>

    <statement>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation.
      </p>
      <ol>
        <li>
          <title>Null space</title>
          <p>
            The <term>null space</term> of <m>T</m>, denoted <m>\NS T</m>, is defined as
            <me>
              \NS T=\{\boldv\in V\colon T(\boldv)=\boldzero_W\}
            </me>.
          </p>
        </li>
        <li>
          <title>Image</title>
          <p>
            The <term>image</term> (or <term>range</term>) of <m>T</m>, denoted <m>\im T</m>, is defined as
            <me>
              \im T=\{\boldw\in W\colon \boldw=T(\boldv) \text{ for some } \boldv\in V\}
            </me>.
          </p>
        </li>
      </ol>
    </statement>
  </definition>
    <remark xml:id="rm_nullspace_image">
  <statement>
    <p>
    A few remarks:
    </p>
    <ol>
      <li>
        <p>
          Let <m>T\colon V\rightarrow W</m>. It is useful to keep in mind where <m>\NS T</m> and <m>\im T</m> <q>live</q> in this picture: we have <m>\NS T\subseteq V</m> and <m>\im T\subseteq W</m>. In other words, the null space is a subset of the domain, and the image is a subset of the codomain.
        </p>
      </li>
      <li>
        <p>
          More generally we can define the image (or range) <m>\im f</m>, for any function of sets <m>f\colon A\rightarrow B</m>: namely,
          <me>
            \im f=\{b\in B\colon b=f(a) \text{ for some } a\in A\}
          </me>.
          To decide whether or not an element <m>b\in B</m> is an element of <m>\im f</m>, we must determine whether there is an element <m>a\in A</m> such that <m>f(a)=b</m>. That is somewhat of a mouthful, and as such we often say simply <q><m>b</m> is hit by <m>f</m></q> (or not) for this condition.
        </p>
      </li>
      <li>
        <p>
          The notion of a null space is analogous to the set of zeros (or roots) of a real-valued function <m>f\colon X\rightarrow \R</m>,
          <me>
            \{x\in X\colon f(x)=0\}
          </me>,
          and <q>the zeros of <m>T</m></q> is a useful English shorthand for <m>\NS T</m>.
           However, there is an important difference between the null space of a linear transformation and the zeros of an arbitrary real-valued function: the null space of a linear transformation comes with the added structure of a vector space (<xref ref="th_nullspace_image"/>), whereas the zeros of an arbitrary function in general do not.
        </p>
        <p>
          The same observation can be made about the image of a linear transformation (<xref ref="th_nullspace_image"/>), in comparison to the image of an arbitrary function.
        </p>
      </li>
    </ol>
  </statement>
</remark>
<theorem xml:id="th_nullspace_image">
  <title>Null space and image</title>

  <statement>
    <p>
    If <m>T\colon V\rightarrow W</m> is a linear transformation, then <m>\NS T</m> is a subspace of <m>V</m>, and <m>\im T</m> is a subspace of <m>W</m>.
    </p>
  </statement>
  <proof>
      <case>
       <title>Null space of <m>T</m></title>
      <p>
      We use the two-step technique to prove <m>\NS T</m> is a subspace.
      </p>
      <ol>
        <li>
          <p>
            Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_V\in \NS T</m>.
          </p>
        </li>
        <li>
          <p>
            Suppose <m>\boldv_1, \boldv_2\in \NS T</m>. Given any <m>c,d\in \R</m>, we have
            <md>
              <mrow>T(c\boldv_1+d\boldv_2) \amp=cT(\boldv_1)+dT(\boldv_2) \amp (T \text{ is linear, } <xref ref="th_trans_props"/>)</mrow>
              <mrow> \amp=c\boldzero_W+d\boldzero_W \amp (\boldv_1, \boldv_2\in \NS T) </mrow>
              <mrow>  \amp = \boldzero_W</mrow>
            </md>.
            This shows that <m>c\boldv_1+d\boldv_2\in \NS T</m>, completing our proof.
          </p>
        </li>
      </ol>
      </case>
      <case>
       <title>Image of <m>T</m></title>
      <p>
        The proof proceeds in a similar manner.
      </p>
      <ol>
        <li>
          <p>
            Since <m>T(\boldzero_V)=\boldzero_W</m> (<xref ref="th_trans_props"/>), we see that <m>\boldzero_W</m> is <q>hit</q> by <m>T</m>, and hence is a member of <m>\im T</m>.
          </p>
        </li>
      </ol>
      </case>
  </proof>
</theorem>
<p>
  <xref ref="th_nullspace_image"/> provides us a with a useful indirect way of proving a subset <m>W\subseteq V</m> is a subspace:
  namely, find a linear transformation $T$ for which <m>W=\NS T</m>. (You could also identify <m>W=\im T</m> for some linear transformation <m>T</m>, but this approach tends to be not as convenient.
</p>
<example>
  <statement>
    <p>
      Prove that the set <m>W\subseteq\R^3</m> of all vectors <m>(x,y,z)</m> satisfying
      <m>x+2y+3z=x-y-z=0</m> is a subspace of <m>\R^3</m>.
    </p>
    <solution>
      <p>
          Let <me>A=\begin{bmatrix}1\amp 2\amp 3\\ 1\amp -1\amp -1 \end{bmatrix}</me>, and let
          <md>
            <mrow>T_A\colon \R^3 \amp\rightarrow \R^2 </mrow>
            <mrow> (x,y,z)\amp\mapsto (x+2y+3z, x-y-z) </mrow>
          </md>
          be its associated linear transformation. Then
          <md>
            <mrow> \NS T_A \amp =\{(x,y,z)\in\R^3\colon T_A(x,y,z)=(0,0)\} </mrow>
            <mrow> \amp = \{(x,y,z)\in\R^3\colon (x+2y+3z,x-y-z)=(0,0)\} </mrow>
            <mrow>  \amp = \{(x,y,z)\in\R^3\colon x+2y+3z=x-y-z=0\}</mrow>
            <mrow>  \amp = W</mrow>
          </md>.
          This shows that <m>W</m> can be identified as the null space of a linear transformation, and hence is a subspace of <m>\R^3</m>.
      </p>
    </solution>

  </statement>
</example>
<definition xml:id="d_symmetric_skewsymmetric">
  <title>Symmetric and skew-symmetric matrices</title>
  <statement>
    <p>
    A matrix <m>A\in M_{nn}</m> is <term>symmetric</term> if <m>A^T=A</m>. It is <term>skew-symmetric</term> if <m>A^T=-A</m>.
    </p>
  </statement>
</definition>
<example xml:id="eg_symmetric_skesymmetric_subspace">
  <title>Vector spaces of symmetric/skew-symmetric matrices</title>

  <statement>
    <p>
      Let <m>V=M_{nn}</m>, let <m>W_1</m> be the set of all symmetric <m>n\times n</m> matrices, and let <m>W_2</m> be the set of all skew-symmetric <m>n\times n</m> matrices. Prove that <m>W_1</m> and <m>W_2</m> are subspaces of <m>V</m>.
    </p>
    <solution>
      <p>
        Define functions <m>T_i\colon M_{nn}\rightarrow M_{nn}</m>, <m>i=1,2</m>, as follows:
        <md>
          <mrow>T_1(A) \amp = A^T-A </mrow>
          <mrow> T_2(A)\amp = A^T+A</mrow>
        </md>.
        Firstly, it is easy to see, using <xref ref="th_trans_props"/> that both <m>T_1</m> and <m>T_2</m> are both linear transformations. For example, for <m>T_1</m> we have
        <md>
          <mrow>T_1(cA+dB) \amp=(cA+dB)^T-(cA+dB) </mrow>
          <mrow> \amp=cA^T+dB^T-cA-cB </mrow>
          <mrow>  \amp = c(A^T-A)+d(B^T-B)</mrow>
          <mrow>  \amp = cT_1(A)+dT_1(B)</mrow>
        </md>.
        As similar proof applies to <m>T_2</m>.
      </p>
      <p>
        Next, it is easy to see that <m>W_1=\NS T_1</m> and <m>W_2=\NS T_2</m>. Taking <m>W_1</m> for example, we have
        <md>
          <mrow>A \text{ symmetric} \amp\iff A^T=A </mrow>
          <mrow> \amp\iff A^T-A=\boldzero_{n\times n} </mrow>
          <mrow>  \amp \iff T_1(A)=\boldzero_{n\times n}</mrow>
          <mrow>  \amp \iff A\in \NS T_1</mrow>
        </md>.
        Having identified <m>W_1</m> and <m>W_2</m> as null spaces of linear transformations, we conclude that they are subspaces of <m>M_{nn}</m>.
      </p>
    </solution>
  </statement>
</example>
<p>
  The next two examples illustrate how to compute the image of a given linear transformation <m>T\colon V\rightarrow W</m>. The examples make clear that computing images (i.e., determining the set of elements of <m>W</m> that are hit by <m>T</m>) is often more onerous than computing null spaces.
</p>
<example>
  <statement>
    <p>
      Let <m>T=T_A\colon \R^2\rightarrow\R^3</m>,
      where <m>A=\begin{bmatrix}1\amp 1\\ 2\amp 1\\ 3\amp 5 \end{bmatrix}</m>.
      According to <xref ref="th_nullspace_image"/>, <m>\im T_A</m> is a subspace of <m>\R^3</m>.
      Identify this subspace as a familiar geometric object.
    </p>
  </statement>
  <solution>
    <p>
      By definition <m>\im T_A</m> is the set
      <me>
        \{\boldy\in\R^3\colon \boldy=T_A(\boldx) \text{ for some \(\boldx\in \R^3\) } \}=\left\{\boldy\colon \boldy=A\boldx \text{ for some \(\boldx\in\R^2\) } \right\}
      </me>.
    </p>
    <p>
      Thus to compute <m>\im T_A</m> we must determine which choice of
      <m>\boldy=(a,b,c)</m> makes the system <m>A\boldx=\boldy</m> consistent.
      We answer this using our old friend Gaussian elimination!
      <md>
        <mrow> \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 2\amp 1\amp b\\ 3\amp 5\amp c \end{amatrix} \amp \xrightarrow[r_3-3r_1]{r_2-2r_1} \begin{amatrix}[rr|r] 1\amp 1\amp a\\ 0\amp 1\amp 2a-b\\ 0\amp 0\amp -7a+2b+c \end{amatrix}</mrow>
      </md>.
      To be consistent we need <m>-7a+2b+c=0</m>.
      We conclude that <m>\im T</m> is the set of all <m>(a,b,c)</m> satisfying <m>-7a+2b+c=0</m>.
      Geometrically this is the plane passing through <m>(0,0,0)</m> with normal vector <m>\boldn=(-7,2,1)</m>.
    </p>
  </solution>
</example>
<example>
  <statement>
    <p>
      Consider again the linear transformation <m>T_1\colon M_{nn}\rightarrow M_{nn}</m>,
      <m>T_1(A)=A^T-A</m>, from <xref ref="eg_symmetric_skesymmetric_subspace"/>.
      We saw that <m>\NS T_1</m> was the space of symmetric matrices.
      Identify <m>\im T_1</m> as a familiar subspace of <m>M_{nn}</m>.
    </p>
  </statement>
  <solution>
    <p>
      Take <m>B\in \im T_1</m>.
      By definition this means <m>B=T(A)=A^T-A</m> for some <m>A</m>.
      So one, somewhat unsatisfying way of describing
      <m>\im T_1</m> is as the set of all matrices of the form <m>A^T-A</m>.
    </p>
    <p>
      Let's investigate further.
      Notice that if <m>B=A^T-A</m>,
      then <m>B^T=(A^T-A)^T=(A^T)^T-A^T=A-A^T=-B</m>.
      Thus every element <m>B\in \im T</m> satisfies <m>B^T=-B</m>: i.e., all elements of the image are skew-symmetric!
    </p>
    <p>
      We claim further that in fact <m>\im T_1</m> is the the set of
      <em>all</em> skew-symmetric matrices.
      To prove this, we need to show that given a skew-symmetric matrix <m>B</m>, we have <m>B\in \im T_1</m>: <ie />, that
      there is a matrix <m>A</m> such that <m>T_1(A)=B</m>.
    </p>
    <p>
      Suppose <m>B</m> satisfies <m>B^T=-B</m>.
      Let <m>A=-\frac{1}{2}B</m>.
      Then
      <me>
        T_1(A)=T_1(-\frac{1}{2}B)=-\frac{1}{2}(B^T-B)=-\frac{1}{2}(-B-B)=B
      </me>.
    </p>
    <p>
      This shows that <m>B\in \im T_1</m>,
      and concludes the proof that
      <m>\im T_1</m> is the set of all skew-symmetric matrices.
    </p>
  </solution>
</example>
<!-- <example>
  <statement>
    <p>
      The set <m>W</m> of all infinitely differentiable functions <m>f</m> satisfying the differential equation
      <m>f''(x)+xf'(x)=3f(x)</m> is a subspace of <m>C^\infty(\R)</m>.
    </p>
    <p>
      Indeed <m>W</m> is <m>\NS T</m>,
      where <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> is the linear transformation defined as <m>T(f)=f''+xf'-3f</m>.
    </p>
    <p>
      (I leave it to you to show <m>T</m> is linear.)
    </p>
  </statement>
</example> -->
    <remark xml:id="rm_lines_planes">
    <title>Lines and planes</title>
  <statement>
    <p>
      Recall that a line <m>\ell</m> in <m>\R^2</m> that <em>passes through the origin </em> can be expressed as the set of solutions
      <m>(x_1,x_2)\in\R^2</m> to an equation of the form
      <me>
      \ell\colon  ax_1+bx_2=0
      </me>.
      Similarly, a plane <m>\mathcal{P}</m> in <m>\R^3</m> that <em>passes through the origin</em> can be expressed as the the set of solutions
      <m>(x_1,x_2,x_3)</m> to an equation of the form
      <me>
        \mathcal{P}\colon ax_1+bx_2+cx_3=0
      </me>.
    Using the language of linear algebra, we see that the line <m>\ell</m> is precisely <m>\NS T_A</m>, where <m>A=[a\ b]</m>, and the plane <m>\mathcal{P}</m> is precisely <m>\NS T_B</m>, where <m>B=[a \ b\ c]</m>. We conclude from <xref ref="th_nullspace_image"/>
      that lines in <m>\R^2</m> and planes in <m>\R^3</m> are subspaces, <em>as long as they pass through the origin</em>.
    </p>
    <p>
      On the other hand, a line or plane that does <em>not</em>
      pass through the origin is not a subspace,
      since it does not contain the zero vector.
    </p>
  </statement>
</remark>

</paragraphs>

    <!-- In fact these are all sub{spaces} of <m>V</m>. Proof: (i) the zero function, <m>h(x)=0</m> for all <m>x</m>, is <m>\infty</m>-differentiable, and thus is an element of each of these sets; (ii)-(iii) these sets being closed under addition and scalar mult. is a consequence of calculus results about sums and scalar multiples of continuous/differentiable functions. -->
    <paragraphs>
    <title>Polynomials</title>
    <p>
      Recall that a <term>polynomial</term>
      is a function that can be written in the form <m>f(x)=\anpoly</m>,
      where <m>a_i\in\R</m> are fixed constants and <m>a_n\ne 0</m>.
      We call <m>n</m> the <term>degree</term>
      of such a function, denoted <m>\deg f=n</m>.
    </p>
    <p>
      Let <m>P</m> be the set of all polynomial functions,
      and let <m>P_n=\{f\colon f(x)=\anpoly,
      a_i\in\R \}</m> be the set of all polynomials of degree
      <em>at most</em> <m>n</m>.
    </p>
    <p>
      We have the set inclusions <m>P_n\subset P\subset C^\infty(X)</m>,
      where <m>X</m> is any nontrivial interval. (The second inclusion holds since any polynomial is infinitely differentiable on any interval. )
    </p>
    <p>
      It is easy to see that in fact <m>P_n</m> is a
      <em>subspace</em> of <m>P</m>,
      and <m>P</m> is a <em>subspace</em> of <m>C^\infty(X)</m>.
      Indeed <m>P_n</m> and <m>P</m> both contain the zero polynomial <m>f(x)=0=0x^n+0x^{n-1}+\cdots 0x+0</m>,
      which is the same thing as the zero function <m>0_X=\boldzero</m>.
      Furthermore,
      sums and scalar multiples of polynomials (resp. of polynomials of at most degree <m>n</m>) are polynomials
      (resp. polynomials of at most degree <m>n</m>).
    </p>
    <paragraphs>
      <title>Important:</title>
      <p>
        a fact we will make use of all the time is that for two polynomials
      </p>
    </paragraphs>
    <me>
      \begin{array}{l}
      f(x)=\anpoly, \ a_n\ne 0 \\
      g(x)=\bmpoly, \ b_m\ne 0
      \end{array}
    </me>
    <p>
      we have <m>f(x)=g(x)</m> if and only if (1) n=m, and (2) <m>a_i=b_i</m> for all <m>0\leq i\leq n</m>.
    </p>
    </paragraphs>
    <paragraphs>
    <title>Intersections and unions</title>
    <theorem>
      <statement>
        <p>
          Let <m>V</m> be a vector space,
          and suppose <m>W_1,W_2,\dots, W_r</m> are each subspaces of <m>V</m>.
          Then the intersection <m>W=W_1\cap W_2\cdots \cap W_r</m> is a subspace of <m>V</m>.
        </p>
      </statement>
    </theorem>
    <paragraphs>
      <title>Comment</title>
      <p>
        Thus intersections of subspaces are subspaces.
        The same is not true of <em>unions</em>.
      </p>
    </paragraphs>
    <p>
      For example, take <m>V=\R^2</m>,
      <m>W_1=\{(t,t)\colon t\in\R\}</m> and <m>W_2=\{(t,-t)\colon t\in\R\}</m>.
      Then each <m>W_i</m> is a subspace,
      but their union <m>W_1\cup W_2</m> is not.
      Why?
    </p>
    <p>
      We have <m>\boldw_1=(1,1)\in W_1\subset W_1\cup W_2</m> and <m>\boldw_2=(1,-1)\in W_2\subset W_1\cup W_2</m>,
      but <m>\boldw_1+\boldw_2=(2,0)\notin W_1\cup W_2</m>.
    </p>
    <p>
      Note that <m>W_1\cup W_2</m> is in fact closed under scalar multiplication.
    </p>
    </paragraphs>
  <subsection>
    <title>Span</title>
    <paragraphs>
      <title>Linear combinations and span</title>
      <p>
        Recall that a <em>linear combination</em>
        in a vector space <m>V</m> is a vector of the form
        <me>
          \boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r
        </me>,
        where <m>k_i\in \R</m> are scalars.
      </p>
      <p>
        We use this notion to define the
        <em>span</em> of a set of vectors.
      </p>
      <definition>
        <statement>
          <p>
            Let <m>V</m> be a vector space,
            and let <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> be a set of vectors of <m>V</m>.
            The <term>span</term> of <m>S</m> is the set
            <md>
              <mrow>\Span(\{\boldv_1,\boldv_2,\dots,\boldv_r\})\amp :=\amp \left(\begin{array}{c} \text{ the set of all linear }  \\ \text{ combinations of the \(\boldv_i\) } \end{array} \right)</mrow>
              <mrow>\amp =\amp \{\boldv\in V\colon \boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r, \text{ for some }  k_i\in\R \}</mrow>
            </md>.
          </p>
        </statement>
      </definition>
    </paragraphs>
    <theorem>
      <statement>
        <p>
          Let <m>V</m> be a vector space,
          <m>S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}</m> a set of vectors of <m>V</m>,
          and <m>W=\Span(S)</m>.
          Then
          <ol>
            <li>
              <p>
                <m>W</m> is a subspace of <m>V</m>;
              </p>
            </li>
            <li>
              <p>
                if <m>W'</m> is a subspace containing all the vectors <m>\boldv_i</m>,
                then <m>W\subseteq W'</m>.
              </p>
            </li>
          </ol>
        </p>
        <p>
          We paraphrase (ii) by saying <m>W=\Span(S)</m> is the
          <q>smallest</q>
          subspace of <m>V</m> containing all the <m>\boldv_i</m>.
        </p>
      </statement>
    </theorem>
    <paragraphs>
      <title>More terminology:</title>
      <p>
        in the spirit of the last theorem,
        given a set of vectors <m>S=\{\boldv_1,\dots, \boldv_r\}</m>,
        we call <m>W=\Span(S)</m> the subspace of <m>V</m> <term>generated by</term>
        the vectors <m>\boldv_i</m>.
        Similarly, given a subspace <m>W</m>,
        a set <m>S=\{\boldv_1,\dots, \boldv_r\}</m> for which <m>W=\Span(S)</m> is called a
        <term>spanning set</term> of <m>W</m>.
      </p>
    </paragraphs>
    <paragraphs>
    <title>Examples</title>
    <p>
      \alert{<m>M_{mn}</m>}. Define <m>E_{ij}</m> to be the matrix whose <m>(i,j)</m>-th entry is 1, and whose every other entry is 0.
      Then the set
      <me>
        \{ E_{ij}\colon 1\leq i\leq m, 1\leq j\leq n\}
      </me>
      is a spanning set for <m>M_{mn}</m>.
    </p>
    <paragraphs>
      <title><m>\R^n</m></title>
      <p>
        In a similar vein,
        define <m>\bolde_i</m> to be the <m>n</m>-tuple whose <m>i</m>-th entry is 1, and whose every other entry is 0.
      </p>
    </paragraphs>
    <p>
      Then <m>\{\bolde_1, \bolde_2, \dots, \bolde_n\}</m> is a spanning set for <m>\R^n</m>.
    </p>
    <paragraphs>
      <title><m>P</m> and <m>P_n</m></title>
      <p>
        The set <m>\{1, x, x^2, \dots, \}</m> is a spanning set for <m>P</m>.
        The set <m>\{1, x, x^2, \dots,
        x^n\}</m> is a spanning set for <m>P_n</m>.
      </p>
    </paragraphs>
    <paragraphs>
      <title><m>\R^\infty</m></title>
      <p>
        As above we can define <m>\bolde_i\in \R^\infty</m> to be the infinite sequence whose <m>i</m>-th entry is 1, and whose every other entry is <m>0</m>.
      </p>
    </paragraphs>
    <p>
      Note, however,
      that the set <m>\{\bolde_1, \bolde_2, \bolde_3, \dots\}</m> is <em>not</em>
      a spanning set for <m>\R^\infty</m>.
    </p>
    <p>
      Indeed, the sequence <m>(1,1,1,\dots)</m> is not a (finite!) linear combination of the <m>\bolde_i</m>.
    </p>
    <p>
      \alert{<m>\R_{>0}</m>} Any
      <m>a\ne 1\in \R_{>0}</m> forms a spanning set for <m>\R_{>0}</m> as a vector space.
      This is because scalar multiplication by <m>r</m> in <m>\R_{>0}</m> is defined as exponentiation.
      Thus <m>\Span(\{a\})=\{a^r\colon r\in\R\}=\R_{>0}</m>.
      The last equality holds since the exponential function
      <m>f(x)=a^x</m> has image all positive reals for any base <m>a\ne 1</m>.
    </p>
    </paragraphs>
    <paragraphs>
    <title>Example</title>
    <p>
      Let <m>V=P_2</m> and let <m>S=\{p_1, p_2\}</m>,
      where <m>p_1(x)=x^2-1</m> and <m>p_2(x)=x^2-x</m>.
      Show that <m>W=\Span(S)</m> is the subspace of all polynomials
      <m>p(x)=a_2x^2+a_1x+a_0</m> for which <m>p(1)=0</m>.
      That is:
      <me>
        \Span(S)=\{p(x)\in P_2\colon p(1)=0\}
      </me>
    </p>
    <proof>
      <p>
        We wish to prove a <em>set equality</em>.
        We do so by showing the <m>\subseteq</m> and
        <m>\supseteq</m> relations separately. (See my proof technique guide!)
      </p>
      <paragraphs>
        <title><m>\subseteq</m></title>
        <p>
          Note first that <m>p_1(1)=p_2(1)=0</m>.
          Given an element <m>q(x)\in \Span\left(\{p_1(x), p_2(x)\}\right)</m>,
          we have <m>q(x)=ap_1(x)+bp_2(x)</m> for some <m>a, b\in \R</m>.
          But then <m>q(1)=ap_1(1)+bp_2(1)=0+0=0</m>.
          Thus <m>q(x)\in \{p(x)\colon p(1)=0\}</m>.
        </p>
      </paragraphs>
      <paragraphs>
        <title><m>\supseteq</m></title>
        <p>
          Now take <m>p(x)=a_2x^2+a_1x+a_0\in \{p(x)\in P_2\colon p(1)=0\}</m>.
          We must find <m>a, b\in \R</m> such that <m>p(x)=ap_1+bp_2</m>.
        </p>
      </paragraphs>
      <p>
        Since <m>p(1)=0</m>, we have <m>a_2+a_1+a_0=0</m>.
        I claim <m>p(x)=(-a_0)p_1+(-a_1)p_2</m>.
        Indeed we have
        <me>
          -a_0p_1-a_1p_2=(-a_0-a_1)x^2+a_1x+a_0=a_2x^2+a_1x+a_0
        </me>,
        since <m>a_2+a_1+a_0=0</m>.
        This shows that <m>p(x)\in\Span\left(\{p_1, p_2\}\right)</m>,
        as desired.
      </p>
    </proof>
    </paragraphs>
    <paragraphs>
      <title>Example</title>
      <p>
        Let <m>V=P_2</m> and let <m>S=\{p_1, p_2, p_3\}</m> where
        <me>
          p_1(x)=x^2+x+1, p_2(x)=x^2+x, p_3(x)=x^2+1
        </me>.
      </p>
      <p>
        Show that <m>\Span(S)=P_2</m>.
      </p>
      <proof>
        <p>
          Again, we are tasked with showing a <em>set equality</em>.
          It is clear that <m>\Span(S)=\{rp_1+sp_2+tp_3\colon r,s,t\in\R\}\subseteq P_2</m>.
          The harder direction is showing <m>P_2\subseteq \Span(S)</m>:
          i.e., given <em>any</em> <m>p(x)=a_2x^2+a_1x+a_0</m> we must show there are
          <m>r,s,t\in\R</m> such that <m>p(x)=rp_1+sp_2+tp_3</m>.
        </p>
        <p>
          We do so by setting up a system of equations.
          Combining like terms and equating coefficients in the polynomial expression
          <m>p(x)=rp_1+sp_2+tp_3</m> yields the linear system
          <md>
            \begin{linsys}{3} r\amp +\amp s\amp +\amp t\amp =\amp a_2\\ r\amp +\amp s\amp \amp \amp =\amp a_1\\ r\amp \amp \amp +\amp t\amp =\amp a_0 \end{linsys}
          </md>
        </p>
        <p>
          GE shows that the system has a solution for <em>any</em>
          choice of <m>a_2, a_1, a_0</m>:
          namely, <m>r=-a_2+a_1+a_0</m>,
          <m>s=a_2-a_0</m>, <m>t=a_2-a_1</m>.
          Thus given any <m>p(x)=a_2x^2+a_1x+a_0</m>,
          we can find <m>r,s,t</m> such that <m>p=rp_1+sp_2+tp_3</m>,
          showing <m>P_2\subseteq \Span(S)</m>.
        </p>
      </proof>
    </paragraphs>
  </subsection>

</section>
