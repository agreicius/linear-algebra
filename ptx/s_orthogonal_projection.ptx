<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_orthogonal_projection">
  <title>Orthogonal projection</title>
  <introduction>
    <p>
      A trick we learn early on in physics-- specifically, in dynamics problems in <m>\R^2</m>-- is to pick a convenient axis and then decompose any relevant vectors (force, acceleration, velocity, position, <etc />) into a sum of two components: one that points along the chosen axis, and one that points perpendicularly to it. As we will see in this section, this technique can be vastly generalized. Namely, instead of <m>\R^2</m> we  can take any inner product space <m>(V, \langle\, , \rangle)</m>; and instead of a chosen axis in <m>\R^2</m>, we can choose any finite-dimensional subspace <m>W\subseteq V</m>; then any <m>\boldv\in V</m> can be decomposed in the form
    <me>
      \boldv=\boldw+\boldw^\perp,
    </me>
    where <m>\boldw\in W</m> and <m>\boldw^\perp</m> is a vector <em>orthogonal</em> to <m>W</m>, in a sense we will make precise below. Just as in our toy physics example, this manner of decomposing vectors helps simplify computations in problems where the subspace <m>W</m> chosen is of central importance.
  </p>
  </introduction>
  <subsection xml:id="ss_ortho_complement">
  <title>Orthogonal complement</title>
  <introduction>
    <p>
      We being by making sense of what it means for a vector to be orthogonal to a subspace.
    </p>
  </introduction>
  <definition xml:id="d_orthogonal_complement">
    <title>Orthogonal complement</title>
    <idx><h>orthogonal</h><h>complement (of a subspace)</h></idx>
    <notation>
      <usage><m>W^\perp</m></usage>
      <description>the orthogonal complement of <m>W</m></description>
    </notation>
    <statement>
      <p>.
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a subspace.
      </p>
      <p>
        A vector <m>\boldv</m> is <term>orthogonal</term> to <m>W</m> if it is orthogonal to every element of <m>W</m>:<ie />, if <m>\langle \boldv, \boldw\rangle=0</m> for all <m>\boldw\in W</m>.
      </p>
      <p>
        The <term>orthogonal complement</term> of <m>W</m>, denoted <m>W^\perp</m>, is the set of all elements of <m>V</m> orthogonal to <m>W</m>: <ie />,
        <me>
          W^\perp=\{\boldv\in V\colon \langle \boldv, \boldw\rangle=0 \text{ for all } \boldw\in W\}
        </me>.
      </p>
    </statement>
  </definition>
      <remark xml:id="rm_computing_ortho_comp">
        <title>Computing <m>W^\perp</m></title>
    <statement>
      <p>
        According to <xref ref="d_orthogonal_complement"/>, to verify that a vector <m>\boldv</m> lies in <m>W^\perp</m>, we must show that <m>\langle \boldv, \boldw\rangle=0</m> for all <m>\boldw\in W</m>. The <q>for all</q> quantifier here can potentially make this an onerous task: there are in principle infinitely many <m>\boldw</m> to check! In the special case where <m>W</m> has a finite spanning set, so that <m> W=\Span \{\boldw_1, \boldw_2,\dots, \boldw_r\}</m> for some vectors <m>\boldw_i</m>, deciding whether <m>\boldv\in W^\perp</m> reduces to checking whether <m>\langle \boldv, \boldw_i\rangle=0</m> for all <m>1\leq i\leq r</m>. In other words, we have
        <me>
          \boldv\in W^\perp\iff \langle \boldv, \boldw_i\rangle=0 \text{ for all } 1\leq i\leq r
        </me>.
        The forward implication of this equivalence is clear: if <m>\boldv</m> is orthogonal to all elements of <m>W</m>, then clearly it is orthogonal to each <m>\boldw_i</m>. The reverse implication is left as an exercise. (See <xref ref="ex_ortho_comp"/>.)
      </p>
      <p>
        We illustrate this computational technique in the next examples.
      </p>
    </statement>
  </remark>

  <example xml:id="eg_ortho_comp_line">
    <statement>
      <p>
        Consider the inner product space <m>\R^2</m> together with the dot product. Let <m>W=\Span\{(1,1)\}=\{(t,t)\colon t\in \R\}</m>: the line <m>\ell\subseteq \R^2</m> with equation <m>y=x</m>. Compute <m>W^\perp</m> and identify it as a familiar geometric object in <m>\R^2</m>.
      </p>
    </statement>
    <solution>
      <p>
        According to <xref ref="rm_computing_ortho_comp"/>, since <m>W=\Span\{(1,1)\}</m>, we have
        <me>
          \boldx\in W^\perp \iff \boldx\cdot (1,1)=0
        </me>.
        Letting <m>\boldx=(x,y)</m>, we see that <m>\boldx\cdot (1,1)=0</m> if and only if <m>x+y=0</m>, if and only if <m>y=-x</m>. Thus <m>W^\perp=\{(x,y)\colon y=-x\}</m> is the line <m>\ell'\subseteq \R^2</m> with equation <m>y=-x</m>. Observe that the lines <m>\ell</m> and <m>\ell'</m> are indeed perpendicular to one another. (Graph them!)
      </p>
    </solution>
  </example>
  <example xml:id="eg_ortho_comp_plane">
    <statement>
      <p>
        Consider the inner product space <m>\R^3</m> together with the dot product. Let <m>W\subseteq \R^3</m> be the plane with equation <m>x-2y-z=0</m>.  Compute <m>W^\perp</m> and identify this as a familiar geometric object in <m>\R^3</m>.
      </p>
    </statement>
    <solution>
      <p>
        First, solving <m>x-2y-z=0</m> for <m>(x,y,z)</m>, we see that
        <me>
          W=\{(2s+t,s,t)\colon s,t\in \R\}=\Span\{(2,1,0),(1,0,1)\}
        </me>.
        Next, according to <xref ref="rm_computing_ortho_comp"/> we have
        <me>
          \boldx\in W^\perp\iff \boldx\cdot (2,1,0)=0 \text{ and } \boldx\cdot (1,0,1)=0
        </me>.
       It follows that <m>W^\perp</m> is the set of vectors <m>\boldx=(x,y,z)</m> satisfying the linear system
       <me>
         \begin{linsys}{3}
         2x\amp +\amp y \amp \amp  \amp = \amp 0\\
         x\amp \amp  \amp +\amp z \amp = \amp 0
       \end{linsys}.
       </me>
       Solving this system using Gaussian elimination we conclude that
       <me>
       W^\perp=\{(t,-2t,-t)\colon t\in \R\}=\Span\{(1,-2,-1)\}
       </me>,
       which we recognize as the line <m>\ell\subseteq \R^3</m> passing through the origin with direction vector <m>(1,-2,-1)</m>. This is none other than the normal line to the plane <m>W</m> passing through the origin.
      </p>
    </solution>
  </example>
  <theorem xml:id="th_orthogonal_complement">
    <title>Orthogonal complement</title>
    <statement>
      <p>
        Let <m>(V,\langle \ , \rangle)</m> be an inner product vector space,
        and let <m>W\subseteq V</m> be a subspace.
        <ol>
          <li>
            <p>
              The orthogonal complement <m>W^\perp</m> is a subspace of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              We have <m>W\cap W^\perp=\{\boldzero\}</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\dim V=n\lt \infty</m> then
              <me>
                \dim W+\dim W^\perp=n
              </me>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <p>
        The proof is left as an exercise. (See <xref first="ex_orthocomp_subspace" last="ex_orthocomp_dim"/>.)
      </p>
    </proof>
  </theorem>
  <example>
    <statement>
      <p>
        Consider the inner product space <m>\R^3</m> with the dot product. Let <m>W=\Span\{(1,1,1)\}\subset \R^3</m>, the line
        passing through the origin with direction vector <m>(1,1,1)</m>.
        The orthogonal complement <m>W^\perp</m> is the set of vectors orthogonal to <m>(1,1,1)</m>. Using the definition of dot product, this is the set of solutions <m>(x,y,z)</m> to the equation
        <me>
          x+y+z=0
        </me>,
          which we recognize as the plane passing through the origin with normal vector <m>(1,1,1)</m>. Note that we have
          <me>
            \dim W+\dim W^\perp=1+2=3,
          </me>
          as predicted in <xref ref="th_orthogonal_complement"/>.

      </p>
    </statement>
  </example>
    <p>
      The notion of orthogonal complement gives us a more conceptual way of understanding the relationship between the various fundamental spaces of a matrix.
    </p>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be <m>m\times n</m>,
          and consider <m>\R^n</m> and <m>\R^m</m> as inner product spaces with respect to the dot product.
          Then:
          <ol>
            <li>
              <p>
                <m>\NS(A)=\left(\RS(A)\right)^\perp</m>,
                and thus <m>\RS(A)=\left(\NS(A)\right)^\perp</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\NS(A^T)=\left(\CS(A)\right)^\perp</m>,
                and thus <m>\CS(A)=\left(\NS(A^T)\right)^\perp</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <ol>
          <li>
            <p>
              Using the dot product method of matrix multiplication,
              we see that a vector <m>\boldx\in\NS(A)</m> if and only if
              <m>\boldx\cdot\boldr_i=0</m> for each row <m>\boldr_i</m> of <m>A</m>, if and only if <m>\boldx\cdot \boldw=0</m> for all <m>\boldw\in \Span \{\boldr_1, \boldr_2, \dots, \boldr_m\}=\RS A</m> (see <xref ref="rm_computing_ortho_comp"/>), if and only if <m>\boldx\in (\RS A)^\perp</m>. This shows <m>\NS A=(\RS A)^\perp</m>.
            </p>
            <p>
              We can use <xref ref="cor_orthocomp_selfdual"/> to conclude <m>\RS A=(\NS A)^\perp</m>. Alternatively, and more directly, the argument above shows that <m>\boldw\in \RS A\implies \boldw\in (\NS A)^\perp</m>, proving <m>\RS A\subseteq (\NS A)^\perp</m>. Next, by the rank-nullity theorem we have <m>\dim \RS A=n-\dim\NS A</m>; and by <xref ref="th_orthogonal_complement"/> we have <m>\dim (\NS A)^\perp=n-\dim\NS A</m>. It follows that <m>\dim\RS A=\dim (\NS A)^\perp</m>. Since <m>\RS A\subseteq (\NS A)^\perp</m> and <m>\dim \RS A=\dim (\NS A)^\perp</m>, we conclude by <xref ref="cor_dimension_subspace"/> that <m>\RS A=(\NS A)^\perp</m>.
            </p>
          </li>
          <li>
            <p>
              This follows from (1) and the fact that <m>\CS(A)=\RS(A^T)</m>.
            </p>
          </li>
        </ol>
      </proof>
    </theorem>

    <example>
      <statement>
        <p>
          Understanding the orthogonal relationship between <m>\NS A </m> and <m>\RS A</m> allows us in many cases to quickly determine/visualize the one from the other. As an example, consider <m>A=\begin{bmatrix}1\amp -1\amp 1\\ 1\amp -1\amp -1 \end{bmatrix}</m>. Looking at the columns, we see easily that <m>\rank A =2</m>,
          which implies that <m>\nullity A=3-2=1</m>.
          Since <m>(1,-1,0)</m> is an element of <m>\NS(A)</m> and <m>\dim\NS A=1</m>,
          we must have <m>\NS A=\Span\{(1,-1,0)\}</m>, a line. By orthogonality, we conclude that
          <me>
            \RS A=(\NS A)^\perp
          </me>,
          which is the plane with normal vector <m>(1,-1,0)</m> passing through the origin.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection>
    <title>Orthogonal Projection</title>
    <theorem xml:id="th_orthogonal_projection">
      <title>Orthogonal projection theorem</title>
      <statement>
        <p>
          Let <m>(V,\langle \ , \rangle)</m> be an inner product space,
          and let <m>W\subseteq V</m> be a
          finite-dimensional subspace.
        </p>
          <ol>
            <li>
              <title>Orthogonal decomposition</title>
              <p>
                For all <m>\boldv\in V</m> there are vectors <m>\boldw</m> and <m>\boldw^\perp</m> satisfying
                <men xml:id="eq_ortho_decomp">
                  \boldv=\boldw+\boldw^\perp, \boldw\in W, \boldw^\perp\in W^\perp
                </men>.
                Furthermore, the pair <m>\boldw, \boldw^\perp</m> is unique in the following sense: if we have <m>\boldv=\boldu+\boldu^\perp</m> for some <m>\boldu\in W</m> and <m>\boldu^\perp\in W^\perp</m>, then <m>\boldu=\boldw</m> and <m>\boldu^\perp=\boldw^\perp</m>. Accordingly, the vector equation <xref ref="eq_ortho_decomp"/> is called the <term>orthogonal decomposition</term> of <m>\boldv</m> with respect to <m>W</m>; and the vector <m>\boldw</m> is called the <term>orthogonal projection</term> of <m>\boldv</m> onto <m>W</m>, denoted <m>\boldw=\proj{\boldv}{W}</m>.
              </p>
            </li>
            <li>
              <title>Orthogonal projection formula</title>
              <p>
                Choose any orthogonal basis <m>B=\{\boldw_1, \boldw_2, \dots, \boldw_r\}</m> of <m>W</m>. We have
                <men xml:id="eq_ortho_proj_formula"> \proj{\boldv}{W}=\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i
                </men>.
              </p>
            </li>
            <li>
              <title>Distance to <m>W</m></title>
              <p>
                The orthogonal projection <m>\boldw=\proj{\boldv}{W}</m> is the element of <m>W</m> that is closest to <m>\boldv</m> in the following sense: for all <m>\boldw'\in W</m> we have
                <me>
                  d(\boldv, \proj{\boldv}{W})\leq d(\boldv, \boldw')
                </me>,
                or equivalently,
                <men xml:id="eq_ortho_proj_distance">
                \norm{\boldv-\proj{\boldv}{W}}\leq\norm{\boldv-\boldw'}
              </men>.
                Accordingly, we define the <term>distance</term> <m>d(\boldv, W)</m> between <m>\boldv</m> and <m>W</m> to be
                <me>
                  d(\boldv, W)=d(\boldv, \proj{\boldv}{W})=\norm{\boldv-\proj{\boldv}{W}}
                </me>.
              </p>
            </li>
          </ol>
      </statement>
      <proof>
        <p>
          Let <m>B=\{\boldw_1, \boldw_2, \dots, \boldw_r\}</m>. We first show that the vectors
          <men xml:id="eq_ortho_proj_formula_proof">
            \boldw=\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i
          </men>
          and <m>\boldw^\perp=\boldv-\boldw</m> satisfy the conditions in <xref ref="eq_ortho_decomp"/>. It is clear that the <m>\boldw</m> defined in <xref ref="eq_ortho_proj_formula_proof"/> is an element of <m>W</m>, since it is a linear combination of the <m>\boldw_i</m>. Furthermore, we see easily that our choice <m>\boldw^\perp=\boldv-\boldw</m> satisfies
          <me>
            \boldw+\boldw^\perp=\boldw+(\boldv-\boldw)=\boldv
          </me>.
          It remains only to show that <m>\boldw^\perp=\boldv-\boldw\in W^\perp</m>. Since <m>B</m> is a basis of <m>W</m>, it suffices to show that <m>\langle \boldw^\perp,\boldw_j\rangle=0</m> for all <m>1\leq i\leq r</m>. We compute:
          <md>
            <mrow>\langle \boldw^\perp, \boldw_j \rangle\amp = \langle \boldv-\proj{\boldv}{W}, \boldw_i \rangle</mrow>
            <mrow> \amp =
            \left\langle \boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\boldw_i, \boldw_j\right\rangle </mrow>
            <mrow>  \amp =
            \langle \boldv, \boldw_j\rangle -\sum_{i=1}^r\frac{\angvec{\boldv,\boldw_i}}{\angvec{\boldw_i, \boldw_i}}\langle \boldw_i, \boldw_j\rangle </mrow>
            <mrow>  \amp =
            \langle \boldv, \boldw_j\rangle -\frac{\langle \boldv, \boldw_j\rangle}{\cancel{\langle \boldw_j, \boldw_j\rangle}}\cancel{\langle \boldw_j, \boldw_j\rangle}</mrow>
            <mrow>  \amp = 0 </mrow>
          </md>,
          as desired.
        </p>
        <p>
          Having shown that a decomposition of <m>\boldv</m> of the form <xref ref="eq_ortho_decomp"/> exists, we now show it is unique in the sense specified. Suppose we have
          <me>
            \boldv=\boldw+\boldw^\perp=\boldu+\boldu^\perp
          </me>,
          where <m>\boldw, \boldu\in W</m> and <m>\boldw^\perp, \boldu^\perp\in W^\perp</m>. Rearranging, we see that
          <me>
            \boldw-\boldu=\boldu^\perp-\boldw^\perp
          </me>.
          We now claim that <m>\boldw-\boldu=\boldu^\perp-\boldw^\perp=\boldzero</m>, in which case <m>\boldw=\boldu</m> and <m>\boldw^\perp=\boldu^\perp</m>, as desired. To see why the claim is true, consider the vector <m>\boldv'=\boldw-\boldu=\boldu^\perp-\boldw^\perp</m>. Since <m>\boldv'=\boldw-\boldu</m>, and <m>\boldw, \boldu\in W</m>, we have <m>\boldv'\in W</m>.
          On the other hand, since <m>\boldv'=\boldu^\perp-\boldw^\perp</m>, and <m>\boldu^\perp, \boldw^\perp\in W^\perp</m>, we have <m>\boldv'\in W^\perp</m>. Thus <m>\boldv'\in W\cap W^\perp</m>. Since <m>W\cap W^\perp=\{\boldzero\}</m> (<xref ref="th_orthogonal_complement"/>), we conclude <m>\boldv'=\boldw-\boldu=\boldu^\perp-\boldw^\perp=\boldzero</m>, as claimed.
        </p>
        <p>
          At this point we have proved both (1) and (2), and it remains only to show that <xref ref="eq_ortho_proj_distance"/> holds for all <m>\boldw'\in W</m>. To this end we compute:
          <md>
            <mrow>\norm{\boldv-\boldw'}^2\amp = \norm{\boldw^\perp+(\boldw-\boldw')}^2</mrow>
            <mrow> \amp =\norm{\boldw^\perp}^2+\norm{\boldw-\boldw'}^2 \amp (<xref ref="ex_ortho_pythag"/>)</mrow>
            <mrow>\amp \geq \norm{\boldw^\perp}^2</mrow>
            <mrow> \amp =\norm{\boldv-\boldw}^2</mrow>
          </md>.
            This shows <m>\norm{\boldv-\boldw'}^2\geq \norm{\boldv-\boldw}^2</m>. Taking square-roots now proves the desired inequality.
        </p>
      </proof>

    </theorem>
      <remark xml:id="rm_ortho_proj_formula">
        <title>Orthogonal projection formula</title>
    <statement>
      <p>
        The formula <xref ref="eq_ortho_proj_formula"/> is very convenient for computing an orthogonal projection <m>\proj{\boldv}{W}</m>, but mark well this important detail: to apply the formula we must first provide an <em>orthogonal</em> basis of <m>W</m>. Thus unless one is provided, our first step in an orthogonal projection computation is to produce an orthogonal basis of <m>W</m>. In some simple cases (<eg />, when <m>W</m> is 1- or 2-dimensional) this can be done by inspection. Otherwise, we use the Gram-Schmidt procedure.
      </p>
    </statement>
  </remark>
  <example>
    <statement>
      <p>
        Consider the inner product space <m>\R^3</m> with the dot product. Let <m>W\subseteq\R^3</m> be the plane with equation <m>x+y+z=0</m>. Compute <m>\proj{\boldv}{W}</m> for each <m>\boldv</m> below.
        <ol>
          <li>
            <p>
              <m>\boldv=(3,-2,2)</m>
            </p>
          </li>
          <li>
            <p>
              <m>\boldv=(2,1,-3)</m>
            </p>
          </li>
          <li>
            <p>
              <m>\boldv=(-7,-7,-7)</m>
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <solution>
      <p>
        According to <xref ref="rm_ortho_proj_formula"/> our first step is to produce an orthogonal basis of <m>W</m>. We do so by inspection. Since <m>\dim W=2</m>, we simply need to find two solutions to <m>x+y+z=0</m> that are orthogonal to one another: <eg />, <m>\boldw_1=(1,-1,0)</m> and <m>\boldw_2=(1,1,-2)</m>. Thus we choose <m>B=\{ (1,-1,0), (1,1,-2)\}</m> as our orthogonal basis, and our computations become a matter of applying <xref ref="eq_ortho_proj_formula"/>, which in this case becomes
        <me>
          \proj{\boldv}{W}=\frac{\boldv\cdot\boldw_1}{\boldw_1\cdot \boldw_1}\boldw_1+\frac{\boldv\cdot\boldw_2}{\boldw_2\cdot \boldw_2}\boldw_2=
          \frac{\boldv\cdot\boldw_1}{2}\boldw_1+\frac{\boldv\cdot\boldw_2}{6}\boldw_2
        </me>.
        Now compute:
        <md>
          <mrow>\proj{(3,-2,2)}{W} \amp =\frac{5}{2}(1,-1,0)+\frac{-3}{6}(1,1,-2)=(2,-3,1)</mrow>
          <mrow>\proj{(2,1,-3)}{W} \amp =\frac{1}{2}(1,-1,0)+\frac{9}{6}(1,1,-2)=(2,1,-3)</mrow>
          <mrow>\proj{(-7,-7,-7)}{W} \amp =\frac{0}{2}(1,-1,0)+\frac{0}{6}(1,1,-2)=(0,0,0)</mrow>
        </md>.
        The last two computations might give you pause. Why do we have <m>\proj{(2,1,-3)}{W}=(2,1,-3)</m> and <m>\proj{(-7,7-7,-7)}{W}=(0,0,0)</m>? The answer is that <m>(2,1,-3)</m> is already an element of <m>W</m>, so it stands to reason that its projection is itself; and <m>(-7,-7,-7)</m> is already orthogonal to <m>W</m> (it is a scalar multiple of <m>(1,1,1)</m>), so it stands to reason that its projection is equal to <m>\boldzero</m>. See <xref ref="ex_orthoproj_props"/> for a rigorous proof of these claims.
      </p>
    </solution>
  </example>


  <corollary xml:id="cor_orthocomp_selfdual">
    <statement>
      <p>
        Let <m>(V,\angvec{\ , \ })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
        Then <m>(W^\perp)^\perp=W</m>.
      </p>
    </statement>
  </corollary>
  <proof>
    <p>
      Clearly <m>W\subseteq (W^\perp)^\perp</m>.
      For the other direction, take <m>\boldv\in (W^\perp)^\perp</m>.
      Using the <em>orthogonal projection theorem</em>,
      we can write <m>\boldv=\boldw+\boldw^\perp</m> with
      <m>\boldw\in W</m> and <m>\boldw^\perp\in W^\perp</m>.
      We will show <m>\boldw^\perp=\boldzero</m>.
    </p>
    <p>
      Since <m>\boldv\in (W^\perp)^\perp</m> we have <m>\angvec{\boldv,\boldw^\perp}=0</m>.
      Then we have
      <md>
        <mrow>0\amp =\angvec{\boldv,\boldw^\perp}</mrow>
        <mrow>\amp =\angvec{\boldw+\boldw^\perp,\boldw^\perp}</mrow>
        <mrow>\amp =\angvec{\boldw,\boldw^\perp}+\angvec{\boldw^\perp,\boldw^\perp} \amp \text{ (since \(W\perp W^\perp\)) }</mrow>
        <mrow>\amp =0+\angvec{\boldw^\perp,\boldw^\perp}</mrow>
      </md>
    </p>
    <p>
      Thus <m>\angvec{\boldw^\perp,\boldw^\perp}=0</m>.
      It follows that <m>\boldw^\perp=\boldzero</m>,
      and hence <m>\boldv=\boldw+\boldzero=\boldw\in W</m>.
    </p>
  </proof>
  <corollary xml:id="cor_orthoproj_linear">
    <title>Orthgonal projection is linear</title>


    <statement>
      <p>
        Let <m>(V,\angvec{\ , \ })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
      </p>
      <ol>
        <li>
          <p>
            The function <m>T\colon V\rightarrow V</m> defined as <m>T(\boldv)=\proj{\boldv}{W}</m> is a linear transformation.
          </p>
        </li>
        <li>
          <p>
            We have <m>\im T=W</m> and <m>\NS T=W^\perp</m>.
          </p>
        </li>
      </ol>
    </statement>
  </corollary>
  <proof>
    <p>
      <ol>
        <li>
          <p>
            We must show that <m>T(c\boldv+d\boldw)=cT(\boldv)+dT(\boldw)</m> for all
            <m>c,d\in\R</m> and <m>\boldv,\boldw\in V</m>.
            We pick an orthogonal basis
            <m>B=\{\boldv_1,\boldv_2, \dots, \boldv_r\}</m> of <m>W</m> and compute, using formula <xref ref="eq_ortho_proj_formula"/>:
            <md>
              <mrow>T(c\boldv+d\boldw) \amp=\sum_{i=1}^{r}\frac{\langle c\boldv+d\boldw, \boldv_i\rangle}{\langle\boldv_i, \boldv_i\rangle }\boldv_i </mrow>
              <mrow> \amp=\sum_{i=1}^r\frac{c\langle \boldv,\boldv_i\rangle+d\langle \boldw, \boldv_i\rangle}{\langle \boldv_i, \boldv_i\rangle}\boldv_i </mrow>
              <mrow>  \amp =c\sum_{i=1}^r\frac{\angvec{\boldv, \boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i+d\sum_{i=1}^r\frac{\angvec{\boldw, \boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i</mrow>
              <mrow>  \amp = cT(\boldv)+dT(\boldw)</mrow>
            </md>.
          </p>
        </li>
        <li>
          <p>
           We have by definition that <m>T(\boldv)=\proj_W(\boldv)\in W</m>, and thus <m>\im T\subseteq W</m>. For the other direction, if <m>\boldw\in W</m>, then <m>\boldw=\proj_W(\boldw)</m> (The statements about <m>\im T</m> and <m>\NS T</m> amount to the two following equivalences:
            <md>
              <mrow>\boldv\in W \amp \iff \proj</mrow>
              <mrow> \amp </mrow>
            </md>
          </p>
        </li>
      </ol>

    </p>
  </proof>
</subsection>

<subsection>
    <title>Orthogonal projection in <m>\R^2</m> and <m>\R^3</m></title>
    <introduction>
      <p>
        For this subsection we will always work within Euclidean space: <ie />, <m>V=\R^n</m> with the dot product. In applications we often want to compute the projection of a point onto a line (in <m>\R^2</m> or <m>\R^3</m>) or plane (in <m>\R^3</m>). According to <xref ref="cor_orthoproj_linear"/> the operation of projecting onto any subspace <m>W\subseteq \R^n</m> is in fact a linear transformation <m>T\colon \R^n\rightarrow \R^n</m>. By <xref ref="cor_matrix_transformations"/> we have <m>T=T_A</m>, where
      <me>
        A=\begin{bmatrix}
        \vert \amp \vert \amp \amp \vert \\
        T(\bolde_1)\amp T(\bolde_2)\amp \cdots \amp T(\bolde_n) \\
        \vert \amp \vert \amp \amp \vert
      \end{bmatrix}
      </me>.
      Lastly, <xref ref="eq_ortho_proj_formula"/> gives us an easy formula for computing <m>T(\bolde_j)</m> for all <m>j</m>, once we have selected an orthogonal basis for <m>W</m>. As a result we can easily derive matrix formulas for projection onto any subspace <m>W</m> of any Euclidean space <m>\R^n</m>. We illustrate this with some examples in <m>\R^2</m> and <m>\R^3</m> below.
    </p>
    </introduction>
    <example>
      <title>Projection onto a line <m>\ell</m></title>
      <statement>
        <p>
          Any line in <m>\R^2</m> passing through the origin can be described as <m>\ell=\Span\{\boldv_0\}</m>,
          for some <m>\boldv_0=(a,b)\ne 0</m>.
          Since <m>\{(a,b)\}</m> is trivially an orthogonal basis of <m>\ell</m>,
          by the orthogonal projection theorem we have,
          for any <m>\boldv=(x,y,z)</m>
          <me>
            \proj{\boldv}{\ell}=\frac{\boldv\cdot \boldv_0}{\boldv_0\cdot\boldv_0}\boldv_0=\frac{ax+by+cz}{a^2+b^2+c^2}(a,b,c)=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} \begin{bmatrix}x\\ y\\ z \end{bmatrix}
          </me>.
        </p>
      </statement>
    </example>


    <example>
      <title>Projection onto planes in <m>\R^3</m></title>
      <statement>
        <p>
          Any plane in <m>\R^3</m> passing through the origin can be described with the equation
          <m>\mathcal{P}\colon ax+by+cz=0</m> for some <m>\boldn=(a,b,c)\ne 0</m>.
          This says precisely that <m>\mathcal{P}</m> is the orthogonal complement of the line <m>\ell=\Span\{(a,b,c)\}</m>:
          i.e., <m>\mathcal{P}=\ell^\perp</m>.
        </p>
        <p>
          From the orthogonal projection theorem, we know that
          <me>
            \boldv=\proj{\boldv}{\ell}+\proj{\boldv}{\ell^\perp}=\proj{\boldv}{\ell}+\proj{\boldv}{\mathcal{P}}
          </me>.
        </p>
        <p>
          But then
          <me>
            \proj{\boldv}{\mathcal{P}}=\boldv-\proj{\boldv}{\ell}=I \ \boldv-\proj{\boldv}{\ell}=(I-A)\boldv
          </me>,
          where <m>A</m> is the matrix formula for
          <m>\proj{\boldv}{\ell}</m> from the previous example.
          We conclude that the matrix defining <m>\proj{\boldv}{\mathcal{P}}</m> is
          <me>
            I-\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} = \frac{1}{a^2+b^2+c^2}\begin{bmatrix}b^2+c^2\amp -ab\amp -ac\\ -ab\amp a^2+c^2\amp -bc\\ -ac\amp -bc\amp a^2+b^2 \end{bmatrix}
          </me>
        </p>
      </statement>
    </example>
</subsection>
  <!-- <subsection>
    <title>Projection onto a plane</title>

  </subsection>
  <p>
    We can express this in terms of matrix multiplication as
  </p>
  <p>
    \item Translate the whole picture by <m>-Q=(-q_1,-q_2, -q_3)</m>,
    which means we replace <m>P=(x,y,z)</m> with
    <m>P-Q=(x-q_1,y-q_2,z-q_3)</m>. \item Apply our formulas from before,
    replacing <m>(x,y,z)</m> with
    <m>(x-q_1,y-q_2,z-q_3)</m> \item Translate back by adding <m>Q</m> to your answer.
  </p> -->

  <subsection>
    <title>Example: sine/cosine series</title>
    <p>
      Let <m>V=C[0,2\pi]</m> with inner product <m>\langle f, g\rangle=\int_0^{2\pi}f(x)g(x) \, dx</m>.
    </p>
    <p>
      We have seen that the set
      <me>
        B=\{1, \cos(x),\sin(x),\cos(2x),\sin(2x), \dots , \cos(nx),\sin(nx)\}
      </me>
      is orthogonal.
      Thus <m>B</m> is an orthogonal basis of <m>W=\Span(B)</m>,
      which we might describe as the space of
      <term>trigonometric polynomials of degree at most <m>n</m></term>.
    </p>
    <p>
      Given an arbitrary function <m>f(x)\in C[0,2\pi]</m>,
      its orthogonal projection onto <m>W</m> is the function
      <me>
        \hat{f}(x)=a_0+a_1\cos(x)+b_1\sin(x)+a_2\cos(2x)+b_2\sin(2x)+\cdots +a_n\cos(nx)+b_n\sin(nx)
      </me>,
      where
      <me>
        a_0=\frac{1}{2\pi}\int_0^{2\pi} f(x) \ dx, \ a_j=\frac{1}{\pi}\int_0^{2\pi}f(x)\cos(jx)\, dx, \ b_k=\frac{1}{\pi}\int_0^{2\pi}f(x)\sin(kx)\, dx
      </me>.
    </p>
    <p>
      The projection theorem tells us that <m>\hat{f}</m> is the
      <q>best</q>
      trigonometric polynomial approximation of <m>f(x)</m>
      (of degree at most <m>n</m>),
      in the sense that for any other sinusoidal <m>g\in W</m>,
      <m>\left\vert\left\vert f-\hat{f}\right\vert\right\vert\leq \norm{f-g}</m>.
    </p>
    <p>
      This means in turn
      <me>
        \int_0^{2\pi} (f-\hat{f})^2\, dx\leq \int_0^{2\pi} (f-g)^2 \, dx
      </me>.
    </p>
  </subsection>
  <subsection>
    <title>Example: least-squares solution to $A\boldx=\boldy$</title>
    <p>
      Often in applications we have an
      <m>m\times n</m> matrix <m>A</m> and vector
      <m>\boldy\in\R^m</m> for which the matrix equation
      <me>
        A\boldx=\boldy
      </me>
      has no solution.
      In terms of fundamental spaces,
      this means simply that <m>\boldy\notin \CS(A)</m>.
      Set <m>W=\CS(A)</m>.
    </p>
    <p>
      In such situations we speak of a <em>least-squares</em>
      solution to the matrix equation.
      This is a vector <m>\hat{\boldx}</m> such that <m>A\hat{\boldx}=\hat{\boldy}</m>,
      where <m>\hat{\boldy}=\proj{\boldy}{W}</m>.
      Here the inner product is taken to be the dot product.
    </p>
    <p>
      Note: the equation <m>A\hat{\boldx}=\hat{\boldy}</m> is guaranteed to have a solution since
      <m>\hat{\boldy}=\proj{\boldy}{W}</m> lies in <m>\CS(A)</m>.
    </p>
    <p>
      The vector <m>\hat{\boldx}</m> is called a least-square solutions because its image
      <m>\hat{\boldy}</m> is the element of <m>\CS(A)</m> that is
      <q>closest</q>
      to <m>\boldy</m> in terms of the dot product.
      Writing <m>\boldy=(y_1,y_2,\dots,y_n)</m> and <m>\hat{\boldy}=(y_1',y_2',\dots,
      y_n')</m>,
      this means that <m>\hat{\boldy}</m> minimizes the distance
      <me>
        \norm{\boldy-\hat{\boldy}}=\sqrt{(y_1-y_1')^2+(y_2-y_2')^2+\cdots +(y_n-y_n')^2}
      </me>.
    </p>
  </subsection>
  <subsection>
    <title>Least-squares example (curve fitting)</title>
    <p>
      Suppose we wish to find an equation of a line <m>y=mx+b</m> that best fits
      (in the least-square's sense)
      the following <m>(x,y)</m> data points:
      <m>P_1=(-3,1), P_2=(1,2), P_3=(2,3)</m>.
    </p>
    <p>
      Then we seek <m>m</m> and <m>b</m> such that
      <md>
        <mrow>1\amp =m(-3)+b</mrow>
        <mrow>2\amp =m(1)+b</mrow>
        <mrow>3\amp =m(2)+b</mrow>
      </md>,
      or equivalently,
      we wish to solve <m>\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} \begin{bmatrix}m \\ b \end{bmatrix} =\begin{bmatrix}1\\ 2\\ 3 \end{bmatrix}</m>.
    </p>
    <p>
      This equation has no solution as
      <m>\boldy=(1,2,3)</m> does no lie in <m>W=\CS(A)=\Span(\{(-3,1,2),(1,1,1)\}</m>.
      So instead we compute <m>\hat{\boldy}=\proj{\boldy}{W}=(13/14,33/14,38/14)</m>. (This was not hard to compute as conveniently the given basis of <m>W</m> was already orthogonal!)
    </p>
    <p>
      Finally we solve <m>A\begin{bmatrix}m\\ b \end{bmatrix} =\hat{\boldy}</m>,
      getting <m>m=5/14</m>, <m>b=28/14=2</m>.
      Thus <m>y=\frac{5}{14}x+2</m> is the line best fitting the data in the least-squares sense.
    </p>
  </subsection>
  <subsection>
    <title>Least-squares example contd.</title>
    <p>
      In what sense does <m>y=\frac{5}{14}x+2</m>
      <q>best</q>
      fit the data?
      <!-- <me>
        <image width="37%" source="images/LeastSquares.png"/>
      </me> -->
    </p>
    <p>
      Let <m>\boldy=(1,2,3)=(y_1,y_2,y_3)</m> be the given <m>y</m>-values of the points,
      and <m>\hat{\boldy}=(y_1',y_2',y_3')</m> be the projection we computed before.
      In the graph the values <m>\epsilon_i</m> denote the vertical difference
      <m>\epsilon_i=y_i-y_i'</m> between the data points, and our fitting line.
    </p>
    <p>
      The projection <m>\hat{\boldy}</m> makes the error
      <m>\norm{\boldy-\hat{\boldy}}=\sqrt{ \epsilon_1^2+\epsilon_2^2+\epsilon_3^2}</m> as small as possible.
    </p>
    <p>
      This means if I draw <em>any other line</em>
      and compute the corresponding differences
      <m>\epsilon_i'</m> at the <m>x</m>-values -3, 1 and 2, then we have
      <me>
        \epsilon_1^2+\epsilon_2^2+\epsilon_3^2\leq (\epsilon_1')^2+(\epsilon_2')^2+(\epsilon_3')^2
      </me>
    </p>
  </subsection>
  <subsection>
  <title>Finding least squares solutions</title>
  <p>
    As the last example illustrated,
    one method of finding a least-squares solution <m>\boldx</m> to
    <m>A\boldx=\boldy</m> is to first produce an orthogonal basis for <m>\CS(A)</m>,
    then compute <m>\hat{\boldy}=\proj{\boldy}{\CS(A)}</m>,
    and then use GE to solve <m>A\boldx=\hat{\boldy}</m>.
  </p>
  <p>
    Alternatively, it turns out
    (through a little trickery)
    that <m>\hat{\boldy}=A\boldx</m>,
    where <m>\boldx</m> is a solution to the equation
    <me>
      A^TA\boldx=A^T\boldy
    </me>.
  </p>
  <p>
    This solves us the hassle of computing an orthogonal basis for <m>\CS(A)</m>;
    to find a least-squares solution <m>\boldx</m> for <m>A\boldx=\boldy</m>,
    we simply use GE to solve the boxed equation. (Some more trickery shows a solution is guaranteed to exist!)
  </p>
  <subsection>
    <title>Example</title>
    <p>
      In the previous example we were seeking a least-squares solution
      <m>\boldx=\colvec{m\\ b}</m> to <m>A\boldx=\boldy</m>,
      where <m>A=\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} , \boldy=\colvec{1\\2\\3}</m>.
    </p>
  </subsection>
  <p>
    The equation <m>A^TA\boldx=A^T\boldy</m> is thus
    <me>
      \begin{bmatrix}14\amp 0\\ 0\amp 3 \end{bmatrix} \boldx= \colvec{5\\ 6}
    </me>
  </p>
  <p>
    As you can see,
    <m>\boldx=\colvec{m\\ b}=\colvec{5/14\\ 2}</m> is a least-squares solution,
    just as before
  </p>
  </subsection>
  <xi:include href="./s_orthogonal_projection_ex.ptx"/>
</section>
