<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_transformation">
  <title>Linear transformations</title>
  <introduction>
    <p>
    As detailed in <xref ref="d_linear_transform"/> a <em>linear transformation</em> is a special type of function between two vector spaces: one that <em>respects</em> in some sense the vector operations of both spaces.
    </p>
    <p>
      This manner of theorizing is typical in mathematics:
      first we introduce a special class of objects defined axiomatically, then we introduce special functions or maps between these objects. Since the original objects of study (e.g. vector spaces) come equipped with special structural properties (e.g. vector operations), the functions we wish to study are the ones that somehow acknowledge this structure.
    </p>
    <p>
      You have already seen this principle at work in your study of calculus.
      First we give <m>\R</m> some structure by defining a notion of proximity (i.e., <m>x</m> is close to <m>y</m> if <m>\val{x-y}</m> is small),
      then we introduce a special family of functions that somehow respects this structure:
      these are precisely the <em>continuous</em> functions!
    </p>
    <p>
      As you will see, linear transformations are not just interesting objects of study in their own right, they also serve as invaluable tools in our continued exploration of the intrinsic properties of vector spaces.
    </p>
    <p>
      In the meantime rejoice in the fact that we can now give a succinct definition of linear algebra:
      it is the theory of vector spaces and the linear transformations between them. Go shout it from the rooftops!
    </p>
  </introduction>
  <paragraphs xml:id="ss_functions">
    <title>Function definitions</title>
    <p>
      Before getting to the main attraction, we first establish some basic definitions and notation regarding functions.
    </p>
    <definition xml:id="d_functions">
      <title>Functions</title>
      <idx><h>function</h></idx>
      <notation>
        <usage>f\colon A\rightarrow B</usage>
        <description>function from <m>A</m> to <m>B</m></description>
      </notation>
      <statement>
        <p>
        Let <m>A</m> and <m>B</m> be two sets. A <term>function from <m>A</m> to <m>B</m></term>, denoted <m>f\colon A\rightarrow B</m>, is a rule which, given any <term>input</term> <m>a\in A</m>, returns an <term>output</term> <m>b\in B</m>. In this case we write <m>b=f(a)</m> and call <m>b</m> the <term>image of <m>a</m> under <m>f</m></term>, or the <term>value of <m>f</m> at <m>a</m> </term>. Similarly, we say <m>f</m> <term>maps</term> (or <term>sends</term>) the input <m>a</m> to the output <m>b</m>.
        </p>
        <p>
          The set <m>A</m> is called the <term>domain</term> of <m>f</m>; the set <m>B</m> is called the <term>codomain</term> of <m>f</m>.
        </p>
        <p>
          When we wish to indicate what rule defines the function, we use the elaborated notation
          <md>
            <mrow>f\colon A \amp\rightarrow B </mrow>
            <mrow> a \amp\mapsto f(a)</mrow>
          </md>.
          This is read as <q>The function <m>f</m> from <m>A</m> to <m>B</m> maps an input <m>a</m> to the output <m>f(a)</m></q>.
        </p>
      </statement>
    </definition>
    <definition xml:id="d_function_equality">
      <title>Function equality</title>
      <idx><h>function</h><h>equality</h></idx>
      <statement>
        <p>
        Functions <m>f</m> and <m>g</m> are <term>equal</term> if
        <ol>
          <li>
            <p>
              they have the same domain <m>A</m> and codomain <m>B</m>, and
            </p>
          </li>
          <li>
            <p>
              for all <m>a\in A</m>, we have <m>f(a)=g(a)</m>.
            </p>
          </li>
        </ol>
        </p>
      </statement>
    </definition>
    <definition xml:id="d_function_composition">
      <title>Function composition</title>
      <idx><h>function</h><h>composition</h></idx>
      <notation>
        <usage>f\circ g</usage>
        <description>the composition of <m>f</m> and <m>g</m></description>
      </notation>
      <statement>
        <p>
          Suppose <m>f\colon C\rightarrow D</m> and <m>g\colon A\rightarrow B</m> are functions satisfying <m>B\subseteq C</m>.  The <term>composition of <m>f</m> and <m>g</m></term>, is the function <m>f\circ g\colon A\rightarrow D</m> defined as follows:
          <md>
            <mrow>f\circ g\colon A \amp\rightarrow D </mrow>
            <mrow> a\amp\mapsto f(g(a)) </mrow>
          </md>.
        In other words, to evaluate <m>f\circ g</m> at the input <m>a</m> we <em>first</em> compute <m>g(a)</m>, and <em>then</em> compute <m>f(g(a))</m>.
        </p>
      </statement>
    </definition>
  </paragraphs>
  <paragraphs xml:id="ss_linear_transform">
    <title>Linear transformations</title>
  <definition xml:id="d_linear_transform">
    <title>Linear transformations</title>

    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
        A function <m>T\colon V\rightarrow W</m> is a <term>linear transformation</term> (or <term>linear</term>)
        if it satisfies the following properties:
        <ol>
          <li>
            <p>
              For all <m>\boldv_1, \boldv_2\in V</m>, we have <m>T(\boldv_1+\boldv_2)=T(\boldv_1)+T(\boldv_2)</m>.
            </p>
          </li>
          <li>
            <p> For all <m>c\in \R</m> and <m>\boldv\in V</m> we have
              <m>T(c\boldv)=cT(\boldv)</m>.
            </p>
          </li>
        </ol>
      </p>
      <p>
        If a function between vector spaces is <term>nonlinear</term> if it is not a linear transformation.
      </p>
    </statement>
  </definition>
    <remark>
  <statement>
    <p>
      How precisely does a linear transformation <q>respect</q> vector space structure?
      In plain English: the image of a sum is the sum of the images,
      and the image of a scalar multiple is the scalar multiple of the image.
    </p>
  </statement>
</remark>
  <p>
    As our first examples of linear transformations, we define the <em>zero transformation</em> and <em>identity transformation</em> on a vector space.
  </p>
  <definition xml:id="d_transform_zero_identity">
    <title>Zero and identity transformation</title>
    <idx><h>linear transformation</h><h>zero transformation</h></idx>
    <idx><h>linear transformation</h><h>identity transformation</h></idx>
    <statement>
        <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
      </p>
      <p>
        The <term>zero transformation from <m>V</m> to <m>W</m></term>, denoted <m>T_0</m>, is defined as follows:
        <md>
          <mrow>T_0\colon V \amp\rightarrow W  </mrow>
          <mrow> \boldv \amp\mapsto T_0(\boldv)=\boldzero_W</mrow>
        </md>,
        where <m>\boldzero_W</m> is the zero vector of <m>W</m>. In other words, <m>T_0</m> is the function that maps all elements of <m>V</m> to the zero vector of <m>W</m>.
      </p>
      <p>
        The <term>identity transformation of <m>V</m></term>, denoted <m>\id_V</m>, is defined as follows:
        <md>
          <mrow> \id_V\colon V \amp\rightarrow V </mrow>
          <mrow> \boldv \amp\mapsto \id_V(\boldv)=\boldv</mrow>
        </md>.
        In other words, <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>.
      </p>
    </statement>
  </definition>
  <remark xml:id="rm_transform_zero_identity">
  <title>Model of linear transformation proof</title>
  <statement>
    <p>
    Let's show that the zero and identity functions are indeed linear transformations.
    </p>
    <p>
    Let <m>V</m> and <m>W</m> be vector spaces, and let <m>T_0\colon V\rightarrow W</m> be the zero function. We verify each defining property separately.
    <ol>
      <li>
        <p>
          Given <m>\boldv, \boldw\in V</m>, we have
          <md>
            <mrow>T_0(\boldv_1+\boldv_2)\amp =\boldzero_W  \amp (\text{by def.}) </mrow>
            <mrow> \amp =\boldzero_W+\boldzero_W </mrow>
            <mrow>  \amp = T_0(\boldv_1)+T_0(\boldv_2) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          Given <m>c\in \R</m> and <m>\boldv\in V</m>, we have
          <md>
            <mrow>T_0(c\boldv) \amp = \boldzero_W \amp (\text{by def.})</mrow>
            <mrow> \amp = c\boldzero_W \amp (<xref ref="th_vectorspace_props"/>) </mrow>
            <mrow>  \amp = cT_0(\boldv) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
    </ol>
    This proves that <m>T_0\colon V\rightarrow W</m> is a linear transformation.
    </p>
    <p>
    Now let <m>V</m> be a vector spaces, and let <m>\id_V\colon V\rightarrow V</m> be the identity function.
    <ol>
      <li>
        <p>
          Given <m>\boldv, \boldw\in V</m>, we have
          <md>
            <mrow>\id_V(\boldv_1+\boldv_2)\amp =\boldv_1+\boldv_2  \amp (\text{by def.})</mrow>
            <mrow> \amp = \id_V(\boldv_1)+\id_V(\boldv_2) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          Given <m>c\in \R</m> and <m>\boldv\in V</m>, we have
          <md>
            <mrow>\id_V(c\boldv) \amp = c\boldv \amp (\text{by def.})</mrow>
            <mrow> \amp = c\id_V(\boldv) \amp (\text{by def.}) </mrow>
          </md>.
        </p>
      </li>
    </ol>
    This proves that <m>\id_V\colon V\rightarrow V</m> is a linear transformation.
    </p>
  </statement>
</remark>
<theorem xml:id="th_transform_basic_props">
  <title>Basic properties of linear transformations</title>
  <statement>
    <p>
      Let <m>T\colon V\rightarrow W</m> be a linear transformation. Let <m>\boldzero_V</m> and <m>\boldzero_W</m> be the zero vectors of <m>V</m> and <m>W</m>, respectively.
    </p>
    <ol>
      <li>
        <p>
          We have <m>T(\boldzero_V)=\boldzero_W</m>.
        </p>
      </li>
      <li>
        <p>
          For all <m>\boldv\in V</m>, we have <m>T(-\boldv)=-T(\boldv)</m>.
        </p>
      </li>
      <li>
        <p>
          For any linear combination
          <me>\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n\in V</me>
          we have
          <me>
            T(\boldv)=T(c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n)=c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots +c_nT(\boldv_n)
          </me>.
        </p>
      </li>
    </ol>
  </statement>
  <proof>
    <p>
      <ol>
        <li>
          <p>
            We employ some similar trickery to what was done in the proof of <xref ref="th_vectorspace_props"/>. Assuming <m>T</m> is linear:
            <md>
              <mrow>T(\boldzero_V)  \amp= T(\boldzero_V+\boldzero_V)</mrow>
              <mrow> \amp =T(\boldzero_V)+T(\boldzero_V) \amp (<xref ref="d_linear_transform"/>) </mrow>
            </md>.
            Thus, whatever <m>T(\boldzero_V)\in W</m> may be, it satisfies
            <me>
              T(\boldzero_V)=T(\boldzero_V)+T(\boldzero_V)
            </me>.
            Canceling <m>T(\boldzero_V)</m> on both sides using <m>-T(\boldzero_V)</m>, we conclude
            <me>
              \boldzero_W=T(\boldzero_V)
            </me>.
          </p>
        </li>
        <li>
          <p>
            The argument is similar:
            <md>
              <mrow>\boldzero_W \amp= T(\boldzero_V) \amp (\text{by (1)})</mrow>
              <mrow> \amp =T(-\boldv+\boldv)</mrow>
              <mrow>  \amp = T(-\boldv)+T(\boldv)</mrow>
            </md>.
            Since <m>\boldzero_W=T(-\boldv)+T(\boldv)</m>, adding <m>-T(\boldv)</m> to both sides of the equation yields
            <me>
              -T(\boldv)=T(-\boldv)
            </me>.

          </p>
        </li>
        <li>
          <p>
            This is an easy proof by induction using the two defining properties of a linear transformation in tandem.
          </p>
        </li>
      </ol>
    </p>
  </proof>
</theorem>
    <remark xml:id="rm_transform_dist">
  <statement>
    <p>
    Statement (3) of <xref ref="th_transform_basic_props"/> provides a more algebraic interpretation of how linear transformations preserve vector space structure: namely, they <em>distribute</em> over linear combinations of vectors.
    </p>
  </statement>
</remark>

    <remark xml:id="rm_transform_prooftip">
  <title>The one-step linear transformation proofs</title>
  <statement>
    <p>
    As a sort of converse to (3) in <xref ref="th_transform_basic_props"/>, observe that if <m>T\colon V\rightarrow W</m> satisfies
    <me>
      T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)
    </me>
    for all <m>c, d\in \R</m> and <m>\boldv_1,\boldv_2\in V</m>, and <m>\boldw\in W</m>, then <m>T</m> is linear. Indeed, taking the special case <m>c=d=1</m> yields (1) of <xref ref="d_linear_transform"/>; and choosing <m>d=0</m> yields (2) of <xref ref="d_linear_transform"/>.
    </p>
    <p>
      This observation gives rise to an alternate one-step technique for proving linearity: given <m>T\colon V\rightarrow W</m>, show that
      <me>
        T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)
      </me>
      for all <m>c, d\in \R</m> and <m>\boldv_1,\boldv_2\in V</m>.
    </p>
  </statement>
</remark>

</paragraphs>

<paragraphs xml:id="ss_transform_examples">
  <title>Examples</title>
  <p>
    Our next example is in fact an entire family of examples of linear transformations: so-called <em>matrix transformations</em> of the form <m>T_A\colon \R^n\rightarrow \R^m</m>, where <m>A</m> is a given <m>m\times n</m> matrix. This is a good place to recall the <xref ref="princ_matrix_mantra" text='custom'> matrix mantra </xref>. Not only can a matrix represent a system of linear equations, it can represent a linear transformation. These are two very different concepts, and the <xref ref="princ_matrix_mantra" text='custom'> matrix mantra </xref> helps us not confuse the two. In the end a matrix is just a matrix: a mathematical tool that can be employed to diverse ends.
  </p>
  <p>
    The definition of a <em>matrix transformation</em> below marks the first point where we take some artistic license and conflate the vector space <m>\R^n</m> with the vector space <m>M_{n\times 1}</m> of column vectors. The rationale here is partly notational in nature (<m>\R^n</m> is less clunky than <m>M_{n\times 1}</m>), and partly conceptual. We will see that the spaces <m>\R^n</m> come equipped with a natural geometric structure, allowing us to interpret the linear transformations among them in a more conceptual manner, especially when <m>n=2, 3</m>.
  </p>
  <definition xml:id="d_matrix_transform">
    <title>Matrix transformations</title>
    <idx><h>matrix transformation</h></idx>
    <notation>
      <usage>T_A</usage>
      <description>the matrix transformation associated to <m>A</m></description>
    </notation>
    <statement>
      <p>
      Let <m>A</m> be an <m>m\times n</m> matrix. Identifying <m>n</m>-tuples <m>(x_1,x_2,\dots, x_n)\in \R^n</m> with their corresponding column vectors <m>\boldx=[x_i]_{n\times 1}</m>, and <m>m</m>-tuples <m>(y_1,y_2,\dots, y_m)\in\R^m</m> with their corresponding column vectors <m>[y_i]_{m\times 1}</m>,  the <term>matrix transformation associated to <m>A</m></term> is the function <m>T_A</m> defined as follows:
      <md>
        <mrow>T_A\colon \R^n \amp\rightarrow \R^m </mrow>
        <mrow> \boldx\amp\mapsto T_A(\boldx)=A\boldx </mrow>
      </md>.
      In other words, given input <m>\boldx_{n\times 1}</m>, the output is the <m>m\times 1</m> column vector <m>A\boldx</m>.
      </p>
    </statement>
  </definition>
  <theorem xml:id="th_matrix_transform_i">
    <title>Matrix transformations I</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>m\times n</m> matrix. The function <m>T_A\colon \R^n\rightarrow \R^m</m> is a linear transformation.
      </p>
    </statement>
    <proof>
      <p>
        We use the one-step technique. For any <m>c,d\in \R</m> and <m>\boldx_1, \boldx_2\in \R^n</m>, we have
        <md>
          <mrow>T_A(c\boldx_1+d\boldx_2) \amp =A(c\boldx_1+d\boldx_2)</mrow>
          <mrow> \amp =A(c\boldx_1)+A(d\boldx_2) \amp (<xref ref="th_matrix_alg_props"/>) </mrow>
          <mrow>  \amp =cA\boldx_1+dA\boldx_2 \amp (<xref ref="th_matrix_alg_props"/>)</mrow>
          <mrow>  \amp =cT_A(\boldx_1)+dT_A(\boldx_2)</mrow>
        </md>.
        This proves <m>T_A</m> is a linear transformation.
      </p>
    </proof>

  </theorem>
    <remark xml:id="rm_matrix_transform">
  <statement>
    <p>
      As the title of <xref ref="th_matrix_transform_i"/> suggests, there is a follow-up theorem, and this states that in fact <em>any</em> linear transformation <m>T\colon\R^n\rightarrow\R^m </m> is of the form <m>T=T_A</m> for some <m>m\times n</m> matrix <m>A</m>. In other words, all linear transformations from <m>\R^n</m> to <m>\R^m</m> are matrix transformations.
    </p>
    <p>
      As general as these two results are, mark well the <em>restriction</em> that remains: they apply only to functions with domain and codomain equal to a vector spaces of tuples. They say nothing for example about functions from <m>\R^\infty</m> to <m>F([0,1],\R)</m>.
    </p>
  </statement>
</remark>
    <remark xml:id="rm_matrix_transform_example">
  <statement>
    <p>
    <xref ref="th_matrix_transform_i"/> gives rise to an alternative technique for showing a function <m>T\colon \R^n\rightarrow \R^m</m> is a linear transformation: show that <m>T=T_A</m> for some matrix <m>A</m>.
    </p>
    <p>
      As an example, consider the function
      <md>
        <mrow>T\colon \R^2 \amp\rightarrow  \R^3 </mrow>
        <mrow> (x,y)\amp\mapsto (7x+2y, -y, x) </mrow>
      </md>.
    </p>
    Conflating tuples with column vectors as described in <xref ref="d_matrix_transform"/> we see that <m>T=T_A</m> where
    <me>
      A=\begin{amatrix}[rr]7\amp 2\\ 0\amp -1\\ 1\amp 0  \end{amatrix}
    </me>.
    In other words, the original formula is just a description in terms of tuples of the function
    <me>
      \begin{amatrix}[c]x\\ y  \end{amatrix}\mapsto A\begin{amatrix}[c]x\\ y  \end{amatrix}=\begin{amatrix}[c]7x+2y\\ -y\\ x  \end{amatrix}
    </me>.
    It follows from <xref ref="th_matrix_transform_i"/> that <m>T=T_A</m> is linear.
  </statement>
</remark>
<p>
  Next we see our first geometric example of a linear transformation: rotation in the plane by an angle <m>\alpha</m>.
</p>
<definition xml:id="d_rotation">
  <title>Rotation in the plane</title>
  <idx><h>rotation</h><h>as linear transformation</h></idx>
  <notation>
    <usage>\rho_\alpha</usage>
    <description>rotation by <m>\alpha</m> in the plane</description>
  </notation>

  <statement>
    <p>
        Fix an angle <m>\alpha</m> and define
        <me>\rho_\alpha\colon \R^2\rightarrow \R^2
        </me>
        to be the function that takes an input <m>P=(x,y)</m>, considered as a point in the <m>xy</m>-plane, and returns the output <m>P'=(x',y')</m> obtained rotating <m>P</m> by an angle of <m>\alpha</m> about the origin.
        The function <m>\rho_\alpha</m> is called <term>rotation by <m>\alpha</m> (about the origin) (in the plane)</term>.
      </p>
      <p>
      We can extract a formula from the rule defining <m>\rho_\alpha</m> by using polar coordinates: if <m>P</m> has polar coordinates <m>(r,\theta)</m>, then <m>P'=\rho_\alpha(P)</m> has polar coordinates <m>(r,\theta+\alpha)</m>.
      </p>
  </statement>
</definition>
<theorem xml:id="th_transform_rotation">
  <title>Rotation is a linear transformation</title>
  <statement>
    <p>
    Fix an angle <m>\alpha</m>. The rotation function <m>\rho_{\alpha}\colon \R^2\rightarrow \R^2</m> is a linear transformation. In fact, we have <m>\rho_\alpha=T_A</m>, where
    <me>
      A=\begin{amatrix}[rr]
        \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
    \end{amatrix}
    </me>.

    </p>
  </statement>
  <proof>
    <p>
      By  <xref ref="rm_matrix_transform_example"/>, we need only show that <m>\rho_\alpha=T_A</m> for the matrix indicated.
    </p>
    <p>
    If the point <m>P=(x,y)</m> has polar coordinates <m>(r,\theta)</m> (so that <m>x=r\cos\theta</m> and <m>y=r\sin\theta</m>), then its image <m>P'=\rho_{\alpha}(P)</m> under our rotation has polar coordinates <m>(r,\theta+\alpha)</m>. Translating back to rectangular coordinates, we see that
    <md>
      <mrow> \rho_\alpha(P)\amp= P' </mrow>
      <mrow>  \amp =\left(r\cos(\theta+\alpha),r\sin(\theta+\alpha)\right)</mrow>
      <mrow> \amp =(r\cos\theta\cos\alpha-r\sin\theta\sin\alpha, r\sin\theta\cos\alpha+r\cos\theta\sin\alpha)
      \amp (\text{trig. identities}) </mrow>
      <mrow>  \amp=(\cos\alpha\, x-\sin\alpha\, y, \sin\alpha\, x+\cos\alpha\, y)
      \amp (\text{since } x=r\cos\theta, y=r\sin\theta) </mrow>
    </md>.
    It follows that <m>\rho_{\alpha}=T_A</m>, where
    <me>
      A=\begin{amatrix}[rr]
        \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
    \end{amatrix}
    </me>,
    as claimed.
    </p>
  </proof>
</theorem>
    <remark xml:id="rm_rotation_matrix">
  <statement>
    <p>
      Observe that it is not at all obvious geometrically that the rotation operation is <em>linear</em>: <ie />, that it preserves sums and scalar multiplications in <m>\R^2</m>. Indeed, our proof does not even show this directly, but instead first gives a matrix formula for rotation, and then uses <xref ref="th_transform_rotation"/>.
    </p>
    <p>
    Since matrices of the form
    <me>
    \begin{amatrix}[rr]
      \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
  \end{amatrix}
    </me>
    can be understood as defining rotations of the plane, we call them <em>rotation matrices</em>.
    </p>
  </statement>
</remark>
<example>
  <statement>
    <p>
      Find formulas for <m>\rho_\pi\colon \R^2\rightarrow \R^2</m> and <m>\rho_{2\pi/3}\colon \R^2\rightarrow \R^2</m>, expressing your answer in terms of pairs (as opposed to column vectors).
    </p>
  </statement>
  <solution>
    <p>
    The rotation matrix corresponding to <m>\alpha=\pi</m> is
    <me>
      A=\begin{amatrix}[rr]\cos\pi\amp -\sin\pi\\ \sin\pi \amp \cos\pi  \end{amatrix}=
      \begin{amatrix}[rr]-1\amp 0\\ 0 \amp -1  \end{amatrix}
    </me>.
    Thus <m>\rho_\pi=T_A</m> has formula
    <me>
      \rho_{\pi}(x,y)=(-x,-y)=-(x,y)
    </me>.
    Note: this is as expected! Rotating by 180 degrees produces the inverse vector.
    </p>
    <p>
    The rotation matrix corresponding to <m>\alpha=2\pi/3</m> is
    <me>
      B=\begin{amatrix}[rr]\cos(2\pi/3)\amp -\sin(2\pi/3)\\ \sin(2\pi/3) \amp \cos(2\pi/3)  \end{amatrix}=
      \begin{amatrix}[rr]-\frac{1}{2}\amp -\frac{\sqrt{3}}{2}\\ \frac{\sqrt{3}}{2} \amp -\frac{1}{2}  \end{amatrix}
    </me>.
    Thus <m>\rho_{2\pi/3}=T_B</m> has formula
    <me>
      \rho_{2\pi/3}(x,y)=\frac{1}{2}(-x-\sqrt{3}y, \sqrt{3}x-y)
    </me>.
    Let's check our formula for <m>\rho_{2\pi/3}</m>for the vectors <m>(1,0)</m> and <m>(0,1)</m>:
    <md>
      <mrow>\rho_{2\pi/3}(1,0) \amp =(-1/2, \sqrt{3}/2) </mrow>
      <mrow>\rho_{2\pi/3}(0,1) \amp =(-\sqrt{3}/2, -1/2) </mrow>
    </md>.
    Confirm for yourself geometrically that these are the points you get by rotating <m>(1,0)</m> and <m>(0,1)</m> by an angle of <m>2\pi/3</m> about the origin.
    </p>

  </solution>
</example>
<p>
  We now proceed to some examples involving our more exotic vector spaces.
</p>
<example>
  <title>Transposition is linear</title>
  <statement>
    <p>
    Fix <m>m,n\geq 1</m>. Define the function <m>f\colon M_{mn}\rightarrow M_{nm}</m> as follows:
    <me>
      f(A)=A^T
    </me>.
    In other words, <m>f</m> maps a matrix to its transpose.
  </p>
  <p>
    Show that <m>f</m> is a linear transformation.
    </p>
  </statement>
  <solution>
    <p>
      We must show <m>f(cA+dB)=cf(A)+df(B)</m> for all scalars
      <m>c,d\in\R</m> and all matrices <m>A, B\in M_{mn}</m>.
      This follows easily from properties of transpose:
      <md>
        <mrow>f(cA+dB)\amp =(cA+dB)^T \amp \text{ (by def.) }</mrow>
        <mrow>\amp =(cA)^T+(dB)^T \amp (<xref ref="th_trans_props"/>)</mrow>
        <mrow>\amp =cA^T+dB^T \amp (<xref ref="th_trans_props"/>)</mrow>
        <mrow>\amp =cf(A)+df(B)</mrow>
      </md>
    </p>
  </solution>
</example>
<example>
  <title>Left-shift transformation</title>

  <statement>
    <p>
    Define the <term>left-shift operation</term>, <m>T_\ell\colon \R^\infty \rightarrow R^{\infty}</m> as follows:
    <me>
      T_\ell\left( (a_{i})_{i=1}^\infty\right)= (a_{i+1})_{i=1}^\infty
    </me>.
    In other words, we have
    <me>
      T_\ell \left( (a_1,a_2,a_3,\dots)\right)=(a_2,a_3,\dots)
    </me>.
    Show that <m>T_\ell</m> is a linear transformation.
    </p>
  </statement>
  <solution>
    <p>
    Let <m>\boldv=(a_i)_{i=1}^\infty</m> and <m>\boldw=(b_i)_{i=1}^\infty</m> be two infinite sequences in <m>\R^\infty</m>. For any <m>c,d\in\R</m> we have
    <md>
      <mrow>T_\ell(c\boldv+d\boldw) \amp=T_\ell\left((ca_i+db_i)_{i=1}^\infty \right)\amp (<xref ref="ex_vs_infinitesequences"/>) </mrow>
      <mrow> \amp= (ca_{i+1}+db_{i+1})_{i=1}^\infty \amp (\text{by def.})</mrow>
      <mrow>  \amp=c(a_{i+1})_{i=1}^\infty+d(b_{i+1})_{i=1}^\infty \amp (<xref ref="ex_vs_infinitesequences"/>)</mrow>
      <mrow>  \amp=cT_\ell(\boldv)+dT_\ell(\boldw)\amp (\text{by def.}) </mrow>
    </md> .
    This proves <m>T_\ell</m> is a linear transformation.
    </p>
  </solution>
</example>

</paragraphs>
<paragraphs xml:id="ss_transormation_video_examples">
  <title>Video examples</title>
<p>
  <figure>
    <title>Linear transformation or not?</title>
  <caption>Linear transformation or not?</caption>
  <video xml:id="vid_transform_1" youtube="STYVF5cprEU" />
  </figure>
  <figure>
    <title>Linear transformation or not?</title>
  <caption>Linear transformation or not?</caption>
  <video xml:id="vid_transform_2" youtube="weETKP8p7cE" />
  </figure>
</p>
</paragraphs>
<paragraphs xml:id="ss_transform_composition">
  <title>Composition of linear transformations and matrix multiplication</title>
<p>
  We end by making good on a promise we made long ago to <em>retroactively</em> make sense of the definition of matrix multiplication. The key connecting concept, as it turns out, is composition of functions. We first need a result showing that composition preserves linearity.
</p>
<theorem xml:id="th_transform_composition">
  <title>Composition of linear transformations</title>
  <statement>
    <p>
    Let <m>V, W, U</m> be vector spaces, and suppose <m>T\colon W\rightarrow U</m> and <m>S\colon V\rightarrow W</m> are linear transformations. Then the composition
    <me>
      T\circ S\colon V\rightarrow U
    </me>
    is a linear transformation.
    </p>
  </statement>
  <proof>
    <p>
      Exercise.
    </p>
  </proof>
</theorem>
<p>
  Turning now to matrix multiplication, suppose <m>A</m> is <m>m\times n</m> and <m>B</m> is <m>n\times r</m>. Let <m>C=AB</m> be their product. These matrices give rise to linear transformations
  <md>
    <mrow>T_A\colon \R^n \amp\rightarrow \R^m \amp T_B\colon \R^r \amp\rightarrow \R^n \amp T_C\colon \R^r \amp\rightarrow \R^m  </mrow>
  </md>.
  According to <xref ref="th_transform_composition"/> the composition <m>T_A\circ T_B</m> is a linear transformation from <m>\R^r</m> (the domain of <m>T_B</m>) to <m>\R^m</m> (the codomain of <m>T_A</m>). We claim that <m>T_A\circ T_B=T_C</m>. Indeed, identifying elements of <m>\R^r</m> with column vectors, for all <m>\boldx\in \R^n</m> we have
  <md>
    <mrow> T_A\circ T_B(\boldx) \amp = T_A(T_B(\boldx)) \amp (<xref ref="d_function_composition"/>) </mrow>
    <mrow> \amp =T_A(B\boldx) \amp (<xref ref="d_matrix_transform"/>)</mrow>
    <mrow> \amp= A(B\boldx) \amp (<xref ref="d_matrix_transform"/>)</mrow>
    <mrow>  \amp = (AB)\boldx \amp (\text{assoc.})</mrow>
    <mrow>  \amp = T_C(\boldx) \amp (\text{since } C=AB)</mrow>
  </md>.
Thus, we can now understand the definition of matrix multiplication as being chosen precisely to encode how to compute the composition of two matrix transformations. The restriction on the dimension of the ingredient matrices is now understood as guaranteeing that the corresponding matrix transformations can be composed!
</p>

</paragraphs>






    <!-- <paragraphs>   ADD THESE TO LATER SECTIONS!!
    <title>Exotic example: \alert{differentiaton}</title>
    <p>
      Let <m>V=C^\infty(\R)</m>,
      the vector space of all infinitely differentiable functions on <m>\R</m>.
      Define <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> by <m>T(f)=f'</m>.
      In other words, <m>T</m> takes as an input a function <m>f</m>,
      and returns as an output its derivative function <m>f'</m>.
    </p>
    <paragraphs>
      <title>Claim</title>
      <p>
        : <m>T</m> is a linear transformation.
      </p>
    </paragraphs>
    <proof>
      <p>
        We must show <m>T(cf+dg)=cT(f)+dT(g)</m> for any scalars
        <m>c,d\in\R</m> and any functions <m>f, g\in C^\infty(\R)</m>.
      </p>
      <p>
        This follows easily from certain properties of the derivative:
        <md>
          <mrow>T(cf+dg)\amp =(cf+dg)' \amp \text{ (def. of \(T\)) }</mrow>
          <mrow>\amp =(cf)'+(dg)'\amp \text{ (addition prop. of derivative) }</mrow>
          <mrow>\amp =cf'+dg' \amp \text{ (scalar prop. of derivative) }</mrow>
          <mrow>\amp =cT(f)+dT(g) \amp \text{ (def. of \(T\)) }</mrow>
        </md>
      </p>
    </proof>
    </paragraphs>
    \item Let <m>V=P_n</m> and <m>W=P_{n+1}</m>,
    the function \item Let <m>V=M_{mn}</m>, <m>W=M_{nm}</m>.
    The function \item Let <m>V=C^1(-\infty,\infty)</m>,
    <m>W=C(-\infty,\infty)</m>.
    The function
  </p>
  <p>
    \item Both <m>\NS(T)</m> and
    <m>\range(T)</m> are indeed <em>subspaces</em>.
    Note that they live in different spaces:
    <m>\NS(T)\subseteq V</m> and <m>\range(T)\subseteq W</m>. \item We define
    <m>\nullity(T)=\dim(\NS(T))</m> and <m>\rank(T)=\dim(\range(T))</m>.
  </p>
  <p>
    \item We must show <m>\boldzero_W\in \range(T)</m>.
    But <m>\boldzero_W=T(\boldzero_V)</m>.
    Thus there is a <m>\boldv\in V</m> with <m>T(\boldv)=\boldzero_W</m>,
    which proves that <m>\boldzero_W\in\range(T)</m>. \item Suppose <m>\boldw_1,\boldw_2\in \range(T)</m>.
    This means there are <m>\boldv_1, \boldv_2\in V</m> such that <m>T(\boldv_i)=\boldw_i</m> for <m>i=1,2</m>.
    We must show <m>\boldw=\boldw_1+\boldw_2\in\range(T)</m>.
  </p>
  <p>
    \item Suppose <m>\boldw\in\range(T)</m>.
    Then there is a <m>\boldv\in V</m> with <m>T(\boldv)=\boldw</m>.
    Then <m>T(k\boldv)=kT(\boldv)=k\boldw</m>,
    showing <m>k\boldw\in\range(T)</m>.
  </p>
  <p>
    Let <m>S=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> be a basis of <m>\NS(T)</m>; \alert{extend} this basis to a full basis <m>\{\boldv_1,\boldv_2,\dots, \boldv_r, \boldu_{1},\dots \boldu_{n-r}\}</m> of <m>V</m>.
    Claim: <m>S'=\{T(\boldu_{1}),\dots, T(\boldu_{n-r})\}</m> is a basis of <m>\range(T)</m>.
    If this is true we are done,
    since we have <m>\dim(\NS(T))=\#S=r</m> and <m>\dim(\range(T))=\#S'=n-r</m>,
    and thus Now ask your professor to prove the claim.
  </p>
  <p>
    \item easily check whether two linear transformations are equal simply by checking that they agree on the basis elements <m>\boldv_i</m>.
  </p>
  <p>
    Then
  </p>
  <p>
    A simple drawing on the unit circle tells us what happens to these vectors when we rotate them by <m>\theta</m>.
    We get:
  </p>
  <p>
    Note further, that if we let <m>\phi</m> be the angle between
    <m>\boldy=(y_1,y_2)</m> and <m>(1,0)</m>, then
  </p>
  <p>
    Thus <m>\text{ proj } _W=T_A</m>,
    where $A=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
  </p>
  <p>
    Thus $\text{ proj } _W=T_A<m>,
    where</m>A=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
  </p> -->
</section>
