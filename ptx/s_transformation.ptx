<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_transformation">
  <title>Linear transformations</title>
  <introduction>
    <p>
    As detailed in <xref ref="d_linear_trans"/> a <em>linear transformation</em> is a special type of function between two vector spaces: one that <em>respects</em> in some sense the vector operations of both spaces.
    </p>
    <p>
      This manner of theorizing is typical in mathematics:
      first we introduce a special class of objects defined axiomatically, then we introduce special functions or maps between these objects. Since the original objects of study (e.g. vector spaces) come equipped with special structural properties (e.g. vector operations), the functions we wish to study are the ones that somehow acknowledge this structre.
    </p>
    <p>
      You have already seen this principle at work in your study of calculus.
      First we give <m>\R</m> some structure by defining a notion of proximity (i.e., <m>x</m> is close to <m>y</m> if <m>\val{x-y}</m> is small),
      then we introduce a special family of functions that somehow respects this structure:
      these are precisely the <em>continuous</em> functions!
    </p>
    <p>
      As you will see, linear transformations are not just interesting objects of study in their own right, they also serve as invaluable tools in our continued exploration of the intrinsic properties of vector spaces.
    </p>
    <p>
      In the meantime rejoice in the fact that we can now give a succinct definition of linear algebra:
      it is the theory of vector spaces and the linear transformations between them. Go shout it from the rooftops!
    </p>
  </introduction>
  <paragraphs xml:id="ss_functions">
    <title>Function definitions</title>
    <p>
      Before getting to the main attraction, we first establish some basic definitions and notation regarding functions.
    </p>
    <definition xml:id="d_functions">
      <title>Functions</title>
      <idx><h>function</h></idx>
      <notation>
        <usage>f\colon A\rightarrow B</usage>
        <description>function from <m>A</m> to <m>B</m></description>
      </notation>
      <statement>
        <p>
        Let <m>A</m> and <m>B</m> be two sets. A <term>function from <m>A</m> to <m>B</m></term>, denoted <m>f\colon A\rightarrow B</m>, is a rule which, given any <term>input</term> <m>a\in A</m>, returns an <term>output</term> <m>b\in B</m>. In this case we write <m>b=f(a)</m> and call <m>b</m> the <term>image of <m>a</m> under <m>f</m></term>, or the <term>value of <m>f</m> at <m>a</m> </term>. Similarly, we say <m>f</m> <term>maps</term> (or <term>sends</term>) the input <m>a</m> to the output <m>b</m>.
        </p>
        <p>
          The set <m>A</m> is called the <term>domain</term> of <m>f</m>; the set <m>B</m> is called the <term>codomain</term> of <m>f</m>.
        </p>
        <p>
          When we wish to indicate what rule defines the function, we use the elaborated notation
          <md>
            <mrow>f\colon A \amp\rightarrow B </mrow>
            <mrow> a \amp\mapsto f(a)</mrow>
          </md>.
          This is read as <q>The function <m>f</m> from <m>A</m> to <m>B</m> maps an input <m>a</m> to the output <m>f(a)</m></q>.
        </p>
      </statement>
    </definition>
    <definition xml:id="d_function_equality">
      <title>Function equality</title>
      <idx><h>function</h><h>equality</h></idx>
      <statement>
        <p>
        Functions <m>f</m> and <m>g</m> are <term>equal</term> if
        <ol>
          <li>
            <p>
              they have the same domain <m>A</m> and codomain <m>B</m>, and
            </p>
          </li>
          <li>
            <p>
              for all <m>a\in A</m>, we have <m>f(a)=g(a)</m>.
            </p>
          </li>
        </ol>
        </p>
      </statement>
    </definition>
    <definition xml:id="d_function_composition">
      <title>Function composition</title>
      <idx><h>function</h><h>composition</h></idx>
      <notation>
        <usage>f\circ g</usage>
        <description>the composition of <m>f</m> and <m>g</m></description>
      </notation>
      <statement>
        <p>
          Suppose <m>f\colon C\rightarrow D</m> and <m>g\colon A\rightarrow B</m> are functions satisfying <m>B\subseteq C</m>.  The <term>composition of <m>f</m> and <m>g</m></term>, is the function <m>f\circ g\colon A\rightarrow D</m> defined as follows:
          <md>
            <mrow>f\circ g\colon A \amp\rightarrow D </mrow>
            <mrow> a\amp\mapsto f(g(a)) </mrow>
          </md>.
        In other words, to evaluate <m>f\circ g</m> at the input <m>a</m> we <em>first</em> compute <m>g(a)</m>, and <em>then</em> compute <m>f(g(a))</m>.
        </p>
      </statement>
    </definition>
  </paragraphs>
  <paragraphs xml:id="ss_linear_trans">
    <title>Linear transformations</title>
  <definition xml:id="d_linear_trans">
    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
        A <term>linear transformation</term>
        is a function <m>T\colon V\rightarrow W</m> that satisfies the following properties:
        <ol>
          <li>
            <p>
              For all <m>\boldv, \boldw\in V</m>, we have <m>T(\boldv+\boldw)=T(\boldv)+T(\boldw)</m>.
            </p>
          </li>
          <li>
            <p> For all <m>c\in \R</m> and <m>\boldv\in V</m> we have
              <m>T(c\boldv)=cT(\boldv)</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </definition>
    <remark>
  <statement>
    <p>
      How does <m>T</m> respect the vector space structure?
      In plain English: the image of a sum is the sum of the images,
      and the image of a scalar multiple is the scalar multiple of the image.
      Alternatively, a linear transformation <em>distributes</em>
      over linear combinations of vectors,
      as we saw in the previous comment.
    </p>
  </statement>
</remark>
  <p>
    As our first examples of linear transformations, we define the <em>zero transformation</em> and <em>identity transformation</em> on a vector space.
  </p>
  <definition xml:id="d_trans_zero_identity">
    <title>Zero and identity transformation</title>
    <idx><h>linear transformation</h><h>zero transformation</h></idx>
    <idx><h>linear transformation</h><h>identity transformation</h></idx>

    <statement>
        <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
      </p>
      <p>
        The <term>zero transformation from <m>V</m> to <m>W</m></term>, denoted <m>T_0</m>, is defined as follows:
        <md>
          <mrow>T_0\colon V \amp\rightarrow W  </mrow>
          <mrow> \boldv \amp\mapsto T_0(\boldv)=\boldzero_W</mrow>
        </md>,
        where <m>\boldzero_W</m> is the zero vector of <m>W</m>. In other words, <m>T_0</m> is the function that maps all elements of <m>V</m> to the zero vector of <m>W</m>.
      </p>
      <p>
        The <term>identity transformation of <m>V</m></term>, denoted <m>\id_V</m>, is defined as follows:
        <md>
          <mrow> \id_V\colon V \amp\rightarrow V </mrow>
          <mrow> \boldv \amp\mapsto \id_V(\boldv)=\boldv</mrow>
        </md>.
        In other words, <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>.
      </p>
    </statement>
  </definition>
  <remark xml:id="rm_trans_zero_identity">
  <title>Model of linear transformation proof</title>
  <statement>
    <p>
    Let's show that <m>T_0\colon V\rightarrow W</m> and <m>\id_V\colon V\rightarrow V</m> are indeed linear transformations.
    </p>

  </statement>
</remark>
<theorem xml:id="th_trans_basic_props">
  <title>Basic properties of linear transformations</title>
  <statement>
    <p>
      Let <m>T\colon V\rightarrow W</m> be a linear transformation. Let <m>\boldzero_V</m> and <m>\boldzero_W</m> be the zero vectors of <m>V</m> and <m>W</m>, respectively.
    </p>
    <ol>
      <li>
        <p>
          We have <m>T(\boldzero_V)=\boldzero_W</m>.
        </p>
      </li>
      <li>
        <p>
          For all <m>\boldv\in V</m>, we have <m>T(-\boldv)=-T(\boldv)</m>.
        </p>
      </li>
      <li>
        <p>
          For any linear combination
          <me>\boldv=c_1\boldv_1+c_2T\boldv_2+\cdots +c_nT\boldv_n\in V</me>
          we have
          <me>
            T(\boldv)=T(c_1\boldv_1+c_2T\boldv_2+\cdots +c_nT\boldv_n)=c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots +c_nT(\boldv_n)
          </me>.
        </p>
      </li>
    </ol>
  </statement>
</theorem>
</paragraphs>






  <paragraphs>
  <title>Example: \alert{the zero transformation}</title>
  <p>
    Given any vector spaces <m>V</m> and <m>W</m>,
    the <term>zero transformation</term>
    <m>T_0\colon V\rightarrow W</m> is defined as
    <m>T_0(\boldv)=\boldzero_W</m> for all <m>\boldv\in V</m>.
  </p>
  <paragraphs>
    <title>Claim</title>
    <p>
      : <m>T_0\colon V\rightarrow W</m> is a linear transformation.
    </p>
  </paragraphs>
  <proof>
    <p>
      We must show
      <me>
        T_0(c_1\boldv_1+c_2\boldv_2)=c_1T_0(\boldv_1)+c_2T_0(\boldv_2)
      </me>
      for all <m>c_i\in\R</m> and <m>\boldv_i\in V</m>.
    </p>
    <p>
      This is obvious:
      since <m>T_0(\boldv)=\boldzero_W</m> for all <m>\boldv\in V</m>,
      the LHS and RHS of the equation above are both equal to <m>\boldzero_W</m>.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
  <title>Example: \alert{the identity transformation}</title>
  <p>
    Given a vector space <m>V</m> the
    <term>identity transformation (on <m>V</m>)</term>
    <m>I_V\colon V\rightarrow V</m> is defined as
    <m>I_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>.
  </p>
  <paragraphs>
    <title>Claim</title>
    <p>
      : <m>I_V\colon V\rightarrow V</m> is a linear transformation.
    </p>
  </paragraphs>
  <proof>
    <p>
      We must show
      <me>
        I_V(c_1\boldv_1+c_2\boldv_2)=c_1I_V(\boldv_1)+c_2I_V(\boldv_2)
      </me>
      for all <m>c_i\in\R</m> and <m>\boldv_i\in V</m>.
    </p>
    <p>
      Again, this is fairly obvious.
    </p>
    <p>
      The LHS of the above equation is <m>I_V(c_1\boldv_1+c_2\boldv_2)=c_1\boldv_1+c_2\boldv_2</m>,
      by definition of the function <m>I_V</m>.
    </p>
    <p>
      The RHS of the equation is <m>c_1I_V(\boldv_1)+c_2I_V(\boldv_2)=c_1\boldv_1+c_2\boldv_2</m>,
      again by definition of the function <m>I_V</m>.
    </p>
    <p>
      This shows LHS=RHS, as desired.
    </p>
  </proof>
  </paragraphs>
  <paragraphs>
    <title>Important example: \alert{matrix transformations}</title>
    <definition>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix.
          The <term>linear transformation associated to <m>A</m></term>,
          denoted <m>T_A</m>,
          is the function <m>T_A\colon\R^n\rightarrow \R^m</m> defined as <m>T_A(\boldx)=A\boldx</m>.
        </p>
        <p>
          Note: here we treat <m>\R^n</m> and <m>\R^m</m> as collections of column vectors.
        </p>
      </statement>
    </definition>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix.
          Then <m>T_A\colon \R^n\rightarrow\R^m</m> is a linear transformation from <m>\R^n</m> to <m>\R^m</m>.
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        As usual, we must show <m>T_A(c_1\boldx_1+c_2\boldx_2)=c_1T_A(\boldx_1)+c_2T_A(\boldx_2)</m>.
        We do so via a chain of equalities:
        <md>
          <mrow>T_A(c_1\boldx_1+c_2\boldx_2)\amp =\amp A(c_1\boldx_1+c_2\boldx_2) \ \text{ (def. of \(T_A\)) }</mrow>
          <mrow>\amp =\amp c_1A\boldx_1+c_2A\boldx_2 \ \text{ (prop. of matrix mult.) }</mrow>
          <mrow>\amp =\amp c_1T_A(\boldx_1)+c_2T_A(\boldx_2) \ \text{ (def. of \(T_A\)) }</mrow>
        </md>
      </p>
      <p>
        This shows <m>T_A</m> is a linear transformation.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Important example: \alert{matrix transformations}</title>
    <p>
      We will show later that in fact <em>all</em>
      linear transformations from <m>\R^n</m> to <m>\R^m</m> are of the form <m>T_A</m> for some <m>m\times n</m> matrix <m>A</m>!
      In other words,
      matrices gives us the full picture in terms of linear transformations.
    </p>
    <p>
      In the meantime,
      matrix transformations give us a way of <em>retroactively</em>
      justifying our definition of matrix multiplication!
      Here's how.
    </p>
    <p>
      Given <m>A\in M_{mn}</m> and <m>B\in M_{np}</m>,
      let <m>C=AB</m> be their product. (Note that <m>C\in M_{mp}</m>).
    </p>
    <p>
      We have associated functions
      <m>T_A\colon \R^n\rightarrow \R^m</m> and <m>T_B\colon \R^p\rightarrow\R^n</m>.
      These functions can be <em>composed</em>
      to form a new function <m>T=T_A\circ T_B\colon \R^p\rightarrow \R^m</m>!
      Here is a diagram of the situation:
    </p>
    <image source="images/b4eb52bfe174fb5fb20b08e96e8629964ba3aade.png"/>
    <p>
      I claim that <m>T</m> is none other than the matrix transformation <m>T_C</m>.
      In other words,
      the definition of matrix multiplication was chosen precisely in order to model the composition of matrix transformations!
    </p>
    <p>
      I leave the proof as an exercise.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Geometric example: \alert{rotation in the plane}</title>
    <p>
      Fix an angle <m>\alpha</m> and define <m>T\colon \R^2\rightarrow \R^2</m> to be
      <term>rotation by <m>\alpha</m></term>:
      in other words,
      <m>T</m> takes a point <m>P=(x,y)</m> in <m>\R^2</m> with polar coordinates
      <m>(r, \theta)</m> and maps it to the point <m>T(P)=Q</m> with polar coordinates <m>(r, \theta+\alpha)</m>.
    </p>
    <p>
      Treating elements of <m>\R^2</m> as column vectors we have
      <me>
        T\left (\begin{bmatrix}x \\ y \end{bmatrix} \right)=T\left( \begin{bmatrix}r\cos \theta\\ r\sin \theta \end{bmatrix} \right)=\begin{bmatrix}r\cos(\theta+\alpha)\\ r\sin(\theta+\alpha) \end{bmatrix}
      </me>,
      where <m>(x,y)</m> has polar coordinates <m>(r,\theta)</m>.
    </p>
    <p>
      Somewhat surprisingly, <m>T</m> is a linear transformation!
    </p>
    <p>
      You can either show this directly,
      by proving <m>T(\boldx_1+\boldx_2)=T(\boldx_1)+T(\boldx_2)</m> and <m>T(c\boldx)=cT(\boldx)</m>,
      or indirectly,
      by proving that <m>T=T_A</m> where <m>A=\begin{bmatrix}\cos\alpha\amp -\sin\alpha\\ \sin\alpha\amp \cos\alpha \end{bmatrix}</m>.
      I leave this as an exercise.
    </p>
  </paragraphs>
  <paragraphs>
  <title>Exotic example: \alert{transposition}</title>
  <p>
    Let <m>V=M_{mn}</m>, <m>W=M_{nm}</m>.
    Define <m>S\colon M_{mn}\rightarrow M_{nm}</m> as <m>S(A)=A^T</m>.
    In other words, <m>S</m> takes an input
    <m>m\times n</m> matrix and returns as an output its transpose.
  </p>
  <p>
    (Note: I name the function <m>S</m> so as not to conflict with <sq>T</sq> in our transpose notation.)
  </p>
  <paragraphs>
    <title>Claim</title>
    <p>
      : <m>S</m> is a linear transformation.
    </p>
  </paragraphs>
  <proof>
    <p>
      We must show <m>S(cA+dB)=cS(A)+dS(B)</m> for all scalars
      <m>c,d\in\R</m> and all matrices <m>A, B\in M_{mn}</m>.
      This follows easily from properties of transpose:
      <md>
        <mrow>S(cA+dB)\amp =(cA+dB)^T \amp \text{ (def. of \(S\)) }</mrow>
        <mrow>\amp =(cA)^T+(dB)^T \amp \text{ (addition prop. of transpose) }</mrow>
        <mrow>\amp =cA^T+dB^T \amp \text{ (scalar prop. of transpose) }</mrow>
        <mrow>\amp =cS(A)+dS(B)</mrow>
      </md>
    </p>
  </proof>
  </paragraphs>
  <p>
    Note further, that if we let <m>\phi</m> be the angle between
    <m>\boldy=(y_1,y_2)</m> and <m>(1,0)</m>, then
    <paragraphs>
    <title>Exotic example: \alert{shift transformations}</title>
    <p>
      Let <m>V=\R^\infty</m>, the vector space of all infinite sequences.
    </p>
    <p>
      The <term>left-shift transformation</term>,
      denoted <m>T_\ell</m> is the function <m>T_\ell\colon \R^\infty\rightarrow\R^\infty</m> defined as
      <me>
        T_\ell\left( (a_1,a_2,a_3,\dots)\right)=(a_2,a_3,\dots)
      </me>.
    </p>
    <p>
      The <term>right-shift transformation</term>,
      denoted <m>T_r</m> is the function <m>T_r\colon \R^\infty\rightarrow\R^\infty</m> defined as
      <me>
        T_r\left( (a_1,a_2,a_3,\dots)\right)=(0,a_1,a_2,a_3,\dots)
      </me>.
    </p>
    <p>
      In other words,
      <m>T_\ell</m> takes an input sequence and returns as an output the sequence obtained by shifting all terms one to the left;
      <m>T_r</m> takes an input sequence and returns as an output the sequence obtained by shifting all terms one to the right and setting the first term equal to 0.
    </p>
    <paragraphs>
      <title>Claim</title>
      <p>
        : <m>T_\ell</m> and <m>T_r</m> are both linear transformations.
      </p>
    </paragraphs>
    <proof>
      <p>
        Exercise.
      </p>
    </proof>
    </paragraphs>
    <paragraphs>
    <title>Exotic example: \alert{differentiaton}</title>
    <p>
      Let <m>V=C^\infty(\R)</m>,
      the vector space of all infinitely differentiable functions on <m>\R</m>.
      Define <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> by <m>T(f)=f'</m>.
      In other words, <m>T</m> takes as an input a function <m>f</m>,
      and returns as an output its derivative function <m>f'</m>.
    </p>
    <paragraphs>
      <title>Claim</title>
      <p>
        : <m>T</m> is a linear transformation.
      </p>
    </paragraphs>
    <proof>
      <p>
        We must show <m>T(cf+dg)=cT(f)+dT(g)</m> for any scalars
        <m>c,d\in\R</m> and any functions <m>f, g\in C^\infty(\R)</m>.
      </p>
      <p>
        This follows easily from certain properties of the derivative:
        <md>
          <mrow>T(cf+dg)\amp =(cf+dg)' \amp \text{ (def. of \(T\)) }</mrow>
          <mrow>\amp =(cf)'+(dg)'\amp \text{ (addition prop. of derivative) }</mrow>
          <mrow>\amp =cf'+dg' \amp \text{ (scalar prop. of derivative) }</mrow>
          <mrow>\amp =cT(f)+dT(g) \amp \text{ (def. of \(T\)) }</mrow>
        </md>
      </p>
    </proof>
    </paragraphs>
    \item Let <m>V=P_n</m> and <m>W=P_{n+1}</m>,
    the function \item Let <m>V=M_{mn}</m>, <m>W=M_{nm}</m>.
    The function \item Let <m>V=C^1(-\infty,\infty)</m>,
    <m>W=C(-\infty,\infty)</m>.
    The function
  </p>
  <p>
    \item Both <m>\NS(T)</m> and
    <m>\range(T)</m> are indeed <em>subspaces</em>.
    Note that they live in different spaces:
    <m>\NS(T)\subseteq V</m> and <m>\range(T)\subseteq W</m>. \item We define
    <m>\nullity(T)=\dim(\NS(T))</m> and <m>\rank(T)=\dim(\range(T))</m>.
  </p>
  <p>
    \item We must show <m>\boldzero_W\in \range(T)</m>.
    But <m>\boldzero_W=T(\boldzero_V)</m>.
    Thus there is a <m>\boldv\in V</m> with <m>T(\boldv)=\boldzero_W</m>,
    which proves that <m>\boldzero_W\in\range(T)</m>. \item Suppose <m>\boldw_1,\boldw_2\in \range(T)</m>.
    This means there are <m>\boldv_1, \boldv_2\in V</m> such that <m>T(\boldv_i)=\boldw_i</m> for <m>i=1,2</m>.
    We must show <m>\boldw=\boldw_1+\boldw_2\in\range(T)</m>.
  </p>
  <p>
    \item Suppose <m>\boldw\in\range(T)</m>.
    Then there is a <m>\boldv\in V</m> with <m>T(\boldv)=\boldw</m>.
    Then <m>T(k\boldv)=kT(\boldv)=k\boldw</m>,
    showing <m>k\boldw\in\range(T)</m>.
  </p>
  <p>
    Let <m>S=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> be a basis of <m>\NS(T)</m>; \alert{extend} this basis to a full basis <m>\{\boldv_1,\boldv_2,\dots, \boldv_r, \boldu_{1},\dots \boldu_{n-r}\}</m> of <m>V</m>.
    Claim: <m>S'=\{T(\boldu_{1}),\dots, T(\boldu_{n-r})\}</m> is a basis of <m>\range(T)</m>.
    If this is true we are done,
    since we have <m>\dim(\NS(T))=\#S=r</m> and <m>\dim(\range(T))=\#S'=n-r</m>,
    and thus Now ask your professor to prove the claim.
  </p>
  <p>
    \item easily check whether two linear transformations are equal simply by checking that they agree on the basis elements <m>\boldv_i</m>.
  </p>
  <p>
    Then
  </p>
  <p>
    A simple drawing on the unit circle tells us what happens to these vectors when we rotate them by <m>\theta</m>.
    We get:
  </p>
  <p>
    Note further, that if we let <m>\phi</m> be the angle between
    <m>\boldy=(y_1,y_2)</m> and <m>(1,0)</m>, then
  </p>
  <p>
    Thus <m>\text{ proj } _W=T_A</m>,
    where $A=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
  </p>
  <p>
    Thus $\text{ proj } _W=T_A<m>,
    where</m>A=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
  </p>
</section>
