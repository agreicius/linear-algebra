<section xml:id="S_linear">
  <title>XXXX</title>
  <p>
    \chapter*{1.1-1.2: linear systems}
  </p>
  <ol>
    <li>
      <p>
        Recall that the set of solutions <m>(x,y)</m> to a single linear equation in 2 variables constitutes a line <m>\ell</m> in <m>\R^2</m>.
        We denote this <m>\ell\colon ax+by=c</m>.
        Similarly, the set of solutions <m>(x,y,z)</m> to a single linear equation in 3 variables
        <m>ax+by+cz=d</m> constitutes a plane <m>\mathcal{P}</m> in <m>\R^3</m>.
        We denote this <m>\mathcal{P}\colon ax+by+cz=d</m>.
      </p>
      <ol>
        <li>
          <p>
            Fix <m>m>1</m> and consider a system of <m>m</m> linear equations in the 2 unknowns <m>x</m> and <m>y</m>.
            What do solutions <m>(x,y)</m> to this <em>system</em>
            of linear equations correspond to geometrically?
          </p>
        </li>
        <li>
          <p>
            Use your interpretation to give a <em>geometric</em>
            argument that a system of <m>m</m> equations in 2 unknowns will have either (i) 0 solutions, (ii) 1 solution,
            or (iii) infinitely many solutions.
          </p>
        </li>
        <li>
          <p>
            Use your geometric interpretation to help produce explicit examples of systems in 2 variables satisfying these three different cases (i)-(iii).
          </p>
        </li>
        <li>
          <p>
            Now repeat (a)-(b) for systems of linear equations in 3 variables <m>x,y, z</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) Geometrically,
          each equation in the system represents a line <m>\ell_i\colon a_ix+b_iy=c_i</m>.
          A solution <m>(x,y)</m> to the <m>i</m>-th equation corresponds to a point on <m>\ell_i</m>.
          Thus a solution <m>(x,y)</m> to the system corresponds to a point lying on
          <em>all</em> of the lines:
          i.e., a point of intersection of the lines.
        </p>
        <p>
          (b) First of all to prove the desired
          <q>or</q>
          statement it suffices to prove that if the number of solutions is greater than 1, then there are infinitely many solutions.
        </p>
        <p>
          Now suppose there is more than one solution.
          Then there are at least two different solutions:
          <m>P_1=(x_1,y_1)</m> and <m>P_2=(x_2,y_2)</m>.
          Take any of the two lines <m>\ell_i, \ell_j</m>.
          By above the intersection of <m>\ell_i</m> and <m>\ell_j</m> contains <m>P_1</m> and <m>P_2</m>.
          But two <em>distinct</em> lines intersect in at most one point.
          It follows that <m>\ell_i</m> and <m>\ell_j</m> must be equal.
          Since <m>\ell_i</m> and <m>\ell_j</m> were arbitrary,
          it follows <em>all</em> of the lines <m>\ell_i</m> are in fact the same line <m>\ell</m>.
          But this means the common intersection of the lines is <m>\ell</m>,
          which has infinitely many points.
          It follows that the system has infinitely many solutions.
        </p>
        <p>
          (c) We will get 0 solutions if the system includes two different parallel lines:
          e.g., <m>\ell_1\colon x+y=5</m> and <m>\ell_2\colon x+y=1</m>.
        </p>
        <p>
          We will get exactly one solution when the slopes of each line in the system are distinct.
        </p>
        <p>
          We will get infinitely many solutions when <em>all</em>
          equations in the system represent the <em>same line</em>.
          This happens when all equations are multiples of one another.
        </p>
        <p>
          (d) Now each equation in our system defines a plane <m>\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i</m>.
          A solution <m>(x,y,z)</m> to the system corresponds to a point
          <m>P=(x,y,z)</m> of intersection of the planes.
          We recall two facts from Euclidean geometry:
          <ol>
            <li>
              <title>Fact 1</title>
              <p>
                Given two distinct points, there is a unique line containing both of them.
              </p>
            </li>
            <li>
              <title>Fact 2</title>
              <p>
                Given any number of distinct planes,
                they either do not intersect, or intersect in a line.
              </p>
            </li>
          </ol>
        </p>
        <p>
          We proceed as in part (b) above:
          that is show that if there are two distinct solutions to the system,
          then there are infinitely many solutions.
          First, for simplicity,
          we may assume that the equations <m>\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i</m> define
          <em>distinct</em> planes;
          if we have two equations defining the same plane,
          we can delete one of them and not change the set of solutions to the system.
        </p>
        <p>
          Now suppose <m>P=(x_1,y_1,z_1)</m> and
          <m>Q=(x_2,y_2,z_2)</m> are two distinct solutions to the system.
          Let <m>\ell</m> be the unique line containing <m>P</m> and <m>Q</m> (Fact 1).
          I claim that <m>\ell</m> is precisely the set of solutions to the system.
          To see this, take any two equations in the system
          <m>\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i</m> and <m>\mathcal{P}_j\colon a_jx+b_jy+c_iz=d_j</m>.
          Since the two corresponding planes are distinct,
          and intersect in at least the points <m>P</m> and <m>Q</m>,
          they must intersect in a line (Fact 2);
          since this line contains <m>P</m> and <m>Q</m>,
          it must be the line <m>\ell</m> (Fact 1).
          Thus any two planes in the system intersect in the line <m>\ell</m>.
          From this it follows that: (a) a point satisfying the system must lie in <m>\ell</m>;
          and (b) all points on <m>\ell</m> satisfy the system
          (since we have shown that <m>\ell</m> lies in all the planes).
          It follows that <m>\ell</m> is precisely the set of solutions,
          and hence that there are infinitely many solutions.
        </p>
      </solution>
    </li>
    <li>
      <p>
        We made the claim that each of our three row operations
        <ol>
          <li>
            <p>
              scalar multiplication (<m>e_i\mapsto c\cdot e_i</m> for <m>c\ne 0</m>),
            </p>
          </li>
          <li>
            <p>
              swap (<m>e_i\leftrightarrow e_j</m>),
            </p>
          </li>
          <li>
            <p>
              addition (<m>e_i\mapsto e_i+c\cdot e_j</m> for some <m>c</m>)
            </p>
          </li>
        </ol>
      </p>
      do not change the set of solutions of a linear system.
      To prove this claim, let <m>L</m> be a general linear system
      <me>
        \numeqsys
      </me>.
      Now consider each type of row operation separately,
      write down the new system <m>L'</m> you get by applying this row operation,
      and prove that an <m>n</m>-tuple
      <m>s=(s_1,s_2,\dots ,s_n)</m> is a solution to the original system <m>L</m> if and only if it is a solution to the new system <m>L'</m>.
      <solution>
        <p>
          Let <m>S</m> be the original system with equations <m>e_1,e_2,\dots ,e_m</m>.
        </p>
        <p>
          For each specified row operation,
          we will call the resulting new system <m>S'</m> and its equations <m>e'_1,e'_2,\dots , e'_m</m>.
          <ul>
            <li>
              <title>Row swap.</title>
              <p>
                In this case systems <m>S</m> and <m>S'</m> have exactly the same equations,
                just written in a different order.
                Thus the <m>n</m>-tuple <m>s</m> satifies <m>S</m> if and only if it satisfies each of the equations <m>e_i</m>,
                if and only if it satisfies each of the equations <m>e'_i</m>,
                since these are the same equations!
                It follows that <m>s</m> is a solution of <m>S</m> if and only if it is a solution to <m>S'</m>.
              </p>
            </li>
            <li>
              <title>Scalar mult.</title>
              <p>
                In this case <m>e_j=e'_j</m> for all <m>j\ne i</m>,
                and <m>e'_i=c\cdot e_i</m> for some <m>c\ne 0</m>.
                Since only the <m>i</m>-th equation has changed,
                it suffices to show that <m>s</m> is a solution to <m>e_i</m> if and only if <m>s</m> is a solution to <m>c\cdot e_i</m>.
                Let's prove each direction of this if and only if separately.
                <md>
                  <mrow>s \text{ satisfies }  e_i\amp \Rightarrow  a_{ii}s_1+a_{i2}s_2+\cdots +a_{in}s_n=b_i \amp  \text{ (by def.) }</mrow>
                  <mrow>\amp \Rightarrow  ca_{i1}s_1+ca_{i2}s_2+\cdots +ca_{in}s_n=cb_i \amp  \text{ (mult both sides by \(c\)) }</mrow>
                  <mrow>\amp \Rightarrow  s \text{ satisfies }  e'_i \amp \text{ (by def) }</mrow>
                </md>
                Now for the other direction of the if and only if.
                <md>
                  <mrow>s \text{ satisfies }  e'_i \amp \Rightarrow ca_{ii}s_1+ca_{i2}s_2+\cdots ca_{in}s_n=cb_i \amp \text{ (by def) }</mrow>
                  <mrow>\amp \Rightarrow \frac{1}{c}(ca_{i1}s_1+ca_{i2}s_2+\cdots +ca_{in}s_n)=\frac{1}{c}\cdot cb_i \amp  \text{ (mult both sides by \(\frac{1}{c}\))}</mrow>
                  <mrow>\amp \Rightarrow a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}x_n=b_i \amp  \text{ (distribute \(\frac{1}{c}\))}</mrow>
                  <mrow>\amp \Rightarrow s \text{ satisfies }  e'_i \amp  \text{ (by def.). }</mrow>
                </md>
              </p>
            </li>
            <li>
              <title>Row addition.</title>
              <p>
                Here <m>e'_i=e_i+ce_j</m>.
                The only equation of <m>S'</m> that differs from <m>S</m> is
                <me>
                  e_i'=e_i+ce_j
                </me>.
                Writing this equation out in terms of coefficients gives us
                <me>
                  e_i': a_{i1}x_1+a_{i2}x_2+\cdots +a_{in}x_n+c(a_{j1}x_1+a_{j2}x_2+\cdots +a_{jn}x_n)=b_i+cb_j
                </me>.
                Now if <m>s</m> satisfies <m>S</m>,
                then it satisfies <m>e_i</m> and <m>e_j</m>,
                in which case evaluating the RHS of the <m>e_i'</m> above at <m>s</m> yields
                <md>
                  <mrow>a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n)\amp =\amp b_i+cb_j</mrow>
                </md>
                showing that <m>s</m> satisfies <m>e_i'</m>.
                Now suppose <m>s=(s_1,s_2,\dots,
                s_n)</m> satisfies <m>S'</m>.
                Since <m>s</m> satisfies <m>e_j'=e_j</m>, we have
                <me>
                  a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n=b_j\tag{\(*\)}
                </me>.
                Since <m>s</m> satisfies <m>e_i'</m>, we have
                <me>
                  a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n)=b_i+cb_j
                </me>
                Substituting <m>(*)</m> into the equation above we get
                <me>
                  a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(b_j)=b_i+cb_j
                </me>,
                and hence
                <me>
                  a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n=b_i
                </me>.
                This shows that <m>s</m> satisfies <m>e_i</m>.
                It follows that <m>s</m> satisfies <m>S</m>.
              </p>
            </li>
          </ul>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Consider the linear system
        <md>
          <mrow>7x_5\amp =\amp 2x_3+12</mrow>
          <mrow>2x_1+4x_2-10x_3+6x_4+12x_5\amp =\amp 28</mrow>
          <mrow>2x_1+4x_2-5x_3+6x_4-5x_5\amp =\amp -1</mrow>
        </md>
        Using augmented matrices,
        try and solve this system by reducing it to a simpler one using row operations.
        How many solutions (0, 1, or <m>\infty</m>) does the system have?
        <solution>
          <p>
            See my lecture notes on Gaussian elimination!
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute the set of solutions <m>S</m> to the following system of linear equations:
        <md>
          \begin{linsys}{4} x_1\amp +\amp x_2\amp -\amp x_3\amp +\amp x_4\amp =\amp 1\\ -2x_1\amp -\amp 2x_2\amp +\amp 2x_3\amp -\amp 2x_4\amp =\amp -2\\ x_1\amp +\amp x_2\amp +\amp x_3\amp +\amp 2x_4\amp =\amp 3 \end{linsys}
        </md>
        Follow the Gaussian elimination algorithm <em>to the letter</em>,
        indicating what row operations are used.
        Make sure to indicate how the <m>x_i</m> are sorted into free and leading variables.
        <term>Your final answer should be described in set notation</term>.
        <solution>
          <md>
            <mrow>\begin{bmatrix}1\amp 1\amp -1\amp 1\amp 1\\ -2\amp -2\amp 2\amp -2\amp -2\\ 1\amp 1\amp 1\amp 2\amp 3 \end{bmatrix} \amp \xrightarrow[\hspace{35pt}]{r_2+2r_1}\amp \begin{bmatrix}1\amp 1\amp -1\amp 1\amp 1\\ 0\amp 0\amp 0\amp 0\amp 0\\ 1\amp 1\amp 1\amp 2\amp 3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{r_3-r_1}\amp \begin{bmatrix}1\amp 1\amp -1\amp 1\amp 1\\ 0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 2\amp 1\amp 2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{r_2\leftrightarrow r_3 }\amp \begin{bmatrix}1\amp 1\amp -1\amp 1\amp 1\\ 0\amp 0\amp 2\amp 1\amp 2\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{\frac{1}{2}r_2}\amp \begin{bmatrix}1\amp 1\amp -1\amp 1\amp 1\\ 0\amp 0\amp 1\amp 1/2\amp 1\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
          </md>
          <p>
            The row echelon matrix tells us that <m>x_2=s</m> and <m>x_4=t</m> are the free variables.
            Back substitution then yields the general solution:
            <md>
              <mrow>x_1\amp =\amp 2-s-\frac{3t}{2}</mrow>
              <mrow>x_2\amp =\amp s</mrow>
              <mrow>x_3\amp =\amp 1-\frac{t}{2}</mrow>
              <mrow>x_4\amp =\amp t</mrow>
            </md>,
          </p>
          <p>
            Alternatively, the set of solutions is
            <me>
              S=\left\{(2-s-\frac{3t}{2},s,1-\frac{t}{2},t)\colon s, t\in\R\right\}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Determine all values of <m>a</m> for which the system has (a) no solutions, (b) exactly one solution,
        or (c) infinitely-many solutions.
        <md>
          \begin{linsys}{3} x\amp +\amp 2y\amp -\amp 3z\amp =\amp 4\\ 3x\amp -\amp y\amp +\amp 5z\amp =\amp 2\\ 4x\amp +\amp y\amp +\amp (a^2-14)z\amp =\amp a+2 \end{linsys}
        </md>
        <solution>
          <p>
            The augmented matrix row reduces to
            <me>
              \begin{bmatrix}1\amp 2\amp 3\amp 4\\ 0\amp -7\amp 14\amp -10\\ 0\amp 0\amp (a^2-16)\amp a-4 \end{bmatrix}
            </me>
          </p>
          <p>
            From this it follows that the system has:
          </p>
          <p>
            a) 0 solutions iff <m>a^2-16=0</m> and <m>a-4\ne 0</m> iff <m>a=-4</m>;
          </p>
          <p>
            (b) exactly one solution iff
            <m>a^2-16\ne 0</m> iff <m>a\ne\pm 4</m>;
          </p>
          <p>
            (c) infinitely many solutions iff
            <m>a^2-16=0</m> and <m>a-4=0</m> iff <m>a=4</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use Gaussian elimination to find the general solution to the following system of linear equations:
        <md>
          \begin{linsys}{4} x_1\amp +\amp 2x_2\amp =\amp x_3\amp +\amp x_4\amp +\amp 3\\ 3x_1\amp +\amp 6x_2\amp =\amp 2x_3\amp -\amp 4x_4\amp +\amp 8\\ -x_1\amp +\amp 2x_3\amp =\amp 2x_2\amp -\amp x_4\amp -\amp 1 \end{linsys}
        </md>
        Follow the Gaussian elimination algorithm to the letter,
        indicating what row operations are used.
        Make sure to indicate how the <m>x_i</m> are sorted into free and leading variables.
        <solution>
          <md>
            <mrow>\begin{bmatrix}1\amp 2\amp -1\amp -1\amp 3\\ 3\amp 6\amp -2\amp 4\amp 8\\ -1\amp -2\amp 2\amp 1\amp -1 \end{bmatrix} \amp \xrightarrow[\hspace{35pt}]{r_2-3r_1}\amp \begin{bmatrix}1\amp 2\amp -1\amp -1\amp 3\\ 0\amp 0\amp 1\amp 7\amp -1\\ -1\amp -2\amp 2\amp 1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{r_3+r_1 }\amp \begin{bmatrix}1\amp 2\amp -1\amp -1\amp 3\\ 0\amp 0\amp 1\amp 7\amp -1\\ 0\amp 0\amp 1\amp 0\amp 2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{r_3-r_2 }\amp \begin{bmatrix}1\amp 2\amp -1\amp -1\amp 3\\ 0\amp 0\amp 1\amp 7\amp -1\\ 0\amp 0\amp 0\amp -7\amp 3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[\hspace{35pt}]{-\frac{1}{7}r_3}\amp \begin{bmatrix}1\amp 2\amp -1\amp -1\amp 3\\ 0\amp 0\amp 1\amp 7\amp -1\\ 0\amp 0\amp 0\amp 1\amp -\frac{3}{7} \end{bmatrix}</mrow>
          </md>
          <p>
            The row echelon matrix tells us that <m>x_2=t</m> is the only free variable.
            Back substitution then yields the general solution:
            <md>
              <mrow>x_1\amp =\amp \frac{32}{7}-2t</mrow>
              <mrow>x_2\amp =\amp t</mrow>
              <mrow>x_3\amp =\amp 2</mrow>
              <mrow>x_4\amp =\amp -\frac{3}{7}</mrow>
            </md>.
          </p>
          <p>
            Alternatively, the set of solutions is
            <me>
              S=\{\left(\frac{32}{7}-2t, t, 2, -3/7\right)\colon t\in\R\}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that a linear system with more unknowns than equations has has either 0 solutions or infinitely many solutions.
        <solution>
          <p>
            Suppose we have a system of <m>m</m> equations in <m>n</m> unknowns <m>x_1,x_2,\dots, x_n</m>.
            We assume <m>n>m</m>.
            Let <m>A</m> be the augmented matrix associated to the system,
            and suppose <m>A</m> is reduced to a matrix <m>U</m> in row echelon form.
          </p>
          <p>
            Since <m>U</m> has <m>m</m> rows,
            there are <em>at most</em> <m>m</m> leading 1's in <m>U</m>,
            which means there are at most <m>m</m> leading variables among the <m>x_i</m>.
            Since <m>n>m</m>, not all the <m>x_i</m> can be leading.
            Thus the system <em>must</em> have a free variable.
          </p>
          <p>
            What does this mean?
            Note that the system could still be inconsistent, meaning no solutions.
            However, the existence of a free variable means if there is a solution,
            then there are infinitely many,
            because the parametric equations for the <m>x_i</m> will involve at least one parameter.
          </p>
          <p>
            We conclude that the system is either inconsistent,
            or has infinitely many solutions.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or false.
        If true you must prove it so;
        if false, you must give an explicit counterexample.
        <ol>
          <li>
            <p>
              Every matrix has a unique row echelon form.
            </p>
          </li>
          <li>
            <p>
              Any homogeneous linear system with more unknowns than equations has infinitely many solutions.
            </p>
          </li>
          <li>
            <p>
              The only solution to a homogeneous system of <m>n</m> equations in <m>n</m> unknowns with <m>n</m> leading 1's is the trivial solution <m>s=(0,0,\dots,0)</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          Let <m>A=\begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}</m>.
          Then <m>A</m> is already in row echelon form,
          but can be further reduced to <m>I=\begin{bmatrix}1\amp 0\\0\amp 1 \end{bmatrix}</m>,
          which is also in row echelon form.
          Thus <m>A</m> and <m>I</m> are two different row echelon forms of <m>A</m>.
        </p>
        <p>
          (b) True.
          Since its homogeneous, it is consistent:
          thus 1 or <m>\infty</m>-many solutions.
          Furthermore,
          we showed above that a system with more variables than equations has a free variable.
          Thus it must have infinitely many solutions.
        </p>
        <p>
          (c) True.
          First note that the given <m>s</m> is always a solution.
          Since the system has <m>n</m> leading ones,
          there will be no free variables,
          which implies this is the only solution.
        </p>
      </solution>
    </li>
    <li xml:id="x1_1-1_2_A">
      <p>
        Use Gaussian elimination to solve the following system:
        <md>
          \begin{linsys}{3} 2x_1 \amp +\amp  2x_2  \amp +\amp 2x_3\amp =\amp 0\\ -2x_1 \amp +\amp  5x_2 \amp +\amp 2x_3\amp =\amp 1\\ 8x_1 \amp +\amp   x_2   \amp +\amp 4x_3\amp =\amp -1 \end{linsys}
        </md>
        <solution>
          <md>
            <mrow>\begin{bmatrix}2\amp 2\amp 2\amp 0\\ -2\amp 5\amp 2\amp 1\\ 8\amp 1\amp 4\amp -1 \end{bmatrix} \amp \xrightarrow[]{\frac{1}{2}r_1}\amp \begin{bmatrix}1\amp 1\amp 1\amp 0\\ -2\amp 5\amp 2\amp 1\\ 8\amp 1\amp 4\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2+2r_1}\amp \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 7\amp 4\amp 1\\ 8\amp 1\amp 4\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3-8r_1}\amp \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 7\amp 4\amp 1\\ 0\amp -7\amp -4\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{7}r_1}\amp \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 1\amp \frac{4}{7}\amp \frac{1}{7}\\ 0\amp -7\amp -4\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3+7r_2}\amp \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 1\amp \frac{4}{7}\amp \frac{1}{7}\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
          </md>
          <p>
            Now solve.
            We set the free variable <m>x_3=r</m> and do back substitution:
            <md>
              <mrow>x_1\amp =\amp -\frac{1}{7}-\frac{3}{7}r</mrow>
              <mrow>x_2\amp =\amp \frac{1}{7}-\frac{4}{7}r</mrow>
              <mrow>x_3\amp =\amp  r</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Solve by Gaussian elimination:
        <md>
          \begin{linsys}{3} \amp \amp -2b  \amp +\amp 3c\amp =\amp 1\\ 3a \amp +\amp  6b \amp -\amp 3c\amp =\amp -2\\ 6a \amp +\amp   6b   \amp +\amp 3c\amp =\amp 5 \end{linsys}
        </md>
        <solution>
          <md>
            <mrow>\begin{bmatrix}[rcl|r] 0\amp -2\amp 3\amp 1</mrow>
            <mrow>3\amp 6\amp -3\amp -2</mrow>
            <mrow>6\amp 6\amp 3\amp 5 \end{bmatrix} \amp \xrightarrow[]{r_1 \leftrightarrow r_2} \begin{bmatrix}[rcl|r] 3\amp 6\amp -3\amp -2</mrow>
            <mrow>0\amp -2\amp 3\amp 1</mrow>
            <mrow>6\amp 6\amp 3\amp 5 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3-2r_1} \begin{bmatrix}[rcl|r] 3\amp 6\amp -3\amp -2</mrow>
            <mrow>0\amp -2\amp 3\amp 1</mrow>
            <mrow>0\amp -6\amp 9\amp 9 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{2}r_2} \begin{bmatrix}[rcl|r] 3\amp 6\amp -3\amp -2</mrow>
            <mrow>0\amp 1\amp -\frac{3}{2}\amp -\frac{1}{2}</mrow>
            <mrow>0\amp -6\amp 9\amp 9 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3+6r_2} \begin{bmatrix}[rcl|r] 3\amp 6\amp -3\amp -2</mrow>
            <mrow>0\amp 1\amp -\frac{3}{2}\amp -\frac{1}{2}</mrow>
            <mrow>0\amp 0\amp 0\amp 6 \end{bmatrix}</mrow>
          </md>
          <p>
            And since <m>0 \neq 6</m>, the system is inconsistent.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Solve the following system using  Gauss-Jordan elimination:
        that is, first reduce the corresponding augmented matrix to reduced row echelon form.
        <md>
          \begin{linsys}{3} 2x_1\amp +\amp 2x_2\amp +\amp 2x_3\amp =\amp 0\\ -2x_1\amp +\amp 5x_2\amp +\amp 2x_3\amp =\amp 1\\ 8x_1\amp +\amp x_2\amp +\amp 4x_3\amp =\amp -1 \end{linsys}
        </md>
        <solution>
          <p>
            The corresponding augmented matrix is
            <me>
              \begin{bmatrix}2\amp 2\amp 2\amp 0\\ -2\amp 5\amp 2\amp 1\\ 8\amp 1\amp 4\amp -1 \end{bmatrix}
            </me>,
            which row reduces first to
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 1\amp \frac{4}{7}\amp \frac{1}{7}\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
            and then further to
            <me>
              \begin{bmatrix}1\amp 0\amp \frac{3}{7}\amp -\frac{1}{7}\\[1ex] 0\amp 1\amp \frac{4}{7}\amp \frac{1}{7}\\[1ex] 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            The corresponding system has solution set
            <me>
              S=\left\{\left(-\frac{1}{7}-\frac{3}{7}r,\frac{1}{7}-\frac{4}{7}r ,r\right)\colon r\in \R\right\}
            </me>
            as in <xref ref="x1_1-1_2_A">Exercise</xref>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Solve the following system of equations by any method:
        <md>
          \begin{linsys}{5} \amp  \amp  \amp \amp  Z_3\amp +\amp Z_4\amp +\amp Z_5 \amp =\amp 0\\ -Z_1\amp -\amp Z_2 \amp +\amp 2Z_3 \amp -\amp 3Z_4\amp +\amp Z_5 \amp =\amp 0\\ Z_1\amp + \amp Z_2 \amp -\amp 2Z_3 \amp \amp \amp -\amp Z_5 \amp =\amp 0\\ 2Z_1\amp + \amp 2Z_2 \amp -\amp Z_3 \amp \amp \amp +\amp Z_5 \amp =\amp 0 \end{linsys}
        </md>
        <solution>
          <md>
            <mrow>\begin{bmatrix}0\amp 0\amp 1\amp 1\amp 1\amp 0\\ -1\amp -1\amp 2\amp -3\amp 1\amp 0\\ 1\amp 1\amp -2\amp 0\amp -1\amp 0\\ 2\amp 2\amp -1\amp 0\amp 1\amp 0 \end{bmatrix} \amp \xrightarrow[]{-r_2 \leftrightarrow r_1} \begin{bmatrix}1\amp 1\amp -2\amp 3\amp -1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 1\amp 1\amp -2\amp 0\amp -1\amp 0\\ 2\amp 2\amp -1\amp 0\amp 1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3-r_1} \begin{bmatrix}1\amp 1\amp -2\amp 3\amp -1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp -3\amp 0\amp 0\\ 2\amp 2\amp -1\amp 0\amp 1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4-2r_1} \begin{bmatrix}1\amp 1\amp -2\amp 3\amp -1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp -3\amp 0\amp 0\\ 0\amp 0\amp 3\amp -6\amp 3\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1-2r_2} \begin{bmatrix}1\amp 1\amp 0\amp 5\amp 1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp -3\amp 0\amp 0\\ 0\amp 0\amp 3\amp -6\amp 3\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4-3r_2} \begin{bmatrix}1\amp 1\amp 0\amp 5\amp 1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp -3\amp 0\amp 0\\ 0\amp 0\amp 0\amp -9\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{3}r_3} \begin{bmatrix}1\amp 1\amp 0\amp 5\amp 1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp -9\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4+9r_3} \begin{bmatrix}1\amp 1\amp 0\amp 5\amp 1\amp 0\\ 0\amp 0\amp 1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2-r_3} \begin{bmatrix}1\amp 1\amp 0\amp 5\amp 1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1-5r_3} \begin{bmatrix}1\amp 1\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
          </md>
          <p>
            Now solve.
            We set the free variables <m>x_2=r</m> and <m>x_5 = s</m> and substitute:
            <md>
              <mrow>Z_1\amp =\amp -r-s</mrow>
              <mrow>Z_2\amp =\amp r</mrow>
              <mrow>Z_3\amp =\amp -s</mrow>
              <mrow>Z_4\amp =\amp 0</mrow>
              <mrow>Z_5\amp =\amp s</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Interpret each matrix below as an augmented matrix of a linear system.
        Asterisks represent an unspecified real number.
        For each matrix,
        determine whether the corresponding system is consistent or inconsistent.
        If the system is consistent,
        decide further whether the solution is unique or not.
        If there is not enough information answer <sq>inconclusive</sq> and back up your claim by giving an explicit example where the system is consistent,
        and an explicit example where the system is inconsistent.
        <ol>
          <li>
            <p>
              <m>\begin{bmatrix}1\amp *\amp *\amp *\\ 0\amp 1\amp *\amp *\\ 0\amp 0\amp 1\amp 1 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}1\amp 0\amp 0\amp *\\ *\amp 1\amp 0\amp *\\ *\amp *\amp 1\amp * \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}1\amp 0\amp 0\amp 0\\ 1\amp 0\amp 0\amp 1\\ 1\amp *\amp *\amp * \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}1\amp *\amp *\amp *\\ 1\amp 0\amp 0\amp 1\\ 1\amp 0\amp 0\amp 1 \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The corresponding system is consistent since the row echelon form of the augmented matrix has no leading 1 in the last column.
          Since the three columns corresponding to the three variables all have leading 1's, there are no free variables.
          Hence the system has a unique solution.
        </p>
        <p>
          (b) This system has a unique solution.
          You can see this either by noting that the
          <q>reverse staircase pattern</q>
          allows us to do
          <q>forwards substitution</q>, solving first for <m>x_1</m>, then for <m>x_2</m>,
          etc., or else by noting that the 1's along the diagonal (and 0's above them) allow us to row reduce the matrix further to one have exactly three leading 1's in the first three columns.
        </p>
        <p>
          (c) Inconsistent.
          Rows 1 and 2 give
          <me>
            x_1 = 0 \hspace{7mm} x_1 = 1
          </me>
        </p>
        <p>
          (d) Inconclusive.
          Consider
          <md>
            <mrow>\begin{bmatrix}1\amp a\amp b\amp c\\ 1\amp 0\amp 0\amp 1\\ 1\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          If <m>a=b=0</m> and <m>c=2</m> the system is inconsistent:
          the matrix row reduces to one with a leading 1 in the last column.
          If <m>a = b = 0</m> and <m>c=1</m>,
          the system has infinitely many solutions:
          the matrix row reduces to one with a leading 1 in the first column only.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine the values of <m>a</m> for which the system below has  no solutions,
        many solutions, or one solution.
        <md>
          \begin{linsys}{3} x\amp +\amp 2y\amp +\amp z\amp =\amp 2\\ 2x\amp -\amp 2y\amp +\amp 3z\amp =\amp 1\\ x\amp +\amp 2y\amp -\amp (a^2-3)z\amp =\amp a \end{linsys}
        </md>
        Solution:
        <md>
          <mrow>\begin{bmatrix}1\amp 2\amp 1\amp 2\\ 2\amp -2\amp 3\amp 1\\ 1\amp 2\amp 3-a^2\amp a \end{bmatrix} \amp \xrightarrow[]{r_1 - r_3} \begin{bmatrix}1\amp 2\amp 1\amp 2\\ 2\amp -2\amp 3\amp 1\\ 0\amp 0\amp a^2-2\amp 2-a \end{bmatrix}</mrow>
          <mrow>\amp \xrightarrow[]{2r_1 - r_2} \begin{bmatrix}1\amp 2\amp 1\amp 2\\ 0\amp 6\amp -1\amp 3\\ 0\amp 0\amp a^2-2\amp 2-a \end{bmatrix}</mrow>
          <mrow>\amp \xrightarrow[]{\frac{1}{6}r_2} \begin{bmatrix}1\amp 2\amp 1\amp 2\\ 0\amp 1\amp -\frac{1}{6}\amp \frac{1}{2}\\ 0\amp 0\amp a^2-2\amp 2-a \end{bmatrix}</mrow>
        </md>
        The row echelon form, and thus the set of solutions,
        now depends on whether <m>a^2-2=0</m> or not:
        equivalently, whether <m>a=\pm\sqrt{2}</m> or not.
        This gives us two cases:
        <ul>
          <li>
            <title>invalidlabel</title>
            <p>
              In this case <m>2-a\ne 0</m>,
              which means the row echelon matrix will end up having a leading 1 in the last column,
              resulting in an inconsistent system.
              There are no solutions in this case.
            </p>
          </li>
          <li>
            <title>invalidlabel</title>
            <p>
              In this case the third column of the row echelon form will have a leading 1, and all variables are leading variables.
              Thus there is a unique solution in this case,
              obtained by back substitution.
            </p>
          </li>
        </ul>
      </p>
    </li>
    <li>
      <p>
        What condition must a,b, and c satisfy for the system to be consistent?
        <md>
          \begin{linsys}{3} x\amp +\amp 3y\amp +\amp z\amp =\amp a\\ -x\amp -\amp 2y\amp +\amp z\amp =\amp b\\ 3x\amp +\amp 7y\amp -\amp z\amp =\amp c \end{linsys}
        </md>
        <solution>
          <md>
            <mrow>\begin{bmatrix}1\amp 3\amp 1\amp a\\ -1\amp -2\amp 1\amp b\\ 3\amp 7\amp -1\amp c \end{bmatrix} \amp \xrightarrow[]{r_1 + r_2}\amp \begin{bmatrix}1\amp 3\amp 1\amp a\\ 0\amp 1\amp 2\amp a+b\\ 3\amp 7\amp -1\amp c \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{3r_1 - r_3}\amp \begin{bmatrix}1\amp 3\amp 1\amp a\\ 0\amp 1\amp 2\amp a+b\\ 0\amp 2\amp 4\amp 3a-c \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{2r_2 - r_3}\amp \begin{bmatrix}1\amp 3\amp 1\amp a\\ 0\amp 1\amp 2\amp a+b\\ 0\amp 0\amp 0\amp a-2b-c \end{bmatrix}</mrow>
          </md>
          <p>
            We see the system is consistent as long as <m>a-2b-c = 0</m>,
            which guarantees there is no leading 1 in the last column.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Solve for <m>x</m>, <m>y</m>, and <m>z</m>:
        <md>
          \begin{linsys}{3} \frac{1}{x}\amp +\amp \frac{2}{y}\amp -\amp \frac{4}{z}\amp =\amp 1\\ \\ \frac{2}{x}\amp +\amp \frac{3}{y}\amp +\amp \frac{8}{z}\amp =\amp 0\\ \\ -\frac{1}{x}\amp +\amp \frac{9}{y}\amp +\amp \frac{10}{z}\amp =\amp 5 \end{linsys}
        </md>
        <solution>
          <p>
            Start by replacing variables.
            Let <m>X = \frac{1}{x}</m>,
            <m>Y = \frac{1}{y}</m>, and <m>Z = \frac{1}{z}</m>.
            Now we can solve the new system as we normally would.
            <md>
              <mrow>\begin{bmatrix}1\amp 2\amp -4\amp 1\\ 2\amp 3\amp 8\amp 0\\ -1\amp 9\amp 10\amp 5 \end{bmatrix} \amp \xrightarrow[]{r_1 + r_3} \begin{bmatrix}1\amp 2\amp -4\amp 1\\ 2\amp 3\amp 8\amp 0\\ 0\amp 11\amp 6\amp 6 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{2r_1 - r_2} \begin{bmatrix}1\amp 2\amp -4\amp 1\\ 0\amp 1\amp -16\amp 2\\ 0\amp 11\amp 6\amp 6 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{11r_2 - r_3} \begin{bmatrix}1\amp 2\amp -4\amp 1\\ 0\amp 1\amp -16\amp 2\\ 0\amp 0\amp -182\amp 16 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Now solve the system for <m>X, Y, Z</m>:
            <md>
              <mrow>X\amp =\amp -\frac{7}{13}</mrow>
              <mrow>Y\amp =\amp \frac{54}{91}</mrow>
              <mrow>Z\amp =\amp -\frac{8}{91}</mrow>
            </md>
          </p>
          <p>
            Now we solve for the original <m>x, y</m>, and <m>z</m>:
            <md>
              <mrow>x\amp =\amp -\frac{13}{7}</mrow>
              <mrow>y\amp =\amp \frac{91}{54}</mrow>
              <mrow>z\amp =\amp -\frac{91}{8}</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        If <m>A</m> is a matrix with three rows and five columns,
        then what is the maximum possible number of leading 1's in its reduced row echelon form?
        <solution>
          <p>
            The maximum possible number of leading 1's in the reduced row echelon form of a matrix with 3 rows and 5 columns is 3.
            It is indeed possible to obtain this maximal number, as the matrix
            <me>
              \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\amp 0 \end{bmatrix}
            </me>
            illustrates.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        If <m>B</m> is a matrix with three rows and six columns,
        then what is the maximum possible number of parameters in the general solution of the linear system with augmented matrix <m>B</m>?
        <solution>
          <p>
            The matrix <m>B</m> corresponds to a linear system of 3 equations in 5 unknowns <m>x_1, x_2, \dots, x_5</m>.
          </p>
          <p>
            Let <m>U</m> be a row echelon form of <m>B</m>,
            and let <m>k</m> be the number of leading 1's
            <em>among the first five columns</em> of <m>U</m>.
            Then the number of parameters in the general solution to the system corresponding to <m>B</m> is <m>5-k</m>.
            Thus we see, that the number of parameters is at most 5
            (when <m>k=5</m>).
          </p>
          <p>
            This case is indeed possible,
            as the matrix <m>B=\underset{3\times 6}{\boldzero}</m> illustrates.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        If <m>C</m> is a matrix with five rows and three columns,
        then what is the minimum possible number of rows of zeros in any row echelon form of <m>C</m>?
        <solution>
          <p>
            If a row echelon form of <m>A</m> has <m>r</m> zero rows,
            then all other rows have leading 1's.
            Thus there are <m>5-r</m> leading 1's in this case.
            Since the number of leading 1's is at most 3
            (the number of columns),
            we have <m>5-r\leq 3</m>.
            It follows that <m>2\leq r</m>,
            and thus there are at least <m>2</m> zero rows in a row echelon form of <m>A</m>.
            It is indeed possible to achieve this minimum number of zero rows,
            as the matrix
            <me>
              U=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1\\ 0\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}
            </me>
            illustrates.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or False.
        If true, prove it; if false,
        give an explicit counterexample.   (a) If a matrix is in reduced row echelon form,
        then it is also in row echelon form.   (b) If an elementary row operation is applied to a matrix that is in row echelon form,
        the resulting matrix will still be in row echelon form.   (c) Every matrix has a unique row echelon form.   (d) A homogeneous linear system in <m>n</m> unknowns whose corresponding augmented matrix has a reduced row echelon form with <m>r</m> leading 1's has <m>n-r</m> free variables.   (e) All leading 1's in a matrix in row echelon form must occur in different columns.   (f ) If every column of a matrix in row echelon form has a leading 1, then all entries that are not leading 1's are zero.   (g) If a homogeneous linear system of <m>n</m> equations in <m>n</m> unknowns has a corresponding augmented matrix with a reduced row echelon form containing <m>n</m> leading 1's, then the linear system has only the trivial solution.   (h) If the reduced row echelon form of the augmented matrix for a linear system has a row of zeros,
        then the system must have infinitely many solutions.   (i) If a linear system has more unknowns than equations,
        then it must have infinitely many solutions.
        <solution>
          <p>
            (a) True.
            A matrix in reduced row echelon form satisfies all conditions for row echelon form,
            plus an additional condition.
          </p>
          <p>
            (b) False.
            Consider
            <me>
              \begin{bmatrix}1\amp 0\\0\amp 1 \end{bmatrix} \xrightarrow[]{r_2+r_1}\begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            (c) False.
            Consider <m>A=\begin{bmatrix}1\amp 1\\0\amp 1 \end{bmatrix}</m>,
            which is already in row echelon form.
            It is further row equivalent to <m>B=\begin{bmatrix}1\amp 0\\0\amp 1 \end{bmatrix}</m>.
          </p>
          <p>
            (d) True.
            Let <m>A</m> be the augmented matrix corresponding to the system,
            let <m>U</m> be a row echelon form of <m>A</m>,
            and let <m>r</m> be the number of pivot columns of <m>U</m>.
            Both matrices have <m>n+1</m> columns,
            and the last column of each is a zero column.
            This means the system is guaranteed to be consistent (i.e., that the <m>r</m> pivot columns are among the first <m>n</m> columns of <m>U</m>),
            and hence that the number of free variables is <m>n-r</m>.
          </p>
          <p>
            (e) True.
            This is part of the definition of row echelon form.
            The leading 1's must form a staircase pattern.
          </p>
          <p>
            (f) False.
            Consider <m>\begin{bmatrix}1\amp 1\\0\amp 1 \end{bmatrix}</m>.
          </p>
          <p>
            (g) True.
            The system is consistent,
            since homogeneous systems are always consistent,
            and the number of free variables is <m>n-\#\text{
            (pivot columns)
            } =n-n=0</m>.
            Hence there are no free variables,
            which means the system has a unique solution.
          </p>
          <p>
            (h) False.
            The system could be inconsistent,
            in which case there are 0 solutions!
            Consider the system corresponding to the matrix <m>A=\begin{bmatrix}0\amp 1\\0\amp 0 \end{bmatrix}</m>.
          </p>
          <p>
            (i) False.
            Consider the system
            <md>
              \begin{linsys}{3} 0x_1\amp +\amp 0x_2\amp =\amp 1 \end{linsys}
            </md>
          </p>
          <p>
            The system is clearly inconsistent.
            Hence it has 0 solutions, not infinitely many.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{1.3-1.4: matrix arithmetic}
  </p>
  <ol>
    <li>
      <p>
        Use the row or column method to quickly compute the following product:
        <me>
          \begin{bmatrix}1\amp -1\amp 1\amp -1\amp 1\\ 1\amp -1\amp 1\amp -1\amp 1\\ 1\amp -1\amp 1\amp -1\amp 1\\ 1\amp -1\amp 1\amp -1\amp 1\\ 1\amp -1\amp 1\amp -1\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 1\amp 1\amp 1\\ -1\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 2\amp 0\\ 0\amp 0\amp 0\amp 3 \end{bmatrix}
        </me>
        <solution>
          <p>
            We did this in class using the column method.
            For kicks, I'll describe the row method here.
            Call the product <m>AB</m>.
          </p>
          <p>
            Note that all the rows of <m>A</m> are all identical,
            and equal to <m>\begin{bmatrix}1 \amp -1 \amp 1 \amp -1 \amp 1 \end{bmatrix}</m>.
            From the row method it follows that each row of <m>AB</m> is given by
            <me>
              \begin{bmatrix}1 \amp -1 \amp 1 \amp -1 \amp 1 \end{bmatrix} B
            </me>.
          </p>
          <p>
            Thus the rows of <m>AB</m> are all identical,
            and the row method computes the product above by taking the corresponding alternating sum of the rows of <m>B</m>:
            <me>
              \begin{bmatrix}1 \amp -1 \amp 1 \amp -1 \amp 1 \end{bmatrix} B=\begin{bmatrix}2\amp 2\amp -1\amp 4 \end{bmatrix}
            </me>.
          </p>
          <p>
            Thus <m>AB</m> is the the <m>5\times 4</m> matrix,
            all of whose rows are <m>\begin{bmatrix}2\amp 2\amp -1\amp 4 \end{bmatrix}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Each of the <m>3\times 3</m> matrices <m>B_i</m> below performs a specific row operation when multiplying a
        <m>3\times n</m> matrix <m>A=\begin{bmatrix}-\boldr_1-\\ -\boldr_2-\\ -\boldr_3- \end{bmatrix}</m> on the left;
        i.e., the matrix <m>B_iA</m> is the result of performing a certain row operation on the matrix <m>A</m>.
        Use the row method of matrix multiplication to decide what row operation each <m>B_i</m> represents.
        <me>
          B_1=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ -2\amp 0\amp 1 \end{bmatrix} , B_2=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp \frac{1}{2}\amp 0\\ 0\amp 0\amp 1 \end{bmatrix} , B_3=\begin{bmatrix}0\amp 0\amp 1\\ 0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}
        </me>.
        <solution>
          <p>
            The matrix <m>B_1</m>, when multiplied on the left,
            replaces the third row of <m>A</m> with <m>\boldr_3-2\boldr_2</m>.
          </p>
          <p>
            The matrix <m>B_2</m>, when multiplied on the left,
            replaces the second row of <m>A</m> with <m>\frac{1}{2}\boldr_2</m>.
          </p>
          <p>
            The matrix <m>B_3</m>, when multiplied on the left,
            swaps <m>\boldr_1</m> and <m>\boldr_3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each part below write down the most general <m>3\times 3</m> matrix
        <m>A=[a_{ij}]</m> satisfying the given condition
        (use letter names <m>a,b,c</m>,etc. for entries).
      </p>
      <ol>
        <li>
          <p>
            <m>a_{ij}=a_{ji}</m> for all <m>i,j</m>.
          </p>
        </li>
        <li>
          <p>
            <m>a_{ij}=-a_{ji}</m> for all <m>i,j</m>
          </p>
        </li>
        <li>
          <p>
            <m>a_{ij}=0</m> for <m>i\ne j</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          <m>A=\begin{bmatrix}a\amp b\amp c\\ b\amp d\amp e\\ c\amp e\amp f \end{bmatrix}</m>
        </p>
        <p>
          <m>A=\begin{bmatrix}0\amp a\amp b\\ -a\amp 0\amp c\\ -b\amp -c\amp 0 \end{bmatrix}</m>
        </p>
        <p>
          <m>A=\begin{bmatrix}a\amp 0\amp 0\\ 0\amp b\amp 0\\ 0\amp 0\amp c \end{bmatrix}</m>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A</m> be a square <m>n\times n</m> matrix.
        We define its <term>square</term>
        to be the matrix <m>A^2:=AA</m>.
        Given a square <m>n\times n</m> matrix <m>B</m>,
        we define a <term>square-root</term>
        of <m>B</m> to be a matrix <m>A</m> such that <m>A^2=B</m>.
        <ol>
          <li>
            <p>
              Find <em>all</em> square-roots of <m>\underset{2\times 2}{\boldzero}</m>.
            </p>
          </li>
          <li>
            <p>
              Find <em>all</em> square-roots of <m>I_2</m>.
            </p>
          </li>
        </ol>
      </p>
      Your answers for both parts should be a useful description of the entries of an arbitrary square-root matrix <m>A=\begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}</m>,
      ideally some sort of parametrization.
      <solution>
        <p>
          For both parts let <m>A=\abcdmatrix{a}{b}{c}{d}</m> be an arbitrary <m>2\times 2</m> matrix.
          Then
          <me>
            A^2=\abcdmatrix{a^2+bc}{ab+bd}{ac+cd}{d^2+cb}
          </me>.
        </p>
        <p>
          Setting <m>A^2=\boldzero_{2\times 2}</m> yields the
          <em>nonlinear</em> system of equations
          <md>
            <mrow>a^2+bc\amp =0</mrow>
            <mrow>ab+bd=b(a+d)\amp =0</mrow>
            <mrow>ac+cd=c(a+d)\amp =0</mrow>
            <mrow>d^2+cb\amp =0</mrow>
          </md>
        </p>
        <p>
          This system has infinitely many solutions.
          For example, if we set <m>a=r</m> and <m>d=-r</m>,
          then the second and third equations are automatically satisfied,
          and the first and four equations reduce to <m>r^2+bc=0</m>.
          If we set <m>b=s</m> for any <m>s\ne 0</m>,
          then we must have <m>c=-r^2/s</m>.
          Thus any
          <me>
            A=\begin{bmatrix}r\amp s\\ -r^2/s\amp  -r \end{bmatrix} , r\in \R, s\ne 0\in \R
          </me>
          satisfies <m>A^2=\boldzero_{2\times 2}</m>.
        </p>
        <p>
          Similarly, setting <m>A^2=I_2</m> yields the
          <em>nonlinear</em> system of equations
          <md>
            <mrow>a^2+bc\amp =1\tag{1}</mrow>
            <mrow>ab+bd=b(a+d)\amp =0\tag{2}</mrow>
            <mrow>ac+cd=c(a+d)\amp =0\tag{3}</mrow>
            <mrow>d^2+cb\amp =1\tag{4}</mrow>
          </md>
        </p>
        <p>
          Let's try to write down <em>all</em>
          solutions to this system.
        </p>
        <p>
          Equation 2 implies either <m>b=0</m>,
          or <m>b\ne 0</m> and <m>a+d=0</m>.
          Consider the two cases separately.
        </p>
        <p>
          <term>Case 1</term>: <m>b=0</m>.
          Then (1) and (4) imply <m>a^2=d^2=1</m>.
          Furthermore, (3) implies either <m>c=0</m> or <m>a+d=0</m>.
          This leads to the following mutually exclusive possibilities for <m>A</m>:
          <me>
            \begin{bmatrix}\pm1\amp 0\\ 0\amp \pm 1 \end{bmatrix} , \begin{bmatrix}1\amp 0\\ r\amp -1 \end{bmatrix} , \begin{bmatrix}-1\amp 0\\ r\amp 1 \end{bmatrix}
          </me>,
          where <m>r</m> can be any nonzero real number.
        </p>
        <p>
          <term>Case 2</term>: <m>b\ne 0</m> and <m>a+d=0</m>.
          Then <m>a=-d</m>.
          Set <m>a=r</m>, <m>d=-r</m>,
          <m>b=s</m> where <m>r</m> and <m>s</m> are any two real numbers.
          Equation 1 (or 4) then implies <m>c=(1-r^2)/s</m>,
          leading to solutions of the form
          <me>
            \begin{bmatrix}r\amp s\\ (1-r^2)/s\amp -r \end{bmatrix}
          </me>
          where <m>r</m> and <m>s</m> are any real numbers.
        </p>
        <p>
          The matrices described in the two cases comprise <em>all</em>
          solutions to <m>A^2=I_2</m>.
          In particular,
          note that <m>I_2</m> has infinitely many square-roots!
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A,B</m> be <m>n\times n</m> matrices.
        Prove: <m>(AB)^T=B^TA^T</m>.
      </p>
    </li>
    <li>
      <p>
        Suppose <m>A</m> is <m>n\times n</m> and satisfies <m>A^2+3A-5I=\boldzero_n</m>.
        Prove <m>A</m> is invertible by providing an explicit inverse.
        <solution>
          <p>
            Set <m>A^{-1}=\frac{1}{5}(A+3I)</m>, and show directly
            (using the above equality)
            that <m>AA^{-1}=A^{-1}A=I</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that in general <m>(AB)^r\ne A^rB^r</m>.
        Why does the argument from real number algebra fail?
        <solution>
          <p>
            Easy to come up with counterexamples.
            I'll leave that part to you.
            Look for <m>2\times 2</m> examples.
          </p>
          <p>
            Why doesn't this work?
            In a word: matrix multiplication is not commutative.
            Look at the <m>r=3</m> case for concreteness.
            <me>
              (AB)^3=(AB)(AB)(AB)=ABABAB
            </me>.
          </p>
          <p>
            Since matrix multiplication is <em>not commutative</em>, I cannot rearrange the <m>A</m>'s and <m>B</m>'s above to get <m>AAABBB=A^3B^3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that <m>(A+B)^2=A^2+2AB+B^2</m> if and only if <m>AB=BA</m>.
        <solution>
          <p>
            Let's do a chain of equivalences,
            where at each step I do something valid
            (and reversible)
            to the given equation:
            <md>
              <mrow>(A+B)^2=A^2+2AB+B^2\amp \Leftrightarrow (A+B)(A+B)=A^2+2AB+B^2 \amp \text{ (def.) }</mrow>
              <mrow>\amp \Leftrightarrow A^2+AB+BA+B^2=A^2+2AB+B^2 \amp \text{ (expand LHS carefully) }</mrow>
              <mrow>\amp \Leftrightarrow AB+BA=2AB \amp \text{ (additive cancel) }</mrow>
              <mrow>\amp \Leftrightarrow BA=AB \amp \text{ (additive cancel) }</mrow>
            </md>
          </p>
          <p>
            Following the chain of equivalences we see that
            <me>
              (A+B)^2=A^2+2AB+B^2 \text{ if and only if }  BA=AB
            </me>,
            as desired.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let
        <me>
          A = \begin{bmatrix}3\amp 0\\ -1\amp 2\\ 1\amp 1 \end{bmatrix} , \hspace{5pt} B = \begin{bmatrix}4\amp -1\\ 0\amp 2 \end{bmatrix} , \hspace{5pt} C = \begin{bmatrix}1\amp 4\amp 2\\ 3\amp 1\amp 5 \end{bmatrix}
        </me>
        <me>
          D = \begin{bmatrix}1\amp 5\amp 2\\ -1\amp 0\amp 1\\ 3\amp 2\amp 4 \end{bmatrix} ,  \hspace{5pt} E = \begin{bmatrix}6\amp 1\amp 3\\ -1\amp 1\amp 2\\ 4\amp 1\amp 3 \end{bmatrix}
        </me>.
        Compute the following matrices,
        or else explain why the given expression is not well defined.
        <ol>
          <li>
            <p>
              <m>(2D^T-E)A</m>
            </p>
          </li>
          <li>
            <p>
              <m>(4B)C+2B</m>
            </p>
          </li>
          <li>
            <p>
              <m>B^T(CC^T-A^TA)</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>\left(\begin{bmatrix}2\amp -2\amp 6\\ 10\amp 0\amp 4\\ 4\amp 2\amp 8 \end{bmatrix} - \begin{bmatrix}6\amp 1\amp 3\\ -1\amp 1\amp 2\\ 4\amp 1\amp 3 \end{bmatrix} \right) \begin{bmatrix}3\amp 0\\ -1\amp 2\\ 1\amp 1 \end{bmatrix} \amp =\amp \begin{bmatrix}-4\amp -3\amp 3\\ 11\amp -1\amp 2\\ 0\amp 1\amp 5 \end{bmatrix} \begin{bmatrix}3\amp 0\\ -1\amp 2\\ 1\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp =\amp \begin{bmatrix}-6\amp -3\\ 36\amp 0\\ 4\amp 7 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}16\amp -4\\ 0\amp 8 \end{bmatrix} \begin{bmatrix}1\amp 4\amp 2\\ 3\amp 1\amp 5 \end{bmatrix} + \begin{bmatrix}8\amp -2\\ 0\amp 4 \end{bmatrix} \amp =\amp \begin{bmatrix}4\amp 60\amp 12\\ 24\amp 8\amp 40 \end{bmatrix} + \begin{bmatrix}8\amp -2\\ 0\amp 4 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          The matrix sum on the right is not defined.
          Thus the operation is not defined.
        </p>
        <p>
          (c)
          <md>
            <mrow>\begin{bmatrix}4\amp 0\\ -1\amp 2 \end{bmatrix} \left( \begin{bmatrix}1\amp 4\amp 2\\ 3\amp 1\amp 5 \end{bmatrix} \begin{bmatrix}1\amp 3\\ 4\amp 1\\ 2\amp 5 \end{bmatrix} - \begin{bmatrix}3\amp -1\amp 1\\ 0\amp 2\amp 1 \end{bmatrix} \begin{bmatrix}3\amp 0\\ -1\amp 2\\ 1\amp 1 \end{bmatrix} \right)</mrow>
            <mrow>= \begin{bmatrix}4\amp 0\\ -1\amp 2 \end{bmatrix} \left( \begin{bmatrix}21\amp 17\\ 17\amp 35 \end{bmatrix} - \begin{bmatrix}11\amp -1\\ -1\amp 5 \end{bmatrix} \right) = \begin{bmatrix}40\amp 72\\ 26\amp 42 \end{bmatrix}</mrow>
          </md>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let
        <me>
          A = \begin{bmatrix}3\amp -2\amp 7\\ 6\amp 5\amp 4\\ 0\amp 4\amp 9 \end{bmatrix} , \hspace{5pt} B = \begin{bmatrix}6\amp -2\amp 4\\ 0\amp 1\amp 3\\ 7\amp 7\amp 5 \end{bmatrix}
        </me>.
        Compute the following using either the row or column method of matrix multiplication:
        <ol>
          <li>
            <p>
              the first column of <m>AB</m>;
            </p>
          </li>
          <li>
            <p>
              the second row of <m>BB</m>;
            </p>
          </li>
          <li>
            <p>
              the third column of <m>AA</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Using expansion by columns,
          the first column of <m>AB</m> is given by <m>A</m> times the first column of <m>B</m>.
          We compute
          <me>
            \begin{bmatrix}3\amp -2\amp 7\\ 6\amp 5\amp 4\\ 0\amp 4\amp 9 \end{bmatrix} \begin{bmatrix}6\\ 0\\ 7 \end{bmatrix} = \begin{bmatrix}67\\ 64\\ 63 \end{bmatrix}
          </me>
        </p>
        <p>
          (b) Using expansion by rows,
          the second row of <m>BB</m> is given by the second row of <m>B</m> times <m>B</m>.
          We compute
          <me>
            \begin{bmatrix}0\amp 1\amp 3 \end{bmatrix} \begin{bmatrix}6\amp -2\amp 4\\ 0\amp 1\amp 3\\ 7\amp 7\amp 5 \end{bmatrix} = \begin{bmatrix}21\amp 22\amp 18 \end{bmatrix}
          </me>
        </p>
        <p>
          (c) Using expansion by columns,
          the third column of <m>AA</m> is given by <m>A</m> times the third column of <m>A</m>.
          We compute
          <me>
            A\colvec{7 \\ 4 \\ 9}=\begin{bmatrix}3\amp -2\amp 7\\ 6\amp 5\amp 4\\ 0\amp 4\amp 9 \end{bmatrix} \colvec{7 \\ 4 \\ 9}=\colvec{76\\98\\97}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find all values of <m>k</m>, if any,
        that satisfy the equation:
        <md>
          <mrow>\begin{bmatrix}2\amp 2\amp k \end{bmatrix} \begin{bmatrix}1\amp 2\amp 0\\ 2\amp 0\amp 3\\ 0\amp 3\amp 1 \end{bmatrix} \begin{bmatrix}2\\ 2\\ k \end{bmatrix} = 0</mrow>
        </md>
        <solution>
          <p>
            Since:
            <md>
              <mrow>\begin{bmatrix}2\amp 2\amp k \end{bmatrix} \begin{bmatrix}1\amp 2\amp 0\\ 2\amp 0\amp 3\\ 0\amp 3\amp 1 \end{bmatrix} \begin{bmatrix}2\\ 2\\ k \end{bmatrix} = \begin{bmatrix}6\amp 4+3k\amp 6+k \end{bmatrix} \begin{bmatrix}2\\ 2\\ k \end{bmatrix} = \begin{bmatrix}k^2 + 12k + 20 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Set <m>k^2 + 12k + 20 = 0</m> and solve for <m>k</m>.
            <me>
              0 = k^2 + 12k + 20 = (k + 2)(k + 10) \rightarrow k = -2, -10
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>\boldzero_{2\times 2}</m> denote the <m>2 \times 2</m> matrix,
        each of whose entries is zero.
        <ol>
          <li>
            <p>
              Is there a 2 <times/> 2 matrix <m>A</m> such that
              <m>A \neq \boldzero</m> and <m>AA = \boldzero</m>?
            </p>
          </li>
          <li>
            <p>
              Is there a 2<times/> 2 matrix <m>A</m> such that <m>A \neq \boldzero</m> and <m>AA = A</m>?
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Yes, there are many examples.
          Here is one:
          <me>
            A = \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}  , AA = \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} = \begin{bmatrix}0\amp 0\\ 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          (b) Again yes, here is one example:
          <me>
            A = \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix}  , AA = \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} = \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Answer true or false.
        If true, provide a proof; if false,
        provide an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>B</m> has a column of zeros,
              then so does <m>AB</m> if this product is defined.
            </p>
          </li>
          <li>
            <p>
              If <m>B</m> has a column of zeros,
              then so does <m>BA</m> if this product is defined.
            </p>
          </li>
          <li>
            <p>
              If <m>A, B, C</m> are <em>nonzero</em>
              <m>n\times n</m> matrices and <m>AC=BC</m>,
              then <m>A=B</m>. (Remember:
              a matrix is nonzero if it is not the zero matrix. )
            </p>
          </li>
          <li>
            <p>
              If <m>AB+BA</m> is well-defined,
              then <m>A</m> and <m>B</m> are square matrices of the same size.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) True.
          Use the column method of matrix multiplication.
          Suppose column <m>j</m> of <m>B</m> is a zero column,
          and denote this column <m>\mathbf{b}_j</m>.
          Then according to the column method of multiplication,
          the <m>j</m>-th column of the product <m>AB</m> is given by <m>A\mathbf{b}_j</m>.
          But clearly, <m>A\mathbf{b}_j</m> is the zero column vector,
          since <m>\mathbf{b}_j</m> contains nothing but zeros.
          Thus the <m>j</m>-th column of <m>AB</m> is a zero column.
        </p>
        <p>
          (b) False.
          Let
          <me>
            A = \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix} , B = \begin{bmatrix}0\amp 1\\ 0\amp 1 \end{bmatrix}
          </me>.
        </p>
        <p>
          Then
          <me>
            BA = \begin{bmatrix}0\amp 1\\ 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix} = \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          (c) False.
          Take <m>A=C=\begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}</m>,
          and <m>B=\begin{bmatrix}0\amp 2\\ 0\amp 0 \end{bmatrix}</m>.
          Then <m>AC=BC=\boldzero_{2\times 2}</m>, but <m>A\ne B</m>.
        </p>
        <p>
          (d) True.
          Suppose <m>A</m> is <m>m\times n</m> and <m>B</m> is <m>r\times s</m>.
          For <m>AB</m> to be well defined, we must have <m>n=r</m>.
          Thus <m>B</m> is in fact <m>n\times s</m>.
          For <m>BA</m> to be well-defined we need <m>s=m</m>.
          Thus <m>B</m> is <m>n\times m</m>,
          and hence <m>AB</m> is <m>m\times m</m> and <m>BA</m> is <m>n\times n</m>.
          For the sum of these matrices to be equal, we need <m>m=n</m>.
          Thus <m>A</m> and <m>B</m> are both <m>n\times n</m> matrices!
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A = \begin{bmatrix}2\amp 0\\ 4\amp 1 \end{bmatrix}</m>.
        Compute <m>A^3</m>, <m>A^{-3}</m> and <m>A^2-2A+I</m>.
        <solution>
          <md>
            <mrow>A^3\amp =AAA</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix} \begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix} \begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] 4\amp 0</mrow>
            <mrow>12\amp 1 \end{bmatrix} \begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] 8\amp 0</mrow>
            <mrow>28\amp 1 \end{bmatrix}</mrow>
            <mrow>A^{-3}\amp =(A^{-1})^3 \amp \text{ (by definition) }</mrow>
            <mrow>\amp =A^{-1}A^{-1}A^{-1}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] \frac{1}{2}\amp 0</mrow>
            <mrow>-2\amp 1 \end{bmatrix} \begin{bmatrix}[rc] \frac{1}{2}\amp 0</mrow>
            <mrow>-2\amp 1 \end{bmatrix} \begin{bmatrix}[rc] \frac{1}{2}\amp 0</mrow>
            <mrow>-2\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] \frac{1}{4}\amp 0</mrow>
            <mrow>-3\amp 1 \end{bmatrix} \begin{bmatrix}[rc] \frac{1}{2}\amp 0</mrow>
            <mrow>-2\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[cc] \frac{1}{8}\amp 0</mrow>
            <mrow>-\frac{7}{2}\amp 1 \end{bmatrix}</mrow>
            <mrow>A^2 -2A + I\amp =\begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix} \begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix} -2\begin{bmatrix}[rc] 2\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix} + \begin{bmatrix}[rc] 1\amp 0</mrow>
            <mrow>0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] 4\amp 0</mrow>
            <mrow>12\amp 1 \end{bmatrix} - \begin{bmatrix}[rc] 4\amp 0</mrow>
            <mrow>8\amp 2 \end{bmatrix} + \begin{bmatrix}[rc] 1\amp 0</mrow>
            <mrow>0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp = \begin{bmatrix}[rc] 1\amp 0</mrow>
            <mrow>4\amp 1 \end{bmatrix}</mrow>
          </md>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A=\begin{bmatrix}2\amp 0\\ 4\amp 1 \end{bmatrix}</m>.
        Compute <m>p(A)</m>, where <m>p(x)=x^3-2x+5</m>.
        <solution>
          <p>
            Replace <m>x</m> with <m>A</m> and the constant term <m>5</m> with <m>5I</m>:
            <md>
              <mrow>\begin{bmatrix}8\amp 0\\ 28\amp 1 \end{bmatrix} - \begin{bmatrix}4\amp 0\\ 8\amp 2 \end{bmatrix} + 5\begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} = \begin{bmatrix}9\amp 0\\ 20\amp 4 \end{bmatrix}</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let
        <md>
          <mrow>A = \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix} , \hspace{5pt} C = \begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix} </mrow>
        </md>.
        Find all choices of <m>a,b,c</m>,
        and <m>d</m> for which <m>AC=CA</m>.
        <solution>
          <md>
            <mrow>AC = \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix} \begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix} = \begin{bmatrix}b\amp 0\\ d\amp 0 \end{bmatrix}</mrow>
            <mrow>CA = \begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix} \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix} = \begin{bmatrix}0\amp 0\\ a\amp b \end{bmatrix}</mrow>
          </md>
          <p>
            Thus <m>A</m> and <m>C</m> commute when:
            <m>a = d</m> and <m>b = 0</m>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Consider the system
        <md>
          \begin{linsys}{2} 2x_1\amp -\amp 2x_2\amp =\amp 4\\ x_1\amp +\amp 4x_2\amp =\amp 4 \end{linsys}
        </md>
        <ol>
          <li>
            <p>
              Find a matrix <m>A</m> and column vector <m>\boldb</m> such that solutions
              <m>(x_1,x_2)</m> to the system correspond to solutions
              <m>\colvec{x_1 \\ x_2}</m> to the matrix equation <m>A\colvec{x_1\\ x_2}=\boldb</m>.
            </p>
          </li>
          <li>
            <p>
              Now solve your matrix equation in (a) algebraically using inverses.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          We let <m>A=\begin{bmatrix}2\amp -2\\ 1\amp 4 \end{bmatrix}</m> and <m>\boldb=\colvec{4 \\ 4}</m>.
          Using our formula for inverses of <m>2\times 2</m> matrices,
          we see that <m>A</m> is invertible and <m>A^{-1}=\begin{bmatrix}2/5\amp 1/5\\ -1/10\amp 1/5 \end{bmatrix}</m>.
          Then we solve the matrix equation as follows:
          <md>
            <mrow>A\colvec{x_1</mrow>
            <mrow>x_2}\amp =\colvec{4</mrow>
            <mrow>4}</mrow>
            <mrow>\colvec{x_1</mrow>
            <mrow>x_2}\amp =A^{-1}\colvec{4</mrow>
            <mrow>4} \amp \text{(mult. both sides on left by \(A^{-1}\))}</mrow>
            <mrow>\colvec{x_1</mrow>
            <mrow>x_2}\amp = \begin{bmatrix}[rc] 2/5\amp 1/5</mrow>
            <mrow>-1/10\amp 1/5 \end{bmatrix} \begin{bmatrix}[r] 4</mrow>
            <mrow>4 \end{bmatrix}</mrow>
            <mrow>\amp =\colvec{12/5</mrow>
            <mrow>2/5}=\frac{1}{5}\colvec{12</mrow>
            <mrow>2}</mrow>
          </md>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find at least nine solutions to <m>A^2 = I_3</m>
        <solution>
          <p>
            Matrices of the form
            <me>
              \begin{bmatrix}\pm 1\amp 0\amp 0\\ 0\amp \pm 1\amp 0\\ 0\amp 0\amp \pm 1 \end{bmatrix}
            </me>
            comprise eight of the desired nine solutions.
            For a ninth solution take
            <me>
              A=\begin{bmatrix}0\amp 1\amp 0\\ 1\amp 0\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Can a matrix with two identical rows or two identical columns have an inverse?
        <solution>
          <p>
            No.
            We see this using the row or column method of multiplication.
          </p>
          <p>
            For example, suppose <m>A</m> has two identical columns;
            that means we have <m>\mathbf{a}_{i}=\mathbf{a}_{j}</m> for two columns
            <m>\mathbf{a}_{i}</m> and <m>\mathbf{a}_{j}</m>.
            Now suppose by contradiction that <m>B</m> is the inverse of <m>A</m>.
            Then <m>BA=I_n</m>.
            The column method tells us that the the <m>i</m>- and <m>j</m>-th columns of <m>BA</m> are
            <m>B\mathbf{a}_{i}</m> and <m>B\mathbf{a}_{j}</m>;
            but then these two columns are equal,
            a contradiction since <m>I_n</m> does not have any identical columns.
          </p>
          <p>
            A similar argument applies if <m>A</m> has two identical rows.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Assuming <m>A, B, C</m> and <m>D</m> are all invertible square matrixes,
        solve the following equation for <m>D</m> in terms of <m>A</m>,
        <m>B</m> and <m>C</m>.
        <me>
          ABC^TDBA^TC=AB^T.\\
        </me>
        <solution>
          <md>
            <mrow>ABC^TDBA^TC\amp =\amp AB^T</mrow>
            <mrow>(ABC^T)^{-1}ABC^TDBA^TC\amp =\amp (ABC^T)^{-1}AB^T</mrow>
            <mrow>DBA^TC\amp =\amp (ABC^T)^{-1}AB^T</mrow>
            <mrow>DBA^TC(BA^TC)^{-1}\amp =\amp (ABC^T)^{-1}AB^T(BA^TC)^{-1}</mrow>
            <mrow>D\amp =\amp (ABC^T)^{-1}AB^T(BA^TC)^{-1}</mrow>
          </md>
          <p>
            Side note: You can also work with one matrix at a time and multiply on the correct side of both sides by the inverse.
            It takes longer, but gives the same result.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove: <m>A+B=B+A</m>, where <m>A</m> and <m>B</m> are any
        <m>m\times n</m> matrices. (This is Theorem 1.18 (a).)
        <solution>
          <p>
            Let <m>A = [a_{ij}], B = [b_{ij}]</m> and
            <m>(A+B)_{ij}</m> denote the <m>ij</m> entry of <m>A+B</m>.
            Show that <m>(A+B)_{ij} = (B+A)_{ij}</m>.
            <md>
              <mrow>(A+B)_{ij} \amp =\amp  a_{ij} + b_{ij}</mrow>
              <mrow>\amp =\amp  b_{ij} + a_{ij}</mrow>
              <mrow>\amp =\amp  (B+A)_{ij}</mrow>
            </md>
          </p>
          <p>
            Note that line two follows from the fact that addition of real numbers is commutative
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove: <m>(AB)^T = B^TA^T</m>,
        where <m>A</m> is any <m>m\times n</m> matrix and <m>B</m> is any <m>n\times r</m> matrix.
        <solution>
          <p>
            Both the LHS and RHS are <m>n\times n</m> matrices.
            It remains to show the <m>ij</m>-th entries are all equal:
            that is, <m>((AB)^T)_{ij}=(B^TA^T)_{ij}</m> for all <m>i,j</m>.
            <md>
              <mrow>((AB)^T)_{ij}\amp =(AB)_{ji} \amp  \text{ (def. of transp.) }</mrow>
              <mrow>\amp =\sum_{\ell=1}^n(A)_{j\ell}(B)_{\ell j} \amp \text{ (def. of mult.) }</mrow>
              <mrow>\amp =\sum_{\ell=1}^n(A^T)_{\ell j}(B^T)_{i\ell} \amp \text{ (def. of transp.) }</mrow>
              <mrow>\amp =\sum_{\ell=1}^n(B^T)_{i\ell}(A^T)_{\ell j} \amp \text{ (real number commutativity) }</mrow>
              <mrow>\amp =(B^TA^T)_{ij}</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Answer true or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are invertible <m>n\times n</m> matrices,
              then <m>(AB)^{-1} = A^{-1}B^{-1}</m>.
            </p>
          </li>
          <li>
            <p>
              A square matrix containing a row or column of zeros cannot be invertible.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are invertible <m>n\times n</m> matrices,
              then <m>A+B</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is invertible, then <m>A^T</m> is invertible.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          Let
          <md>
            <mrow>A = \begin{bmatrix}2\amp 1\\ 1\amp 1 \end{bmatrix} : B = \begin{bmatrix}2\amp 2\\ 3\amp 2 \end{bmatrix}</mrow>
            <mrow>A^{-1}B^{-1} = \begin{bmatrix}-5/2\amp 2\\ 4\amp -3 \end{bmatrix}</mrow>
            <mrow>(AB)^{-1} = B^{-1}A^{-1} = \begin{bmatrix}-2\amp 3\\ 5/2\amp -7/2 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          (b) True.
          If <m>A</m> has a row of zeroes,
          then <m>AB</m> has a row of zeroes;
          it follows that there cannot be a matrix <m>B</m> such that <m>AB=I</m>,
          and thus that <m>A</m> is not invertible.
        </p>
        <p>
          Similarly, if <m>A</m> has a column of zeroes,
          then <m>BA</m> has a column of zeroes,
          and hence cannot be equal to <m>I</m>.
          Thus <m>A</m> cannot be invertible in this case.
        </p>
        <p>
          (c) False.
          Take <m>A=I_2</m> and <m>B=-I_2</m>.
          Both are invertible, yet <m>A+B=\boldzero</m> is not invertible.
        </p>
        <p>
          (d) True.
          In fact, we have a theorem that says in this case that <m>(A^T)^{-1}=(A^{-1})^T</m>.
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{1.5-1.6: inverses, diagonal and symmetric matrices}
  </p>
  <ol>
    <li>
      <p>
        Let
        <me>
          B = \begin{bmatrix}8\amp 1\amp 5\\ 2\amp -7\amp -1\\ 3\amp 4\amp 1 \end{bmatrix} , \hspace{5pt} D = \begin{bmatrix}8\amp 1\amp 5\\ -6\amp 21\amp 3\\ 3\amp 4\amp 1 \end{bmatrix} , \hspace{5pt} F = \begin{bmatrix}8\amp 1\amp 5\\ 8\amp 1\amp 1\\ 3\amp 4\amp 1 \end{bmatrix} , \hspace{5pt}
        </me>
        For each part below, find an explicit
        <em>elementary matrix</em>
        <m>E</m> satisfying the given matrix equation.
        Justify your answer by first explaining what row operation <m>E</m> should perform.
      </p>
      <ol>
        <li>
          <p>
            <m>EB = D</m>
          </p>
        </li>
        <li>
          <p>
            <m>ED=B</m>
          </p>
        </li>
        <li>
          <p>
            <m>EB=F</m>
          </p>
        </li>
        <li>
          <p>
            <m>EF=B</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) To obtain <m>D</m> from <m>B</m> we must scale the second row of <m>B</m> by <m>-3</m>.
          Thus
          <me>
            E = \begin{bmatrix}1\amp 0\amp 0\\ 0\amp -3\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>.
        </p>
        <p>
          (c) To obtain <m>F</m> from <m>B</m> we must replace the second row of <m>B</m> with <m>r_2+2r_3</m>.
          Thus
          <me>
            E = \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 2\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        For each matrix below use the inversion algorithm to find the inverse,
        if it exists.
        <ol>
          <li>
            <p>
              <m>A=\begin{bmatrix}1/5\amp 1/5\amp -2/5\\ 1/5\amp 1/5\amp 1/10\\ 1/5\amp -4/5\amp 1/10 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}1/5\amp 1/5\amp -2/5\\ 2/5\amp -3/5\amp -3/10\\ 1/5\amp -4/5\amp 1/10 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}0\amp 0\amp 2\amp 0\\ 1\amp 0\amp 0\amp 1\\ 0\amp -1\amp 3\amp 0\\ 2\amp 1\amp 5\amp -3 \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>\begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 1/5\amp 1/5\amp 1/10\amp 0\amp 1\amp 0\\ 1/5\amp -4/5\amp 1/10\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_1 - r_2}\amp \begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 0\amp 0\amp -1/2\amp 1\amp -1\amp 0\\ 1/5\amp -4/5\amp 1/10\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 - r_3}\amp \begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 0\amp 0\amp -1/2\amp 1\amp -1\amp 0\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{5r_1}\amp \begin{bmatrix}1\amp 1\amp -2\amp 5\amp 0\amp 0\\ 0\amp 0\amp -1/2\amp 1\amp -1\amp 0\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 \leftrightarrow r_3}\amp \begin{bmatrix}1\amp 1\amp -2\amp 5\amp 0\amp 0\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1\\ 0\amp 0\amp -1/2\amp 1\amp -1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1- r_2}\amp \begin{bmatrix}1\amp 0\amp -3/2\amp 4\amp 0\amp 1\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1\\ 0\amp 0\amp -1/2\amp 1\amp -1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-2r_3}\amp \begin{bmatrix}1\amp 0\amp -3/2\amp 4\amp 0\amp 1\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1\\ 0\amp 0\amp 1\amp -2\amp 2\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 + \frac{1}{2}r_3}\amp \begin{bmatrix}1\amp 0\amp -3/2\amp 4\amp 0\amp 1\\ 0\amp 1\amp 0\amp 0\amp 1\amp -1\\ 0\amp 0\amp 1\amp -2\amp 2\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 + \frac{3}{2}r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 3\amp 1\\ 0\amp 1\amp 0\amp 0\amp 1\amp -1\\ 0\amp 0\amp 1\amp -2\amp 2\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 2/5\amp -3/5\amp -3/10\amp 0\amp 1\amp 0\\ 1/5\amp -4/5\amp 1/10\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{2r_1 - r_2}\amp \begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 0\amp 1\amp -1/2\amp 2\amp -1\amp 0\\ 1/5\amp -4/5\amp 1/10\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 - r_3}\amp \begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 0\amp 1\amp -1/2\amp 2\amp -1\amp 0\\ 0\amp 1\amp -1/2\amp 1\amp 0\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 - r_3}\amp \begin{bmatrix}1/5\amp 1/5\amp -2/5\amp 1\amp 0\amp 0\\ 0\amp 1\amp -1/2\amp 2\amp -1\amp 0\\ 0\amp 0\amp 0\amp 1\amp -1\amp 1 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Since we have row reduced our matrix to a matrix with a row of zeros,
          we can conclude that our original matrix was singular.
          Thus no inverse exists.
        </p>
        <p>
          (c)
          <md>
            <mrow>\begin{bmatrix}0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp -1\amp 3\amp 0\amp 0\amp 0\amp 1\amp 0\\ 2\amp 1\amp 5\amp -3\amp 0\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_1 \leftrightarrow r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp -1\amp 3\amp 0\amp 0\amp 0\amp 1\amp 0\\ 2\amp 1\amp 5\amp -3\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4-2r_1}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp -1\amp 3\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp -1\amp -5\amp 5\amp 0\amp 2\amp 0\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp 1\amp -3\amp 0\amp 0\amp 0\amp -1\amp 0\\ 0\amp -1\amp -5\amp 5\amp 0\amp 2\amp 0\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4+r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp 1\amp -3\amp 0\amp 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp -8\amp 5\amp 0\amp 2\amp -1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 \leftrightarrow r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp -3\amp 0\amp 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp 0\amp -8\amp 5\amp 0\amp 2\amp -1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_4+4r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp -3\amp 0\amp 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 2\amp 0\amp 1\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 5\amp 4\amp 2\amp -1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{2}r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp -3\amp 0\amp 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/2\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 5\amp 4\amp 2\amp -1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 + 3r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp 3/2\amp 0\amp -1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/2\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 5\amp 4\amp 2\amp -1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{5}r_4}\amp \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp 3/2\amp 0\amp -1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/2\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 4/5\amp 2/5\amp -1/5\amp -1/5 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 - r_4}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp -4/5\amp 3/5\amp 1/5\amp 1/5\\ 0\amp 1\amp 0\amp 0\amp 3/2\amp 0\amp -1\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/2\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 4/5\amp 2/5\amp -1/5\amp -1/5 \end{bmatrix}</mrow>
          </md>
        </p>
      </solution>
    </li>
    <li>
      <p>
        For each matrix below find the inverse using the inverse algorithm.
        Assume <m>k_1,k_2,k_3,k_4,k \neq 0</m>.
        <ol>
          <li>
            <p>
              <m>A=\begin{bmatrix}0\amp 0\amp 0\amp k_1\\ 0\amp 0\amp k_2\amp 0\\ 0\amp k_3\amp 0\amp 0\\ k_4\amp 0\amp 0\amp 0 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}k\amp 0\amp 0\amp 0\\ 1\amp k\amp 0\amp 0\\ 0\amp 1\amp k\amp 0\\ 0\amp 0\amp 1\amp k \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>\begin{bmatrix}0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0\\ 0\amp 0\amp k_2\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp k_3\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ k_4\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_1 \leftrightarrow r_4}\amp \begin{bmatrix}k_4\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp k_2\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp k_3\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 \leftrightarrow r_3}\amp \begin{bmatrix}k_4\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1\\ 0\amp k_3\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp k_2\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{k_4}r_1}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1/k_4\\ 0\amp k_3\amp 0\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp k_2\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{k_3}r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1/k_4\\ 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 1/k_3\amp 0\\ 0\amp 0\amp k_2\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{k_2}r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1/k_4\\ 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 1/k_3\amp 0\\ 0\amp 0\amp 1\amp 0\amp 0\amp 1/k_2\amp 0\amp 0\\ 0\amp 0\amp 0\amp k_1\amp 1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{k_1}r_4}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\amp 0\amp 0\amp 1/k_4\\ 0\amp 1\amp 0\amp 0\amp 0\amp 0\amp 1/k_3\amp 0\\ 0\amp 0\amp 1\amp 0\amp 0\amp 1/k_2\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 1/k_1\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Thus <m>A^{-1}=\begin{bmatrix}0\amp 0\amp 0\amp 1/k_4\\ 0\amp 0\amp 1/k_3\amp 0\\ 0\amp 1/k_2\amp 0\amp 0\\ 1/k_1\amp 0\amp 0\amp 0 \end{bmatrix}</m>
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}k\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\amp 0\\ 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{\frac{1}{k}r_1}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 - r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp -k\amp 0\amp 0\amp 1/k\amp -1\amp 0\amp 0\\ 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{k}r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp -1/k^2\amp 1/k\amp 0\amp 0\\ 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 - r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp -1/k^2\amp 1/k\amp 0\amp 0\\ 0\amp 0\amp -k\amp 0\amp -1/k^2\amp 1/k\amp -1\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{k}r_3}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp -1/k^2\amp 1/k\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/k^3\amp -1/k^2\amp 1/k\amp 0\\ 0\amp 0\amp 1\amp k\amp 0\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3 - r_4}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp -1/k^2\amp 1/k\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/k^3\amp -1/k^2\amp 1/k\amp 0\\ 0\amp 0\amp 0\amp -k\amp 1/k^3\amp -1/k^2\amp 1/k\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{k}r_4}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1/k\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp -1/k^2\amp 1/k\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\amp 1/k^3\amp -1/k^2\amp 1/k\amp 0\\ 0\amp 0\amp 0\amp 1\amp -1/k^4\amp 1/k^3\amp -1/k^2\amp 1/k \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Thus <m>A^{-1}=\begin{bmatrix}1/k\amp 0\amp 0\amp 0\\ -1/k^2\amp 1/k\amp 0\amp 0\\ 1/k^3\amp -1/k^2\amp 1/k\amp 0\\ -1/k^4\amp 1/k^3\amp -1/k^2\amp 1/k \end{bmatrix}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find all values of <m>c</m>, if any,
        making the matrix <m>A=\begin{bmatrix}c\amp 1\amp 0\\ 1\amp c\amp 1\\ 0\amp 1\amp c \end{bmatrix}</m> invertible.
        <solution>
          <p>
            The matrix is row equivalent to <m>U=\begin{bmatrix}1\amp c\amp 1\\ 0\amp 1\amp c\\ 0\amp 0\amp -2c+c^3 \end{bmatrix}</m>.
          </p>
          <p>
            We know <m>A</m> is invertible if and only if <m>U</m> is invertible.
            Furthermore <m>U</m> is invertible if and only if <m>-2c+c^3=c(c^2-2)\ne 0</m>.
            This is the case if and only if <m>c\ne 0</m> and <m>c\ne \pm \sqrt{2}</m>.
            Thus <m>A</m> is invertible if and only if <m>c \neq 0, \pm\sqrt{2}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each <m>A</m> below,
        express <m>A</m> and <m>A^{-1}</m> as products of elementary matrices.
        <ol>
          <li>
            <p>
              <m>A=\begin{bmatrix}1\amp 0\\-5\amp 2 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}1\amp 1\amp 0\\ 1\amp 1\amp 1\\ 0\amp 1\amp 1 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}1\amp 1\amp 1\\ 1\amp -1\amp 0\\ 1\amp 0\amp -1 \end{bmatrix}</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>A = \begin{bmatrix}1\amp 0\\ -5\amp 2 \end{bmatrix} \amp \xrightarrow[]{r_2+5r_1}\amp \begin{bmatrix}1\amp 0\\ 0\amp 2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{\frac{1}{2}r_2}\amp \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Thus we have elementary matrices <m>E_1 = \begin{bmatrix}1\amp 0\\ 5\amp 1 \end{bmatrix} , E_2 = \begin{bmatrix}1\amp 0\\ 0\amp 1/2 \end{bmatrix}</m> such that:
        </p>
        <p>
          <m>E_2E_1 = \begin{bmatrix}1\amp 0\\ 0\amp 1/2 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 5\amp 1 \end{bmatrix} = A^{-1}</m> and <m>E^{-1}_1E^{-1}_2 = \begin{bmatrix}1\amp 0\\ 0\amp -2 \end{bmatrix} \begin{bmatrix}1\amp 0\\ -5/2\amp -1 \end{bmatrix} = A</m>
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}1\amp 1\amp 0\amp 1\amp 0\amp 0\\ 1\amp 1\amp 1\amp 0\amp 1\amp 0\\ 0\amp 1\amp 1\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_2-r_1}\amp \begin{bmatrix}1\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 1\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 \leftrightarrow r_3}\amp \begin{bmatrix}1\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 1\amp 0\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2-r_3}\amp \begin{bmatrix}1\amp 1\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 1\amp -1\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1-r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 1\amp -1\\ 0\amp 1\amp 0\amp 1\amp -1\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Then the elementary matrices are:
        </p>
        <p>
          <m>E_1 = \begin{bmatrix}1\amp 0\amp 0\\ -1\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix} , E_2 = \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 0\amp 1\\ 0\amp 1\amp 0 \end{bmatrix} , E_3 = \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp -1\\ 0\amp 0\amp 1 \end{bmatrix} , E_4 = \begin{bmatrix}1\amp -1\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m>
        </p>
        <p>
          And
        </p>
        <p>
          <m>A^{-1} = E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}</m>
        </p>
        <p>
          (c)
          <md>
            <mrow>\begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>1\amp -1\amp 0\amp 0\amp 1\amp 0</mrow>
            <mrow>1\amp 0\amp -1\amp 0\amp 0\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_2-r_1} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp -2\amp -1\amp -1\amp 1\amp 0</mrow>
            <mrow>1\amp 0\amp -1\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3-r_1} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp -2\amp -1\amp -1\amp 1\amp 0</mrow>
            <mrow>0\amp -1\amp -2\amp -1\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-\frac{1}{2}r_2} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 1/2\amp 1/2\amp -1/2\amp 0</mrow>
            <mrow>0\amp -1\amp -2\amp -1\amp 0\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_3+r_1} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 1/2\amp 1/2\amp -1/2\amp 0</mrow>
            <mrow>0\amp 0\amp -3/2\amp -1/2\amp -1/2\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-2/3 r_3} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 1/2\amp 1/2\amp -1/2\amp 0</mrow>
            <mrow>0\amp 0\amp 1\amp 1/3\amp 1/3\amp -2/3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2-1/2r_3} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 1\amp 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0\amp 1/3\amp -2/3\amp 2/3</mrow>
            <mrow>0\amp 0\amp 1\amp 1/3\amp 1/3\amp -2/3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1-r_3} \begin{bmatrix}[rcl|rcl] 1\amp 1\amp 0\amp 2/3\amp -1/3\amp 2/3</mrow>
            <mrow>0\amp 1\amp 0\amp 1/3\amp -2/3\amp 2/3</mrow>
            <mrow>0\amp 0\amp 1\amp 1/3\amp 1/3\amp -2/3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1-r_2} \begin{bmatrix}[rcl|rcl] 1\amp 0\amp 0\amp 1/3\amp 1/3\amp 0</mrow>
            <mrow>0\amp 1\amp 0\amp 1/3\amp -2/3\amp 2/3</mrow>
            <mrow>0\amp 0\amp 1\amp 1/3\amp 1/3\amp -2/3 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          The corresponding elementary matrices are
          <md>
            <mrow>E_1\amp =\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>-1\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp 1 \end{bmatrix}, \hspace{5pt} E_2=\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>-1\amp 0\amp 1 \end{bmatrix}, \hspace{5pt} E_3=\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp -\frac{1}{2}\amp 0</mrow>
            <mrow>0\amp 0\amp 1 \end{bmatrix}, \hspace{5pt}</mrow>
            <mrow>E_4\amp =\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>1\amp 0\amp 1 \end{bmatrix}, \hspace{5pt} E_5=\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp -\frac{2}{3} \end{bmatrix}, \hspace{5pt} E_6=\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp -\frac{1}{2}</mrow>
            <mrow>0\amp 0\amp 1 \end{bmatrix}, \hspace{5pt}</mrow>
            <mrow>E_7\amp =\begin{bmatrix}[rrr] 1\amp 0\amp -1</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp 1 \end{bmatrix}, \hspace{5pt} E_8=\begin{bmatrix}[rrr] 1\amp -1\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp 1 \end{bmatrix}, \hspace{5pt}</mrow>
          </md>
        </p>
        <p>
          We have <m>A^{-1}=E_8E_7E_6E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}E_6^{-1}E_7^{-1}E_8^{-1}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let
        <me>
          A=\begin{bmatrix}2\amp 1\amp 0\\ -1\amp 1\amp 0\\ 3\amp 0\amp -1 \end{bmatrix} , \hspace{5pt} B=\begin{bmatrix}6\amp 9\amp 4\\ -5\amp -1\amp 0\\ -1\amp -2\amp -1 \end{bmatrix}
        </me>.
        Produce <m>B</m> from <m>A</m> using row operations,
        the use this sequence of row operations to find <m>C</m> such that <m>CA =B</m>.
        <solution>
          <md>
            <mrow>\begin{bmatrix}2\amp 1\amp 0\\ -1\amp 1\amp 0\\ 3\amp 0\amp -1 \end{bmatrix} \amp \xrightarrow[]{r_3 - 2r_1}\amp \begin{bmatrix}2\amp 1\amp 0\\ -1\amp 1\amp 0\\ -1\amp -2\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 - 2r_1}\amp \begin{bmatrix}2\amp 1\amp 0\\ -5\amp -1\amp 0\\ -1\amp -2\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 - 4r_3}\amp \begin{bmatrix}6\amp 9\amp 4\\ -5\amp -1\amp 0\\ -1\amp -2\amp -1 \end{bmatrix}</mrow>
          </md>
          <md>
            <mrow>C = \begin{bmatrix}1\amp 0\amp -4\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 0\amp 0\\ -2\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ -2\amp 0\amp 1 \end{bmatrix} = \begin{bmatrix}9\amp 0\amp -4\\ -2\amp 1\amp 0\\ -2\amp 0\amp 1 \end{bmatrix}</mrow>
          </md>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> be <m>n\times n</m>.
        Show that the following are equivalent by proving a
        <q>cycle of implications</q>.
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              For any column vector <m>\boldb</m> the matrix equation
              <m>A\boldx=\boldb</m> has a unique solution.
            </p>
          </li>
          <li>
            <p>
              For any column vector <m>\boldb</m> the matrix equation <m>A\boldx=\boldb</m> has a solution.
            </p>
          </li>
        </ol>
      </p>
      This completes the proof of the invertibility theorem in its current form.
      <solution>
        <p>
          <m>(i)\Rightarrow (ii)</m>.
          Suppose <m>A</m> is invertible.
          Given any <m>\underset{n\times 1}{\boldb}</m>, we have
          <md>
            <mrow>A\boldx=\boldb\Rightarrow A^{-1}A\bold=A^{-1}\boldb</mrow>
            <mrow>\amp \Rightarrow I\boldx=A^{-1}\boldb</mrow>
            <mrow>\amp \Rightarrow \boldx=A^{-1}\boldb</mrow>
          </md>
        </p>
        <p>
          This shows that for any <m>\boldb</m>,
          there is a unique solution to the equation <m>A\boldx=\boldb</m>:
          namely, <m>\boldx=A^{-1}\boldb</m>.
        </p>
        <p>
          <m>(ii)\Rightarrow (iii)</m>.
          Easy.
        </p>
        <p>
          <m>(iii)\Rightarrow (i)</m>.
        </p>
        <p>
          Assume <m>A\boldx=\boldb</m> has a solution for any vector <m>\boldb</m>.
          For each <m>1\leq j\leq n</m> let
          <m>\bolde_j</m> be the <m>j</m>-th column of the identity matrix <m>I_n</m>:
          i.e., <m>\bolde_j</m> has a 1 in the <m>j</m>-th row, and 0's elsewhere.
          By assumption for each such <m>\bolde_j</m> I can find a vector
          <m>\boldc_j</m> such that <m>A\boldc_j=\bolde_j</m>.
        </p>
        <p>
          I claim the matrix
          <me>
            C=\begin{bmatrix}\vert\amp \vert \amp \dots \amp \vert\\ \boldc_1\amp \boldc_2\amp \dots\amp \boldc_n\\\vert\amp \vert \amp \dots \amp \vert \end{bmatrix}
          </me>
          is the inverse of <m>A</m>.
        </p>
        <p>
          Indeed, using the column method of matrix multiplication we see that
          <md>
            <mrow>AC\amp =A\begin{bmatrix}[cccc]\vert\amp \vert \amp \dots \amp \vert</mrow>
            <mrow>\boldc_1\amp \boldc_2\amp \dots\amp \boldc_n</mrow>
            <mrow>\vert\amp \vert \amp \dots \amp \vert \end{bmatrix}</mrow>
            <mrow>\amp =\begin{bmatrix}[cccc]\vert\amp \vert \amp \dots \amp \vert</mrow>
            <mrow>A\boldc_1\amp A\boldc_2\amp \dots\amp A\boldc_n</mrow>
            <mrow>\vert\amp \vert \amp \dots \amp \vert \end{bmatrix}</mrow>
            <mrow>\amp =\begin{bmatrix}[cccc]\vert\amp \vert \amp \dots \amp \vert</mrow>
            <mrow>\bolde_1\amp \bolde_2\amp \dots\amp \bolde_n</mrow>
            <mrow>\vert\amp \vert \amp \dots \amp \vert \end{bmatrix}</mrow>
            <mrow>\amp =I_n</mrow>
          </md>,
          where the last equality follows by construction:
          i.e, we picked <m>\bolde_j</m> precisely to be the <m>j</m>-th column <m>I_n</m> !!
        </p>
        <p>
          Note that this only shows <m>AC=I_n</m>;
          i.e., only that <m>C</m> is a <em>right-inverse</em> of <m>A</m>.
          Luckily we proved a corollary in the lecture notes that says you are a right inverse if and only if you are a left inverse.
          This allows us to claim that we also have <m>CA=I_n</m>,
          and hence that <m>C=A^{-1}</m>.
        </p>
        <p>
          (You might object that the corollary cited was proved using the invertibility theorem,
          which the desired statement is a part of!
          However, if you look closely at the proof of the corollary,
          you see that it only uses statement (b) from the invertibility theorem. )
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A</m> be <m>n\times n</m> and suppose <m>A</m> has two identical columns.
        Use an appropriate statement from the invertibility theorem to show <m>A</m> is not invertible.
        <solution>
          <p>
            Let <m>\boldc_i</m> be the columns of <m>A</m> and suppose that columns
            <m>\boldc_i</m> and <m>\boldc_j</m> are equal.
            Define <m>\boldx</m> to be the column vector with a 1 in the <m>i</m>-th row,
            a <m>-1</m> in the <m>j</m>-th column, and 0's elsewhere.
          </p>
          <p>
            Then the column method of matrix multiplication tells us that
            <me>
              A\boldx=\boldc_i-\boldc_j=\boldzero
            </me>.
          </p>
          <p>
            Since <m>\boldx</m> is nontrivial,
            we have found a nontrivial solution to <m>A\boldx=\boldzero</m>.
            By the invertibility theorem we now conclude <m>A</m> is not invertible.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove that if A is an invertible matrix and B is row equivalent to A, then B is also invertible.
        <solution>
          <p>
            Given that B is row equivalent to A, we can write B as the product of elementary matrices and A.
            <me>
              B = E_n\dots E_1A
            </me>
          </p>
          <p>
            By Theorem 1.5.2, all of the elementary matrices are invertible.
            And we know that the product of any number of invertible matrices is also invertible.
            Thus B is also invertible.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Determine conditions on <m>b_i</m>'s to guarantee a consistent system.
        <md>
          <mrow>x_1-2x_2-x_3\amp =\amp b_1</mrow>
          <mrow>-4x_1+5x_2+2x_3\amp =\amp b_2</mrow>
          <mrow>-4x_1+7x_2+4x_3\amp =\amp b_3</mrow>
        </md>
        <solution>
          <md>
            <mrow>\begin{bmatrix}1\amp -2\amp -1\amp b_1\\ -4\amp 5\amp 2\amp b_2\\ -4\amp 7\amp 4\amp b_3 \end{bmatrix} \amp \xrightarrow[]{4r_1+r_2}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp -3\amp -2\amp 4b_1+b_2\\ -4\amp 7\amp 4\amp b_3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{4r_1+r_3}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp -3\amp -2\amp 4b_1+b_2\\ 0\amp -1\amp 0\amp 4b_1+b_3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 \leftrightarrow r_3}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp -1\amp 0\amp 4b_1+b_3\\ 0\amp -3\amp -2\amp 4b_1+b_2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-r_2}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp 1\amp 0\amp -4b_1-b_3\\ 0\amp -3\amp -2\amp 4b_1+b_2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{3r_2+r_3}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp 1\amp 0\amp -4b_1-b_3\\ 0\amp 0\amp -2\amp -8b_1+b_2-3b_3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-1/2r_3}\amp \begin{bmatrix}1\amp -2\amp -1\amp b_1\\ 0\amp 1\amp 0\amp -4b_1-b_3\\ 0\amp 0\amp 1\amp 4b_1-1/2b_2+3/2b_3 \end{bmatrix}</mrow>
          </md>
          <p>
            Thus, this system is consistent for all values of <m>b_i</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>Ax=0</m> be a homogeneous system of <m>n</m> linear equations in <m>n</m> unknowns,
        and let <m>Q</m> be an invertible n x n matrix.
        Prove that <m>Ax=0</m> has only the trivial solution if and only if <m>(QA)x=0</m> has only the trivial solution.
        <solution>
          <p>
            Claim: <m>A\boldx=\boldzero\Leftrightarrow QA\boldx=\boldzero</m>.
            From this claim it follows that the set of solutions to
            <m>A\boldx=\boldzero</m> is exactly equal to the set of solutions to <m>QA\boldx=\boldzero</m>,
            and hence that the former equation has a unique solution if and only if the latter equation has a unique solution.
          </p>
          <p>
            Proof of claim:
            <md>
              <mrow>QA\boldx=\boldzero\amp \Leftrightarrow Q^{-1}QA\boldx=Q^{-1}\boldzero</mrow>
              <mrow>\amp \Leftrightarrow A\boldx=\boldzero</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Answer true or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              The product of two elementary matrices of the same size must be an elementary matrix.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is row equivalent to <m>B</m>,
              and <m>B</m> is row equivalent to <m>C</m>,
              then <m>A</m> is row equivalent to <m>C</m>.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a singular <m>n\times n</m> matrix,
              then the linear system <m>A\boldx=\boldzero</m> has infinitely many solutions.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is invertible,
              then if the second row of <m>A</m> is replaced with the second row plus the first row,
              the resulting matrix is invertible.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a square matrix,
              and if the linear system <m>Ax = b</m> has a unique solution,
              then the linear system <m>Ax = c</m> also must have a unique solution.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are row equivalent matrices,
              then the linear systems <m>Ax=0</m> and <m>Bx=0</m> have the same solution set.
            </p>
          </li>
          <li>
            <p>
              Let <m>A</m> and <m>B</m> be n x n matrices.
              If <m>A</m> or <m>B</m>
              (or both)
              are not invertible, then neither is <m>AB</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          Consider:
          <md>
            <mrow>\begin{bmatrix}1\amp 0\\ 0\amp -3 \end{bmatrix} \begin{bmatrix}1\amp 4\\ 0\amp 1 \end{bmatrix} = \begin{bmatrix}1\amp 4\\ 0\amp -3 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          The matrix on the right is not an elementary matrix,
          as it performs TWO row operations on a matrix: (1) <m>r_1\rightarrow r_1+4r_2</m>,
          and (2) <m>r_2\rightarrow -3r_2</m>.
        </p>
        <p>
          (b) True.
          Simply apply row operations to first transorm <m>A</m> to <m>B</m>,
          then <m>B</m> to <m>C</m>.
        </p>
        <p>
          (c) True.
          Singular means noninvertible, btw.
          The invertibility theorem tells us that
          <m>A\boldx=\boldzero</m> has more than one solution.
          Since linear systems have either 0, 1 or infinitely-many solutions,
          we conclude that it has infinitely many solutions.
        </p>
        <p>
          (d) True.
          The new matrix <m>B</m> is just <m>EA</m>,
          where <m>E</m> is an elementary matrix.
          Since elementary matrices are invertible,
          <m>B=EA</m> is the product of invertible matrices, hence invertible.
        </p>
        <p>
          (e) False.
          Take <m>A=\begin{bmatrix}1\amp 1\\0\amp 0 \end{bmatrix}</m>,
          <m>\boldb=\begin{bmatrix}1\\0 \end{bmatrix}</m> and <m>\boldc=\begin{bmatrix}0\\1 \end{bmatrix}</m>.
        </p>
        <p>
          Then the equation <m>A\boldx=\boldb</m> has a solution:
          namely <m>\boldx=\begin{bmatrix}1\\0 \end{bmatrix}</m>.
          But the equation <m>A\boldx=\boldc</m> does not have a solution.
        </p>
        <p>
          (f) True.
          Since <m>A</m> and <m>B</m> are row equivalent,
          we can <m>B = E_r \cdots E_1A=QA</m>,
          where <m>Q=E_r \cdots E_1</m> is invertible,
          being the product of invertible matrices.
        </p>
        <p>
          I claim <m>A\boldx=\boldzero</m> if and only if <m>QA\boldx=\boldzero</m>.
          Indeed, the <m>\Rightarrow</m> direction follows by multiplying both sides of <m>A\boldx=\boldzero</m> by <m>Q</m>;
          the <m>\Leftarrow</m> direction follows by multiplying both sides of <m>QA\boldx=\boldzero</m> by <m>Q^{-1}</m>.
          Since <m>QA=B</m>, we conclude that
          <m>A\boldx=\boldzero</m> if and only if <m>B\boldx=\boldzero</m>.
          This means the two equations have the same solution set.
        </p>
        <p>
          (g) True.
          We know now that for any <m>n\times n</m> matrices <m>A,B</m> we have the if and only if statement:
          <me>
            A \text{ and }  B \text{ are invertible } \Leftrightarrow AB \text{ is invertible. }
          </me>
        </p>
        <p>
          Thus if either one of <m>A</m> or <m>B</m> is not invertible,
          then the product <m>AB</m> is not invertible.
        </p>
      </solution>
    </li>
    <li xml:id="ex_diag">
      <p>
        Let <m>D=\begin{bmatrix}d_1\amp 0\amp \cdots\\ 0\amp d_2\amp 0\amp \cdots\\ \vdots \amp \amp \amp \vdots\\ 0\amp 0\amp \cdots \amp d_n \end{bmatrix}</m> be an <m>n\times n</m> diagonal matrix.
        <ol>
          <li>
            <p>
              Let <m>A</m> be <m>n\times r</m>.
              Use either the row/column method to describe the product <m>DA</m> as on operation on the rows/columns of <m>A</m>.
              (You choose which is appropriate, rows or columns.)
            </p>
          </li>
          <li>
            <p>
              Let <m>B</m> be <m>m\times n</m>.
              Describe the product <m>BD</m> as on operation of the rows/columns of <m>B</m>.
              (You choose which is appropriate, rows or columns.)
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) To understand what multiplying on the left by <m>D</m> does,
          we use the row method.
          This tells us that <m>DA</m> returns the matrix whose <m>i</m>-th row is <m>d_i</m> times the <m>i</m>-th row of <m>A</m>.
        </p>
        <p>
          (b) To understand what multiplying on the right by <m>D</m> does,
          we use the column method.
          This tells us that <m>BD</m> returns the matrix whose <m>j</m>-th column is <m>d_j</m> times the <m>j</m>-th column of <m>B</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A=[a_{ij}]</m> be a triangular <m>n\times n</m> matrix.
        Prove: <m>A</m> is invertible if and only if
        <m>a_{ii}\ne 0</m> for all <m>1\leq i\leq n</m>.
        Hint: use a convenient equivalent statement of invertibility from the invertibility theorem.
      </p>
      <solution>
        <p>
          Note: this statement is extremely easy to prove once we can invoke the determinant.
          Without this tool,
          it is intuitively true but not so easy to prove as it turns out!
        </p>
        <ul>
          <li>
            <title>Case 1</title>
            <p>
              assume <m>A</m> is upper triangular.
              We show the two implications separately.
            </p>
            <ul>
              <li>
                <title>invalidlabel</title>
                <p>
                  this is the easy direction.
                  If all <m>a_{ii}\ne 0</m>, then since <m>A</m> is upper triangular,
                  it is easy to see that <m>A</m> can be row reduced to the identity matrix.
                  First, for all <m>i</m> we scale the <m>i</m>-th row by <m>\frac{1}{a_{i}}</m>;
                  this transforms <m>A</m> into a row echelon matrix with a leading 1 in every column.
                  Reducing the resulting matrix to reduced row echelon form then yields the identity matrix.
                </p>
              </li>
              <li>
                <title>invalidlabel</title>
                <p>
                  we prove this implication by proving its contrapositive.
                  Assume one of the diagonal entries of <m>A</m> is equal to 0.
                  Let <m>a_{jj}</m> be the <em>first</em> zero diagonal entry,
                  so that <m>a_{ii}\ne 0</m> for all <m>i\lt j</m>.
                  Consider the matrix equation <m>A\boldx=\boldzero</m>,
                  and its corresponding homogenous linear system.
                  I claim there is a solution
                  <m>(x_1,x_2,\dots,
                  x_n)</m> where <m>x_j=1</m> and <m>x_k=0</m> for all <m>k>j</m>.
                  This would yield a <em>non-trivial</em>
                  solution to the homogenous system;
                  the invertibility theorem then would imply <m>A</m> is not invertible.
                  So why can we build such a solution?
                  Since <m>A</m> is upper triangular,
                  looking at the corresponding system,
                  the equations <m>j</m> through <m>n</m> only involve the variables <m>x_k</m> for all <m>k\geq j</m>.
                  Since <m>a_{jj}=0</m>, clearly the choice <m>x_j=1</m>,
                  <m>x_k=0</m> for <m>k>j</m> satisfies equations <m>j</m> through <m>n</m>.
                  Next, by assumption the coefficients
                  <m>a_{ii}\ne 0</m> for all <m>i\lt j</m>.
                  This allows me now to back-substitute and solve for each <m>x_i</m>,
                  <m>i\lt j</m>.
                  This concludes the proof that if <m>A</m> is invertible,
                  then <m>a_{ii}\ne 0</m> for all <m>i</m>.
                </p>
              </li>
            </ul>
          </li>
          <li>
            <title>Case 2</title>
            <p>
              assume <m>A</m> is lower triangular.
              The result in this case follows from Case 1 as follows:
              <md>
                <mrow>\text{ \(A\) is invertible } \amp \Longleftrightarrow \text{ \(A^T\) is invertible }  \amp \text{ (since \(A\) inv. if and only if \(A^T\) inv.) }</mrow>
                <mrow>\amp \Longleftrightarrow \text{ \(A^T\) has nonzero diagonal entries }  \amp \text{ (since \(A^T\) is upper triangular, using Case 1) }</mrow>
                <mrow>\amp \Longleftrightarrow \text{ \(A\) has nonzero diagonal entries }  \amp \text{ (since \(A\), \(A^T\) have the same diagonal entries). }</mrow>
              </md>
            </p>
          </li>
        </ul>
      </solution>
    </li>
    <li>
      <p>
        Prove that if <m>A</m> and <m>B</m> are symmetric,
        then <m>cA+dB</m> is symmetric for any <m>c,d\in\R</m>.
        <solution>
          <p>
            We use the definition that <m>A</m> is symmetric if and only if <m>A^T=A</m>.
          </p>
          <p>
            Assume <m>A</m> and <m>B</m> are symmetric.
            Then <m>A^T=A</m> and <m>B^T=B</m>.
            But then
            <md>
              <mrow>(cA+dB)^T\amp =(cA)^T+(dB)^T \amp \text{ (additive prop. of transp.) }</mrow>
              <mrow>\amp =cA^T+dB^T \amp \text{ (scaling prop. of transp. } \amp =cA+dB \amp \text{ (since \(A,B\) symmetric) }</mrow>
            </md>
          </p>
          <p>
            This shows <m>(cA+dB)^T=cA+dB</m>,
            which means <m>A+B</m> is symmetric.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Suppose <m>A</m> is symmetric and invertible.
        Prove <m>A^{-1}</m> is symmetric.
        <solution>
          <p>
            We use the alternative definition that <m>A</m> is symmetric if and only if <m>A^T=A</m>.
          </p>
          <p>
            Assume <m>A</m> is symmetric.
            Then <m>A^T=A</m>.
            But then
            <md>
              <mrow>(A^{-1})^T\amp =(A^T)^{-1} \amp \text{ (prop. of transp.) }</mrow>
              <mrow>\amp =A^{-1} \amp \text{ (since \(A\) symmetric) }</mrow>
            </md>
          </p>
          <p>
            This shows <m>(A^{-1})^T=A^{-1}</m>,
            which means <m>A^{-1}</m> is symmetric.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> and <m>B</m> be symmetric.
        Prove that <m>AB</m> is symmetric if and only if <m>AB=BA</m>.
        <solution>
          <p>
            I will prove this if and only if statement by showing both implications separately.
            Note that we assume throughout that <m>A</m> and <m>B</m> are symmetric:
            thus that <m>A^T=A</m> and <m>B^T=B</m>.
          </p>
          <p>
            <m>(\Rightarrow)</m>: suppose <m>AB</m> is symmetric.
            Then <m>(AB)^T=(AB)</m>.
            On the other hand, we have
            <me>
              (AB)^T=B^TA^T=BA
            </me>,
            where the last equality holds since <m>A</m> and <m>B</m> are symmetric.
          </p>
          <p>
            But then we have <m>AB=(AB)^T=BA</m>, showing <m>AB=BA</m>.
          </p>
          <p>
            <m>(\Leftarrow)</m>: suppose <m>AB=BA</m>.
            Then
            <md>
              <mrow>(AB)^T\amp =B^TA^T \amp \text{ (prop. of transp.) }</mrow>
              <mrow>\amp =BA \amp \text{ (\(A\),\(B\) are symmetric) }</mrow>
              <mrow>\amp =AB \amp \text{ (by assumption) } </mrow>
            </md>.
          </p>
          <p>
            This shows that <m>(AB)^T=AB</m>,
            which proves <m>AB</m> is symmetric.
          </p>
          <p>
            Having shown that
            <me>
              \text{ \(AB\) is symmetric } \Rightarrow AB=BA
            </me>,
          </p>
          <p>
            AND
            <me>
              AB=BA\Rightarrow \text{ \(AB\) is symmetric }
            </me>,
            we conclude that
            <me>
              \text{ \(AB\) is symmetric } \Leftrightarrow AB=BA
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each matrix below (i) decide whether it is lower triangular,
        upper triangular,
        diagonal or neither, and (ii) decide whether it is invertible.
        <ol>
          <li>
            <p>
              <m>\begin{bmatrix}4\amp 0\\ 1\amp 7 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}0\amp -3\\ 0\amp 0 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}4\amp 0\amp 0\\ 0\amp \frac{3}{5}\amp 0\\ 0\amp 0\amp -2 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}3\amp 0\amp 0\\ 3\amp 1\amp 0\\ 7\amp 0\amp 0 \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          a. Lower Triangular and Invertible
        </p>
        <p>
          b. Upper Triangular and Not Invertible
        </p>
        <p>
          c. Diagonal (Upper and Lower Triangular) and Invertible
        </p>
        <p>
          d. Lower Triangular and Not Invertible
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find the product by inspection:
        <me>
          \begin{bmatrix}2\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} \begin{bmatrix}4\amp -1\amp 3\\ 1\amp 2\amp 0\\ -5\amp 1\amp -2 \end{bmatrix} \begin{bmatrix}-3\amp 0\amp 0\\ 0\amp 5\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}
        </me>
        <solution>
          <p>
            Multiply the first row of the second matrix by 2, the second row by -1, and the third row by 4.
            Now we have:
            <me>
              \begin{bmatrix}8\amp -2\amp 6\\ -1\amp -1\amp 0\\ -20\amp 4\amp -8 \end{bmatrix} \begin{bmatrix}-3\amp 0\amp 0\\ 0\amp 5\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}
            </me>
          </p>
          <p>
            Now multiply the first column of the new matrix by -3, the second column by 5, and the third column by 2.
            Which gives the matrix:
            <me>
              \begin{bmatrix}-24\amp -10\amp 12\\ 3\amp -10\amp 0\\ 60\amp 20\amp -16 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find <m>A^2</m>, <m>A^{-2}</m>,
        and <m>A^{-k}</m> by inspection:
        <me>
          A = \begin{bmatrix}-6\amp 0\amp 0\\ 0\amp 3\amp 0\\ 0\amp 0\amp 5 \end{bmatrix}
        </me>
        <solution>
          <p>
            <m>A^2 = \begin{bmatrix}36\amp 0\amp 0\\ 0\amp 9\amp 0\\ 0\amp 0\amp 25 \end{bmatrix} , A^{-2} = \begin{bmatrix}1/36\amp 0\amp 0\\ 0\amp 1/9\amp 0\\ 0\amp 0\amp 1/25 \end{bmatrix} , A^{-k} = \begin{bmatrix}1/(-6)^k\amp 0\amp 0\\ 0\amp 1/(3)^k\amp 0\\ 0\amp 0\amp 1/(5)^k \end{bmatrix}</m>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find the diagonal entries of <m>AB</m> by inspection.
        <me>
          A = \begin{bmatrix}4\amp 0\amp 0\\ -2\amp 0\amp 0\\ -3\amp 0\amp 7 \end{bmatrix} , B = \begin{bmatrix}6\amp 0\amp 0\\ 1\amp 5\amp 0\\ 3\amp 2\amp 6 \end{bmatrix}
        </me>
        <solution>
          <p>
            The diagonal entries of a product of two lower triangular matrices are obtained simply by multiplying the corresponding diagonal entries.
            Thus the diagonal entries of the product
            (in order)
            are <m>4\cdot 6=24, 0\cdot 5=0, 7\cdot 6=42</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find all values of <m>x</m> for which A is invertible.
        <me>
          \begin{bmatrix}x - 1/2\amp 0\amp 0\\ x\amp x-1/3\amp 0\\ x^2\amp x^3\amp x+1/4 \end{bmatrix} \\
        </me>
        <solution>
          <p>
            Since matrix A is lower triangular,
            we can apply Theorem 1.7.1, which states that a triangular matrix is invertible if and only if its diagonal entries are all non zero.
            Thus <m>x \neq 1/2, 1/3, -1/4</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that if <m>A</m> is a symmetric
        <m>n\times n</m> matrix and <m>B</m> is any <m>n\times m</m> matrix,
        the the following products are symmetric:
        <m>B^TB</m>, <m>BB^T</m>, <m>B^TAB</m>.
        <solution>
          <p>
            We will use the general facts that:
            <m>(A^T)^T = A</m> and <m>(AB)^T = B^TA^T</m>.
            As usual, to show a matrix <m>C</m> is symmetric,
            we must show <m>C^T=C</m>.
            <md>
              <mrow>(B^TB)^T\amp =\amp B^T(B^T)^T</mrow>
              <mrow>\amp =\amp  B^TB</mrow>
            </md>
            <md>
              <mrow>(BB^T)^T\amp =\amp  (B^T)^TB^T</mrow>
              <mrow>\amp =\amp BB^T</mrow>
            </md>
            <md>
              <mrow>(B^TAB)^T\amp =\amp  B^TA^TB</mrow>
              <mrow>\amp =\amp  B^TAB \ \text{ (since \(A^T=A\)) }</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find all <m>3\times 3</m> diagonal matrices <m>A</m> such that
        <me>
          A^2 - 3A - 4I = 0
        </me>
        <solution>
          <p>
            <m>\begin{bmatrix}a^2\amp 0\amp 0\\ 0\amp b^2\amp 0\\ 0\amp 0\amp c^2 \end{bmatrix} - \begin{bmatrix}3a\amp 0\amp 0\\ 0\amp 3b\amp 0\\ 0\amp 0\amp 3c \end{bmatrix} - \begin{bmatrix}4\amp 0\amp 0\\ 0\amp 4\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} = \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}</m> This produces three equations,
            each with one unknown.
            <md>
              <mrow>a^2-3a-4\amp =\amp 0</mrow>
              <mrow>b^2-3b-4\amp =\amp 0</mrow>
              <mrow>c^2-3c-4\amp =\amp 0</mrow>
            </md>
          </p>
          <p>
            Solving for one will solve the other two.
          </p>
          <p>
            <m>a^2 - 3a - 4 = 0 \rightarrow (a-4)(a+1) = 0 \rightarrow a=4,-1</m>
          </p>
          <p>
            Thus <m>b = 4,-1</m> and <m>c=4,-1</m>.
            It is important to note that in matrix <m>A</m>,
            <m>a</m> need not be equal to <m>b</m>.
            So there are 8 solutions to <m>A</m>
          </p>
          <p>
            <m>\begin{bmatrix}4\amp 0\amp 0\\ 0\amp 4\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} : \begin{bmatrix}4\amp 0\amp 0\\ 0\amp 4\amp 0\\ 0\amp 0\amp -1 \end{bmatrix} : \begin{bmatrix}4\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} : \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp 4\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} \begin{bmatrix}4\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix} : \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp 4\amp 0\\ 0\amp 0\amp -1 \end{bmatrix} : \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp 4 \end{bmatrix} : \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}</m>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove: If <m>A^TA =A</m>, then <m>A</m> is symmetric and <m>A=A^2</m>.
        <solution>
          <p>
            First we prove <m>A^TA</m> is symmetric by showing <m>(A^TA)^T=A^TA</m>:
            <md>
              <mrow>(A^TA)^T\amp =A^T(A^T)^T \amp ((AB)^T=B^TA^T)</mrow>
              <mrow>\amp =A^TA \amp (\text{ since \((A^T)^T=A\) } )</mrow>
            </md>
          </p>
          <p>
            Thus <m>A^TA</m> is symmetric.
            Since we assume <m>A^TA=A</m>,
            it follows that <m>A</m> is symmetric; thus <m>A^T=A</m>.
            This means
            <md>
              <mrow>A\amp =A^TA\amp (\text{ since \(A^TA=A\) } )</mrow>
              <mrow>\amp =AA \amp (\text{ since \(A^T=A\) } )</mrow>
              <mrow>\amp =A^2</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{2.1-2.3: the determinant}
  </p>
  <ol>
    <li>
      <p>
        Let <m>A</m> be <m>n\times n</m>.
        Let <m>c</m> be any constant.
        State and prove a formula relating <m>\det(cA)</m> with <m>\det(A)</m>.
        (Look at the <m>n=2</m> case to see what this should be.)
        <solution>
          <p>
            WARNING: <m>\det(cA)\ne c\det(A)</m>!!!
          </p>
          <p>
            We claim <m>\det(cA)=c^n\det(A)</m>,
            where <m>A</m> is <m>n\times n</m>.
          </p>
          <p>
            Proof: first observe that <m>cA=(cI_n)A=\begin{bmatrix}c\amp 0\amp \dots\amp 0\\ 0\amp c\amp 0\amp \dots\\ \vdots \amp \\ 0\amp 0\amp \dots\amp c \end{bmatrix} A</m>,
            using the result of <xref ref="ex_diag">Exercise</xref> of the previous section.
            Then
            <md>
              <mrow>\det(cA)\amp =\det((cI_n)A)</mrow>
              <mrow>\amp =\det(cI_n)\det(A)</mrow>
              <mrow>\amp =c^n\det(A)</mrow>
            </md>,
            where the last equality follows since <m>cI_n</m> is a diagonal matrix with <m>c</m>'s along the diagonal.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A,B</m> be <m>n\times n</m>,
        and suppose <m>B</m> is invertible.
        Prove the following:
      </p>
      <ol>
        <li>
          <p>
            <m>\ds \det(B^{-1})=\frac{1}{\det(B)}</m>.
          </p>
        </li>
        <li>
          <p>
            <m>\det(B^{-1}AB)=\det(A)</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) Since <m>BB^{-1}=I</m>, we have
          <md>
            <mrow>1\amp =\det(I)</mrow>
            <mrow>\amp =\det(BB^{-1})</mrow>
            <mrow>\amp =\det(B)\det(B^{-1})</mrow>
          </md>
        </p>
        <p>
          Dividing both sides by <m>\det(B)</m> (remember <m>\det(B)</m> is just a number!) we get <m>\det(B^{-1})=\frac{1}{\det(B)}</m>.
        </p>
        <p>
          (b) Start with the LHS and compute:
          <md>
            <mrow>\det(B^{-1}AB)\amp =\det(B^{-1})\det(A)\det(B) \amp \text{ (det. is multiplicative) }</mrow>
            <mrow>\amp =\det(A)\det(B^{-1})\det(B) \amp \text{ (real mult. is commutative) }</mrow>
            <mrow>\amp =\det(A)\frac{1}{\det(B)}\det(B) \amp \text{ (prev. problem) }</mrow>
            <mrow>\amp =\det(A)</mrow>
          </md>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A</m> be <m>n\times n</m>,
        and suppose two rows of <m>A</m> are identical.
        Show that <m>\det(A)=0</m>.
        Now prove the same result for a matrix with two identical columns.
        <solution>
          <p>
            Let the rows of <m>A</m> be <m>\boldr_k</m>,
            and suppose <m>\boldr_i=\boldr_j</m>.
          </p>
          <p>
            Let <m>E</m> be the elementary matrix which replaces
            <m>\boldr_i</m> with <m>\boldr_i-\boldr_j</m>.
            Then <m>EA=B</m>, where the <m>i</m>-th row of <m>B</m> is 0
            (since <m>\boldr_i=\boldr_j</m> in <m>A</m>).
            This clearly implies <m>\det(B)=0</m>.
            Furthermore, we know <m>\det(E)=1</m>.
            Though this is already a proven result,
            it is easy to see why it is true.
            Such an elementary matrix is triangular
            (maybe upper or lower depending),
            with 1's along the diagonal;
            thus its determinant, being the product of the diagonal elements, is 1.
          </p>
          <p>
            Now we compute
            <md>
              <mrow>0\amp =\det(B)</mrow>
              <mrow>\amp =\det(EA)</mrow>
              <mrow>\amp =\det(E)\det(A)</mrow>
              <mrow>\amp =\det(A)</mrow>
            </md>,
            showing <m>\det(A)=0</m>.
          </p>
          <p>
            Now suppose <m>A</m> has two identical columns.
            Then <m>A^T</m> has two identical rows, and we have
            <me>
              \det(A)=\det(A^T)=0
            </me>,
            where the last equality follows from the previous result.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Suppose <m>A</m> is <m>3\times 3</m> and satisfies:
        <me>
          \underset{r_1-r_2}{E}\ \underset{2r_2}{E}\ \underset{r_1\leftrightarrow r_2}{E}A=\begin{bmatrix}1\amp 0\amp 0\\ 85\amp 2\amp 0\\ -18\amp 201\amp 3 \end{bmatrix}
        </me>.
        Find <m>\det(A)</m>.
        <solution>
          <p>
            We have
            <md>
              <mrow>\amp \det\left(\underset{r_1-r_2}{E}\right)\det\left(\underset{2r_2}{E}\right)\det\left(\underset{r_1\leftrightarrow r_2}{E}\right)\det(A)=\det\left(\begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
              <mrow>85\amp 2\amp 0</mrow>
              <mrow>-18\amp 201\amp 3 \end{bmatrix}\right)</mrow>
              <mrow>\amp \Rightarrow 1(2)(-1)\det(A)=1(2)(3)</mrow>
              <mrow>\amp \Rightarrow -2\det(A)=6</mrow>
              <mrow>\amp \Rightarrow \det(A)=-3</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        State and prove a formula for <m>\det(A)</m> where
        <me>
          A=\begin{bmatrix}a\amp b\amp b\dots \\ b\amp a\amp b\amp \dots\\ \vdots \amp  \amp \vdots \\ b\amp b\amp \dots \amp a \end{bmatrix}
        </me>,
        the matrix with <m>a</m>'s along the diagonal and <m>b</m>'s everywhere else.
        Look at the <m>n=2</m> and <m>n=3</m> cases first.
        Your proof of the formula in the general case will involve row operations.
        <solution>
          <p>
            We perform the following row operations:
            replace <m>r_1</m> with <m>r_1-r_2</m>;
            replace <m>r_2</m> with <m>r_2-r_3</m>;
            replace <m>r_3</m> with <m>r_3-r_4</m>,
            etc.These are row operations of the third type (<m>r_i+cr_j</m>),
            and thus do NOT change the determinant.
            (Recall of course that the other types of row operatins DO change the determinant.)
          </p>
          <p>
            These operations result in the matrix
            <me>
              B= \begin{bmatrix}(a-b)\amp -(a-b)\amp 0\amp 0\amp \dots\\ 0\amp (a-b)\amp -(a-b)\amp 0\amp \dots\\ \vdots\\ 0\amp 0\amp \amp \dots\amp  (a-b)\amp -(a-b)\\ b\amp b\amp b\amp \dots \amp b \amp a \end{bmatrix}
            </me>
          </p>
          <p>
            Now replace column <m>\boldc_2</m> with <m>\boldc_2+\boldc_1</m>,
            column <m>\boldc_3</m> with <m>\boldc_3+\boldc_2, \dots</m>,
            column <m>\boldc_n</m> with <m>\boldc_n+\boldc_{n-1}</m>.
            These operations, which also do not change the determinant,
            result in the lower triangular matrix
            <me>
              C=\begin{bmatrix}(a-b)\amp 0\amp 0\amp 0\amp \dots\\ 0\amp (a-b)\amp 0\amp 0\amp \dots\\ 0\amp 0\amp (a-b)\amp 0\amp \dots \vdots\\ 0\amp 0\amp \amp \dots\amp  (a-b)\amp 0\\ b\amp 2b\amp 3b\amp \dots \amp (n-1)b \amp a+(n-1)b \end{bmatrix}
            </me>
          </p>
          <p>
            We conclude that <m>\det(A)=\det(B)=\det(C)=(a-b)^{n-1}(a+(n-1)b)</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Given <m>r_1, r_2, \dots,
        r_n\in \R</m> the <em>Vandermonde</em> matrix is defined as
        <me>
          A_{r_1,r_2,\dots, r_n}=\begin{bmatrix}1\amp 1\amp \cdots \amp 1\\ r_1\amp r_2\amp \cdots \amp  r_n\\ r_1^2\amp r_2^2\amp \cdots \amp r_n^2\\ \vdots \amp  \vdots \amp \cdots \amp \vdots\\ r_1^{n-1}\amp r_2^{n-1}\amp \cdots \amp r_n^{n-1} \end{bmatrix} =[r_j^{i-1}]_{1\leq i,j\leq n}
        </me>.
        Prove: <m>\ds\det (A_{r_1,r_2,\dots,
        r_n})=\prod_{1\leq i\lt j\leq n}(r_j-r_i)</m>.
        <term>Hint</term>.
        Do a proof by induction.
        For the induction step,
        first clear out the entries under the 1 of the first column,
        working from the bottom up.
        You will also use linearity in columns.
        <solution>
          <p>
            This is a well known result.
            Look it up or else follow my instructions.
            The sequence of row operations
            (working from bottom up)
            is <m>\boldr_n-r_1\boldr_{n-1}</m>,
            <m>\boldr_{n-1}-r_1\boldr_{n-2},\dots</m>,
            <m>\boldr_2-r_1\boldr_1</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let
        <me>
          A = \begin{bmatrix}2\amp 3\amp -1\amp 1\\ -3\amp 2\amp 0\amp 3\\ 3\amp -2\amp 1\amp 0\\ 3\amp -2\amp 1\amp 4 \end{bmatrix}
        </me>
        <ol>
          <li>
            <p>
              Find <m>M_{32}</m> and <m>C_{32}</m>.
            </p>
          </li>
          <li>
            <p>
              Find <m>M_{44}</m> and <m>C_{44}</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Delete row 3 and column 2 from A to and compute the determinant:
          <md>
            \begin{vmatrix}[rrr] 2\amp -1\amp 1\\ -3\amp 0\amp 3\\ 3\amp 1\amp 4 \end{vmatrix} =2(-3)+1(-21)+-3=-30.
          </md>
        </p>
        <p>
          Thus <m>M_{32} = -30</m> and <m>C_{32} = (-1)^{3+2}M_{32} = 30</m>
        </p>
        <p>
          (b) Delete the 4th row and column and compute the determinant:
          <md>
            \begin{vmatrix}[rrr] 2\amp 3\amp -1\\ -3\amp 2\amp 0\\ 3\amp -2\amp 1 \end{vmatrix} =2(2)-3(-3)+(-1)(0)=13.
          </md>
        </p>
        <p>
          Thus <m>M_{44} = 13</m> and <m>C_{44} = (-1)^{4+4}M_{44} = 13</m>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let
        <me>
          A = \begin{bmatrix}-1\amp 1\amp 2\\ 3\amp 0\amp -5\\ 1\amp 7\amp 2 \end{bmatrix}
        </me>
        <ol>
          <li>
            <p>
              Compute <m>\det(A)</m> by expanding along the second row.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\det(A)</m> by expanding along the third column.
            </p>
          </li>
        </ol>
      </p>
    </li>
    <li>
      <p>
        Compute
        <me>
          \det \begin{bmatrix}3\amp 3\amp 0\amp 5\\ 2\amp 2\amp 0\amp -2\\ 4\amp 1\amp -3\amp 0\\ 2\amp 10\amp 3\amp 2 \end{bmatrix}
        </me>
        <solution>
          <p>
            Since column three has two zeros, lets expand by it:
            <md>
              \det(A) = -3 \begin{vmatrix}[rrr] 3\amp 3\amp 5\\ 2\amp 2\amp -2\\ 2\amp 10\amp 2 \end{vmatrix} -3 \begin{vmatrix}[rrr] 3\amp 3\amp 5\\ 2\amp 2\amp -2\\ 4\amp 1\amp 0 \end{vmatrix}
            </md>
          </p>
          <p>
            Now we compute these <m>3\times 3</m> deterimants:
            <md>
              <mrow>\det(A) \amp =\amp -3(3(24)-3(8)+5(16))-3(4(-16)-1(-16))</mrow>
              <mrow>\amp =\amp -3(8\cdot 16)-3(-3\cdot 16)</mrow>
              <mrow>\amp =\amp -15\cdot 16=-240</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute
        <me>
          \det \begin{bmatrix}4\amp 0\amp 0\amp 1\amp 0\\ 3\amp 3\amp 3\amp -1\amp 0\\ 1\amp 2\amp 4\amp 2\amp 3\\ 9\amp 4\amp 6\amp 2\amp 3\\ 2\amp 2\amp 4\amp 2\amp 3 \end{bmatrix}
        </me>
        <solution>
          <p>
            Since row one has three zeros, lets expand by row one.
            <md>
              \det(A) = 4 \begin{vmatrix}[rrrr] 3\amp 3\amp -1\amp 0\\ 2\amp 4\amp 2\amp 3\\ 4\amp 6\amp 2\amp 3\\ 2\amp 4\amp 2\amp 3 \end{vmatrix} -1 \begin{vmatrix}[rrrr] 3\amp 3\amp 3\amp 0\\ 1\amp 2\amp 4\amp 3\\ 9\amp 4\amp 6\amp 3\\ 2\amp 2\amp 4\amp 3 \end{vmatrix}
            </md>
          </p>
          <p>
            Expanding by the 4th columns gives:
            <md>
              <mrow>\amp =\amp  4\left(3 \begin{vmatrix}[rrr] 3\amp 3\amp -1</mrow>
              <mrow>4\amp 6\amp 2</mrow>
              <mrow>2\amp 4\amp 2 \end{vmatrix} -3 \begin{vmatrix}[rrr] 3\amp 3\amp -1</mrow>
              <mrow>2\amp 4\amp 2</mrow>
              <mrow>2\amp 4\amp 2 \end{vmatrix} +3 \begin{vmatrix}[rrr] 3\amp 3\amp -1</mrow>
              <mrow>2\amp 4\amp 2</mrow>
              <mrow>4\amp 6\amp 2 \end{vmatrix} \right)</mrow>
              <mrow>\amp -1 \left(3 \begin{vmatrix}[rrr] 3\amp 3\amp 3</mrow>
              <mrow>9\amp 4\amp 6</mrow>
              <mrow>2\amp 2\amp 4 \end{vmatrix} -3 \begin{vmatrix}[rrr] 3\amp 3\amp 3</mrow>
              <mrow>1\amp 2\amp 4</mrow>
              <mrow>2\amp 2\amp 4 \end{vmatrix} +3 \begin{vmatrix}[rrr] 3\amp 3\amp 3</mrow>
              <mrow>1\amp 2\amp 4</mrow>
              <mrow>9\amp 4\amp 6 \end{vmatrix} \right)</mrow>
              <mrow>\amp =\amp 12((32-36)-(28-28)+(36-32))-3((138-168)-(54-48)+(156-120))</mrow>
              <mrow>\amp =\amp 12(-4 +4)-3(-30-6+36) = 0</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_detcolumnops">
      <p>
        We know now how elementary row operations affect the determinant.
        State and prove analogous statements about how elementary
        <em>column operations</em>
        affect the determinant of a matrix.
        <solution>
          <p>
            Multiplying a matrix <m>A</m> on the <em>right</em>
            by an elementary matrix <m>E</m> has the effect of performing the corresponding elementary
            <em>column operation</em>.
            In more detail:
          </p>
          <p>
            <m>AE_{c\boldr_i}</m> scales the <m>i</m>-th column of <m>A</m> by <m>c</m>;
          </p>
          <p>
            <m>AE_{\boldr_i\leftrightarrow \boldr_j}</m> swaps columns <m>i</m> and <m>j</m>;
          </p>
          <p>
            <m>AE_{\boldr_i+c\boldr_j}</m> replaces column <m>i</m> with column <m>i</m> plus <m>c</m> times column <m>j</m>.
          </p>
          <p>
            To see what happens to the determinant,
            we use the multiplicative property:
          </p>
          <p>
            <m>\det(AE_{c\boldr_i})=\det A\det E_{c\boldr_i}=c\det A</m>;
          </p>
          <p>
            <m>\det(AE_{\boldr_i\leftrightarrow \boldr_j})=\det A\det E_{\boldr_i\leftrightarrow \boldr_j}=-\det A</m>;
          </p>
          <p>
            <m>\det(AE_{\boldr_i+c\boldr_j})=\det A\det E_{\boldr_i+c\boldr_j}=\det A</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let
        <me>
          A=\begin{bmatrix}-4\amp 1\amp 1\amp 1\amp 1\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix}
        </me>
        Show that <m>\det(A) = 0</m> without directly computing the determinant.
        <solution>
          <p>
            First solution: notice that each row sums to 0.
            Using the column method of matrix multiplication, this implies that
            <me>
              A\begin{bmatrix}1\\ 1\\ 1\\ 1\\ 1 \end{bmatrix} =\underset{5\times 1}{\boldzero}
            </me>
          </p>
          <p>
            Thus <m>A\boldx=\boldzero</m> has a nontrivial solution:
            namey, <m>\boldx=(1,1,1,1,1)</m>.
            It follows from the invertibility theorem that <m>A</m> is not invertible.
            Thus <m>\det A=0</m>.
          </p>
          <p>
            Second solution: each column also sums to 0.
            This means if we apply successive row operations,
            replacing <m>\boldr_1</m> with <m>\boldr_1+\boldr_2</m>,
            then replacing the resulting
            <m>\boldr_1</m> with <m>\boldr_1+\boldr_2</m>, and so on,
            that we will eventually end up with a row of zeros:
            <md>
              <mrow>\begin{bmatrix}-4\amp 1\amp 1\amp 1\amp 1\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix} \amp \xrightarrow[]{r_1 + r_2}\amp \begin{bmatrix}-3\amp -3\amp 2\amp 2\amp 2\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{r_1 + r_3}\amp \begin{bmatrix}-2\amp -2\amp -2\amp 3\amp 3\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{r_1 + r_4}\amp \begin{bmatrix}-1\amp -1\amp -1\amp -1\amp 4\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{r_1 + r_5}\amp \begin{bmatrix}0\amp 0\amp 0\amp 0\amp 0\\ 1\amp -4\amp 1\amp 1\amp 1\\ 1\amp 1\amp -4\amp 1\amp 1\\ 1\amp 1\amp 1\amp -4\amp 1\\ 1\amp 1\amp 1\amp 1\amp -4 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Let <m>B</m> be the matrix we row reduced <m>A</m> to.
            Since all of our row operations were of the row addition type,
            we have <m>\det A=\det B</m>.
            Since <m>B</m> has a row of zeros, <m>\det B=0</m>.
            It follows that <m>\det A=\det B=0</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use determinants to decide whether the given matrix is invertible.
        <me>
          A = \begin{bmatrix}2\amp 0\amp 3\\ 0\amp 3\amp 2\\ -2\amp 0\amp -4 \end{bmatrix}
        </me>
        <solution>
          <p>
            Expanding by the first column gives:
            <me>
              \det(A) = 2 \begin{bmatrix}3\amp 2\\ 0\amp -4 \end{bmatrix} -2 \begin{bmatrix}0\amp 3\\ 3\amp 2 \end{bmatrix} =-24 + 18 =-6
            </me>
          </p>
          <p>
            Since the determinant is non-zero, the matrix is invertible.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use determinants to decide whether the given matrix is invertible.
        <me>
          A = \begin{bmatrix}\sqrt{2}\amp -\sqrt{7}\amp 0\\ 3\sqrt{2}\amp -3\sqrt{7}\amp 0\\ 5\amp -9\amp 0 \end{bmatrix}
        </me>
        <solution>
          <p>
            Expanding by the third column gives:
            <me>
              \det(A) = 0
            </me>
          </p>
          <p>
            Thus <m>A</m> is not invertible.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find the values of <m>k</m> for which the matrix <m>A</m> is invertible.
        <me>
          \begin{bmatrix}1\amp 2\amp 0\\ k\amp 1\amp k\\ 0\amp 2\amp 1 \end{bmatrix}
        </me>
        <solution>
          <p>
            Expanding by the first row gives:
            <me>
              \det(A) = \begin{bmatrix}1\amp k\\ 2\amp 1 \end{bmatrix} -k \begin{bmatrix}2\amp 0\\ 2\amp 1 \end{bmatrix} =1-2k-2k = 1-4k
            </me>.
          </p>
          <p>
            Now we set <m>1-4k = 0</m>.
            Solving for k gives: <m>k = \frac{1}{2}</m>.
            Thus matrix <m>A</m> is invertible for all <m>k\neq \frac{1}{2}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use the adjoint formula to compute the inverse of
        <me>
          A=\begin{bmatrix}1\amp 2\amp -3\\ 1\amp 1\amp 1\\ 1\amp 0\amp -1 \end{bmatrix}
        </me>.
        <solution>
          <p>
            First compute <m>\det A=1(-1)-2(-2)-3(-1)=6</m>.
            Next compute
            <me>
              \adjoint A=([C_{ij}])^T=\left( \begin{bmatrix}-1\amp 2\amp -1\\ 2\amp 2\amp 2\\ 5\amp -4\amp -1 \end{bmatrix} \right)^T =\begin{bmatrix}-1\amp 2\amp 5\\ 2\amp 2\amp -4\\ -1\amp 2\amp -1 \end{bmatrix}
            </me>.
          </p>
          <p>
            We conclude that
            <me>
              A^{-1}=\frac{1}{6}\begin{bmatrix}-1\amp 2\amp 5\\ 2\amp 2\amp -4\\ -1\amp 2\amp -1 \end{bmatrix}
            </me>.
          </p>
          <p>
            Verify this yourself by checking <m>AA^{-1}=I_3</m> !!
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        In each part,
        find the determinant given that <m>A</m> is a 4x4 matrix for which <m>\det(A) = -2</m>.
        <ol>
          <li>
            <p>
              <m>\det(-A)</m>
            </p>
          </li>
          <li>
            <p>
              <m>\det(A^{-1})</m>
            </p>
          </li>
          <li>
            <p>
              <m>\det(2A^T)</m>
            </p>
          </li>
          <li>
            <p>
              <m>\det(A^3)</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <me>
            \det(-A)=\det(-1 \cdot A) = (-1)^4\det(A) = \det(A) = -2
          </me>
        </p>
        <p>
          (b)
          <me>
            \det(-A) = \frac{1}{\det(A)} = \frac{1}{-2} = -\frac{1}{2}
          </me>
        </p>
        <p>
          (c)
          <me>
            \det(2A^T)=2^4det(A)=2^4(-2)=-32
          </me>
        </p>
        <p>
          (d)
          <me>
            \det(A^3)=\det(AAA)=\det(A)\det(A)\det(A)=-2\cdot -2\cdot -2=-8
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Prove that a square matrix <m>A</m> is invertible if and only if <m>A^TA</m> is invertible.
        <solution>
          <p>
            <m>(\Rightarrow)</m> Assume that <m>A</m> is invertible:
          </p>
          <p>
            Since <m>A</m> is square, we know that <m>\det(A^T) = \det(A)</m>.
            Since <m>A</m> is invertible, <m>\det(A)\neq 0</m>.
            Now:
            <me>
              \det(A^TA)=\det(A^T)\det(A)=\det(A)\det(A)\neq 0
            </me>
          </p>
          <p>
            Thus <m>A^TA</m> is invertible.
          </p>
          <p>
            <m>(\Leftarrow)</m> Assume that <m>A^TA</m> is invertible:
          </p>
          <p>
            Since <m>A^TA</m> is invertible,
            we know that <m>\det(A^TA) \neq 0</m>.
            Thus
            <me>
              0\neq \det(A^TA) = \det(A^T)\det(A) = \det(A)\det(A)
            </me>
            which makes <m>\det(A) \neq 0</m>.
            So matrix <m>A</m> is invertible.
          </p>
          <p>
            Alternatively, we have
            <md>
              <mrow>A \text{ inv. } \amp \Leftrightarrow A \text{ and }  A^T \text{ inv. }  \amp (A \text{ inv. } \Leftrightarrow A^T \text{ inv. } )</mrow>
              <mrow>\amp \Leftrightarrow A^TA \text{ inv. } \amp (A, B \text{ inv. } \Leftrightarrow A\cdot B \text{ inv. } )</mrow>
            </md>
          </p>
          <p>
            That paranthetical justification of the first
            <m>\Leftrightarrow</m> was never explicity stated thus far;
            we only proved the direction <m>A \text{ invertible } \Rightarrow A^T \text{ invertible }</m>.
            However, we can prove the reverse direction using the first implication:
            <md>
              <mrow>A^T \text{ invertible } \amp \Rightarrow (A^T)^T \text{ invertible }  \amp (\text{ by first implication } )</mrow>
              <mrow>\amp \Rightarrow A \text{ invertible } \amp ((A^T)^T=A)</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>A</m> is a 4x4 matrix and <m>B</m> is obtained from <m>A</m> by interchanging the first two rows and then interchanging the last two rows,
              then <m>\det(A)=\det(B)</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a 3x3 matrix and <m>B</m> is obtained from <m>A</m> by multiplying the first column by 4 and multiplying the third column by <m>\frac{3}{4}</m>,
              then <m>\det(B)=3\det(A)</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a 3x3 matrix and <m>B</m> is obtained from <m>A</m> by adding 5 times the first row to each of the second and third rows,
              then <m>\det(B) = 25\det(A)</m>.
            </p>
          </li>
          <li>
            <p>
              If the sum of the second and fourth row vectors of a 6x6 matrix <m>A</m> is equal to the last row vector,
              then <m>\det(A) = 0</m>
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a 3x3 matrix, then <m>\det(2A) = 2\det(A)</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are square matrices of the same size and <m>A</m> is invertible,
              then <m>\det(A^{-1}BA)=\det(B)</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a square matrix and the linear system <m>Ax = 0</m> has multiple solutions for <m>x</m>,
              then <m>\det(A)=0</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>E</m> is an elementary matrix,
              then <m>Ex = 0</m> has only the trivial solution.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) True.
          The row operations and determinant theorem states that interchanging two rows of a matrix flips the sign on the determinant.
          Since we did the interchanging twice, the sign flips twice.
          Thus <m>\det(A)=\det(B)</m>.
        </p>
        <p>
          (b) True.
          We saw in <xref ref="ex_detcolumnops">Exercise</xref>
          that performing elementary column operations on a matrix has the same effect on the determinant as the corresponding row operation.
          Thus in this case we have <m>\det(B) =\frac{3}{4}\cdot 4 \det(A) = 3\det(A)</m>.
        </p>
        <p>
          (c) False.
          By the row operations and determinant theorem we know that multiplying one row and adding it to another row does not change the value of the determinant.
          Thus <m>\det(A)=\det(B)</m>.
        </p>
        <p>
          (d) True.
          By GE: Add <m>r_2</m> to <m>r_4</m> to replace <m>r_2</m>.
          Then subtract <m>r_6</m> from <m>r_2</m> to replace <m>r_6</m>.
          Now <m>r_6</m> is all zeros, thus <m>\det(A) = 0</m>.
        </p>
        <p>
          (e) False.
          In fact we have
          <me>
            \det(2A) = 2^3\det(A) = 8\det(A) \neq 2\det(A)
          </me>,
          as long as <m>\det A\ne 0</m>.
          For an explicit counterexample, take <m>A=I_3</m>.
          Then <m>\det(2I_3)=8</m> and <m>2\det I_n=2</m>.
        </p>
        <p>
          (f) True.
          We have
          <md>
            <mrow>\det(A^{-1}BA)\amp =\amp \det(A^{-1})\det(B)\det(A)</mrow>
            <mrow>\amp =\amp \frac{1}{\det(A)}\det(B)\det(A)</mrow>
            <mrow>\amp =\amp \det(B)</mrow>
          </md>
        </p>
        <p>
          (g) True.
          By the invertibility theorem know that the following statements are equivalent:
          <m>Ax = 0</m> has only the trivial solution
        </p>
        <p>
          <m>\det(A)\neq 0</m> By our assumptions,
          the first statement is false,
          thus the second statement is also false.
        </p>
        <p>
          (h) True.
          Elementary matrices are invertible.
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{3.1: vector spaces}
  </p>
  <ol>
    <li>
      <p>
        Let <m>V=\{A\in M_{nn}\colon A \text{ is invertible } \}</m>.
        Show that the usual matrix addition and scalar multiplication do NOT make <m>V</m> into a vector space.
        <solution>
          <p>
            We know already that matrix addition and scalar multiplication satisfy <em>most</em>
            of the vector space axioms:
            e.g., commutativity, distributivity, etc.
          </p>
          <p>
            However the operations fail to be well-defined defined on the given set!
            That is the sum of two matrices in <m>V</m> does not necessarily lie in <m>V</m>.
          </p>
          <p>
            As a concrete counterexample,
            take <m>I_n</m> and <m>-I_n</m>.
            Both are invertible matrices, and thus lie in <m>V</m>.
            However, <m>I_n+(-I_n)=\boldzero_n</m>, the zero matrix,
            which does not lie in <m>V</m>.
          </p>
          <p>
            Note also that scalar multiplication is also not well-defined on <m>V</m>:
            if we scale any invertible matrix <m>A</m> by the scalar <m>c=0</m>,
            then we get the zero matrix, which is not in <m>V</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\R^n</m>.
        Define vector addition on <m>V</m> to be the usual <m>n</m>-vector addition,
        but define scalar multiplication as
        <me>
          r(v_1,v_2,\dots ,v_n)=(r^2v_1,r^2v_2,\dots, r^2v_n)
        </me>.
        Show that <m>V</m> is NOT a vector space with these choices of vector addition and scalar multiplication.
        <solution>
          <p>
            This choice of vector addition and scalar multiplication fails the following distributivity axiom:
            <me>
              (r+s)\boldv=r\boldv+s\boldv
            </me>.
          </p>
          <p>
            Indeed take <m>r=s=1</m>, and <m>\boldv=(1,1,\dots,1)</m>.
            Then on the one hand we have
            <me>
              (r+s)\boldv=(1+1)\boldv=2(1,1,\dots,1)=(4,4,\dots, 4)
            </me>,
            using the given definition of scalar multiplication.
          </p>
          <p>
            On the other hand we have
            <me>
              r\boldv+s\boldv=1(1,1,\dots,1)+1(1,1,\dots,1)=(1,1,\dots,1)+(1,1,\dots,1)=(2,2,\dots,2)
            </me>,
            again using the given definitions of scalar mult. and vector addition.
          </p>
          <p>
            Thus we see that
            <me>
              (r+s)\boldv\ne r\boldv+s\boldv
            </me>
            for this choice of <m>r,s</m> and <m>\boldv</m>,
            and conclude the given set and operations do not form a vector space.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\{(x,y)\in\R^2\colon x>0, \ y\lt 0\}</m>:
        i.e., <m>V</m> is the set of pairs whose first component is positive,
        and whose second component is negative.
        The following operations satisfy the axioms of a vector space.
        <term>Vector addition</term>:
        <m>(x_1,\ y_1)+(x_2,\ y_2)=(x_1 x_2,\ -y_1 y_2)</m>. (In other words,
        vector addition takes the product of the first components,
        and takes the negative of the product of the second components. )
        <term>Scalar multiplication</term>:
        <m>r(x,\ y)=(x^r,\ -\val{y}^r)</m>.
        Below you are asked to verify some
        (but not all!)
        of the vector space axioms.
      </p>
      <ol>
        <li>
          <p>
            Explicitly identify the element of <m>V</m> that acts as the additive identity
            <m>\boldzero</m> with respect to the above vector operations.
            Show directly that your choice of <m>\boldzero</m> satisfies
            <m>\boldzero+\boldv=\boldv</m> for all <m>\boldv=(x,y)\in V</m>.
            Caution: at the very least,
            the <m>\boldzero</m> you provide should be an actual element of the given <m>V</m>.
            For example,
            note that <m>(0,0)</m> is not even an element of <m>V</m> !!
          </p>
        </li>
        <li>
          <p>
            Given <m>\boldv=(x,y)\in V</m>,
            give a formula in terms of <m>x</m> and <m>y</m> for its additive inverse <m>-\boldv</m> with respect to the above vector operations.
            Show directly that your formula for <m>-\boldv</m> satisfies <m>\boldv+ (-\boldv)=\boldzero</m>.
            Again, you must always use the definition of the vector space operations given above.
          </p>
        </li>
        <li>
          <p>
            Prove that Axiom (h) holds:
            i.e., that <m>1\boldv=\boldv</m> for all <m>\boldv\in V</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) The additive identity for the given vector space is <m>\boldzero=(1,-1)</m>.
        </p>
        <p>
          Proof:
          <md>
            <mrow>(1,-1)+(x,y)\amp =(1x,-(-1)y) \amp \text{ (def. of vect. add.) }</mrow>
            <mrow>\amp =(x,y)</mrow>
          </md>
        </p>
        <p>
          (b) Given element <m>\boldv=(x,y)\in V</m>,
          its additive inverse is <m>-\boldv=\left(\frac{1}{x},\frac{1}{y}\right)</m>.
        </p>
        <p>
          Proof:
          <md>
            <mrow>(x,y)+\left(\frac{1}{x},\frac{1}{y}\right)\amp =\left(x\ \frac{1}{x},-y\ \frac{1}{y}\right) \amp \text{ (def. of vect. add.) }</mrow>
            <mrow>\amp =(1,-1)</mrow>
            <mrow>\amp =\boldzero \amp \text{ (since \(\boldzero=(1,-1)\)) }</mrow>
          </md>
        </p>
        <p>
          (c) Proof:
          <md>
            <mrow>1(x,y)\amp =(x^1,-\val{y}^1) \amp \text{ (def. of scalar mult.) }</mrow>
            <mrow>\amp =(x,-\val{y})</mrow>
            <mrow>\amp =(x,y)</mrow>
          </md>.
        </p>
        <p>
          The last equality in the proof above follows from the definition of absolute value:
          for <m>y\lt 0</m>, we have by definition <m>\val{y}=-y</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=\{f(x)=1+ax\colon a\in\R\}</m> be the set of all linear polynomials <m>f(x)</m> with constant coefficient 1: i.e., <m>f(0)=1</m>.
        Define:
        <md>
          <mrow>\text{ Vector addition: }  \ (1+ax)+(1+bx)\amp := (1+ax)(1+bx)-abx^2</mrow>
          <mrow>\text{ Scalar multiplication: } \  r\cdot (1+ax):=1+rax</mrow>
        </md>
        Prove <m>V</m> along with these operations forms a vector space.
        Make explicit what the additive identity and inverses are.
        <solution>
          <p>
            If you expand and simplify the expression
            <m>(1+ax)(1+bx)-abx^2</m> you get <m>1+(a+b)x</m>.
            This shows that our vector addition is well-defined on the set <m>V</m>;
            it is clear that our scalar multiplication is also well-defined.
          </p>
          <p>
            An element <m>f(x)=1+ax</m> is completely determined by the coefficient <m>a</m>,
            so let's denote <m>f_a(x)=1+ax</m>.
            Our vector operations have a very simple description using this notation:
            <md>
              <mrow>f_a(x)+f_b(x)\amp =f_{a+b}(x)</mrow>
              <mrow>r\cdot f_a(x)\amp =f_{ra}(x)</mrow>
            </md>.
          </p>
          <p>
            In other words we see that our
            <q>fancy</q>
            vector operations boil down to the usual addition <m>a+b</m>,
            and multiplication <m>r\cdot a</m>.
            Once this it is clear,
            it is obvious that the operations satisfy the vector space axioms.
          </p>
          <p>
            The additive identity is <m>\boldzero=f_0(x)=1+0x=1</m>,
            the constant function equal to 1.
            Additive inverses are given by the formula <m>-f_a(x)=f_{-a}(x)</m>;
            i.e., the additive inverse of <m>f(x)=1+ax</m> is <m>g(x)=1-ax</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\R^2</m>.
        The following operations do <term>NOT</term>
        satisfy all of the vector space axioms.
        <term>Vector addition</term>:
        <m>(x_1,y_1)+(x_2,y_2)=(x_1+y_1,x_2+y_2)</m>.
        (The usual vector addition on <m>\R^2</m>.)
        <term>Scalar multiplciation</term>:
        <m>r(x,y)=(rx,0)</m>.
        <ol>
          <li>
            <p>
              Verify that these operations satisfy Axiom (f):
              i.e., that <m>(r+s)\boldv=r\boldv+s\boldv</m> for all
              <m>r,s\in\R</m> and all <m>(x,y)\in\R^2</m>.
            </p>
          </li>
          <li>
            <p>
              Find an axiom that these operations do <term>NOT</term> satisfy,
              and provide an explicit counterexample showing that the axiom fails.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) We have
          <md>
            <mrow>(r+s)(x,y)\amp =((r+s)x,0) \amp \text{ (by def. of scal. mult.) }</mrow>
            <mrow>\amp =(rx+sx,0) \amp \text{ (real number arith.) }</mrow>
            <mrow>\amp =(rx,0)+(sx,0) \amp \text{ (by def. of vector add.) }</mrow>
            <mrow>\amp =r(x,0)+s(x,0) \amp \text{ (by def. of scal. mult.) } </mrow>
          </md>.
        </p>
        <p>
          (b) The given operations do not satisfy axiom (h).
          By definition we have <m>1(x,y)=(x,0)</m>.
          This will not be equal to <m>(x,y)</m> unless <m>y=0</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Prove the vector space properties theorem. (Some of these parts have been proved in the lecture notes or elsewhere.
        No matter, recreate those proofs or produce new ones!)
      </p>
      <solution>
        <p>
          The theorem states that,
          given a vector space <m>V</m>,
          the properties below hold.
          <ol>
            <li>
              <p>
                <m>0\boldv=\boldzero</m> for all <m>\boldv\in V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>k\boldzero=\boldzero</m> for all <m>k\in \R</m>.
              </p>
            </li>
            <li>
              <p>
                <m>(-1)\boldv=-\boldv</m> for all <m>\boldv\in V</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>k\boldu=\boldzero</m>, then <m>k=0</m> or <m>\boldu=\boldzero</m>.
              </p>
            </li>
          </ol>
        </p>
        <p>
          We have proved (a) and (c) somewhere
          (lecture notes, video examples, class, etc.).
        </p>
        <p>
          It remains to prove (b) and (d).
          I may make use of properties (a) and (c),
          since these are already proven.
        </p>
        <p>
          Let's prove (b).
          From (a) we know that if <m>k=0</m>,
          then <m>k\boldzero=0\boldzero=\boldzero</m>.
          Suppose <m>k\ne 0</m>.
          Then we have
          <md>
            <mrow>k\boldzero\amp =k(\boldzero+\boldzero) \amp \text{ (\(\boldzero+\boldzero=\boldzero\)) }</mrow>
            <mrow>\amp =k\boldzero+k\boldzero \amp \text{ (dist. prop.) }</mrow>
          </md>
        </p>
        <p>
          Thus <m>k\boldzero=k\boldzero+k\boldzero</m>.
          Now add the additive identity
          <m>-k\boldzero</m> to both sides of this equation to
          <q>cancel</q>
          one of the <m>k\boldzero</m>.
          We are left with <m>\boldzero=k\boldzero</m>, as desired.
        </p>
        <p>
          Finally, we prove (d).
          Suppose <m>k\boldu=\boldzero</m> and that <m>k\ne 0</m>.
          Then <m>k</m> has a multiplicative inverse <m>\frac{1}{k}</m>.
          Then
          <md>
            <mrow>k\boldu=\boldzero\amp \Rightarrow \frac{1}{k}(k\boldu)=\frac{1}{k}\boldzero</mrow>
            <mrow>\amp \Rightarrow (\frac{1}{k}k)\boldu=\boldzero \amp \text{ (assoc. and property (b)) }</mrow>
            <mrow>\amp \Rightarrow 1\boldu=\boldzero</mrow>
            <mrow>\amp \Rightarrow \boldu=\boldzero \amp \text{ (since \(1\boldu=\boldu\)) } </mrow>
          </md>.
        </p>
        <p>
          We have shown that if <m>k\boldu=\boldzero</m> and <m>k\ne 0</m>,
          then <m>\boldu=\boldzero</m>.
          Logically, this is equivalent to showing if <m>k\boldu=\boldzero</m>,
          then <m>k=0</m> or <m>\boldu=0</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Prove that a vector space <m>V</m> is either trivial (i.e., <m>V=\{\boldzero\}</m>) or infinite.
        Hint: if <m>\boldv\ne\boldzero</m>,
        show that <m>r\boldv\ne s\boldv</m> for two distinct real numbers <m>r\ne s</m>.
        <solution>
          <p>
            Suppose <m>V</m> is nontrivial.
            Then there is a nonzero <m>\boldv\ne \boldzero</m> in <m>V</m>.
            Then for each <m>r\in\R</m> the vector <m>r\boldv</m> is also an element of <m>V</m>.
            To show that <m>V</m> infinite we need only show that all of these <m>r\boldv</m> are distinct;
            i.e., we must show that if <m>r\ne s</m>,
            then <m>r\boldv\ne s\boldv</m>.
          </p>
          <p>
            (Note: we cannot take this for granted,
            but must actually prove it.
            For all we know we could have
            <m>r\boldv=\boldv</m> for all <m>r\in\R</m>.)
          </p>
          <p>
            We prove the contrapositive.
            Suppose <m>r\boldv=s\boldv</m>.
            Then <m>r\boldv-s\boldv=\boldzero</m>.
            Then <m>(r-s)\boldv=\boldzero</m>.
            I now claim that since <m>\boldv\ne\boldzero</m>,
            we must have <m>r-s=0</m>, and hence <m>r=s</m>.
          </p>
          <p>
            Indeed suppose <m>a\boldv=\boldzero</m> and <m>a\ne 0</m>.
            Since <m>a\ne 0</m>,
            we can multiply both sides of this equation by <m>\frac{1}{a}</m>:
            <md>
              <mrow>\frac{1}{a}(a\boldv)\amp =\frac{1}{a}\boldzero</mrow>
              <mrow>(\frac{1}{a}\cdot a)\boldv\amp =\boldzero \amp \text{ (assoc. of scalar mult; \(c\boldzero=\boldzero\)) }</mrow>
              <mrow>1\boldv\amp =\boldzero</mrow>
              <mrow>\boldv\amp =\boldzero \amp \text{ (Axiom: \(1\boldv=\boldv\)) }</mrow>
            </md>
          </p>
          <p>
            That concludes our
            (longer than expected)
            proof that if <m>r\ne s</m>,
            then <m>r\boldv\ne s\boldv</m>,
            which proves that <m>V</m> is infinite.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V</m> be a vector space.
        Show that for all <m>\boldv\in V</m> there is a unique additive inverse for <m>\boldv</m>.
        That is, show if <m>\boldv+\boldw=\boldzero</m>,
        then <m>\boldw=-\boldv</m>.
        <solution>
          <p>
            Fix <m>\boldv</m> and suppose <m>\boldw</m> also satisfies <m>\boldv+\boldw=\boldzero</m>.
            Then
            <me>
              \boldv+-\boldv=\boldzero=\boldv+\boldw
            </me>.
          </p>
          <p>
            Then we have
            <md>
              <mrow>\boldv+(-\boldv)\amp =\boldv+\boldw</mrow>
              <mrow>-\boldv+\boldv+(-\boldv)\amp =-\boldv+\boldv+\boldw</mrow>
              <mrow>\boldzero+(-\boldv)\amp =\boldzero+\boldw</mrow>
              <mrow>-\boldv\amp =\boldw</mrow>
            </md>,
            as desired.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V</m> be a vector space.
        Prove, using only the axioms of a vector space:
        if <m>\boldu + \boldw = \boldv + \boldw</m>,
        then <m>\boldu = \boldv</m>.
        <solution>
          <md>
            <mrow>\boldu + \boldw = \boldv + \boldw\amp \text{ (Hypothesis) }</mrow>
            <mrow>(\boldu + \boldw) + (-\boldw) = (\boldv + \boldw) + (-\boldw) \amp \text{ (Add -w to both sides) }</mrow>
            <mrow>\boldu + [\boldw + (-\boldw)] = \boldv + [\boldw + (-\boldw)] \amp \text{ (Axiom 3) }</mrow>
            <mrow>\boldu + \bold0 = \boldv + \bold0 \amp \text{ (Axiom 5) }</mrow>
            <mrow>\boldu = \boldv \amp \text{ (Axiom 4) }</mrow>
          </md>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{3.2: linear transformations}
  </p>
  <ol>
    \begin{samepage}
    <li>
      <p>
        For each of the following functions <m>T</m>,
        decide whether <m>T</m> is linear.
        If yes, provide a proof; if no,
        given an explicit example that violates one of the axioms.
      </p>
      <ol>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow M_{nn}</m>, <m>T(A)=A^2</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow \R</m>, <m>T(A)=\tr A</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow \R</m>, <m>T(A)=\det A</m>.
          </p>
        </li>
        <li>
          <p>
            <m>T\colon F((-\infty, \infty))\rightarrow F((-\infty, \infty))</m>,
            <m>T(f)=1+f</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon F((-\infty, \infty))\rightarrow F((-\infty, \infty))</m>,
            <m>T(f(x))=f(x+3)</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon\R^3\rightarrow \R^2</m>, <m>T(x,y,z)=(xy,yz)</m>.
          </p>
        </li>
      </ol>
      \end{samepage}
      <solution>
        <p>
          (a) Nonlinear.
          Take <m>A=I_2</m>.
          Then <m>T(2A)=4I\ne 2T(A)=2I</m>.
        </p>
        <p>
          (b) Linear.
          Easy proof.
        </p>
        <p>
          (c) Nonlinear.
          Let <m>A=I_2</m>.
          Then <m>\det(2A)=4\ne 2\det(A)=2</m>.
        </p>
        <p>
          (d) Nonlinear.
          Notice that <m>T(\boldzero)=1\ne\boldzero</m>;
          yet any linear transformation sends the zero vector to the zero vector.
        </p>
        <p>
          (e) Linear.
          <md>
            <mrow>T(cf+dg)\amp =(cf+g)(x+3)\amp \text{ (by def) }</mrow>
            <mrow>\amp =cf(x+3)+dg(x+3)\amp \text{ (function arith.) }</mrow>
            <mrow>\amp =cT(f)+dT(g) \amp \text{ (by def) }</mrow>
          </md>
        </p>
        <p>
          (f) Nonlinear.
          <m>T(2(1,1,1))=T(2,2,2)=(4,4)</m> and <m>2T(1,1,1)=2(1,1)=(2,2)</m>,
          so <m>T</m> does not respect scalar multiplication.
          (Nor does it respect vector addition.)
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> by <m>T(f)=g</m>,
        where <m>g(x)=f(x)+f(-x)</m>.
        Show that <m>T</m> is a linear transformation.
        <solution>
          <p>
            We show <m>T</m> is linear by showing <m>T(cf+dg)=cT(f)+dT(g)</m> for any
            <m>c,d\in\R</m> and any <m>f,g\in C^\infty(\R)</m>.
            <md>
              <mrow>T(cf+dg)\amp =(cf+dg)(x)+(cf+dg)(-x) \amp \text{ (by def) }</mrow>
              <mrow>\amp =c(f(x)+f(-x))+d(g(x)+g(-x)) \amp \text{ (arith) }</mrow>
              <mrow>\amp =cT(f)+dT(g) \amp \text{ (by def) }</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_linereflection">
      <p>
        <em>Reflections in <m>\R^2</m></em>.
        Given a fixed angle <m>\alpha</m>,
        <m>0\leq \alpha\lt \pi</m>,
        let <m>\ell_{\alpha}</m> be the line through the origin that makes an angle of <m>\alpha</m> with the positive <m>x</m>-axis.
        We define <m>T_{\alpha}\colon\R^2\rightarrow \R^2</m> as <m>T_\alpha(P)=\text{ the reflection \(P'\) of \(P\) through \(\ell_\alpha\) }</m> In more detail,
        given point <m>P\in\R^2</m>,
        let <m>\ell'</m> be the line passing through <m>P</m> that is perpendicular to <m>\ell_{\alpha}</m>,
        and let <m>Q</m> be the intersection of <m>\ell</m> and <m>\ell_\alpha</m>.
        If <m>P</m> already lies on <m>\ell_{\alpha}</m>,
        then <m>T_\alpha(P)=P</m>;
        otherwise <m>T_\alpha(P)</m> is the <em>other</em>
        point <m>P'</m> lying on <m>\ell</m>,
        whose distance to <m>Q</m> is equal to the distance between <m>P</m> and <m>Q</m>.
        <ol>
          <li>
            <p>
              Give a geometric argument (i.e., draw a picture) that strongly suggests <m>T_\alpha</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Now prove <m>T_\alpha</m> is linear by finding a
              <m>2\times 2</m> matrix <m>A</m> such that <m>T_\alpha=T_A</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) As the diagram below illustrates,
          reflection preserves sums of vectors and scalar multiples of vectors.
          This is a result of the fact that reflection preserves angles between position vectors.
        </p>
        <p>
          (b) As suggestive as the diagram is,
          it falls somewhat short of a proof.
          We will find the matrix <m>\underset{2\times 2}{A}</m> such that <m>T_{\alpha}=T_A</m>.
        </p>
        <p>
          It is easiest to see what reflection does by thinking in terms of polar coordinates.
          Recall: any point <m>P=(x,y)</m> can be expressed in the form <m>(r\cos\theta,
          r\sin\theta)</m>.
          The values <m>r</m> and <m>\theta</m> are called polar coordinates of the vector.
          In general there are many choices for <m>r</m> and <m>\theta</m> for any given vector <m>(x,y)</m>.
          We will stipulate that <m>\boldr=\sqrt{x^2+y^2}</m>
          (the length of the vector),
          and that <m>\theta</m> is the measure of any oriented angle that <em>starts from</em>
          the positive <m>x</m>-axis and <em>ends at</em>
          <m>\overrightarrow{OP}</m>.
          Note that for <m>(x,y)\ne (0,0)</m> the angle <m>\theta</m> is defined only up to multiples of <m>2\pi n</m>,
          where <m>n</m> is an integer.
        </p>
        <p>
          Suppose <m>P=(x,y)</m> has polar coordinates <m>(r,\theta)</m>.
          As is illustrated in the diagram below,
          the oriented angle <em>starting from</em>
          the position vector <m>\overrightarrow{OP}</m> and <em>ending at</em>
          <m>\ell_\alpha</m> can be described as <m>\alpha-\theta</m>
          (up to a multiple of <m>2\pi</m>).
          By symmetry
          (or congruent triangles),
          the oriented angle from the line
          <m>\ell_\alpha</m> to the position vector
          <m>\overrightarrow{OP'}</m> of the reflection <m>P'</m> can also be described as <m>\alpha-\theta</m>.
          It follows that <m>\overrightarrow{OP'}</m> has length <m>r</m>
          (reflection preserves length),
          and oriented angle <m>\theta+(\alpha-\theta)+(\alpha-\theta)=2\alpha-\theta</m>.
          Thus, in terms of polar coordinates we have
          <md>
            <mrow>T_{\alpha}(x,y)\amp =T(r\cos\theta,r\sin\theta)</mrow>
            <mrow>\amp =(r\cos(2\alpha-\theta),r\sin(2\alpha-\theta) \hspace{20pt} \text{ (from the discussion above) }</mrow>
            <mrow>\amp =(r\cos(2\alpha)\cos(\theta)+r\sin(2\alpha)\sin(\theta),r\sin(2\alpha)\cos(\theta)-r\cos(2\alpha)\sin(\theta))  \hspace{20pt}\text{ (trig. identities) }</mrow>
            <mrow>\amp =(\cos(2\alpha)r\cos(\theta)+\sin(2\alpha)r\sin(\theta),\sin(2\alpha)r\cos(\theta)-\cos(2\alpha)r\sin(\theta))  \hspace{20pt}\text{ (algebra) }</mrow>
            <mrow>\amp =\begin{bmatrix}[rr] \cos(2\alpha)\amp \sin(2\alpha)</mrow>
            <mrow>\sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix} \begin{bmatrix}[r] r\cos\theta</mrow>
            <mrow>r\sin\theta \end{bmatrix}  \hspace{20pt}\text{ (identifying 2-vectors with column vectors) }</mrow>
            <mrow>\amp =\begin{bmatrix}[rr] \cos(2\alpha)\amp \sin(2\alpha)</mrow>
            <mrow>\sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix} \begin{bmatrix}[r] x</mrow>
            <mrow>y \end{bmatrix}</mrow>
          </md>.
        </p>
        <p>
          This proves <m>T_\alpha=T_A</m> where
          <me>
            A=\begin{bmatrix}\cos(2\alpha)\amp \sin(2\alpha)\\ \sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix}
          </me>.
        </p>
        <p>
          Since any function of the form <m>T_A</m> is linear,
          we conclude <m>T_\alpha</m> is linear.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=\R^\infty=\{(a_1,a_2,\dots, )\colon a_i\in\R\}</m>,
        the space of all infinite sequences.
        Define the
        <q>shift left function</q>, <m>T_\ell</m>, and
        <q>shift right function</q>, <m>T_r</m>, as follows:
        <md>
          <mrow>T_\ell\colon \R^\infty\amp \rightarrow \R^\infty \amp  T_r\colon \R^\infty\amp \rightarrow \R^\infty</mrow>
          <mrow>s=(a_1,a_2, a_3,\dots )\amp \longmapsto T_\ell(s)=(a_2, a_3,\dots) \amp  s=(a_1,a_2, a_3,\dots )\amp \longmapsto T_r(s)=(0,a_1,a_2,\dots)</mrow>
        </md>
        Prove that <m>T_\ell</m> and <m>T_r</m> are linear transformations.
        <solution>
          <p>
            Let <m>\boldsymbol=(a_1,a_2,a_3,\dots)</m> and
            <m>\boldt=(b_1,b_2,b_3,\dots)</m> be two infinite sequences.
            Then for any scalars <m>c,d\in\R</m> we have
            <md>
              <mrow>T_\ell(c\boldsymbol+d\boldr)\amp =T_\ell(ca_1+db_1,ca_2+db_2,\dots) \amp \text{ (arith. of sequences) }</mrow>
              <mrow>\amp =(ca_2+db_2,ca_3+db_3,\dots) \amp \text{ (def. of \(T_\ell\)) }</mrow>
              <mrow>\amp =c(a_2,a_3,\dots)+d(b_2,b_3,\dots) \amp \text{ (arith. of sequences) }</mrow>
              <mrow>\amp =cT_\ell(\boldsymbol)+dT_\ell(\boldt) \amp \text{ (def. of \(T_\ell\)) } </mrow>
            </md>,
            and
            <md>
              <mrow>T_r(c\boldsymbol+d\boldr)\amp =T_r(ca_1+db_1,ca_2+db_2,\dots) \amp \text{ (arith. of sequences) }</mrow>
              <mrow>\amp =(0,ca_1+db_1,ca_2+db_2,\dots) \amp \text{ (def. of \(T_r\)) }</mrow>
              <mrow>\amp =c(0,a_1,a_1,\dots)+d(0,b_1,b_2,\dots) \amp \text{ (arith. of sequences) }</mrow>
              <mrow>\amp =cT_r(\boldsymbol)+dT_r(\boldt) \amp \text{ (def. of \(T_r\)) } </mrow>
            </md>.
          </p>
          <p>
            This proves <m>T_\ell</m> and <m>T_r</m> are both linear transformations.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        (Conjugation).
        Fix an invertible matrix <m>Q\in M_{nn}</m>.
        Define <m>T\colon M_{nn}\rightarrow M_{nn}</m> as <m>T(A)=QAQ^{-1}</m>.
        Show that <m>T</m> is a linear transformation.
        (This operation is called <term>conjugation by <m>Q</m></term>.)
        <solution>
          <p>
            We show that <m>T(cA+dB)=cT(A)+dT(B)</m> for any
            <m>c,d\in\R</m> and any <m>A,B\in M_{nn}</m>:
            <md>
              <mrow>T(cA+dB)\amp =Q(cA+dB)Q^{-1} \amp \text{ (def. of \(T\)) }</mrow>
              <mrow>\amp =QcAQ^{-1}+QdBQ^{-1} \amp \text{ (left- and right-distributivity of matrix mult.) }</mrow>
              <mrow>\amp =cQAQ^{-1}+dQBQ^{-1} \amp \text{ (scalars commute) }</mrow>
              <mrow>\amp =cT(A)+dT(B) \amp \text{ (def. of \(T\)) }</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{3.3: subspaces}
  </p>
  <ol>
    <li xml:id="ex_trace">
      <p>
        Given any square matrix <m>A=[a_{ij}]\in M_{nn}</m>,
        we define the <term>trace</term> of <m>A</m> as <m>\tr A=\sum_{i=1}^n a_{ii}</m>.
        In other words,
        the trace <m>\tr A</m> is the sum of the diagonal entries of <m>A</m>.
        Let <m>V=M_{22}</m>,
        and define <m>W=\{A\in M_{22}\colon \tr(A)=0\}</m>.
      </p>
      <ol>
        <li>
          <p>
            Prove that <m>W</m> is a subspace.
          </p>
        </li>
        <li>
          <p>
            Find a spanning set for the subspace
            <me>
              W=\{A\in M_{22}\colon \tr(A)=0\}
            </me>.
          </p>
        </li>
      </ol>
    </li>
    <li>
      <p>
        Let <m>V=M_{22}</m>,
        and define <m>A_1=\begin{bmatrix}1\amp 1\\1\amp 1 \end{bmatrix}</m>,
        <m>A_2=\begin{bmatrix}0\amp 1\\1\amp 0 \end{bmatrix}</m>,
        <m>A_3=\begin{bmatrix}1\amp 1\\ 1\amp 0 \end{bmatrix}</m>.
        Compute <m>W=\Span(\{A_1,A_2,A_3\})</m>,
        identifying it as a certain <em>familiar</em> set of matrices.
        <solution>
          <p>
            Using the definition of <m>\Span</m>, we compute
            <md>
              <mrow>\Span\{A_1,A_2,A_3\}\amp =\{c_1A_1+c_2A_2+c_3A_3\colon c_i\in\R\}</mrow>
              <mrow>\amp =\left\{\begin{bmatrix} c_1+c_3\amp c_1+c_2+c_3</mrow>
              <mrow>c_1+c_2+c_3 \amp  c_1\end{bmatrix}\colon c_i\in\R\right\}</mrow>
            </md>.
          </p>
          <p>
            We claim this last set is none other than the set of all symmetric matrices.
            It is clear that each element in the set above is symmetric,
            so it remains only to show that if <m>A</m> is symmetric,
            then <m>A</m> can be written in the above form for some choice of <m>c_1,c_2,c_3</m>.
          </p>
          <p>
            Write <m>A=\begin{bmatrix}a\amp b\\b\amp c \end{bmatrix}</m>.
            We need to find <m>c_1,c_2,c_3</m> such that
            <me>
              \begin{bmatrix}c_1+c_3\amp c_1+c_2+c_3\\ c_1+c_2+c_3 \amp  c_1 \end{bmatrix} =\begin{bmatrix}a\amp b\\b\amp c \end{bmatrix}
            </me>
          </p>
          <p>
            The bottom-right entry tells us that we must pick <m>c_1=c</m>.
            Then the top-left entry tells us that <m>c_3=a-c</m>.
            Lastly, the off diagonal entries tells us that <m>c_2=b-a</m>.
          </p>
          <p>
            Thus we can write <m>A=cA_1+(b-a)A_2+(a-c)A_3</m>,
            showing <m>A\in\Span\{A_1,A_2,A_3\}</m>,
            and proving that <m>\Span\{A_1,A_2,A_3\}=(\text{ set of symmetric matrices) }</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\R^4</m> and define <m>W=\{(x_1,x_2,x_3,x_4)\colon x_1-x_4=x_2+x_3=x_1+x_2+x_3=0\}</m>.
        <ol>
          <li>
            <p>
              Show <m>W</m> is a subspace by identifying it as <m>\NS T</m> for an explicit linear transformation.
            </p>
          </li>
          <li>
            <p>
              Describe <m>W</m> in as simple a manner as possible.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The subset <m>W</m> can be described as the set of solutions to the linear system
          <md>
            \begin{linsys}{4} x_1\amp +\amp  \amp  \amp  \amp - \amp x_4\amp =0\\ \amp  \amp x_2\amp +\amp x_3\amp  \amp  \amp =0\\ x_1\amp +\amp x_2\amp +\amp x_3\amp  \amp  \amp =0 \end{linsys}\tag{<m>*</m>}
          </md>
        </p>
        <p>
          We can express this with the matrix equation <m>A\boldx=\boldzero</m>, where
          <me>
            A=\begin{bmatrix}1\amp 0\amp 0\amp -1\\ 0\amp 1\amp 1\amp 0\\ 1\amp 1\amp 1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          We see then that <m>W=\NS T_A=\NS A</m>.
          This proves <m>W</m> is a subspace,
          since <m>T_A</m> is a linear transformation.
        </p>
        <p>
          (b) We use Gaussian elimination to help describe <m>W</m>.
          The system <m>(*)</m> has corresponding augmented maqtrix
          <m>[A\vert \boldzero]</m>, which row reduces to
          <me>
            \begin{bmatrix}1\amp 0\amp 0\amp -1\amp 0\\ 0\amp 1\amp 1\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus <m>x_3=t</m> is the only free variable,
          and we back-substitute to find <m>(x_1,x_2,x_3,x_4)=(0,-t,t,0)</m>.
          We conclude that <m>W=\{(0,-t,t,0)\colon t\in\R\}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine which of the following subsets <m>W</m> are subspaces of <m>M_{nn}</m>.
        <ol>
          <li>
            <p>
              <m>W=\{\text{ diagonal \(n\times n\) matrices } \}</m>
            </p>
          </li>
          <li>
            <p>
              <m>W=\{A\in M_{nn}\colon \det A=0\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>W=\{\text{ upper triangular \(n\times n\) matrices } \}</m>
            </p>
          </li>
          <li>
            <p>
              <m>W=\{A\in M_{nn}\colon \tr A=0\}</m>. (See <xref ref="ex_trace">Exercise</xref>
              above for the definition of <m>\tr A</m>. )
            </p>
          </li>
          <li>
            <p>
              <m>W=\{\text{ symmetric \(n\times n\) matrices } \}=\{A\in M_{nn}\colon A^T=A\}</m>.
            </p>
          </li>
          <li>
            <p>
              <m>W=\{A\in M_{nn}\colon A^T = -A\}</m>.
              (These are called <term>skew-symmetric matrices</term>.)
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> of all <m>n</m> by <m>n</m> matrices <m>A</m> for which
              <m>A\boldx = \bold0</m> has only the trivial solution.
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> of all <m>n</m> by <m>n</m> matrices <m>A</m> such that <m>AB=BA</m>,
              where <m>B</m> is a fixed <m>n\times n</m> matrix.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Yes, let
          <me>
            A = \begin{bmatrix}a_{11} \amp  0 \amp  \cdots \amp  0 \\ 0 \amp  a_{22} \amp  \cdots \amp  0 \\ \vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\ 0 \amp  0 \amp  \cdots \amp  a_{nn} \end{bmatrix} B = \begin{bmatrix}b_{11} \amp  0 \amp  \cdots \amp  0 \\ 0 \amp  b_{22} \amp  \cdots \amp  0 \\ \vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\ 0 \amp  0 \amp  \cdots \amp  b_{nn} \end{bmatrix}
          </me>
        </p>
        <p>
          Then,
          <me>
            A + B = \begin{bmatrix}a_{11} + b_{11} \amp  0 \amp  \cdots \amp  0 \\ 0 \amp  a_{22}+b_{22} \amp  \cdots \amp  0 \\ \vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\ 0 \amp  0 \amp  \cdots \amp  a_{nn}+b_{nn} \end{bmatrix} \in M
          </me>
        </p>
        <p>
          And, for some scalar k
          <me>
            kA = k\begin{bmatrix}a_{11} \amp  0 \amp  \cdots \amp  0 \\ 0 \amp  a_{22} \amp  \cdots \amp  0 \\ \vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\ 0 \amp  0 \amp  \cdots \amp  a_{nn} \end{bmatrix} = \begin{bmatrix}ka_{11} \amp  0 \amp  \cdots \amp  0 \\ 0 \amp  ka_{22} \amp  \cdots \amp  0 \\ \vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\ 0 \amp  0 \amp  \cdots \amp  ka_{nn} \end{bmatrix} \in M
          </me>
        </p>
        <p>
          (b) No, consider
          <me>
            A = \begin{bmatrix}1\amp -1\\ -1\amp 1 \end{bmatrix} B = \begin{bmatrix}3\amp 3\\ 3\amp 3 \end{bmatrix} A+B = \begin{bmatrix}4\amp 2\\ 2\amp 4 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus <m>\det(A) = 0</m> and <m>\det(B)=0</m>,
          but <m>\det(A+B) = 16 - 4 = 12</m>.
          So addition is not closed.
        </p>
        <p>
          (c) Yes.
          Since <m>\underset{n\times n}{\boldzero}</m> is upper triangular,
          we have <m>\underset{n\times n}{\boldzero}\in W</m>.
          If <m>A=[a_{ij}]</m> and <m>B=[b_{ij}]</m> are upper triangular,
          then <m>a_{ij}=b_{ij}=0</m> for all <m>j>1</m>.
          Then we also have <m>a_{ij}+b_{ij}=0</m> for all <m>j>i</m>,
          showing that <m>A+B</m> is upper triangular.
          Hence <m>W</m> is closed under addition.
          A similar argument shows <m>W</m> is closed under scalar multiplication.
        </p>
        <p>
          (d) Yes.
          We prove by first showing that the map
          <m>\tr\colon M_{nn}\rightarrow \R</m> is a linear transformation.
          Since <m>W=\{A\in M_{nn}\colon \tr A=0\}=\NS \tr</m>,
          our null space theorem implies <m>W</m> is a subspace.
        </p>
        <p>
          To prove <m>\tr\colon M_{nn}\rightarrow\R</m> is linear,
          we observe that
          <md>
            <mrow>\tr(cA+dB)\amp =\sum_{i=1}^n (cA+dB)_{ii}</mrow>
            <mrow>\amp =\sum_{i=1}^n c(A)_{ii}+d(B)_{ii}</mrow>
            <mrow>\amp =c\sum_{i=1}^n(A)_{ii}+d\sum_{i=1}^n(B)_{ii}</mrow>
            <mrow>\amp =c\tr A+d\tr B</mrow>
          </md>
        </p>
        <p>
          (e) Yes.
          We prove this in the usual fashion.
          Remember: a matrix <m>A</m> is symmetric iff <m>A^T=A</m>.
          Thus <m>W=\{A\in M_{nn}\colon A^T=A\}</m>.
          <ol>
            <li>
              <p>
                <m>\boldzero=0_{nn}</m> is clearly symmetric.
                Hence <m>\boldzero\in W</m>.
              </p>
            </li>
            <li>
              <p>
                Suppose <m>A_1,A_2\in W</m>.
                Then <m>(A_i)^T=A_i</m> for <m>i=1,2</m>.
                But then
                <md>
                  <mrow>(A_1+A_2)^T\amp =A_1^T+A_2^T \amp \text{ (prop. of transp.) }</mrow>
                  <mrow>\amp =A_1+A_2 \amp \text{ (since \(A_i\in W\)) }</mrow>
                </md>
                Thus <m>(A_1+A_2)^T=A_1+A_2</m>,
                showing <m>A_1+A_2\in W</m>.
              </p>
            </li>
            <li>
              <p>
                Suppose <m>A\in W</m>.
                Then <m>A^T=A</m>.
                But then
                <md>
                  <mrow>(kA)^T\amp =kA^T \amp \text{ (prop. of transp.) }</mrow>
                  <mrow>\amp =kA \amp \text{ (since \(A\in W\)) }</mrow>
                </md>
                Thus <m>kA\in W</m>.
              </p>
            </li>
          </ol>
        </p>
        <p>
          (f) Yes.
          The proof of this looks nearly exactly like the one above,
          only now we are always showing a matrix <m>B</m> satisfies <m>B^T=-B</m>.
          The exact same properties of transpose will be invoked.
        </p>
        <p>
          (g) No.
          The zero vector in <m>M_{nn}</m> is <m>\underset{n\times n}{\boldzero}</m>,
          the matrix with all zeros.
          Clearly the matrix equation
          <m>\underset{n\times n}{\boldzero}\boldx=\boldzero</m> has more than just the trivial solution;
          in fact all vectors <m>\boldx</m> satisfy this equation!
          Thus <m>\underset{n\times n}{\boldzero}\notin W</m>,
          showing that <m>W</m> is not a subspace.
        </p>
        <p>
          (h) Yes.
          <ol>
            <li>
              <p>
                Clearly <m>\boldzero=0_{nn}</m> satisfies <m>0_{nn}B=B0_{nn}</m>.
                Thus <m>\boldzero\in W</m>.
              </p>
            </li>
            <li>
              <p>
                Next, let <m>A_1,A_2 \in W</m>.
                Then
                <me>
                  (A_1+A_2)B = A_1B +A_2B = BA_1+BA_2 + B(A_1+A_2)
                </me>
                So, <m>A_1 + A_2 \in W</m>.
              </p>
            </li>
            <li>
              <p>
                Finally, suppose <m>A_1\in W</m> and let <m>k</m> be any scalar.
                Then
                <me>
                  (kA_1)B = k(A_1B) = k(BA_1) = (kB)A_1 = B(kA_1)
                </me>
                Thus <m>kA_1 \in W</m>.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine which of the following are subspaces of <m>P_3</m>.
        <ol>
          <li>
            <p>
              The set <m>W</m> of all polynomials
              <m>a_0+a_1x+a_2x^2+a_3x^3</m> for which <m>a_0 = 0</m>
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> of all polynomials
              <m>a_0+a_1x+a_2x^2+a_3x^3</m> for which <m>a_0 + a_1 + a_2 + a_3 = 0</m>.
            </p>
          </li>
          <li>
            <p>
              All polynomials of the form <m>a_0+a_1x+a_2x^2+a_3x^3</m> in which
              <m>a_0,a_1,a_2,a_3</m> are rational numbers.
            </p>
          </li>
          <li>
            <p>
              All polynomials of the form <m>a_0 +a_1x</m>,
              where <m>a_0,a_1</m> are real numbers.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Yes.
          <ol>
            <li>
              <p>
                First off the zero vector <m>\boldzero</m> in this case is the zero polynomial <m>p(x)=0+0x+0x^2+0x^3</m>.
                This is clearly an element of <m>W</m>.
              </p>
            </li>
            <li>
              <p>
                Next, suppose <m>a_0 = b_0 = 0</m>.
                Then,
                <md>
                  <mrow>a_0+a_1x+a_2x^2+a_3x^3 + b_0+b_1x+b_2x^2+b_3x^3 \amp =\amp  (a_0+b_0) + (a_1 +b_1)x + (a_2 +b_2)x^2 + (a_3 + b_3)x^3</mrow>
                  <mrow>\amp =\amp  0 + (a_1 +b_1)x + (a_2 +b_2)x^2 + (a_3 + b_3)x^3</mrow>
                </md>
                Thus addition is closed.
              </p>
            </li>
            <li>
              <p>
                Finally, let <m>k</m> be some scalar.
                Then,
                <md>
                  <mrow>k(a_0+a_1x+a_2x^2+a_3x^3) \amp =\amp  ka_0+ka_1x+ka_2x^2+ka_3x^3</mrow>
                  <mrow>\amp =\amp  0 + ka_1x+ka_2x^2+ka_3x^3</mrow>
                </md>
                Thus scalar multiplication is also closed.
              </p>
            </li>
          </ol>
        </p>
        <p>
          (b) Yes.
          <ol>
            <li>
              <p>
                First off, clearly the zero element <m>p(x)=0+0x+0x^2+0x^3</m> is in <m>W</m>.
              </p>
            </li>
            <li>
              <p>
                Next, suppose <m>a_0 + a_1 + a_2 + a_3 = 0</m> and <m>b_0 + b_1 + b_2 + b_3 = 0</m>.
                Then,
                <md>
                  <mrow>(a_0 + a_1x + a_2x^2 + a_3x^3) + (b_0 + b_1x + b_2x^2 + b_3x^3)</mrow>
                  <mrow>= (a_0+b_0) + (a_1+b_1)x + (a_2+b_2)x^2+(a_3+b_3)x^3</mrow>
                </md>
                So,
                <md>
                  <mrow>(a_0+b_0) + (a_1+b_1) + (a_2+b_2) + (a_3+b_3)</mrow>
                  <mrow>= (a_0 + a_1 + a_2 + a_3) + (b_0 + b_1 + b_2 + b_3)</mrow>
                  <mrow>=0 + 0 = 0</mrow>
                </md>
                Thus addition is closed.
              </p>
            </li>
            <li>
              <p>
                Lastly, let <m>k</m> be some scalar.
                Then,
                <md>
                  <mrow>k(a_0+a_1x+a_2x^2+a_3x^3) = ka_0+ka_1x+ka_2x^2+ka_3x^3</mrow>
                </md>
                So,
                <me>
                  ka_0+ka_1+ka_2+ka_3 = k(a_0 + a_1 + a_2 + a_3) = k(0)=0
                </me>
                Thus scalar multiplication is also closed.
              </p>
            </li>
          </ol>
        </p>
        <p>
          (c) No.
          Let <m>p(x)=1+x+x^2</m>, a polynomial with rational coefficients.
          Then <m>\sqrt{2}p(x)=\sqrt{2}+\sqrt{2}x+\sqrt{2}x^2</m> does not have rational coefficients,
          hence does not lie in the set.
        </p>
        <p>
          (d) Yes.
          This is just the subspace <m>P_1</m>,
          which we have already shown to be a subspace.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Which of the following are subspaces of <m>F(-\infty,\infty)</m>?
        <ol>
          <li>
            <p>
              The set <m>W</m> of all functions <m>f</m> in
              <m>F(-\infty,\infty)</m> for which <m>f(0) = 0</m>.
            </p>
          </li>
          <li>
            <p>
              All functions <m>f</m> in <m>F(-\infty,\infty)</m> for which <m>f(0) = 1</m>.
            </p>
          </li>
          <li>
            <p>
              The set <m>W</m> of all functions <m>f</m> in
              <m>F(-\infty,\infty)</m> for which <m>f(-x) = x</m>.
            </p>
          </li>
          <li>
            <p>
              All polynomials of degree 2.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Yes.
          First note that the zero function clearly satisfies the stated condition.
          Thus <m>\boldzero\in W</m>.
        </p>
        <p>
          Now suppose <m>f(0) = 0</m> and <m>g(0) = 0</m>.
          Then
          <me>
            (f + g)(0) = f(0) + g(0) = 0 + 0 = 0
          </me>
        </p>
        <p>
          And for some scalar <m>k</m>,
          <me>
            k(f(0)) = k(0) = 0
          </me>
        </p>
        <p>
          (b) No not closed under addition.
          Consider <m>f(0) = 1</m> and <m>g(0) = 1</m>, then
          <me>
            (f+g)(0) = f(0)+g(0) = 1 + 1 =2 \neq 1
          </me>
        </p>
        <p>
          (c) Yes.
          Again, clearly the zero function satisfies this condition.
          Thus <m>\boldzero\in W</m>.
        </p>
        <p>
          Next, suppose <m>f(-x) = x</m> and <m>g(-x) = x</m>, then
          <me>
            (f+g)(x) = f(x) + g(x) = f(-x) +g(-x) = (f+g)(-x)
          </me>
        </p>
        <p>
          And for some scalar <m>k</m>,
          <me>
            (kf)(x)=k(f(x))=k(f(-x)) = (kf)(-x)
          </me>
        </p>
        <p>
          (d) No, consider <m>-x^2</m> and <m>x^2 +3x +1</m>.
          Then
          <me>
            -x^2 +(x^2 +3x +1) = 3x +1
          </me>
          which is not a polynomial of degree 2.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Express the following as linear combinations of <m>\boldu = (2,1,4)</m>,
        <m>\boldv = (1,-1,3)</m>, and <m>\boldw = (3,2,5)</m>
        <ol>
          <li>
            <p>
              <m>(-9,-7,-15)</m>
            </p>
          </li>
          <li>
            <p>
              <m>(6,11,6)</m>
            </p>
          </li>
          <li>
            <p>
              <m>(0,0,0)</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>\begin{bmatrix}2\amp 1\amp 3\amp -9\\ 1\amp -1\amp 2\amp -7\\ 4\amp 3\amp 5\amp -15 \end{bmatrix} \amp \xrightarrow[]{1/2r_1 - r_2}\amp \begin{bmatrix}2\amp 1\amp 3\amp -9\\ 0\amp 3/2\amp -1/2\amp 5/2\\ 4\amp 3\amp 5\amp -15 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{2r_1 - r_3}\amp \begin{bmatrix}2\amp 1\amp 3\amp -9\\ 0\amp 3/2\amp -1/2\amp 5/2\\ 0\amp -1\amp 1\amp -3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{1/2r_1}\amp \begin{bmatrix}1\amp 1/2\amp 3/2\amp -9/2\\ 0\amp 3/2\amp -1/2\amp 5/2\\ 0\amp -1\amp 1\amp -3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{2/3r_2}\amp \begin{bmatrix}1\amp 1/2\amp 3/2\amp -9/2\\ 0\amp 1\amp -1/3\amp 5/3\\ 0\amp -1\amp 1\amp -3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 + r_3}\amp \begin{bmatrix}1\amp 1/2\amp 3/2\amp -9/2\\ 0\amp 1\amp -1/3\amp 5/3\\ 0\amp 0\amp 2/3\amp -4/3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{3/2r_3}\amp \begin{bmatrix}1\amp 1/2\amp 3/2\amp -9/2\\ 0\amp 1\amp -1/3\amp 5/3\\ 0\amp 0\amp 1\amp -2 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Solving by back substitution,
          <md>
            <mrow>x_3 \amp =\amp  -2</mrow>
            <mrow>x_2 \amp =\amp  5/3 - 2/3 = 1</mrow>
            <mrow>x_1 \amp =\amp  -9/2 + 3 - 1/2 = -2</mrow>
          </md>
        </p>
        <p>
          Thus
          <me>
            -2\boldu + 1\boldv +(-2)\boldw = (-9,-7,-15)
          </me>
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}2\amp 1\amp 3\amp 6\\ 1\amp -1\amp 2\amp 11\\ 4\amp 3\amp 5\amp 6 \end{bmatrix} \amp \xrightarrow[]{r_1 \leftrightarrow r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 11\\ 2\amp 1\amp 3\amp 6\\ 4\amp 3\amp 5\amp 6 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{2r_1 - r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 11\\ 0\amp -3\amp 1\amp 16\\ 4\amp 3\amp 5\amp 6 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{4r_1 - r_3}\amp \begin{bmatrix}1\amp -1\amp 2\amp 11\\ 0\amp -3\amp 1\amp 16\\ 0\amp -7\amp 3\amp 38 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-1/3r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 11\\ 0\amp 1\amp -1/3\amp -16/3\\ 0\amp -7\amp 3\amp 38 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{7r_2 + r_3}\amp \begin{bmatrix}1\amp -1\amp 2\amp 11\\ 0\amp 1\amp -1/3\amp -16/3\\ 0\amp 0\amp 2/3\amp 2/3 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Solving by back substitution,
          <md>
            <mrow>x_3 \amp =\amp  1</mrow>
            <mrow>x_2 \amp =\amp  -16/3 + 1/3 = -5</mrow>
            <mrow>x_1 \amp =\amp  11 -2 -5 = 4</mrow>
          </md>
        </p>
        <p>
          Thus
          <me>
            4\boldu + (-5)\boldv +1\boldw = (6,11,6)
          </me>
        </p>
        <p>
          (c)
          <me>
            0\boldu + 0\boldv + 0\boldw = (0,0,0)
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Which are the following are linear combinations of:
        <me>
          A = \begin{bmatrix}4\amp 0\\ -2\amp -2 \end{bmatrix} \hspace{5pt}, B = \begin{bmatrix}1\amp -1\\ 2\amp 3 \end{bmatrix} \hspace{5pt}, C= \begin{bmatrix}0\amp 2\\ 1\amp 4 \end{bmatrix} ?
        </me>
        <ol>
          <li>
            <p>
              <m>\begin{bmatrix}6\amp -8\\ -1\amp -8 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}0\amp 0\\ 0\amp 0 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\begin{bmatrix}-1\amp 5\\ 7\amp 1 \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Yes, this is a linear combination.
          <md>
            <mrow>\begin{bmatrix}4\amp 1\amp 0\amp 6\\ 0\amp -1\amp 2\amp -8\\ -2\amp 2\amp 1\amp -1\\ -2\amp 3\amp 4\amp -8 \end{bmatrix} \amp \xrightarrow[]{r_1 + 2r_3}\amp \begin{bmatrix}4\amp 1\amp 0\amp 6\\ 0\amp -1\amp 2\amp -8\\ 0\amp 5\amp 2\amp 4\\ -2\amp 3\amp 4\amp -8 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 + 2r_4}\amp \begin{bmatrix}4\amp 1\amp 0\amp 6\\ 0\amp -1\amp 2\amp -8\\ 0\amp 5\amp 2\amp 4\\ 0\amp 7\amp 8\amp -10 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-r_2}\amp \begin{bmatrix}4\amp 1\amp 0\amp 6\\ 0\amp 1\amp -2\amp 8\\ 0\amp 5\amp 2\amp 4\\ 0\amp 7\amp 8\amp -10 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{1/4r_1}\amp \begin{bmatrix}1\amp 1/4\amp 0\amp 3/2\\ 0\amp 1\amp -2\amp 8\\ 0\amp 5\amp 2\amp 4\\ 0\amp 7\amp 8\amp -10 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{5r_2-r_3}\amp \begin{bmatrix}1\amp 1/4\amp 0\amp 3/2\\ 0\amp 1\amp -2\amp 8\\ 0\amp 0\amp -12\amp 36\\ 0\amp 7\amp 8\amp -10 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{7r_2-r_4}\amp \begin{bmatrix}1\amp 1/4\amp 0\amp 3/2\\ 0\amp 1\amp -2\amp 8\\ 0\amp 0\amp -12\amp 36\\ 0\amp 0\amp -22\amp 66 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-1/12r_3}\amp \begin{bmatrix}1\amp 1/4\amp 0\amp 3/2\\ 0\amp 1\amp -2\amp 8\\ 0\amp 0\amp 1\amp -3\\ 0\amp 0\amp -22\amp 66 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{22r_3+r_4}\amp \begin{bmatrix}1\amp 1/4\amp 0\amp 3/2\\ 0\amp 1\amp -2\amp 8\\ 0\amp 0\amp 1\amp -3\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Now by substitution:
          <md>
            <mrow>x_3 \amp =\amp  -3</mrow>
            <mrow>x_2 \amp =\amp  8 +2(-3) = 2</mrow>
            <mrow>x_1 \amp =\amp  3/2 - 1/4(2) = 1</mrow>
          </md>
        </p>
        <p>
          (b) Yes, this is a linear combination.
          <me>
            0A + 0B +0C = \begin{bmatrix}0\amp 0\\ 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          (c) No, this is not a linear combination
          <md>
            <mrow>\begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp -1\amp 2\amp 5\\ -2\amp 2\amp 1\amp 7\\ -2\amp 3\amp 4\amp 1 \end{bmatrix} \amp \xrightarrow[]{1/2r_1+r_3}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp -1\amp 2\amp 5\\ 0\amp 5/2\amp 1\amp 13/2\\ -2\amp 3\amp 4\amp 1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{1/2r_1+r_4}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp -1\amp 2\amp 5\\ 0\amp 5/2\amp 1\amp 13/2\\ 0\amp 7/2\amp 4\amp 1/2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-r_2}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp 1\amp -2\amp -5\\ 0\amp 5/2\amp 1\amp 13/2\\ 0\amp 7/2\amp 4\amp 1/2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{5/2r_2 - r_3}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp 1\amp -2\amp -5\\ 0\amp 0\amp -6\amp -19\\ 0\amp 7/2\amp 4\amp 1/2 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{7/2r_2 - r_4}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp 1\amp -2\amp -5\\ 0\amp 0\amp -6\amp -19\\ 0\amp 0\amp -11\amp -18 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-1/6r_3}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp 1\amp -2\amp -5\\ 0\amp 0\amp 1\amp 19/6\\ 0\amp 0\amp -11\amp -18 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{11r_3 + r_4}\amp \begin{bmatrix}4\amp 1\amp 0\amp -1\\ 0\amp 1\amp -2\amp -5\\ 0\amp 0\amp 1\amp 19/6\\ 0\amp 0\amp 0\amp 101/6 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          The system is not consistent,
          thus it cannot be a linear combination.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Express <m>p(x)=7 +8x + 9x^2</m> as a linear combination of
        <me>
          p_1 = 2 + x + 4x^2, p_2=1-x+3x^2, p_3=3+2x+5x^2
        </me>
        <solution>
          <p>
            We have <m>p</m> equal to a linear combination of <m>p_1, p_2, p_3</m> if and only if
            <m>p=ap_1+bp_2+cp_3</m> for some <m>a,b,c\in\R</m> iff
            <me>
              7 +8x + 9x^2=a(2+x+4x^2)+b(1-x+3x^2)+c(3+2x+5x^2)=(2a+b+3c)+(a-b+2c)x+(4a+3b+5c)x^2
            </me>.
          </p>
          <p>
            Equating coefficients, we see that <m>p</m> is a linear combination of
            <m>p_1,p_2,p_3</m> iff the following system is consistent:
            <md>
              \begin{linsys}{3} 2a\amp +\amp b\amp +\amp 3c\amp =\amp 7\\ a\amp -\amp b\amp +\amp 2c\amp =\amp 8\\ 4a\amp +\amp 3b\amp +\amp 5c\amp =\amp 9 \end{linsys}
            </md>
          </p>
          <p>
            Now perform Gaussian elimination:
            <md>
              <mrow>\begin{bmatrix}2\amp 1\amp 3\amp 7\\ 1\amp -1\amp 2\amp 8\\ 4\amp 3\amp 5\amp 9 \end{bmatrix} \amp \xrightarrow[]{r_1 \leftrightarrow r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 2\amp 1\amp 3\amp 7\\ 4\amp 3\amp 5\amp 9 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{2r_1 - r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 0\amp -3\amp 1\amp 9\\ 4\amp 3\amp 5\amp 9 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{4r_1 - r_3}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 0\amp -3\amp 1\amp 9\\ 0\amp -7\amp 3\amp 23 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{-1/3r_2}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 0\amp 1\amp -1/3\amp -3\\ 0\amp -7\amp 3\amp 23 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{7r_2 + r_3}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 0\amp 1\amp -1/3\amp -3\\ 0\amp 0\amp 2/3\amp 2 \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{3/2r_3}\amp \begin{bmatrix}1\amp -1\amp 2\amp 8\\ 0\amp 1\amp -1/3\amp -3\\ 0\amp 0\amp 1\amp 3 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Now by substitution
            <md>
              <mrow>c \amp =\amp  3</mrow>
              <mrow>b \amp =\amp  -3 + 1 = -2</mrow>
              <mrow>a \amp =\amp  8-6-2 = 0</mrow>
            </md>
          </p>
          <p>
            Thus
            <me>
              7 +8x + 9x^2 = 0p_1 -2p_2 + 3p_3
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Determine whether the following polynomials span <m>P_2</m>.
        <me>
          p_1= 1-x+2x^2, \ p_2=3+x, \ p_3 = 5 -x +4x^2,\  p_4 = -2-2x+2x^2
        </me>
        <solution>
          <p>
            Reasoning exactly as in the previous exercise,
            we see that a polynomial <m>p(x)=a+bx+cx^2</m> is a linear combination of the <m>p_i</m> if and only if we have
            <m>p=k_1p_1+k_2p_2+k_3p_3+k_4p_4</m> for some <m>k_i\in\R</m>.
            After expanding the expression on the right and equating coefficients,
            we conclude that <m>p</m> lies in the span of the <m>p_i</m> if and only if the system
            <md>
              \begin{linsys}{4} k_1\amp +\amp 3k_2\amp +\amp 5k_3\amp -\amp 2k_4\amp =\amp a\\ -k_1\amp +\amp k_2\amp -\amp k_3\amp -\amp 2k_4\amp =\amp b\\ 2k_0\amp +\amp 0k_2\amp +\amp 4k_3\amp +\amp 2k_4\amp =\amp c \end{linsys}
            </md>
          </p>
          <p>
            Now apply Gaussian elimination.
            <md>
              <mrow>\begin{bmatrix}1\amp 3\amp 5\amp -2\amp a\\ -1\amp 1\amp -1\amp -2\amp b\\ 2\amp 0\amp 4\amp 2\amp c \end{bmatrix} \amp \xrightarrow[]{r_2+r_1}\amp \begin{bmatrix}1\amp 3\amp 5\amp -2\amp a\\ 0\amp 4\amp 4\amp -4\amp a+b\\ 2\amp 0\amp 4\amp 2\amp c \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{r_3-2r_1}\amp \begin{bmatrix}1\amp 3\amp 5\amp -2\amp a\\ 0\amp 4\amp 4\amp -4\amp a+b\\ 0\amp 6\amp 6\amp -6\amp c-2a \end{bmatrix}</mrow>
              <mrow>\amp \xrightarrow[]{r_3-3/2r_2}\amp \begin{bmatrix}1\amp 3\amp 5\amp -2\amp a\\ 0\amp 4\amp 4\amp -4\amp a+b\\ 0\amp 0\amp 0\amp 0\amp -7/2a-3/2b+c \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            We see the system will be consistent if and only if <m>-7/2a-3/2b+c=0</m>
            (to avoid having a leading 1 in the last column).
            For example, if we pick <m>a=0, b=0, c=1</m>,
            the system is not consistent.
            This means the corresponding polynomial,
            <m>p(x)=0+0x+1x^2=x^2</m> is not in the span of the <m>p_i</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>f(x)= \cos^2x</m> and <m>g(x)= \sin^2x</m>.
        Determine whether the following functions lie in the space spanned by <m>f</m> and <m>g</m>.
        If yes, provide an explicit linear combination;
        if no, show no linear combination exists.
        (You may want to review your trig identities.)
        <ol>
          <li>
            <p>
              <m>q(x)=\cos 2x</m>
            </p>
          </li>
          <li>
            <p>
              <m>q(x)=3+x^2</m>
            </p>
          </li>
          <li>
            <p>
              <m>q(x)=1</m>
            </p>
          </li>
          <li>
            <p>
              <m>q(x)=\sin x</m>
            </p>
          </li>
          <li>
            <p>
              <m>q(x)=0</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Yes.
          <me>
            f - g = \cos^2x - \sin^2x = \cos2x
          </me>
        </p>
        <p>
          (b) No.
          Assume we can find <m>k_1, k_2</m> such that
          <me>
            3+x^2 = k_1\cos^2x +k_2\sin^2x
          </me>
          for all <m>x</m>.
        </p>
        <p>
          Evaluating this identity at <m>x=0</m>,
          we see that we must have <m>3=k_1</m>.
        </p>
        <p>
          However, evaluating the same identity at <m>x=\pi</m> we see that <m>3+\pi^2=k_1</m>.
          Thus <m>k_1=3</m>, and <m>k_1=3+\pi^2</m>, a contradiction.
        </p>
        <p>
          We conclude there can be no such <m>k_1, k_2</m>.
        </p>
        <p>
          (c) Yes.
          <me>
            f + g = \cos^2x + \sin^2x = 1
          </me>
        </p>
        <p>
          (d) No, assume we can find <m>k_1, k_2</m> such that
          <me>
            \sin x = k_1\cos^2x + k_2\sin^2x
          </me>
          for all <m>x</m>.
          Then
          <md>
            <mrow>\sin(-x) \amp =\amp  k_1\cos^2(-x) + k_2\sin^2(-x)</mrow>
            <mrow>\amp =\amp  k_1\cos^2(x) + k_2\sin^2(x)</mrow>
            <mrow>\amp =\amp \sin(x)</mrow>
          </md>
        </p>
        <p>
          But it is not the case that <m>\sin(x)=\sin(-x)</m> for all <m>x</m>.
          Thus there are no such <m>k_i</m>,
          and <m>\sin(x)</m> is not in the span.
        </p>
        <p>
          (e) Yes.
          <me>
            0f + 0g = 0
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Show that the solution vectors of a consistent nonhomogeneous system of <m>m</m> linear equations in <m>n</m> unknowns do not form a subspace of <m>R^n</m>.
        <solution>
          <p>
            Represent the system as a matrix equation of the form
            <me>
              A\boldx=\boldy
            </me>,
            where <m>\boldy\ne\boldzero</m>,
            since the system is assumed to be nonhomogeneous.
          </p>
          <p>
            We can describe the set of solutions as <m>W=\{\boldx\colon A\boldx=\boldy\}</m>.
            Then <m>W</m> is not a subspace as <m>\boldzero\notin W</m>.
            Indeed <m>A\boldzero=\boldzero\ne\boldy</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Define the function <m>T\colon P_2\rightarrow \R^2</m> by
        <m>T(p(x))=\begin{bmatrix}p(1)\\ p(-1) \end{bmatrix} </m>.   (a) Prove <m>T</m> is a linear transformation.   (b) Compute  <m>\NS(T)</m> and
        <m>\range(T)</m> and describe these in as simple a manner as possible.
        <solution>
          <p>
            (a)
            <md>
              <mrow>T(cp_1+dp_2)\amp =\begin{bmatrix} (cp_1+dp_2)(1)</mrow>
              <mrow>(cp_1+dp_2)(-1)\end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix} cp_1(1)+dp_2(1)</mrow>
              <mrow>cp_1(-1)+dp_2(-1)\end{bmatrix}</mrow>
              <mrow>\amp =c\begin{bmatrix} p_1(1)</mrow>
              <mrow>p_1(-1)\end{bmatrix}+ d\begin{bmatrix}p_2(1)</mrow>
              <mrow>p_2(-1)\end{bmatrix}</mrow>
              <mrow>\amp =cT(p_1)+dT(p_2)</mrow>
            </md>
          </p>
          <p>
            (b)
            <md>
              <mrow>\NS(T)\amp =\{p\in P_2\colon T(p)=\boldzero\}</mrow>
              <mrow>\amp =\{p\in P_2\colon \begin{bmatrix} p(1)</mrow>
              <mrow>p(-1) \end{bmatrix}=\begin{bmatrix} 0</mrow>
              <mrow>0 \end{bmatrix}\}</mrow>
              <mrow>\amp =\{p\in P_2\colon p(1)=p(-1)=0\}</mrow>
            </md>.
          </p>
          <p>
            We must now describe all polynomials
            <m>p(x)=ax^2+bx+c</m> satisfying <m>p(1)=p(-1)=0</m>.
            You could set up the system of equations by evaluating and then solve directly for <m>a,
            b, c</m>.
            Alternatively,
            you can observe that a degree 2 polynomial satisfying
            <m>p(1)=p(-1)=0</m> must <em>factor</em> as
            <me>
              p(x)=c(x-1)(x+1)=c(x^2-1)=cx^2-c
            </me>.
          </p>
          <p>
            It follows that <m>\NS T=\{p(x)\in P_2\colon p(x)=cx^2-c \text{ for some } c\in\R\}=\Span(\{x^2-1\})</m>.
          </p>
          <p>
            The range of <m>T</m> is the set of vectors <m>(b_1,b_2)</m> such that
            <m>(b_1,b_2)=(p(1),p(-1))</m> for some <m>p(x)=a_0+a_1x+a_2x^2</m>:
            i.e., the set of pairs <m>(b_1,b_2)</m> for which the system
            <md>
              \begin{linsys}{3} a_0\amp +\amp a_1\amp +\amp a_2\amp =\amp b_1\\ a_0\amp -\amp a_1\amp +\amp a_2\amp =\amp b_2 \end{linsys}
            </md>
            is consisent.
            Perform Gaussian elimination on the corresponding augmented matrix:
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp b_1\\ 1\amp -1\amp 1\amp b_2 \end{bmatrix} \xrightarrow{\text{ row red. } } \begin{bmatrix}1\amp 1\amp 1\amp b_1\\ 0\amp 1\amp 0\amp \frac{b_1-b_2}{2} \end{bmatrix}
            </me>
          </p>
          <p>
            We see the system is always consistent,
            and hence that all <m>2</m>-vectors are included in the range:
            i.e., <m>\range T=\R^2</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Define <m>T\colon P_2\rightarrow P_3</m> by <m>T(p)=q</m>,
        where <m>\ds q(x)=\int_0^x p(t) \ dt</m>.
        <ol>
          <li>
            <p>
              Prove that <m>T</m> is a linear transformation.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\NS(T)</m> and <m>\range(T)</m> and describe these spaces in as simple a manner as possible.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>T(cp+dq)\amp =\int_0^x cp(t)+dq(t) \ dt \amp \text{ (by def.) }</mrow>
            <mrow>\amp =c\int_0^x p(t) \ dt+d\int_0^x q(t) \ dt \amp \text{ (by calc.) }</mrow>
            <mrow>\amp =cT(p)+dT(q) \amp \text{ (by def) }</mrow>
          </md>
        </p>
        <p>
          (b) For an arbitrary polynomial <m>p(x)=ax^2+bx+c</m>,
          we have <m>T(p)=\frac{1}{3}ax^3+\frac{1}{2}bx^2+cx</m>.
          Clearly <m>T(p)=\boldzero</m> if and only if <m>a=b=c=0</m>.
          Thus <m>\NS(T)=\{\boldzero\}</m>.
        </p>
        <p>
          We clearly have <m>\range(T)\subseteq W=\{q(x)\in P_3\colon q(x)=dx^3+ex^2+fx\}</m>:
          the set of polynomials whose constant term is zero.
          Furthermore, given any <m>q(x)=dx^3+ex^2+fx\in W</m>,
          we have <m>q=T(3dx^2+2ex+f)</m>,
          showing that <m>W\subseteq \range(T)</m>.
          We conclude <m>W=\range(T)</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>S\colon M_{nn}\rightarrow M_{nn}</m> by <m>S(A)=A^T-A</m>.
        <ol>
          <li>
            <p>
              Show <m>S</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\NS(S)</m> and <m>\range(S)</m> and identify both of these spaces as certain special named families of matrices.
              Make sure to prove your claims.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>S(cA+dB)\amp =(cA+dB)^T-(cA+dB) \amp \text{ (by def) }</mrow>
            <mrow>\amp =cA^T+dB^T-cA-cB \amp \text{ (transpose prop.) }</mrow>
            <mrow>\amp =c(A^T-A)+d(B^T-B)\amp \text{ (matrix arith.) }</mrow>
            <mrow>\amp =cS(A)+dS(B) \amp \text{ (by def) }</mrow>
          </md>
        </p>
        <p>
          (b) <m>S(A)=\boldzero</m> iff <m>A^T-A=\boldzero</m> iff <m>A^T=A</m>.
          Thus <m>\NS(A)</m> is the set of symmetric matrices.
        </p>
        <p>
          Now we identify <m>\range T</m>.
          Let <m>B=S(A)=A^T-A</m>.
          Then <m>B^T=(A-A^T)=-(A^T-A)=-B</m>.
          This shows <m>\range(T)</m> is a subset of the space of skew-symmetric matrices.
          Does <m>\range T</m> include <em>all</em> skew-symmetric matrices?
          Suppose <m>B</m> is skew, so that <m>B^T=-B</m>.
          Set <m>A=-\frac{1}{2}B</m>.
          Then
          <me>
            T(A)=(-\frac{1}{2}B)^T-\frac{1}{2}B)=-\frac{1}{2}(B^T-B)=-\frac{1}{2}(-B-B)=B
          </me>.
        </p>
        <p>
          This shows directly that <m>B\in \range T</m>,
          and thus that <m>\range T=\{\text{ skew-symmetric \(n\times n\) matrices } \}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>T\colon P_3\rightarrow M_{22}</m> by <m>T(p)=\begin{bmatrix}p(1)\amp p(-1)\\ p(2)\amp p(-2) \end{bmatrix}</m>.
        <ol>
          <li>
            <p>
              Show <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\NS(T)</m> and <m>\range(T)</m>.
              Hint: a nonzero polynomial of degree 3 can have at most 3 roots!
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a)
          <md>
            <mrow>T(cp+dq)\amp =\begin{bmatrix}(cp+dq)(1)\amp (cp+dq)(-1)</mrow>
            <mrow>(cp+dq)(2)\amp (cp+dq)(-2)\end{bmatrix} \amp \text{ (by def.) }</mrow>
            <mrow>\amp =\begin{bmatrix}cp(1)+dq(1)\amp cp(-1)+dq(-1)</mrow>
            <mrow>cp(2)+dq(2)\amp cp(-2)+dq(-2)\end{bmatrix} \amp \text{ (function arith.) } \</mrow>
            <mrow>\amp =c\begin{bmatrix}p(1)\amp p(-1)</mrow>
            <mrow>p(2)\amp p(-2)\end{bmatrix}+d\begin{bmatrix}q(1)\amp q(-1)</mrow>
            <mrow>q(2)\amp q(-2)\end{bmatrix}\amp \text{ (matrix arith.) }</mrow>
            <mrow>\amp =cT(p)+dT(q) \amp \text{ (by def) }</mrow>
          </md>
        </p>
        <p>
          (b) We have <m>T(p)=\boldzero</m> iff
          <m>p(1)=p(-1)=p(2)=p(-2)=0</m> iff <m>p(x)=\boldzero</m>,
          since a nonzero polynomial of degree at most 3 can have at most 3 distinct roots!
          Thus <m>\NS(T)=\{\boldzero\}</m> is the trivial subspace.
        </p>
        <p>
          For given <m>A=\begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}</m>,
          deciding whether there is a
          <m>p(x)=a_0+a_1x+a_2x^2+a_3</m> such that <m>T(p)=A</m> amounts to deciding whether the system
          <md>
            \begin{linsys}{4} a_0\amp +\amp a_1\amp +\amp a_2\amp +\amp a_3\amp =\amp a\\ a_0\amp -\amp a_1\amp +\amp a_2\amp -\amp a_3\amp =\amp b\\ a_0\amp +\amp 2a_1\amp +\amp 4a_2\amp +\amp 8a_3\amp =\amp c\\ a_0\amp -\amp 2a_1\amp +\amp 4a_2\amp -\amp 8a_3\amp =\amp d \end{linsys}
          </md>
          is consistent.
          This in turn can answered by deciding when the following matrix equation can be solved:
          <me>
            \begin{bmatrix}1\amp 1\amp 1\amp 1\\ 1\amp -1\amp 1\amp -1\\ 1\amp 2\amp 4\amp 8\\ 1\amp -2\amp 4\amp -8 \end{bmatrix} \begin{bmatrix}a_0\\ a_1\\ a_2\\ a_3 \end{bmatrix} =\begin{bmatrix}a\\b\\c\\d \end{bmatrix}
          </me>
        </p>
        <p>
          A determinant computation shows that the given
          <m>4\times 4</m> matrix is invertible! (In fact this is an instance of the Vandermonde determinant!) By the invertibility theorem,
          for any choice of <m>a,b,c,d</m> there is
          <em>always</em> a solution.
          Thus <m>\range T=M_{22}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define the function <m>T\colon C^\infty(\R)\rightarrow C^\infty(\R)</m> by <m>T(f)=f'</m>.
        <ol>
          <li>
            <p>
              Show <m>T</m> is a linear transformation.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\NS(T)</m> and <m>\range T</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Shown in the notes.
          To repeat: <m>T(cf+dg)=(cf+dg)'=cf'+dg'</m>,
          using familiar properties of the derivative.
        </p>
        <p>
          (b) <m>\NS(T)=\{f\colon f'=0\}=\{\text{ constant functions } \}</m>,
          again using a familiar fact from calculus.
        </p>
        <p>
          I claim <m>\range T=C^\infty(\R)</m>.
          Indeed, any <m>g\in C^\infty(\R)</m> is continuous,
          and thus has an antiderivative by the fundamental theorem of calculus.
          Thus there is a function <m>f</m> such that <m>f'=g</m>.
          Since <m>g</m> is infinitely differentiable,
          it follows that this <m>f</m> is also infinitely differentiable.
          Thus we have found an <m>f\in C^\infty(\R)</m> such that <m>T(f)=f'=g</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>T\colon F(-\infty,\infty)\rightarrow F(-\infty, \infty)</m> by <m>T(f)=g</m>,
        where <m>g(x)=f(x)+f(-x)</m>.
        Show that <m>T</m> is linear,
        and identify <m>\NS(T)</m> and
        <m>\range(T)</m> as certain special named types of functions.
        Make sure to prove your claims.
        <solution>
          <p>
            Linearity:
            <md>
              <mrow>T(cf+dg)\amp =(cf+dg)(x)+(cf+dg)(-x) \amp \text{ (by def) }</mrow>
              <mrow>\amp =c(f(x)+f(-x))+d(g(x)+g(-x)) \amp \text{ (arith) }</mrow>
              <mrow>\amp =cT(f)+dT(g) \amp \text{ (by def) }</mrow>
            </md>
          </p>
          <p>
            <m>T(f)=\boldzero</m> iff <m>f(x)+f(-x)=\boldzero</m> iff <m>f(-x)=-f(x)</m>.
            Thus <m>\NS(T)</m> is the set of odd functions.
          </p>
          <p>
            I claim <m>\range(T)</m> is the space of even functions.
            Indeed, given any <m>g=T(f)=f(x)+f(-x)\in\range(T)</m>,
            we have <m>g(-x)=f(-x)+f(x)=g(x)</m>.
            Thus <m>\range(T)</m> is a subspace of the space of even functions.
            For the other direction,
            given even function <m>g(x)</m>,
            set <m>f(x)=\frac{1}{2}g(x)</m>.
            Then
            <md>
              <mrow>T(f)\amp =\frac{1}{2}g(x)+\frac{1}{2}g(-x)</mrow>
              <mrow>\amp =\frac{1}{2}g(x)+\frac{1}{2}g(x) \amp \text{ (since \(g\) is even) }</mrow>
              <mrow>\amp =g(x)</mrow>
            </md>
          </p>
          <p>
            Thus the space of even functions is a subspace of <m>\range(T)</m>.
            We conclude the two spaces are equal.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{3.4-3.5: linear independence and bases}
  </p>
  <ol>
    <li>
      <p>
        Let <m>V=\R^n</m>.
        Let <m>S=\{\boldv_1,\boldv_2,\dots ,\boldv_r\}</m> be a set of <m>r</m> vectors.
        Prove (using some Gaussian elimination theory) that if <m>r>n</m>,
        then <m>S</m> is linearly dependent.
        In other words: in <m>\R^n</m> you can have <em>at most</em>
        <m>n</m> linearly independent vectors.
        <solution>
          <p>
            Write
            <me>
              \boldv_j=\begin{bmatrix}a_{1j} \\a_{2j} \\ \vdots \\ a_{nj} \end{bmatrix}
            </me>
          </p>
          <p>
            Let <m>A=[a_{ij}]</m> be the matrix obtained by placing
            <m>\boldv_j</m> as the <m>j</m>-th column.
            This is a <m>n\times r</m> matrix, <m>r>n</m>.
          </p>
          <p>
            Now observe that
            <md>
              <mrow>c_1\boldv_1+c_2\boldv_2+\cdots +c_r\boldv_r=\boldzero \amp \Leftrightarrow A\begin{bmatrix} c_1</mrow>
              <mrow>c_2</mrow>
              <mrow>\vdots</mrow>
              <mrow>c_r \end{bmatrix} =\begin{bmatrix}0</mrow>
              <mrow>0</mrow>
              <mrow>\vdots</mrow>
              <mrow>0  \end{bmatrix}</mrow>
              <mrow>\amp \Leftrightarrow (c_1,c_2,\dots, c_r) \text{ is a solution to }  A\boldx=\boldzero</mrow>
            </md>
          </p>
          <p>
            But the linear system corresponding to
            <m>A\boldx=\boldzero</m> has <m>n</m> equations in <m>r</m> unknowns.
            Since <m>r>n</m>, there are more unknowns than equations.
            Gaussian elimination now tells us that this system has a free variable,
            and hence infinitely many solutions.
            In particular, there is a nontrivial solution,
            which proves the vectors <m>\boldv_j</m> are linearly dependent.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=M_{22}</m>,
        and define <m>A_1=\begin{bmatrix}1\amp 1\\1\amp 1 \end{bmatrix}</m>,
        <m>A_2=\begin{bmatrix}0\amp 1\\1\amp 0 \end{bmatrix}</m>,
        <m>A_3=\begin{bmatrix}1\amp 1\\ 1\amp 0 \end{bmatrix}</m>.
      </p>
      <ol>
        <li>
          <p>
            Compute <m>W=\Span(\{A_1,A_2,A_3\})</m>,
            identifying it as a certain <em>familiar</em> set of matrices.
          </p>
        </li>
        <li>
          <p>
            Decide whether <m>S=\{A_1,A_2,A_3\}</m> is independent.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) Using the definition of <m>\Span</m>, we compute
          <md>
            <mrow>\Span\{A_1,A_2,A_3\}\amp =\{c_1A_1+c_2A_2+c_3A_3\colon c_i\in\R\}</mrow>
            <mrow>\amp =\left\{\begin{bmatrix} c_1+c_3\amp c_1+c_2+c_3</mrow>
            <mrow>c_1+c_2+c_3 \amp  c_1\end{bmatrix}\colon c_i\in\R\right\}</mrow>
          </md>.
        </p>
        <p>
          We claim this last set is none other than the set of all symmetric matrices.
          It is clear that each element in the set above is symmetric,
          so it remains only to show that if <m>A</m> is symmetric,
          then <m>A</m> can be written in the above form for some choice of <m>c_1,c_2,c_3</m>.
        </p>
        <p>
          Write <m>A=\begin{bmatrix}a\amp b\\b\amp c \end{bmatrix}</m>.
          We need to find <m>c_1,c_2,c_3</m> such that
          <me>
            \begin{bmatrix}c_1+c_3\amp c_1+c_2+c_3\\ c_1+c_2+c_3 \amp  c_1 \end{bmatrix} =\begin{bmatrix}a\amp b\\b\amp c \end{bmatrix}
          </me>
        </p>
        <p>
          The bottom-right entry tells us that we must pick <m>c_1=c</m>.
          Then the top-left entry tells us that <m>c_3=a-c</m>.
          Lastly, the off diagonal entries tells us that <m>c_2=b-a</m>.
        </p>
        <p>
          Thus we can write <m>A=cA_1+(b-a)A_2+(a-c)A_3</m>,
          showing <m>A\in\Span\{A_1,A_2,A_3\}</m>,
          and proving that <m>\Span\{A_1,A_2,A_3\}=(\text{ set of symmetric matrices) }</m>.
        </p>
        <p>
          (b) From the above description of a general linear combination <m>c_1A_1+c_2A_2+c_3A_3</m>,
          it is clear that his will be the zero matrix iff <m>c_1=c_2=c_3=0</m>.
          Thus <m>S</m> is linearly independent.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A=\begin{bmatrix}1\amp 1\amp 1\amp 1\\ 1\amp 1\amp -1\amp -2 \end{bmatrix}</m>.
        Find a basis for <m>W=\NS(A)</m> and compute its dimension.
        You should first use GE to solve the defining matrix equation of <m>\NS(A)</m>.
        <solution>
          <p>
            Using GE techniques we see that the set of solutions to <m>A\boldx=\boldzero</m> is given by
            <me>
              W=\{(\frac{t}{2}-s ,s, -\frac{3t}{2},t)\colon s,t\in\R\}
            </me>
          </p>
          <p>
            To find a basis for <m>W</m> we express this as a linear combination of two vectors:
            <md>
              <mrow>W\amp =\{(\frac{t}{2}-s ,s, -\frac{3t}{2},t)\colon s,t\in\R\}</mrow>
              <mrow>\amp =\{s(-1,1,0,0)+t\left(\frac{1}{2},0,-\frac{3}{2},1\right)\colon s,t\in\R\}</mrow>
              <mrow>\amp =\Span\left\{\left(-1,1,0,0\right),\left(\frac{1}{2},0,\frac{3}{2},1\right)\right\}</mrow>
            </md>
          </p>
          <p>
            This shows that <m>\boldv_1=(-1,1,0,0)</m> and
            <m>\boldv_2=(1/2,0,-3/2,1)</m> span <m>W=\NS(A)</m>.
            Since they are clearly linearly independent, they form a basis.
          </p>
          <p>
            Since <m>W=\NS(A)</m> has a basis consisting of two vectors,
            we conclude <m>\dim(W)=2</m>.
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_matrixbases">
      <p>
        Let <m>V=M_{nn}</m>.
        For each of the following subspaces <m>W\subset M_{nn}</m>,
        give a basis <m>B</m> of <m>W</m> and compute <m>\dim(W)</m>.
        Justify your answer.  (a) <m>W</m> is the set of upper triangular
        <m>n\times n</m> matrices.  (b) <m>W</m> is the set of symmetric
        <m>n\times n</m> matrices.  (c) <m>W</m> is the set of skew-symmetric <m>n\times n</m> matrices
        (<m>A^T=-A</m>).
        <solution>
          <p>
            In what follows,
            <m>E_{ij}</m> is the matrix with a 1 in the <m>ij</m>-th entry,
            and 0's elsewhere.
            To come up with bases for the listed spaces,
            you should first write down what an arbitrary matrix of the given description looks like,
            then understand this description as a linear combination of some <m>E_{ij}</m>'s.
          </p>
          <p>
            (a) An upper triangular matrix <m>A</m> can have arbitrary entries <m>a_{ij}</m> for all pairs <m>i,j</m> with <m>i\leq j</m>,
            and must have 0's elsewhere.
            That means we can write
            <me>
              A=\sum_{i\leq j}a_{ij}E_{ij}
            </me>
            as a linear combination <em>only</em>
            of the <m>E_{ij}</m> with <m>i\leq j</m>.
            This shows the set
            <me>
              B=\{E_{ij}\colon i\leq j\}
            </me>
            spans <m>W</m>.
            Since it is a subset of the standard basis for <m>M_{nn}</m>,
            <m>B</m> is automatically independent.
            Thus it is a basis.
          </p>
          <p>
            We have <m>\dim(W)=\#B=1+2+\cdots + n=\frac{n(n+1)}{2}</m>.
          </p>
          <p>
            (b) An symmetric matrix <m>A</m> can have arbitrary diagonal entries <m>a_{ii}</m>,
            but once you pick an entry <m>a_{ij}</m> with <m>i\lt j</m>,
            then the corresponding entry <m>a_{ji}</m> is determined since <m>a_{ji}=a_{ij}</m>.
          </p>
          <p>
            Let
            <me>
              S=\{E_{11},\dots ,E_{nn}\}\cup\{D_{ij}\}_{1\leq i\lt j\leq n}
            </me>
            where <m>E_{ii}</m> is exactly as above,
            and <m>D_{ij}</m> is the matrix with a 1 in the <m>ij</m>-th entry,
            a 1 in the <m>ji</m>-th entry, and 0's elsewhere.
            I claim <m>S</m> is a basis.
          </p>
          <p>
            <m>S</m> <em>spans</em>.
            As noted above an arbitrary symmetric matrix <m>A</m> has any coefficients <m>a_{ii}</m> along the diagonal;
            if the <m>ij</m>-th entry is <m>a_{ij}</m>,
            then the <m>ji</m>-th entry is <m>a_{ji}</m>, and thus it is clear that
            <me>
              A=a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i\lt j\leq n}a_{ij}D_{ij}
            </me>.
          </p>
          <p>
            Thus <m>S</m> spans.
          </p>
          <p>
            <m>S</m> <em>is linearly independent</em>.
            Suppose
            <me>
              a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i\lt j\leq n}a_{ij}D_{ij}=0_{nn}
            </me>.
          </p>
          <p>
            From the definition of <m>E_{ii}</m> and <m>D_{ij}</m>,
            if follows that the <m>ij</m>-th entry of the matrix on the LHS is just <m>a_{ij}</m>.
            Thus <m>a_{ij}=0</m> for all <m>i,j</m>.
            This shows <m>S</m> is independent.
          </p>
          <p>
            Finally, The number of elements in <m>S</m> is equal to the number of positions in an
            <m>n\times n</m> matrix on and above the diagonal.
            Counting from the bottom of the matrix up, we see there are
            <me>
              1+2+\cdots +n=\frac{n(n+1)}{2}
            </me>
            of these.
            Thus <m>\dim(W)=\frac{n(n+1)}{2}</m>.
          </p>
          <p>
            (c) Very similar to (b),
            except a skew-symmetric matrix <m>A</m> must <m>a_{ii}=0</m> for all <m>i</m>,
            and <m>a_{ij}=-a_{ji}</m> for all <m>i\lt j</m>.
            Setting <m>S_{ij}</m> to be the matrix with a 1 in the <m>ij</m>-th position,
            a -1 in the <m>ji</m>-th position and 0's elsewhere, we see that
            <me>
              B=\{S_{ij}\colon 1\leq i\lt j\leq n\}
            </me>
            is a basis for <m>W</m> in this case.
            We count the basis in a similar manner to (b) to conclude
            <me>
              \dim(W)=1+2+\cdots +(n-1)=\frac{n(n-1)}{2}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Determine if the vectors are linearly independent or not in <m>\R^3</m>
        <ol>
          <li>
            <p>
              <m>(-3,0,4), (5,-1,2), (1,1,3)</m>
            </p>
          </li>
          <li>
            <p>
              <m>(-2,0,1), (3,2,5), (6,-1,1), (7,0,-2)</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          The relevant system of linear equations has augmented matrix:
          <md>
            <mrow>\begin{bmatrix}-3\amp 5\amp 1\amp 0\\ 0\amp -1\amp 1\amp 0\\ 4\amp 2\amp 3\amp 0 \end{bmatrix} \amp \xrightarrow[]{-1/3r_1}\amp \begin{bmatrix}1\amp -5/3\amp -1/3\amp 0\\ 0\amp -1\amp 1\amp 0\\ 4\amp 2\amp 3\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{4r_1 - r_3}\amp \begin{bmatrix}1\amp -5/3\amp -1/3\amp 0\\ 0\amp -1\amp 1\amp 0\\ 0\amp -26/3\amp -13/3\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-r_2}\amp \begin{bmatrix}1\amp -5/3\amp -1/3\amp 0\\ 0\amp 1\amp -1\amp 0\\ 0\amp -26/3\amp -13/3\amp 0 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{26/3r_2 +r_3}\amp \begin{bmatrix}1\amp -5/3\amp -1/3\amp 0\\ 0\amp 1\amp -1\amp 0\\ 0\amp 0\amp -13\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Thus the only solution to the corresponding system is the trivial one,
          which means the vectors are linearly independent.
        </p>
        <p>
          (b) Consider the matrix
          <me>
            \begin{bmatrix}-2\amp 3\amp 6\amp 7\amp 0\\ 0\amp 2\amp -1\amp 0\amp 0\\ 1\amp 5\amp 1\amp -2\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          When you reduce it, you will have a free variable,
          which means we do not have just the trivial solution.
          Thus the vectors are linearly dependent.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine if the vectors are linearly independent or not in <m>\R^4</m>
        <ol>
          <li>
            <p>
              <m>(3,8,7,-3), (1,5,3,-1), (2,-1,2,6), (4,2,6,4)</m>
            </p>
          </li>
          <li>
            <p>
              <m>(3,0,-3,6), (0,2,3,1), (0,-2,-2,0), (-2,1,2,1)</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Dependent.
          The relevant linear system has augmented matrix
          <me>
            \begin{bmatrix}3\amp 1\amp 2\amp 4\amp 0\\ 8\amp 5\amp -1\amp 2\amp 0\\ 7\amp 3\amp 2\amp 6\amp 0\\ -3\amp -1\amp 6\amp 4\amp 0 \end{bmatrix}
          </me>
          which row reduces to
          <me>
            \begin{bmatrix}1\amp 1/3\amp 2/3\amp 4/3\amp 0\\ 0\amp 7/3\amp -19/3\amp -26/3\amp 0\\ 0\amp 0\amp 3\amp 3\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the fourth column has no leading 1, the corresponding variable is free,
          which means the linear system has nontrivial solutions.
          This in turn implies the vectors are dependent.
        </p>
        <p>
          (b) Independent.
          The relevant linear system has augmented matrix
          <me>
            \begin{bmatrix}3\amp 0\amp 0\amp -2\amp 0\\ 0\amp 2\amp -2\amp 1\amp 0\\ -3\amp 3\amp -2\amp 2\amp 0\\ 6\amp 1\amp 0\amp 1\amp 0 \end{bmatrix}
          </me>
          which row reduces to
          <me>
            \begin{bmatrix}1\amp 0\amp 0\amp -2/3\amp 0\\ 0\amp 1\amp 0\amp 5\amp 0\\ 0\amp 0\amp 1\amp 15/2\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the first four columns are all pivot columns,
          the corresponding linear system has no free variables,
          and hence only the trivial solution.
          This in turn implies the vectors are independent.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine whether the vectors are linearly independent or are linearly dependent in <m>P_2</m>.
        <ol>
          <li>
            <p>
              <m>2-x+4x^2</m>, <m>3+6x+2x^2</m>, <m>2+10x-4x^2</m>
            </p>
          </li>
          <li>
            <p>
              <m>1+3x+3x^2</m>, <m>x+4x^2</m>,
              <m>5+6x+3x^2</m>, <m>7+2x-x^2</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Independent.
          The relevant system of equations arising from looking at linear combinations yielding the zero polynomial has augmented matrix:
          <me>
            \begin{bmatrix}2\amp 3\amp 2\amp 0\\ -1\amp 6\amp 10\amp 0\\ 4\amp 2\amp -4\amp 0 \end{bmatrix}
          </me>
          which row reduces to
          <me>
            \begin{bmatrix}1\amp -6\amp -10\amp 0\\ 0\amp 1\amp 22/15\amp 0\\ 0\amp 0\amp 1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the first three columns have leading 1's, the corresponding system has no free variables,
          and hence only the trivial solution.
          This implies the polynomials are independent.
        </p>
        <p>
          (b) Dependent.
          The relevant system of equations arising from looking at linear combinations yielding the zero polynomial has augmented matrix
          <me>
            \begin{bmatrix}1\amp 0\amp 5\amp 7\amp 0\\ 3\amp 1\amp 6\amp 2\amp 0\\ 3\amp 4\amp 3\amp -1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Gaussian elimination shows the system has a free variable,
          which means there is a nontrivial linear combination yielding the zero polynomial,
          and hence that the polynomials are linearly dependent.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine whether the matrices are linearly independent or dependent.
        <ol>
          <li>
            <p>
              In <m>M_{22}</m>
              <me>
                \begin{bmatrix}1\amp 0\\ 1\amp 2 \end{bmatrix} ; \begin{bmatrix}1\amp 2\\ 2\amp 1 \end{bmatrix} ; \begin{bmatrix}0\amp 1\\ 2\amp 1 \end{bmatrix}
              </me>
            </p>
          </li>
          <li>
            <p>
              In <m>M_{23}</m>
              <me>
                \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix} ; \begin{bmatrix}0\amp 0\amp 1\\ 0\amp 0\amp 0 \end{bmatrix} ; \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 1\amp 0 \end{bmatrix}
              </me>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Independent.
          Looking at linear combinations of the matrices equal to the zero matrix gives rise to the linear system with augmented matrix
          <me>
            \begin{bmatrix}1\amp 1\amp 0\amp 0\\ 0\amp 2\amp 1\amp 0\\ 1\amp 2\amp 2\amp 0\\ 2\amp 1\amp 1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          This row reduces to
          <me>
            \begin{bmatrix}1\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus <m>c_1=c_2=c_3=0</m> is the only linear combination of the matrices yielding
          <m>\boldzero</m> and we can see that the matrices are independent.
        </p>
        <p>
          (b) Independent.
          Consider
          <me>
            k_1 \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix} +k_2 \begin{bmatrix}0\amp 0\amp 1\\ 0\amp 0\amp 0 \end{bmatrix} +k_3 \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 1\amp 0 \end{bmatrix} = \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the first matrix is the only one to contribute to the zero in the <m>a_{11}</m> slot,
          <m>k_1</m> must be zero.
          Since the second matrix is the only one to contribute to the zero in the <m>a_{13}</m> slot,
          <m>k_2</m> must also be zero.
          Similarly, <m>k_3</m> must also be zero.
          This means only the trivial solution exits,
          thus the 3 given matrices are independent.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Determine all values of <m>k</m> for which the following matrices are linearly independent in <m>M_{22}</m>.
        <me>
          \begin{bmatrix}1\amp 0\\ 1\amp k \end{bmatrix} ; \begin{bmatrix}-1\amp 0\\ k\amp 1 \end{bmatrix} ; \begin{bmatrix}2\amp 0\\ 1\amp 3 \end{bmatrix}
        </me>
        <solution>
          <p>
            Looking at linear combinations of the matrices equal to the zero matrix gives rise to the linear system with augmented matrix
            <me>
              \begin{bmatrix}1\amp -1\amp 2\amp 0\\ 0\amp 0\amp 0\amp 0\\ 1\amp k\amp 1\amp 0\\ k\amp 1\amp 3\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            This row reduces to
            <me>
              \begin{bmatrix}1\amp -1\amp 2\amp 0\\ 0\amp -k-1\amp 2k-3\amp 0\\ 0\amp 0\amp 2k-4\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            The system has only the trivial solution iff the first three columns are pivot columns iff <m>k+1\ne 0</m> and
            <m>2k-4\ne 0</m> iff <m>k\ne -1</m> and <m>k\ne 2</m>.
            Thus the matrices are independent for all values of <m>k</m> except for <m>k=-1</m> and <m>k=2</m>.
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_basis">
      <p>
        Show that the following polynomials form a basis for <m>P_3</m>.
        <me>
          p_1=1+x, p_2=1-x, p_3=1-x^2, p_4=1-x^3
        </me>
        <solution>
          <p>
            To prove that the <m>p_i</m> form a basis,
            we must show that for any <m>p(x)=b_0+b_1x+b_2x^2+b_3x^3</m>,
            we there is a <em>unique</em> choice of
            <m>c_1,c_2,\dots,
            c_4</m> satisfying <m>c_1p_1+c_2p_2+c_3p_3+c_4p_4=p(x)</m>.
            After combining like terms and equating like coefficients,
            this boils down to the matrix equation
            <me>
              A\colvec{c_1\\c_2\\c_3\\c_4}=\colvec{b_0\\ b_1\\ b_2\\ b_3}
            </me>
            having a unique solution for any choice of <m>b_i</m>, where
            <me>
              A = \begin{bmatrix}1\amp 1\amp 1\amp 1\\ 1\amp -1\amp 0\amp 0\\ 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 0\amp -1 \end{bmatrix}
            </me>.
          </p>
          <p>
            The invertibility theorem tells us that this is the case if and only if <m>A</m> is invertible if and only if <m>\det A\ne 0</m>.
            We compute
            <me>
              \det(A) = -1\det \begin{bmatrix}1\amp 1\amp 1\\ 1\amp -1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix} = -1\left(-1\det \begin{bmatrix}1\amp 1\\ 1\amp -1 \end{bmatrix} \right) =-1(-1(-1-1)) = -2 \neq 0
            </me>
          </p>
          <p>
            Thus <m>A</m> is invertible,
            and hence the <m>p_i</m> form a basis for <m>P_3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that the following matrices form a basis for <m>M_{22}</m>.
        <me>
          A_1=\begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix} ; \ A_2=\begin{bmatrix}1\amp -1\\ 0\amp 0 \end{bmatrix} ; \ A_3=\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} ; \ A_4=\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix}
        </me>
        <solution>
          <p>
            Reasoning as in <xref ref="ex_basis">Exercise</xref>,
            this boils down to the invertibility of the following matrix:
            <me>
              B= \begin{bmatrix}1\amp 1\amp 0\amp 1\\ 1\amp -1\amp -1\amp 0\\ 1\amp 0\amp 1\amp 0\\ 1\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            We test this using the determinant:
            <md>
              <mrow>\det(B) = -1\det \begin{bmatrix}1\amp 0\amp 1\\ -1\amp -1\amp 0\\ 0\amp 1\amp 0 \end{bmatrix} = -1\left(1\det \begin{bmatrix}1\amp 0\\ -1\amp -1 \end{bmatrix} \right) =-1(1(-1-0)) = 1 \neq 0</mrow>
            </md>
          </p>
          <p>
            Thus <m>B</m> is not invertible,
            which implies the matrices <m>A_i</m> do not form a basis of <m>M_{22}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that the following vectors do not form a basis for <m>P_2</m>
        <me>
          p_1=1-3x+2x^2, p_2=1+x+4x^2, p_3=1 - 7x
        </me>
        <solution>
          <p>
            Reasoning as in <xref ref="ex_basis">Exercise</xref>, this amounts to showing
            <me>
              A = \begin{bmatrix}1\amp 1\amp 1\\ -3\amp 1\amp -7\\ 2\amp 4\amp 0 \end{bmatrix}
            </me>
            is invertible.
            Again, we can test this using the determinant:
            <md>
              <mrow>\det(A) = 2\det \begin{bmatrix}1\amp 1\\ 1\amp -7 \end{bmatrix} -4\det \begin{bmatrix}1\amp 1\\ -3\amp -7 \end{bmatrix} = 2(-7-1) -4(-7+3) = -16 + 16 = 0</mrow>
            </md>
          </p>
          <p>
            Thus <m>A</m> is not invertible,
            and hence the polynomials do not form a basis.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V</m> be the space spanned by <m>\boldv_1 = \cos^2x</m>,
        <m>\boldv_2 = \sin^2x</m>, <m>\boldv_3=\cos2x</m>.
        <ol>
          <li>
            <p>
              Show that <m>S = {\boldv_1,\boldv_2,\boldv_3}</m> is not a basis for <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              Find a basis for <m>V</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          The trig identity <m>\cos^2x - \sin^2x =\cos(2x)</m> provides a linear relation between the elements of <m>S</m>.
          Thus <m>S</m> is not linearly independent,
          and hence does not form a basis.
        </p>
        <p>
          (b) Remove <m>\boldv_3</m>.
          The span remains the same and
          <m>\boldv_1</m> and <m>\boldv_2</m> are independent.
          So
          <me>
            S'=\{\cos^2x,\sin^2x\}
          </me>
          is a basis for <m>V</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=\R^n</m>, considered as column vectors.
        Prove: vectors <m>\boldv_1,\boldv_2,\dots, \boldv_n</m> form a basis of <m>\R^n</m> if and only if the matrix <m>A=\begin{bmatrix}\vert \amp \vert \amp \cdots \amp \vert\\ \boldv_1\amp \boldv_2\amp  \amp \boldv_n\\ \vert \amp \vert \amp \cdots \amp \vert \end{bmatrix}</m> is invertible.
        <solution>
          <p>
            We prove the statement using a <em>chain of equivalences</em>:
            <md>
              <mrow>\amp \{\boldv_1, \boldv_2,\dots, \boldv_n\} \text{ is a basis of \(\R^n\) }</mrow>
              <mrow>\amp \Leftrightarrow \text{ for all }  \boldb\in \R^n \text{ we can write \(\boldb=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n\) in a \emph{unique} way}  \amp \text{ (Theorem 3.15) }</mrow>
              <mrow>\amp \Leftrightarrow \text{ for all \(\boldb\in \R^n\) the matrix equation \(A\boldc=\boldb\) has a unique solution }  \amp \text{ (column method) }</mrow>
              <mrow>\amp \Leftrightarrow \text{ \(A\) is invertible } \amp \text{ (invertibility theorem) }</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>\boldv_1,\boldv_2,\dots, \boldv_n</m> be a basis for <m>\R^n</m>
        (considered as column vectors),
        and let <m>Q</m> be an invertible <m>n\times n</m> matrix.
        Show that the vectors
        <md>
          <mrow>\boldw_1\amp =Q\boldv_1</mrow>
          <mrow>\boldw_2\amp =Q\boldv_2</mrow>
          <mrow>\ \vdots</mrow>
          <mrow>\boldw_n\amp =Q\boldv_n</mrow>
        </md>
        form a basis for <m>\R^n</m>.
        <solution>
          <p>
            Let <m>A=\begin{bmatrix}\vert \amp \vert \amp \cdots \amp \vert\\ \boldv_1\amp \boldv_2\amp \amp \boldv_n\\ \vert \amp \vert \amp \cdots \amp \vert \end{bmatrix}</m>.
          </p>
          <p>
            Then <m>A</m> is invertible, by the previous exercise.
            Let <m>B=QA</m>.
            Using the column method of multiplication, we have
            <me>
              B=QA=\begin{bmatrix}\vert \amp \vert \amp \cdots \amp \vert\\ Q\boldv_1\amp Q\boldv_2\amp  \amp Q\boldv_n\\ \vert \amp \vert \amp \cdots \amp \vert \end{bmatrix} =\begin{bmatrix}\vert \amp \vert \amp \cdots \amp \vert\\ \boldw_1\amp \boldw_2\amp  \amp \boldw_n\\ \vert \amp \vert \amp \cdots \amp \vert \end{bmatrix}
            </me>
          </p>
          <p>
            Since <m>Q</m> and <m>A</m> are invertible, so is <m>B</m>.
            The previous exercise then implies that the <m>\boldw_i</m> form a basis.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>V = \Span\{\boldv_1,\dots ,\boldv_n\}</m>,
              then <m>\{\boldv_1,\dots ,\boldv_n\}</m> is a basis for <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              Every linearly independent subset of a vector space <m>V</m> is a basis for <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\{\boldv_1, \boldv_2, \dots , \boldv_n\}</m> is a basis for a vector space <m>V</m>,
              then every vector in <m>V</m> can be expressed as a linear combination of <m>\boldv_1, \boldv_2, \dots , \boldv_n</m>.
            </p>
          </li>
          <li>
            <p>
              Every basis of <m>P_4</m> contains at least one polynomial of degree 3 or less.
            </p>
          </li>
          <li>
            <p>
              There is a basis for <m>M_{22}</m> consisting of invertible matrices.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          A basis must also be linearly independent.
          Counter example:
          <m>V = R^2</m> and the vectors <m>\{(1,0),(0,1),(1,1)\}</m>.
          Then <m>span\{(1,0),(0,1),(1,1)\} = R^2</m> but since
          <m>(1,0) + (0,1) = (1,1)</m> the vector set is not independent and therefore not a basis.
        </p>
        <p>
          (b) False.
          The vector set must span <m>V</m>.
          Consider <m>V = R^3</m> and the vector set <m>\{(1,0,0),(0,1,0)\}</m>.
          Clearly the set is independent,
          but it cannot span <m>V</m>.
        </p>
        <p>
          (c) True.
          A basis spans <m>V</m> by definition.
        </p>
        <p>
          (d) False.
          Consider the vector set
          <me>
            1+x+x^2+x^3+x^4, x+x^2+x^3+x^4, x^2+x^3+x^4, x^3+x^4, x^4
          </me>
        </p>
        <p>
          The degree of each polynomial is 4.
          To show this set forms a basis,
          we set up the usual linear combination equation <m>c_1p_1+c_2p_2+c_3p_3+c_4p_4=p</m>,
          where <m>p</m> is an arbitrary polynomial,
          which boils down to a matrix equation of the form
          <me>
            A\colvec{c_1\\ c_2\\ c_3\\ c_4\\c_5}=\colvec{a_0\\ a_1\\ a_2\\ a_3\\a_4} \text{ (\(A\boldc=\bolda\)) }
          </me>
          where
          <me>
            A = \begin{bmatrix}1\amp 0\amp 0\amp 0\amp 0\\ 1\amp 1\amp 0\amp 0\amp 0\\ 1\amp 1\amp 1\amp 0\amp 0\\ 1\amp 1\amp 1\amp 1\amp 0\\ 1\amp 1\amp 1\amp 1\amp 1 \end{bmatrix}
          </me>.
        </p>
        <p>
          The set forms a basis if and only if this equation has a unique solution for each <m>\bolda=(a_0,a_1,a_2,a_3,a_4)</m>,
          if and only if <m>A</m> is invertible.
          Since <m>\det A=1</m>
          (lower triangular matrix),
          we conclude <m>A</m> is invertible,
          and hence the given polynomials form a basis.
        </p>
        <p>
          (e) True.
          You can check for yourself that the following matrices are invertible and form a basis:
          <me>
            \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} , \begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix} , \begin{bmatrix}1\amp 0\\ 1\amp 1 \end{bmatrix} , \begin{bmatrix}1\amp 1\\ 1\amp 0 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{3.6: dimension}
  </p>
  <ol>
    <li>
      <p>
        Prove the following statement from the
        <q>street smarts theorem</q>.   Let <m>V</m> be a vector space with basis <m>B=\{\boldv_1,\dots, \boldv_n\}</m>.
        Then any collection of <m>r</m> vectors,
        with <m>r>n</m>, is linearly dependent.
        Hint: suppose <m>S=\{\boldw_1,\boldw_2,\dots ,\boldw_r\}</m> with <m>r>n</m>.
        Begin by setting up the vector equation <m>c_1\boldw_1+c_2\boldw_2+\cdots +c_r\boldw_r=\boldzero_V</m>;
        then write each <m>\boldw_j</m> in this equation as a linear combination of the
        <m>\boldv_i</m> (possible since <m>\boldv_i</m> span);
        then collect like terms and use the fact that the
        <m>\boldv_i</m> are linearly independent.
        <solution>
          <p>
            Since <m>B</m> spans we can write each
            <me>
              \boldw_j=a_{1j}\boldv_1+a_{2j}\boldv_2+\cdots+ a_{nj}\boldv_{nj}=\sum_{i=1}^na_{ij}\boldv_i
            </me>.
          </p>
          <p>
            Now, we have {
            <md>
              <mrow>c_1\boldw_1+c_2\boldw_2\cdots +c_r\boldw_r=\boldzero_V\amp \Leftrightarrow c_1\sum_{i=1}^na_{i1}\boldv_i+c_2\sum_{i=1}^na_{i2}\boldv_i\cdots +c_r\sum_{i=1}^na_{ir}\boldv_i=\boldzero</mrow>
              <mrow>\amp \Leftrightarrow (\sum_{j=1}^ra_{1j}c_j)\boldv_1+(\sum_{j=1}^ra_{2j}c_j)\boldv_2+\cdots +(\sum_{j=1}^ra_{nj}c_j)\boldv_n=\boldzero \amp \text{ (grouping like terms) }</mrow>
              <mrow>\amp \Leftrightarrow (a_{i1}c_1+a_{i2}c_{2}+\cdots a_{ir}c_r)=0 \text{ for all \(i\), }  \amp \text{ (\(\boldv_i\) are lin. ind.) }</mrow>
              <mrow>\amp \Leftrightarrow (c_1,\dots ,c_r) \text{ is a solution to system }</mrow>
              <mrow>\amp \begin{linsys}{4} a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\amp \cdots \amp +\amp a_{1r}x_r\amp =\amp 0</mrow>
              <mrow>a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\amp \cdots \amp +\amp a_{2r}x_r\amp =\amp 0</mrow>
              <mrow>\amp \vdots\amp  \amp \vdots\amp  \amp \vdots\amp</mrow>
              <mrow>a_{n1}x_{1}\amp +\amp a_{n2}x_{2}\amp +\amp \cdots \amp +\amp a_{nr}x_r\amp =\amp 0 \end{linsys}</mrow>
            </md>
          </p>
          <p>
            } Now this is a homogenous linear system of <m>n</m> equations in <m>r</m> unknowns,
            where <m>r>n</m>.
            Gaussian elimination theory tells us that this has a nontrivial solution.
            It follows that the <m>\boldw_i</m> are linearly dependent.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each of the following choices of a vector space <m>V</m> and subset
        <m>S\subseteq V</m> decide whether <m>S</m> is a basis for <m>V</m>.
        Keep an eye out for potential shortcuts using the dimension theorem compendium.   (a) <m>V=P_3</m>,
        <m>S=\{1-x^3, x-x^3, x^2-x^3\}</m>.   (b) <m>V=\R^3</m>,
        <m>S=\{(1,2,3), (2,5,6), (1,-1,3)\}</m>.
        <solution>
          <p>
            (a) The set <m>S</m> is guaranteed not to span <m>P_3</m>.
            Indeed, we have <m>\dim P_3=4</m>, <m>\#S=3</m>.
            Street smarts! (Dimension theorem compendium.)
          </p>
          <p>
            (b) Since <m>\# S=\dim\R^3=3</m>,
            the dimension theorem compendium asserts that <m>S</m> is a basis iff <m>S</m> is linearly independent iff <m>S</m> spans.
            This means I need only check one of the two usual conditions to being a basis:
            linear independence, or span.
          </p>
          <p>
            Let's check whether <m>S</m> is linearly independent.
            We have
            <md>
              <mrow>c_1(1,2,3)+c_2(2,5,6)+c_3(1,-1,3)=(0,0,0) \amp \Longleftrightarrow \begin{bmatrix}[rrr] 1\amp 2\amp 1</mrow>
              <mrow>2\amp 5\amp -1</mrow>
              <mrow>3\amp 6\amp 3 \end{bmatrix} \begin{bmatrix} c_1</mrow>
              <mrow>c_2</mrow>
              <mrow>c_3 \end{bmatrix}=\begin{bmatrix} 0</mrow>
              <mrow>0</mrow>
              <mrow>0 \end{bmatrix}</mrow>
            </md>.
          </p>
          <p>
            Let <m>A=\begin{bmatrix}1\amp 2\amp 1\\ 2\amp 5\amp -1\\3\amp 6\amp 3 \end{bmatrix}</m>.
            We see via simple computation that
            <m>\det A=0</m> and thus that <m>A</m> is not invertible.
            The invertibility theorem then implies that there is a nontrivial solution
            <m>(c_1,c_2,c_3)\ne (0,0,0)</m> to the matrix equation,
            and hence a nontrivial linear combination of the vectors yielding <m>\boldzero</m>.
            This proves <m>S</m> is not linearly independent, and hence not a basis.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        In multivariable calculus,
        a plane in <m>\R^3</m> is defined as the set of solutions to an equation of the form <m>ax+by+cz=d</m>,
        where at least one of <m>a, b, c</m> is nonzero.
        In particular,
        a plane passing through the origin <m>(0,0,0)</m> is the set of solutions to an equation of the form <m>ax+by+cz=0</m>,
        where at least one of <m>a, b, c</m> is nonzero.
      </p>
      <ol>
        <li>
          <p>
            Show that a plane passing through the origin is a subspace of <m>\R^3</m> of dimension 2;
            conversely, show that a subspace of <m>\R^3</m> of dimension 2 is a plane passing through the origin.
            In other words,
            show that the set of all planes passing through the origin is precisely the set of all 2-dimensional subspaces of <m>\R^3</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>\mathcal{P}\colon ax+by+cz+d</m> be any plane in <m>\R^3</m>,
            and let <m>P=(x_0,y_0,z_0)</m> be a point of <m>\mathcal{P}</m>.
            Show that there is a 2-dimensional subspace
            <m>W\subseteq \R^3</m> such that <m>\mathcal{P}=P+W:=\{(x_0,y_0,z_0)+\boldw\colon \boldw\in W\}</m>.
            The set <m>P+W</m> is called the
            <term>translate of <m>W</m> by <m>P</m></term>.
            This result shows that all planes in <m>\R^3</m> are translates of 2-dimensional subspaces.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) Let <m>\mathcal{P}</m> be the plane defined by <m>ax+by+cz=0</m>.
          Gaussian elimination allows us to describe the points of <m>\mathcal{P}</m> parametrically.
          For example, if <m>a\ne 0</m>,
          then <m>\mathcal{P}=\{(-(bs+ct)/a, s, t)\colon s, t\}=\Span\{(-b/a, 1,0),(-c/a,0,1)\}</m>.
          The set <m>\{\{(-b/a, 1,0),(-c/a,0,1)\}</m> is clearly linearly independent:
          the vectors are not scalar multiples of one another.
          Thus <m>\mathcal{P}</m> is a subspace of dimension 2 in this case.
          The cases <m>b\ne 0</m> and <m>c\ne 0</m> are dealt with similarly.
          In all cases,
          we see that <m>\mathcal{P}</m> is a subspace of dimension 2.
        </p>
        <p>
          Going the other way, suppose
          <m>W\subseteq \R^3</m> is a 2-dimensional subspace.
          By definition of dimension,
          <m>W</m> has a a basis <m>\{(x_1,y_1,z_1), (x_2, y_2, z_2)\}</m> consisting of two linearly independent vectors.
          I claim that there is a nonzero vector <m>(a,b,c)</m> for which
          <me>
            ax_1+by_1+cz_1=ax_2+by_2+cz_3=0
          </me>.
        </p>
        <p>
          To see this, consider the corresponding matrix equation
          <me>
            \begin{bmatrix}x_1\amp y_1\amp z_1\\ x_2\amp y_2\amp z_2 \end{bmatrix} \begin{bmatrix}a\\ b\\ c \end{bmatrix} =\begin{bmatrix}0 \\ 0 \end{bmatrix}
          </me>.
        </p>
        <p>
          Thinking of <m>a, b, c</m> as the unknowns here,
          we see that we have a homogenous linear system with more unknowns than equations.
          It follows that there is a nontrivial solution <m>(a,b,c)\ne (0,0,0)</m> to this equation.
        </p>
        <p>
          Fix one such solution <m>(a,b,c)</m>,
          and let <m>\mathcal{P}</m> be the plane with defining equation <m>ax+by+cz=0</m>.
          We showed above that <m>\mathcal{P}</m> is a 2-dimensional subspace.
          Since <m>(x_1,y_1,z_1)</m> and
          <m>(x_2,y_2,z_2)</m> both satisfy this equation we must have <m>W=\Span \{(x_1,y_1,z_1), (x_2, y_2, z_2)\}\subseteq\mathcal{P}</m>.
          Lastly, since <m>\dim W=\dim\mathcal{P}=2</m>,
          we conclude <m>W=\mathcal{P}</m>
          (dimension theorem compendium).
        </p>
        <p>
          (b) This claim follows from part (a) and the fact that an arbitrary plane
          <m>\mathcal{P}\colon ax+by+cz=d</m> can be obtained by taking the parallel plane
          <m>\mathcal{P}_0\colon ax+by+cz=0</m> that passes through the origin,
          and translating it by <m>P=(x_0,y_0,z_0)</m>,
          where <m>P</m> is any point in <m>\mathcal{P}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=M_{33}</m>,
        <m>W=\{A\in M_{33}\colon A^T=-A\}</m> (i.e., the subspace of skew-symmetric matrices),
        and  <m>W'=\Span\left\{A_1=\begin{bmatrix}0\amp 1\amp 2\\ -1\amp 0\amp 1\\ -2\amp -1\amp 0 \end{bmatrix} , A_2=\begin{bmatrix}0\amp 1\amp -1\\ -1\amp 0\amp 1\\ 1\amp -1\amp 0 \end{bmatrix} , A_3=\begin{bmatrix}0\amp 1\amp 0\\ -1\amp 0\amp -1\\ 0\amp 1\amp 0 \end{bmatrix} \right\}</m> Show that <m>W'=W</m> as follows:   (a) Show that
        <m>W'\subseteq W</m> (easy);   (b) Compute the dimensions of <m>W'</m> and <m>W</m> and use the dimension theorem
        (compendium).
        <solution>
          <p>
            (a) Since each <m>A_i</m> is clearly skew-symmetric,
            then any linear combination of the <m>A_i</m>'s is skew-symmetric.
            Thus <m>W'=\Span(\{A_1,A_2,A_3\})\subseteq W</m>.
          </p>
          <p>
            (b) Since <m>W=\left\{\begin{bmatrix}0\amp a\amp b\\ -a\amp 0\amp c\\ -b\amp -c\amp 0 \end{bmatrix} \colon a,b,c\in \R\right\}</m>,
            we see essentially by inspection that
            <me>
              B=\left\{ \begin{bmatrix}0\amp 1\amp 0\\ -1\amp 0\amp 0\\ 0\amp 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 1\\ 0\amp 0\amp 0\\ -1\amp 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 1\\ 0\amp -1\amp 0 \end{bmatrix} \right\}
            </me>
            is basis for <m>W</m>, and hence that <m>\dim W=3</m>.
          </p>
          <p>
            Since <m>W'\subseteq W</m>,
            it follows that <m>W=W'</m> iff
            <m>\dim W'=3</m>, by the dimension theorem
            (compendium).
            To show <m>\dim W'=3</m>, we need only show that the given matrices,
            which span <m>W'</m> by definition,
            are linearly independent and hence a basis.
            This is done in the usual way.
            I leave it to you.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find a basis for the solution space of the homogeneous linear system,
        and find the dimension of that space.
        <md>
          <mrow>x_1-4x_2+3x_3-x_4 \amp =\amp  0</mrow>
          <mrow>2x_1 - 8x_2 +6x_3 -2x_4 \amp =\amp 0</mrow>
        </md>
        <solution>
          <p>
            Let <m>W</m> be the set of solutions to this homogenous system of equations.
            The augmented matrix we obtain to determine <m>W</m> is
            <me>
              \begin{bmatrix}1\amp -4\amp 3\amp -1\amp 0\\ 2\amp -8\amp 6\amp -2\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Since row two is just 2 times row one, the reduced matrix is:
            <me>
              \begin{bmatrix}1\amp -4\amp 3\amp -1\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Writing the solution with free variables <m>x_2= t,x_3=r,x_4=s</m>
            <me>
              x_4 = s, x_3 = r, x_2 = t, x_1 = 4t-3s+r
            </me>
          </p>
          <p>
            Writing the solution in vector form gives:
            <md>
              <mrow>(x_1,x_2,x_3,x_4) \amp = (4t-3s+r,t,s,r)\tag{$*$}</mrow>
              <mrow>\amp = t(4,1,0,0) + s(-3,0,1,0) + r(1,0,0,1)\tag{$**$}</mrow>
            </md>
          </p>
          <p>
            Thus the vectors <m>\{(4,1,0,0),(-3,0,1,0),(1,0,0,1)\}</m> span the vector space.
            For linear independence, note that
            <me>
              t(4,1,0,0) + s(-3,0,1,0) + r(1,0,0,1)=(0,0,0,0)\Rightarrow (4t-3s+r,t,s,r)=(0,0,0,0)\Rightarrow r=s=t=0
            </me>.
          </p>
          <p>
            This shows the three vectors form a basis of <m>W</m>,
            and hence that <m>\dim W=3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find a basis for the solution space of the homogeneous linear system,
        and find the dimension of that space.
        <md>
          <mrow>x+y+z\amp =\amp  0</mrow>
          <mrow>3x+2y-2z\amp =\amp 0</mrow>
          <mrow>4x+3y-z \amp =\amp  0</mrow>
          <mrow>6x+5y+z \amp =\amp  0</mrow>
        </md>
        <solution>
          <p>
            The matrix we obtain to find the basis is:
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 3\amp 2\amp -2\amp 0\\ 4\amp 3\amp -1\amp 0\\ 6\amp 5\amp 1\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Which reduces to the matrix:
            <me>
              \begin{bmatrix}1\amp 0\amp -4\amp 0\\ 0\amp 1\amp 5\amp 0\\ 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Writing the solution with free variable <m>x_3 = r</m>
            <me>
              x_3 = r, x_2 = -5r, x_1 = 4r
            </me>
          </p>
          <p>
            Writing the solution in vector form gives:
            <md>
              <mrow>(x_1,x_2,x_3) \amp =\amp  (4r,-5,r)</mrow>
              <mrow>\amp =\amp  r(4,-5,1)</mrow>
            </md>
          </p>
          <p>
            Thus the vector <m>\{(4,-5,1)\}</m> spans the vector space,
            and by the remark at the end of example 3 on page 223,
            they also form a basis for the vector space.
            Since the basis has 1 vector,
            the dimension of the vector space is 1.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let
        <me>
          S=\{\boldv_1=(1,1,1,1), \boldv_2=(2,2,2,0) \}\subseteq\R^4
        </me>.
        Enlarge <m>S</m> into a basis of <m>\R^4</m>.
        <solution>
          <p>
            We will have a systematic way of doing this later.
            For now we proceed in the spirit of the proof that every linearly independent set can be extended to a basis.
          </p>
          <p>
            Namely, we search for a vector
            <m>\boldv\in \R^4</m> such that <m>\boldv\notin\Span S</m>.
            Consider <m>\boldv_3=(1,0,0,0)</m>.
            A simple application of Gaussian elimination shows that
            <m>(1,0,0,0)</m> is not a linear combination of <m>\boldv_1</m> and <m>\boldv_2</m>.
            Thus <m>S'=\{\boldv_1,\boldv_2,\boldv_3\}</m> is linearly independent.
          </p>
          <p>
            Next, I claim <m>\boldv_4=(0,1,0,0)</m> is not in <m>\Span S'</m>.
            Again you can confirm this with a Gaussian elimination argument.
            We conclude that
            <me>
              S''=\{(1,1,1,1), (2,2,2,0),(1,0,0,0), (0,1,0,0)\}
            </me>
            is linearly independent.
            Since <m>\#S''=\dim\R^4=4</m>,
            we conclude <m>S''</m> is a basis for <m>\R^4</m> extending our original set <m>S</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute bases and dimensions for the following subspaces of <m>\R^4</m>.
        <ol>
          <li>
            <p>
              <m>W=\{(a,b,c,d)\in\R^4\colon d=a+b, c=a-b\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>W=\{(a,b,c,d)\in \R^4\colon a+b=c+d\}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Both spaces can be described as null spaces of a linear transformation.
        </p>
        <p>
          In (a) we have
          <me>
            W=\{(a,b,c,d)\in\R^4\colon d=a+b, c=a-b\}=\{(a,b,c,d)\in\R^4\colon a+b-d=0 \text{ and }  a-b-c=0\}
          </me>.
        </p>
        <p>
          Thus <m>W=\NS \begin{bmatrix}1\amp 1\amp 0\amp -1\\ 1\amp -1\amp -1\amp 0 \end{bmatrix}</m>.
          After some Gaussian elimination,
          we see that <m>W=\{(\frac{1}{2}(s+t), \frac{1}{2}(-s+t),
          s, t)\colon s,t\in\R\}</m>.
          From this parametric description,
          we can then easily extract the basis <m>\{(1/2, -1/2, 1, 0), (1/2, 1/2, 0, 1) \}</m>.
          We conclude <m>\dim W=2</m>.
        </p>
        <p>
          In (b) we have <m>W=\{(a,b,c,d)\in\R^4\colon a+b-c-d=0\}=\NS [1 1 -1 -1]</m>.
          Again Gaussian elimination tells us that
          <me>
            W=\{(-r+s+t, r, s, t)\colon r,s,t\in\R\}=\Span\underset{S}{\underbrace{\{(-1,1,0,0), (1,0,1,0), (1,0,0,1)\}}}=\Span S
          </me>.
        </p>
        <p>
          The spanning set <m>S</m> on the right is easily seen to be linearly independent:
          <m>a(-1,1,0,0)+b(1,0,1,0)+c(1,0,0,1)=(-a+b+c,a, b, c)=(0,0,0)\Rightarrow a=b=c=0</m>.
          Thus <m>S</m> is a basis for <m>W</m> and we conclude <m>\dim W=3</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=M_{nn}</m>, <m>W_1</m> the set of diagonal matrices,
        <m>W_2</m> the set of symmetric matrices,
        <m>W_3</m> the set of upper triangular matrices.
        Compute the dimensions of all these spaces.
        You may use your results from <xref ref="ex_matrixbases">Exercise</xref> in Section 3.3-3.4.
      </p>
      <solution>
        <p>
          <ol>
            <li>
              <p>
                <m>W_1</m>, the set of diagonal matrices.
                Let
                <me>
                  S=\{E_{11}, E_{22}, \dots , E_{nn}\}
                </me>,
                where as usual <m>E_{ij}</m> denotes the matrix with a 1 in the <m>ij</m>-th entry,
                and 0's elsewhere.
                I claim <m>S</m> is a basis of <m>W_1</m>,
                and hence that <m>\dim(W_1)=n</m>.
                <m>S</m> <em>spans</em>.
                An arbitrary diagonal matrix <m>A</m> has entries <m>a_ii</m> along the diagonal and 0 elsewhere.
                Then clearly
                <me>
                  A=a_{11}E_11+\cdots +a_{nn}E_nn
                </me>.
                This shows <m>S</m> spans <m>W_1</m>.
                <m>S</m> <em>is linearly independent</em>.
                Note that <m>S</m> is a subset of the standard basis for <m>M_{nn}</m>.
                Since the standard basis is linearly independent,
                any subset of it is also linearly independent.
                Thus <m>S</m> is independent.
              </p>
            </li>
            <li>
              <p>
                <m>W_2</m>, the set of symmetric matrices.
                Let
                <me>
                  S=\{E_{11},\dots ,E_{nn}\}\cup\{F_{ij}\}_{1\leq i\lt j\leq n}
                </me>
                where <m>E_{ii}</m> is exactly as above,
                and <m>F_{ij}</m> is the matrix with a 1 in the <m>ij</m>-th entry,
                a 1 in the <m>ji</m>-th entry, and 0's elsewhere.
                I claim <m>S</m> is a basis.
                <m>S</m> <em>spans</em>.
                Similar argument as above.
                An arbitrary symmetric matrix <m>A</m> has any coefficients <m>a_{ii}</m> along the diagonal;
                if the <m>ij</m>-th entry is <m>a_{ij}</m>,
                then the <m>ji</m>-th entry is <m>a_{ji}</m>, and thus it is clear that
                <me>
                  A=a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i\lt j\leq n}a_{ij}F_{ij}
                </me>.
                Thus <m>S</m> spans.
                <m>S</m> <em>is linearly independent</em>.
                Suppose
                <me>
                  a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i\lt j\leq n}a_{ij}F_{ij}=0_{nn}
                </me>.
                From the definition of <m>E_{ii}</m> and <m>F_{ij}</m>,
                if follows that the <m>ij</m>-th entry of the matrix on the LHS is just <m>a_{ij}</m>.
                Thus <m>a_{ij}=0</m> for all <m>i,j</m>.
                This shows <m>S</m> is independent.
                Finally, The number of elements in <m>S</m> is equal to the number of positions in an
                <m>n\times n</m> matrix on and above the diagonal.
                Counting from the bottom of the matrix up, we see there are
                <me>
                  1+2+\cdots +n=\frac{n(n+1)}{2}
                </me>
                of these.
                Thus <m>\dim(W_2)=\frac{n(n+1)}{2}</m>.
              </p>
            </li>
            <li>
              <p>
                <m>W_3</m>, the set of upper triangular matrices.
                The argument is very similar to the above arguments.
                The set
                <me>
                  S=\{E_{ij}\}_{1\leq i\leq j\leq n}
                </me>
                is a basis.
                This set also has <m>\frac{n(n+1)}{2}</m> elements.
                Thus <m>\dim(W_3)=\frac{n(n+1)}{2}</m>.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>\{\boldv_1,\boldv_2,\boldv_3\}</m> be a basis for a vector space <m>V</m>.
        Show that <m>\{\boldu_1,\boldu_2,\boldu_3\}</m> is also a basis,
        where <m>\boldu_1 = \boldv_1</m>,
        <m>\boldu_2 = \boldv_1 + \boldv_2</m>,
        and <m>\boldu_3 = \boldv_1 +\boldv_2 + \boldv_3</m>.
        <solution>
          <p>
            Since <m>\{\boldv_1,\boldv_2,\boldv_3\}</m> is a basis for <m>V</m>,
            any other basis for <m>V</m> must also have exactly 3 vectors.
            Since the set <m>\{\boldu_1,\boldu_2,\boldu_3\}</m> also has 3 vectors,
            then by the dimension theorem (compendium),
            we only need to show that this set spans <m>V</m> or that it is linearly independent.
            Let's show that it is linearly independent.
            If we can show that if <m>c_1\boldu_1 + c_2\boldu_2 + c_3\boldu_3 = 0</m>,
            then <m>c_1=c_2=c_3=0</m>, we will be done.
            <md>
              <mrow>c_1\boldu_1 + c_2\boldu_2 + c_3\boldu_3=0\amp \Rightarrow c_1\boldv_1 + c_2(\boldv_1+\boldv_2) + c_3(\boldv_1+\boldv_2+\boldv_3)=0</mrow>
              <mrow>\amp \Rightarrow (c_1+c_2+c_3)\boldv_1 + (c_2+c_3)\boldv_2 + c_3\boldv_3 = 0</mrow>
              <mrow>\amp \Rightarrow (c_1+c_2+c_3)=(c_2+c_3)=c_3=0 \amp \text{ (since \(\boldv_i\) are independent) }</mrow>
              <mrow>\amp \Rightarrow c_3=c_2=c_1=0 \amp \text{ (work backwards) }</mrow>
            </md>
          </p>
          <p>
            This shows the <m>\boldu_i</m> are independent,
            and hence constitute a basis.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=M_{23}</m> and define
        <me>
          W=\{A\in M_{23}\colon \text{ rows and columns of \(A\) sum to 0 } \}
        </me>.
        Find a basis for <m>W</m> and compute its dimension.
        <solution>
          <p>
            We can proceed by inspection.
            The general element of <m>W</m> looks like
            <me>
              A=\begin{bmatrix}r\amp s\amp -(r+s)\\ -r\amp -s\amp (r+s) \end{bmatrix} =r\begin{bmatrix}1\amp 0\amp -1\\ -1\amp 0\amp 1 \end{bmatrix} +s\begin{bmatrix}0\amp 1\amp -1\\ 0\amp -1\amp 1 \end{bmatrix}
            </me>.
          </p>
          <p>
            From this description it is obvious that
            <me>
              B=\left\{ \begin{bmatrix}1\amp 0\amp -1\\ -1\amp 0\amp 1 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp -1\\ 0\amp -1\amp 1 \end{bmatrix} \right\}
            </me>.
            spans <m>W</m>.
            It is also fairly clear that the given matrices are linearly independent
            (look at the relevant matrix equation yourself).
            Thus <m>B</m> is a basis.
          </p>
          <p>
            Since <m>B</m> has 2 elements, we have <m>\dim(W)=2</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>C^\infty(-\infty,\infty)</m> and consider the subspace
        <me>
          W=\Span(\{\cos(2x),\sin(2x), \sin(x)\cos(x), \cos^2(x),\sin^2(x) \})
        </me>.
        Compute the dimension of <m>W</m> by providing an explicit basis.
        <solution>
          <p>
            Some trig identities:
            <md>
              <mrow>\cos(2x)\amp =\cos^2(x)-\sin^2(x)</mrow>
              <mrow>\sin(2x)\amp =2\sin(x)\cos(x)</mrow>
            </md>.
          </p>
          <p>
            These identities show us that <m>\cos(2x)</m> and
            <m>\sin(2x)</m> are in the span of <m>\{\cos^2(x),\sin^2(x),\sin(x)\cos(x)\}</m>,
            and thus that <m>W=\Span(\{\cos^2(x),\sin^2(x),\sin(x)\cos(x)\}</m>.
            Is this new set linearly independent?
          </p>
          <p>
            Suppose we have
            <me>
              k_1\cos^2(x)+k_2\sin^2(x)+k_3\sin(x)\cos(x)=0
            </me>
            for all <m>x</m>.
          </p>
          <p>
            Evaluating at <m>x=0</m> yields <m>k_1=0</m>.
          </p>
          <p>
            Evaluating at <m>x=\pi/2</m> yields <m>k_2=0</m>.
          </p>
          <p>
            Thus the identity becomes <m>k_3\sin(x)\cos(x)=0</m>,
            which is true for all <m>x</m> if and only if <m>k_3=0</m>.
          </p>
          <p>
            This shows that <m>\{\cos^2(x),\sin^2(x),\sin(x)\cos(x)\}</m> is linearly independent.
            Since it also spans <m>W</m>,
            it is a basis of <m>W</m>.
            Since <m>W</m> has a basis consisting of 3 elements,
            we conclude <m>\dim(W)=3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A\in M_{nn}</m>.
        Show that there is a nonzero polynomial <m>p(x)=a_{n^2}x^{n^2}+a_{n^2-1}x^{n^2-1}+\cdots +a_1x+a_0</m> such that
        <me>
          p(A)=a_{n^2}A^{n^2}+a_{n^2-1}A^{n^2-1}+\cdots +a_1A+a_0I_n=\underset{n\times n}{\boldzero}
        </me>.
        Hint: consider the set <m>S=\{I_n, A, A^2, \dots, A^{n^2}\}\subseteq M_{nn}</m> and use dimension theory.
        <solution>
          <p>
            We are tempted to quote the dimension theorem compendium:
            <m>\dim M_{nn}=n^2</m>, <m>\#S=n^2+1</m>,
            thus <m>S</m> is linearly dependent.
            This would then imply there is a linear combination of the elements of <m>S</m> equal to 0:
            i.e., there is a nontrivial choice of <m>a_i</m> such that <m>a_{n^2}A^{n^2}+a_{n^2-1}A^{n^2-1}+\cdots +a_1A+a_0I_n=\underset{n\times n}{\boldzero}</m>,
            as desired.
          </p>
          <p>
            The problem with this reasoning is that we cannot be sure that <m>\#S=n+1</m> !! In other words we might have <m>A^i=A^k</m> for some pair <m>0\leq i\lt j\leq n^2</m>.
            But if this is the case,
            then we have <m>A^j-A^i=\underset{n\times n}{\boldzero}</m>,
            and thus <m>p(A)=\underset{n\times n}{\boldzero}</m>,
            where <m>p(x)=x^j-x^i</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove that <m>V=C^\infty(\R)</m> is infinite dimensional.
        Hint: use a proof by contradiction.
        More specifically, assume <m>\dim V=n</m> for some <m>n</m>,
        and try and derive a contradiction to one of our dimension theorem compendium statements.
        <solution>
          <p>
            By contradiction.
            Assume <m>\dim V=n</m> for some <m>n\lt \infty</m>.
            Consider the set <m>S=\{f_0, f_1, f_2,\dots,
            f_{n}\}\subseteq C^\infty(\R)</m> where <m>f_j(x)=x^j</m>.
            The set <m>S</m> contains <m>n+1</m> elements,
            and is linearly independent:
            indeed it is the standard basis of <m>P_{n}</m>.
            This contradicts the dimension theorem compendium:
            a set containing more than <m>n</m> elements in an <m>n</m>-dimensional space is guaranteed to be linearly dependent!
            Thus we cannot have <m>\dim V=n</m> for any <m>n</m>.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{4.1-4.2: linear transformations, bases, rank-nullity}
  </p>
  <ol>
    <li>
      <p>
        Suppose <m>T\colon\R^2\rightarrow \R^3</m> is linear,
        and satisfies <m>T(1,1)=(1,1,1)</m>, <m>T(1,-1)=(1,2,1)</m>.
        Find the matrix <m>A</m> such that <m>T=T_A</m>.
        Hint: first write <m>(1,0)</m> and <m>(0,1)</m> as linear combinations of <m>(1,1)</m> and <m>(1,-1)</m>.
        <solution>
          <p>
            Write <m>(1,0)=\frac{1}{2}(1,1)+\frac{1}{2}(1,-1)</m>,
            and <m>(0,1)=\frac{1}{2}(1,1)-\frac{1}{2}(1,-1)</m>.
            Then
            <md>
              <mrow>T(1,0)\amp =T(\frac{1}{2}(1,1)+\frac{1}{2}(1,-1))</mrow>
              <mrow>\amp =\frac{1}{2}T(1,1)+\frac{1}{2}T(1,-1) \amp \text{ (\(T\) is linear) }</mrow>
              <mrow>\amp =\frac{1}{2}(1,1,1)+\frac{1}{2}(1,2,1) \amp \text{ (given) }</mrow>
              <mrow>\amp =(1,3/2,1)</mrow>
            </md>
            and
            <md>
              <mrow>T(0,1)\amp =T(\frac{1}{2}(1,1)-\frac{1}{2}(1,-1))</mrow>
              <mrow>\amp =\frac{1}{2}T(1,1)-\frac{1}{2}T(1,-1) \amp \text{ (\(T\) is linear) }</mrow>
              <mrow>\amp =\frac{1}{2}(1,1,1)-\frac{1}{2}(1,2,1) \amp \text{ (given) }</mrow>
              <mrow>\amp =(0,-1/2,0)</mrow>
            </md>.
          </p>
          <p>
            It follows, following our recipe, that <m>T=T_A</m>, where
            <me>
              A=\begin{bmatrix}1\amp 0\\ 3/2\amp -1/2\\ 1\amp 0 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Suppose <m>T\colon \R^n\rightarrow \R^n</m> is linear,
        that <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}</m> is a basis for <m>\R^n</m> and that
        <m>T(\boldv_i)=~2\boldv_i</m> for all <m>1\leq i\leq n</m>.
        Compute the <m>n\times n</m> matrix <m>A</m> such that <m>T=T_A</m>.
        Justify your answer.
        <solution>
          <p>
            I claim <m>T=T_A</m>, where
            <me>
              A=2I_n=\begin{bmatrix}2\amp 0\amp \dots\amp 0\\ 0\amp 2\amp \dots\amp 0\\ \vdots \\ 0\amp 0\amp \dots\amp 2 \end{bmatrix}
            </me>.
          </p>
          <p>
            Indeed, observe that <m>T_A</m> is linear
            (since it's a matrix equation),
            and that
            <me>
              T_A(\boldx)=2I_n\underset{n\times 1}{\boldx}=2\boldx
            </me>
            for any <m>\boldx\in\R^n</m>.
            In particular, <m>T_A(\boldv_i)=2\boldv_i</m> for the elements
            <m>\boldv_i</m> of the given basis <m>B</m>.
            But then <m>T(\boldv_i)=T_A(\boldv_i)</m> for all <m>\boldv_i\in B</m>.
            We have a theorem that states any two linear transformations that agree on a basis must agree everywhere,
            and hence are equal.
            We conclude <m>T=T_A</m>, as claimed.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use the rank-nullity theorem to compute the rank of the linear transformation <m>T</m> described.
      </p>
      <ol>
        <li>
          <p>
            <m>T\colon\R^7\rightarrow M_{32}</m> has nullity 2.
          </p>
        </li>
        <li>
          <p>
            <m>T\colon P_3\rightarrow \R</m> has nullity 1.
          </p>
        </li>
        <li>
          <p>
            The null space of <m>T\colon P_5 \rightarrow P_5</m> is <m>P_4</m>.
          </p>
        </li>
        <li>
          <p>
            <m>T\colon P_n\rightarrow M_{mn}</m> has nullity 3.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) <m>\rank(T) = \dim(R^7) - 2 = 7-2=5</m>
        </p>
        <p>
          (b) <m>\rank(T) = \dim(P_3) - 1 = 4-1=3</m>
        </p>
        <p>
          (c)<m>\rank(T) = \dim(P_5) - \nullity(T) = \dim(P_5)-\dim(P_4) = 1</m>
        </p>
        <p>
          (d) <m>\rank(T) = \dim(P_n)-3 = (n+1) - 3 = n-2</m>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation,
        where <m>V</m> is finite dimensional.
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\R^\infty=\{(a_1,a_2,\dots, )\colon a_i\in\R\}</m>,
        the space of all infinite sequences.
        Define the
        <q>shift left</q>
        and
        <q>shift right</q>
        functions as follows:
      </p>
    </li>
    <li>
      <p>
        Prove that <m>T_L</m> and <m>T_R</m> are linear.
      </p>
    </li>
    <li>
      <p>
        Show that <m>T_L</m> is onto,
        but not one-to-one. (Compute
        <m>\NS(T_L)</m> and <m>\range(T_L)</m>.)
      </p>
    </li>
    <li>
      <p>
        Show that <m>T_R</m> is one-to-one,
        but not onto. (Compute <m>\NS(T_R)</m> and <m>\range(T_R)</m>. )
      </p>
    </li>
    <li>
      <p>
        Show that <m>T_L\circ T_R=I_{\R^\infty}</m> but <m>T_R\circ T_L\ne I_{\R^\infty}</m>.
      </p>
    </li>
    <li>
      <p>
        Give an explicit example of a matrix <m>A</m> such that <m>\CS(A)\ne \CS(U)</m>.
        <solution>
          <p>
            Take <m>A=\begin{bmatrix}1\amp 1\\1\amp 1 \end{bmatrix}</m>,
            which row reduces to <m>U=\begin{bmatrix}1\amp 1\\0\amp 0 \end{bmatrix}</m>.
          </p>
          <p>
            Then <m>\CS(A)=\Span(\{(1,1\})</m>,
            which is the line <m>y=x</m>,
            and <m>\CS(U)=\Span(\{(1,0)\})</m>,
            which is the <m>x</m>-axis.
            Clearly these two lines are not equal.
          </p>
          <p>
            Note: any explicit counterexample you give must be non-invertible,
            since otherwise <m>\CS A=\CS U=\R^n</m>,
            by the invertibility theorem.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove that <m>\CS(A)=\{\boldb\in\R^m\colon A\boldx=\boldb \text{ has a solution } \}</m>.
        <solution>
          <p>
            A complete proof of this is given in my lecture notes.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find bases and dimensions for all fundamental spaces of
        <me>
          A=\begin{bmatrix}1\amp -1\amp 2\amp 0\\ 2\amp 0\amp 3\amp 1\\ 1\amp 1\amp 1\amp 1 \end{bmatrix}
        </me>
        <solution>
          <p>
            Routine.
            Follow the usual algorithms.
            The matrix row reduces to
            <me>
              U=\begin{bmatrix}1\amp -1\amp 2\amp 0\\ 0\amp 1\amp -1/2\amp 1/2\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>.
          </p>
          <p>
            From this we compute the following bases:
            <md>
              <mrow>\NS(A)\colon B_1\amp =\{(1,1,0,-2), (-3,1,2,0) \} \amp  \dim(\NS(A))\amp =2</mrow>
              <mrow>\CS(A)\colon B_2\amp =\{ (1,2,1), (-1,0,1) \} \amp  \dim(\CS(A))\amp =2</mrow>
              <mrow>\RS(A)\colon B_3\amp =\{ (1,-1,2,0), (0,1,-\frac{1}{2},\frac{1}{2}) \amp  \dim(RS(A))\amp =2</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> be <m>m\times n</m>.
        Look closely at our description of the method of computing the fundamental spaces to do prove the following:
      </p>
    </li>
    <li>
      <p>
        Let <m>S=\{p_1=1+x+2x^2,p_2=1-x,p_3=1+x^2,p_4=1+x-x^2\}</m> and let <m>W=\Span(S)\subset P</m>.
        (Recall <m>P</m> is the space of all polynomials.)
      </p>
    </li>
    <li>
      <p>
        Use
        <q>street smarts</q>
        to decide whether <m>S</m> is linearly independent.
      </p>
    </li>
    <li>
      <p>
        Use coordinate vectors and an appropriate fundamental space algorithm to choose a basis of <m>W=\Span(S)</m>
        <em>from among the elements of <m>S</m></em>.
      </p>
    </li>
    <li>
      <p>
        Give a satisfying description of <m>W</m>.
      </p>
    </li>
    <li>
      <p>
        For each matrix below use the rank-nullity theorem to help determine the fundamental spaces
        <q>by inspection</q>
        <ndash/>i.e., without having to do Gaussian elimination.
        The various fundamental spaces will live either in <m>\R^2</m> or <m>\R^3</m>.
        For each matrix give two sketches showing (a) <m>\NS(A)</m> and <m>\RS(A)</m> in one sketch,
        and (b) <m>\CS(A)</m> in another.
        <ol>
          <li>
            <p>
              <m>A=\begin{bmatrix}3\amp 2\amp 1\\ -6\amp -4\amp -2 \end{bmatrix}</m>
            </p>
          </li>
          <li>
            <p>
              <m>A=\begin{bmatrix}2\amp 1\\ 8\amp 4\\ 6\amp 3 \end{bmatrix}</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The columns of <m>A</m> are all scalar multiples of each other,
          so clearly <m>B_1=\{(1,-2)\}</m> is a basis of <m>\CS(A)</m>,
          and we have <m>\rank(A)=1</m>.
          When you sketch <m>\CS(A)</m> you get the line in <m>\R^2</m> connecting <m>(0,0)</m> and <m>(1,-2)</m>.
        </p>
        <p>
          It follows that <m>\RS(A)</m> is also 1-dimensional,
          and thus that any nonzero row of <m>A</m> will do as a basis:
          e.g., <m>B_2=\{(3,2,1)\}</m>.
          When you sketch <m>\RS(A)</m> you get the line in <m>\R^3</m> passing through <m>(0,0,0)</m> and <m>(3,2,1)</m>.
        </p>
        <p>
          Lastly rank-nullity implies <m>\dim(\NS(A))=3-1=2</m>.
          Furthermore,
          by the orthogonality relation <m>\NS(A)</m> is the plane orthogonal to <m>\RS(A)</m>.
          Having already sketched <m>\RS(A)</m> above,
          this is now easy to add to your sketch.
          You do it!
        </p>
        <p>
          (b) The reasoning is similar to the above.
        </p>
        <p>
          A basis for <m>\CS(A)</m> is <m>B_1=\{(1,4,3)\}</m>.
          Thus <m>\CS(A)</m> is the span of this vector,
          which defines a line in <m>\R^3</m>.
        </p>
        <p>
          A basis for <m>\RS(A)</m> is <m>B_2=(2,1)</m>,
          and thus <m>\RS(A)</m> is the line passing through the origin and <m>(2,1)</m>.
          Then <m>\NS(A)</m> is just the orthogonal complement of this in <m>\R^2</m>,
          which now is the line perpendicular to this line.
          Sketch it!
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A</m> be <m>m\times n</m> with <m>n\lt m</m>.
        Use fundamental spaces to show that there is a
        <m>\boldb\in\R^m</m> such that the system <m>A\boldx=\boldb</m> is inconsistent.
        <solution>
          <p>
            Using the fact that <m>\CS(A)=\{\boldb\in\R^m\colon A\boldx=\boldb \text{ has a solution } \}</m>,
            we see that we need only show that <m>\CS(A)\subsetneq R^m</m>.
            We do so using rank-nullity.
          </p>
          <p>
            We have <m>\rank(A)\leq\min\{m,n\}</m>.
            Since <m>n\lt m</m>, <m>\min\{m,n\}=n</m>.
            Thus <m>\rank(A)\leq n</m>.
          </p>
          <p>
            But <m>\rank(A)=\dim(\CS(A))</m>.
            Thus
            <me>
              \dim(\CS(A))\leq n\lt m=\dim(\R^m)
            </me>.
          </p>
          <p>
            Thus we have <m>\CS(A)\subset \R^m</m> and <m>\dim(\CS(A))\lt \dim(\R^m)</m>.
            By our theorem about dimensions of subspaces,
            it follows that <m>\CS(A)\ne \R^m</m>, and we are done.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A=\begin{bmatrix}1\amp 1\amp 1\amp 1\\ 2\amp 1\amp 2\amp 1\\ 1\amp 1\amp 1\amp 1 \end{bmatrix}</m>.
        Use the rank-nullity theorem to find bases for all the fundamental spaces of <m>A</m>
        <q>by inspection</q>.
        <solution>
          <p>
            More of the same.
            There are clearly only 2 linearly independent columns.
            So a basis for <m>\CS(A)</m> is <m>B_1=\{(1,2,1),(1,1,1)\}</m>.
          </p>
          <p>
            Since <m>\dim(\RS(A))=\dim(\CS(A))=2</m>,
            we see easily that <m>B_2=\{(1,1,1,1),(2,1,2,1)\}</m> is a basis for <m>\RS(A)</m>.
          </p>
          <p>
            Lastly, rank-nullity tells us that <m>\dim(\NS(A))=n-2=4-2=2</m>.
            Thus to come up with a basis for <m>\NS(A)</m> I need only find two linearly independent elements of <m>\NS(A)</m>.
          </p>
          <p>
            I can come up with two such solutions to
            <m>A\boldx=\boldzero</m> easily by inspection,
            thinking of <m>A\boldx</m> as a linear combination of the columns of <m>A</m>.
            For example,
            since the first and third columns of <m>A</m> are identical, I see that
            <m>(1,0,-1,0)</m> is an element of <m>\NS(A)</m>. (Do the multiplication yourself!) Since the second and fourth columns are identical, I see that
            <m>(0,1,0,-1)</m> is an element of <m>\NS(A)</m>.
          </p>
          <p>
            Since these two vectors are clearly linearly independent and <m>\NS(A)</m> has dimension 2, we see that
            <m>B_3=\{(1,0,-1,0), (0,1,0,-1)\}</m> is a basis for <m>\NS(A)</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find bases for all the fundamental spaces of the following matrices.
        <ol>
          <li>
            <p>
              <me>
                A = \begin{bmatrix}1\amp 4\amp 5\amp 2\\ 2\amp 1\amp 3\amp 0\\ -1\amp 3\amp 2\amp 2 \end{bmatrix}
              </me>
            </p>
          </li>
          <li>
            <p>
              <me>
                A = \begin{bmatrix}1\amp 4\amp 5\amp 6\amp 9\\ 3\amp -2\amp 1\amp 4\amp -1\\ -1\amp 0\amp -1\amp -2\amp -1\\ 2\amp 3\amp 5\amp 7\amp 8 \end{bmatrix}
              </me>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The matrix <m>A</m> reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp 1\amp -2/7\\ 0\amp 1\amp 1\amp 4/7\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The non-zero rows of the reduced matrix form a basis for the row space:
          <me>
            \{ \begin{bmatrix}1\amp 0\amp 1\amp -2/7 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp 1\amp 4/7 \end{bmatrix} \}
          </me>
        </p>
        <p>
          Since the null space is the solution space of the homogeneous linear system <m>A\boldx = 0</m>,
          we need to look at the first two rows of the reduced matrix.
          <md>
            <mrow>x_1 + x_3 - 2/7x_4 \amp =\amp  0</mrow>
            <mrow>x_2 + x_3 +4/7x_4 \amp =\amp  0</mrow>
          </md>
        </p>
        <p>
          Setting the free variables <m>x_3 = r</m> and <m>x_4 = s</m>.
          <md>
            <mrow>(x_1,x_2,x_3,x_4) \amp =\amp  (-r + 2/7s, -r-4/7s,r,s)</mrow>
            <mrow>\amp =\amp  r(-1,-1,1,0) + s(2/7,-4/7,0,1)</mrow>
          </md>
        </p>
        <p>
          Thus the basis for the null space is:
          <me>
            \left\{ \begin{bmatrix}-1\\ -1\\ 1\\ 0 \end{bmatrix} , \begin{bmatrix}2/7\\ -4/7\\ 0\\ 1 \end{bmatrix} \right\}
          </me>
        </p>
        <p>
          (b) The matrix <m>A</m> reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp 1\amp 2\amp 1\\ 0\amp 1\amp 1\amp 1\amp 2\\ 0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus the basis for the row space is:
          <me>
            \{ \begin{bmatrix}1\amp 0\amp 1\amp 2\amp 1 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp 1\amp 1\amp 2 \end{bmatrix} \}
          </me>
        </p>
        <p>
          Setting up the equations for the null space basis:
          <md>
            <mrow>x_1 + x_3+ 2x_4+x_5 \amp =\amp  0</mrow>
            <mrow>x_2+x_3+x_4+2x_5 \amp =\amp  0</mrow>
          </md>
        </p>
        <p>
          Setting the free variables <m>x_3 = r</m>,
          <m>x_4 = s</m>, and <m>x_5 = t</m>.
          <md>
            <mrow>(x_1,x_2,x_3,x_4,x_5,) \amp =\amp  (-r-2s-t,-r-s-2t,r,s,t)</mrow>
            <mrow>\amp =\amp  r(-1,-1,1,0,0) + s(-2,-1,0,1,0) + t(-1,-2,0,0,1)</mrow>
          </md>
        </p>
        <p>
          Thus the basis for the null space is:
          <me>
            \left\{ \begin{bmatrix}-1\\ -1\\ 1\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}-2\\ -1\\ 0\\ 1\\ 0 \end{bmatrix} , \begin{bmatrix}-1\\ -2\\ 0\\ 0\\ 1 \end{bmatrix} \right\}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find bases for the row space and for the column space of the given matrix.
        <ol>
          <li>
            <p>
              <me>
                A= \begin{bmatrix}1\amp 2\amp 4\amp 5\\ 0\amp 1\amp -3\amp 0\\ 0\amp 0\amp 1\amp -3\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
              </me>
            </p>
          </li>
          <li>
            <p>
              <me>
                A= \begin{bmatrix}1\amp 2\amp -1\amp 5\\ 0\amp 1\amp 4\amp 3\\ 0\amp 0\amp 1\amp -7\\ 0\amp 0\amp 0\amp 1 \end{bmatrix}
              </me>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The non-zero rows of the matrix for a basis for the row space:
          <me>
            \{ \begin{bmatrix}1\amp 2\amp 4\amp 5 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp -3\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 1\amp -3 \end{bmatrix} \}
          </me>
        </p>
        <p>
          The columns that have the leading 1's of the row vectors form a basis for the column space:
          <me>
            \left\{ \begin{bmatrix}1\\ 0\\ 0\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}2\\ 1\\ 0\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}4\\ -3\\ 1\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}5\\ 0\\ -3\\ 1\\ 0 \end{bmatrix} \right\}
          </me>
        </p>
        <p>
          (b) The non-zero rows of the matrix for a basis for the row space:
          <me>
            \{ \begin{bmatrix}1\amp 2\amp -1\amp 5 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp 4\amp 3 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 1\amp -7 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 0\amp 1 \end{bmatrix} \}
          </me>
        </p>
        <p>
          The columns that have the leading 1's of the row vectors form a basis for the column space:
          <me>
            \left\{ \begin{bmatrix}1\\ 0\\ 0\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}2\\ 1\\ 0\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}-1\\ 4\\ 1\\ 0\\ 0 \end{bmatrix} , \begin{bmatrix}5\\ 3\\ -7\\ 1\\ 0 \end{bmatrix} \right\}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>\ds A= \begin{bmatrix}1\amp -2\amp 5\amp 0\amp 3\\ -2\amp 5\amp -7\amp 0\amp -6\\ -1\amp 3\amp -2\amp 1\amp -3\\ -3\amp 8\amp -9\amp 1\amp -9 \end{bmatrix}</m>.
        <ol>
          <li>
            <p>
              Use the usual procedure to find bases for <m>\CS(A)</m> and <m>\RS(A)</m>.
            </p>
          </li>
          <li>
            <p>
              Now compute a basis for <m>\RS(A)</m> consisting of a
              <em>subset of the rows</em> of <m>A</m>.
              Hint: look at <m>A^T</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          The given matrix reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp 11\amp 0\amp 3\\ 0\amp 1\amp 3\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The non-zero rows of the reduced matrix for a basis for the row space:
          <me>
            \{ \begin{bmatrix}1\amp 0\amp 11\amp 0\amp 3 \end{bmatrix} , \begin{bmatrix}0\amp 1\amp 3\amp 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\amp 0\amp 1\amp 0 \end{bmatrix} \}
          </me>
        </p>
        <p>
          The columns of the original matrix that correspond the the columns of the reduced matrix that have the leading 1's of the row vectors form a basis for the column space:
          <me>
            \left\{ \begin{bmatrix}1\\ -2\\ -1\\ -3 \end{bmatrix} , \begin{bmatrix}-2\\ 5\\ 3\\ 8 \end{bmatrix} , \begin{bmatrix}0\\ 0\\ 1\\ 1 \end{bmatrix} \right\}
          </me>
        </p>
        <p>
          (b) We have <m>\RS(A)=\CS(A^T)</m>.
          Applying the usual procedure to compute a basis of <m>\CS(A^T)</m> produces a basis for
          <m>\RS(A)=\CS(A^T)</m> from among the columns of <m>A^T</m>,
          which are the rows of <m>A</m>.
        </p>
        <p>
          We have
          <me>
            A^T= \begin{bmatrix}1\amp -2\amp -1\amp -3\\ -2\amp 5\amp 3\amp 8\\ 5\amp -7\amp -2\amp -9\\ 0\amp 0\amp 1\amp 1\\ 3\amp -6\amp -3\amp -9 \end{bmatrix}
          </me>
          which reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp 0\amp 0\\ 0\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp 1\\ 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since columns 1,2,and 3 have the leading 1's, the corresponding rows of the original matrix form a basis:
          <me>
            \{ \begin{bmatrix}1\amp -2\amp 5\amp 0\amp 3 \end{bmatrix} , \begin{bmatrix}-2\amp 5\amp -7\amp 0\amp -6 \end{bmatrix} , \begin{bmatrix}-1\amp 3\amp -2\amp 1\amp -3 \end{bmatrix} \}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find a subset of the given vectors that forms a basis for the space spanned by those vectors,
        and then express each vector that is not in the basis as a linear combination of the basis vectors.
        <me>
          \boldv_1 = (1,0,1,1), \boldv_2 = (-3,3,7,1), \boldv_3 = (-1,3,9,3), \boldv_4 = (-5,3,5,-1)
        </me>
        <solution>
          <p>
            Start by writing each vector as a column in a matrix.
            <me>
              \begin{bmatrix}1\amp -3\amp -1\amp -5\\ 0\amp 3\amp 3\amp 3\\ 1\amp 7\amp 9\amp 5\\ 1\amp 1\amp 3\amp -1 \end{bmatrix}
            </me>
          </p>
          <p>
            Like the previous problems we reduce the matrix.
            <me>
              \begin{bmatrix}1\amp 0\amp 2\amp -2\\ 0\amp 1\amp 1\amp 1\\ 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Now the columns of the original matrix that correspond to the columns of the reduced matrix that have the leading 1's form a basis.
            Thus column 1 and 2 form the basis.
            Since our columns are vectors, the basis is:
            <me>
              \{\boldv_1,\boldv_2\}
            </me>
          </p>
          <p>
            Now we find a set of dependency equations for each column in the reduced matrix that does not have a leading 1.
            Let <m>\boldw_i</m> be the columns of the reduced matrix.
            Then
            <md>
              <mrow>\boldw_3 \amp =\amp  2\boldw_1 + \boldw_2</mrow>
              <mrow>\boldw_4 \amp =\amp  -2\boldw_1 + \boldw_2</mrow>
            </md>
          </p>
          <p>
            Replacing each <m>\boldw_i</m> with
            <m>\boldv_i</m> gives the required result:
            <md>
              <mrow>\boldv_3 \amp =\amp  2\boldv_1 + \boldv_2</mrow>
              <mrow>\boldv_4 \amp =\amp  -2\boldv_1 + \boldv_2</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find the rank and nullity of each matrix by reducing it to row echelon form.
        <ol>
          <li>
            <p>
              <me>
                A = \begin{bmatrix}1\amp 0\amp -2\amp 1\amp 0\\ 0\amp -1\amp -3\amp 1\amp 3\\ -2\amp -1\amp 1\amp -1\amp 3\\ 0\amp 1\amp 3\amp 0\amp -4 \end{bmatrix}
              </me>
            </p>
          </li>
          <li>
            <p>
              <me>
                A = \begin{bmatrix}1\amp 3\amp 1\amp 3\\ 0\amp 1\amp 1\amp 0\\ -3\amp 0\amp 6\amp -1\\ 3\amp 4\amp -2\amp 1\\ 2\amp 0\amp -4\amp -2 \end{bmatrix}
              </me>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The matrix reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp -2\amp 0\amp 1\\ 0\amp 1\amp 3\amp 0\amp -4\\ 0\amp 0\amp 0\amp 1\amp -1\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the matrix has 3 leading 1's, <m>\rank(A)=3</m>.
          The rank-nullity theorem now tells us that <m>\nullity(A)=n-3=5-3=2</m>.
        </p>
        <p>
          (b) <m>A</m> reduces to:
          <me>
            \begin{bmatrix}1\amp 0\amp -2\amp 0\\ 0\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          This matrix has three leading 1's, which means
          <m>\rank(A)=3</m> and <m>\nullity(A)=4-3=1</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Matrix <m>R</m> is the reduced row echelon form of the matrix <m>A</m>.
        <me>
          A= \begin{bmatrix}0\amp 2\amp 2\amp 4\\ 1\amp 0\amp -1\amp -3\\ 2\amp 3\amp 1\amp 1\\ -2\amp 1\amp 3\amp -2 \end{bmatrix} , R = \begin{bmatrix}1\amp 0\amp -1\amp 0\\ 0\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
        </me>
        <ol>
          <li>
            <p>
              Compute the rank and nullity of A.
            </p>
          </li>
          <li>
            <p>
              Confirm that the rank an nullity satisfy the rank-nullity theorem.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Since <m>R</m> has three non-zero rows, <m>\rank(A) = 3</m>.
          There is one free variable, so <m>\nullity(A) = 1</m>.
        </p>
        <p>
          (b) The matrix <m>A</m> is <m>4\times 4</m>.
          Thus <m>n = 4</m>.
          In our example we have
          <md>
            <mrow>\rank(A) + \nullity(A) \amp =3+1 \amp \text{ (from (a)) }</mrow>
            <mrow>\amp =4</mrow>
          </md>
        </p>
        <p>
          Thus <m>\rank(A) + \nullity(A)=n</m> as predicted by the theorem.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Give two explicit matrices <m>A</m> and <m>B</m> of the same size satisfying <m>\rank(A)=\rank(B)</m>,
        but <m>\rank(A^2)\ne \rank(B^2)</m>.
        <solution>
          <p>
            Let <m>A=\begin{bmatrix}1\amp 1\\ -1\amp -1 \end{bmatrix}</m> and <m>B=\begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</m>.
            Both clearly have rank 1.
            Since <m>A^2=\boldzero</m>, we have <m>\rank(A^2)=0</m>.
            On the other hand <m>B^2=\begin{bmatrix}2\amp 2\\ 2\amp 2 \end{bmatrix}</m>,
            which has rank 1.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove: an <m>n\times n</m> matrix <m>A</m> satisfies
        <m>\CS(A)\subseteq \NS(A)</m> if and only if  <m>A^2=\boldzero</m>.
        Produce an explicit nonzero example of such a matrix.
        <solution>
          <p>
            Write <m>A=\begin{bmatrix}\vert \amp \vert \amp  \amp \vert\\ \hspace{5pt} \boldv_1\amp \hspace{5pt}\boldv_2\amp \cdots \amp \hspace{5pt}\boldv_n\\ \vert \amp \vert \amp  \amp \vert \end{bmatrix}</m>.
            Then we have
            <md>
              <mrow>A^2=\boldzero \amp \Longleftrightarrow \begin{bmatrix} \vert \amp \vert \amp  \amp \vert</mrow>
              <mrow>\hspace{5pt} A\boldv_1\amp \hspace{5pt}A\boldv_2\amp \cdots \amp \hspace{5pt}A\boldv_n</mrow>
              <mrow>\vert \amp \vert \amp  \amp \vert \end{bmatrix}=\boldzero \amp \text{ (column method) }</mrow>
              <mrow>\amp \Longleftrightarrow A\boldv_j=\boldzero \text{ for all \(1\leq j\leq n\) }</mrow>
              <mrow>\amp \Longleftrightarrow \boldv_j\in\NS(A) \text{ for all \(1\leq j\leq n\) }</mrow>
              <mrow>\amp \Longleftrightarrow \Span\left(\{ \boldv_1,\boldv_2,\dots, \boldv_n\}\right)\subseteq\NS(A)</mrow>
              <mrow>\amp \Longleftrightarrow \CS(A)\subseteq \NS(A)</mrow>
            </md>
          </p>
          <p>
            The penultimate if and only only is a consequence of the general fact that a subspace <m>W</m> contains
            <m>\Span(\{\boldw_1,\dots, \boldw_r\})</m> if and only if it contains <m>\boldw_i</m> for all <m>i</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        If <m>A</m> is an <m>m</m> by <m>n</m> matrix,
        what is the largest possible value for its rank and the smallest possible value for its nullity?
        <solution>
          <p>
            Since the row vectors of <m>A</m> lie in <m>R^n</m> and the column vectors in <m>R^m</m>,
            the row space of <m>A</m> is at most <m>n</m>-dimensional and the column space is at most <m>m</m>-dimensional.
            Since the rank of <m>A</m> is the common dimension of its row and column space,
            it follows that the rank is at most the smaller of <m>m</m> and <m>n</m>.
            Thus:
            <me>
              \rank(A)\leq \min(m,n)
            </me>
          </p>
          <p>
            Using the rank-nullity theorem we can find a similar bound for the nullity.
            <md>
              <mrow>\nullity(A) \amp = n - \rank(A) \amp \text{ (Thm. 4.8.2) }</mrow>
              <mrow>\amp \geq n - \min(m,n) \amp \text{ (since \(\rank(A)\leq\min(m,n)\)) }</mrow>
              <mrow>\amp = \max(n-m,0) \amp</mrow>
            </md>
          </p>
          <p>
            The last equality follows from the more general rule that
            <me>
              r-\min\{m,n\}=\max\{r-m,r-n\}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Complete the following statements.
        Justify your answer.
        <ol>
          <li>
            <p>
              If <m>A</m> is a 3 by 5 matrix,
              then the rank of <m>A</m> is at most <m>\dots</m>
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a 3 by 5 matrix,
              then the nullity of <m>A</m> is at most <m>\dots</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) By the previous exercise
          <m>\rank(A)</m> is at most <m>\min(3,5) = 3</m>.
        </p>
        <p>
          (b) The nullity is at most 5.
          Consider
          <me>
            A = \begin{bmatrix}0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The rank of this matrix is 0.
          Thus the nullity must be 5.
          Since the rank can never be negative,
          the largest the nullity can be is 5.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A</m> be a 5 by 7 matrix with rank 4.
        <ol>
          <li>
            <p>
              What is the dimension of the solution space of <m>A\boldx = 0</m>?
            </p>
          </li>
          <li>
            <p>
              Is <m>A\boldx = b</m> consistent for all vectors <m>b</m> in <m>\R^5</m>?
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The solution space of <m>A\boldx = 0</m> is <m>\NS(A)</m>.
          Thus:
          <me>
            \nullity(A) =n- \rank(A) = 7 - 4 = 3
          </me>
        </p>
        <p>
          (b) No.
          Since <m>\rank(A)=4</m>, <m>\dim(\CS(A))=4</m>.
          Since <m>\CS(A)\subset\R^5</m> and
          <m>\dim(\R^5)=5</m> it follows that <m>\CS(A)\subsetneq \R^5</m>.
          Then there is <m>\boldb\in\R^5</m> such that <m>\boldb\notin\CS(A)</m>.
          Since
          <me>
            \CS(A)=\{\boldb\in\R^5\colon A\boldx=\boldb \text{ is consistent } \}
          </me>
          it follows that we cannot solve
          <m>A\boldx=\boldb</m> for this <m>\boldb</m>.
        </p>
      </solution>
    </li>
    <li xml:id="ex_rowscolumns">
      <p>
        Prove: If a matrix A is not a square,
        then either the row vectors or the column vectors of <m>A</m> are linearly dependent.
        <solution>
          <p>
            Suppose <m>A</m> is a <m>m</m> by <m>n</m> matrix with <m>n>m</m>.
            Then we have <m>n</m> columns in <m>A</m> which we can think of as being vectors in <m>R^m</m>.
            The columns of <m>A</m> cannot be linearly independent since <m>R^m</m> has dimension <m>m</m>,
            so any collection of vectors from <m>R^m</m> with more than <m>m</m> vectors must be linearly dependent.
            So the columns are dependent.
            Similarly, if <m>n\lt m</m>,
            then you can think of the rows of <m>A</m> as a collection of <m>m</m> vectors in <m>R^n</m>.
            Using the same argument we can see that rows must be dependent.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              Either the row vectors or the column vectors of a square matrix are linearly independent.
            </p>
          </li>
          <li>
            <p>
              A matrix with linearly independent row vectors and linearly independent column vectors is square.
            </p>
          </li>
          <li>
            <p>
              The nullity of a nonzero <m>m</m> by <m>n</m> matrix is at most <m>m</m>.
            </p>
          </li>
          <li>
            <p>
              Adding one additional column to a matrix increases its rank by one.
            </p>
          </li>
          <li>
            <p>
              The nullity of a square matrix with linearly dependent rows is at least one.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is a square and
              <m>A\boldx = b</m> is inconsistent for some vector <m>b</m>,
              the the nullity of <m>A</m> is zero.
            </p>
          </li>
          <li>
            <p>
              If a matrix <m>A</m> has more rows than columns,
              then the dimension of the row space is greater than the dimension of the column space.
            </p>
          </li>
          <li>
            <p>
              <m>\rank(A)=\rank(A^T)</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\nullity(A^T)=\nullity(A)</m>, then <m>A</m> is square.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          Consider the matrix:
          <me>
            \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          Both the row vectors and the column vectors are linearly dependent.
        </p>
        <p>
          (b) True.
          Let <m>A</m> be an <m>m</m> by <m>n</m> matrix with linearly independent row vectors and linearly independent column vectors.
          Assume for the purpose of contradiction that <m>m \neq n</m>.
          Then by <xref ref="ex_rowscolumns">Exercise</xref>
          either the row vectors or the column vectors are linearly dependent.
          This contradicts the fact that <m>A</m> has linearly independent row vectors and linearly independent column vectors.
          Thus the assumption is false and <m>m = n</m>,
          so <m>A</m> is a square matrix.
        </p>
        <p>
          (c) False.
          The <m>m\times n</m> zero matrix has nullity <m>n</m>.
          So if <m>n>m</m> the nullity of <m>0_{mn}</m> is greater than <m>n</m>.
        </p>
        <p>
          (d) False.
          Consider:
          <me>
            \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the matrix has 3 leading 1's, the rank is 3.
          Now if we add a column of 1's
          <me>
            \begin{bmatrix}1\amp 0\amp 0\amp 1\\ 0\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          Since the matrix has 3 leading 1's, the rank is 3.
        </p>
        <p>
          (e) True.
          Let <m>A</m> be an <m>n</m> by <m>n</m> matrix.
          Since the rows are linearly dependent,
          the reduced form of <m>A</m> has a row of zeros for the bottom row.
          Thus
          <me>
            \rank(A) \lt  n
          </me>
        </p>
        <p>
          And
          <me>
            \nullity(A) = n - \rank(A) > 0
          </me>
        </p>
        <p>
          (f) False.
          Use the invertibility theorem.
          Since <m>A</m> is inconsistent for some vector <m>b</m>,
          part (e) is false.
          Thus part (o) is false.
          So <m>A</m> does not have nullity 0.
        </p>
        <p>
          (g) False.
          By rank-nullity,
          the row space and the column space always have the same dimension.
        </p>
        <p>
          (h) True.
          We have <m>\rank(A)=\dim\CS(A)=\dim\RS(A^T)=\rank(A^T)</m>.
        </p>
        <p>
          (i) True.
          Let <m>A</m> be <m>m\times n</m>,
          and assume <m>\nullity(A)=\nullity(A^T)</m>.
          Then by the rank-nullity theorem
          <md>
            <mrow>n\amp =\rank(A)+\nullity(A)\amp \text{ (by rank-nullity theorem, since \(A\) is \(m\times n\)) }</mrow>
            <mrow>\amp =\rank(A^T)+\nullity(A) \amp \text{ (since \(\rank(A)=\rank(A^T)\)) }</mrow>
            <mrow>\amp =\rank(A^T)+\nullity(A^T) \amp \text{ (by assumption) }</mrow>
            <mrow>\amp =m \amp \text{ (by rank-nullity theorem, since \(A^T\) is \(n\times m\)) }</mrow>
          </md>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Suppose <m>\underset{m\times n}{A}</m> reduces to the row echelon matrix <m>\underset{m\times n}{U}</m>.
        <ol>
          <li>
            <p>
              Prove: the columns <m>\boldu_{i_1}, \boldu_{i_2}, \dots, \boldu_{i_r}</m> of <m>U</m> form a basis for <m>\CS U</m> if and only if the corresponding columns
              <m>\bolda_{i_1}, \bolda_{i_2}, \dots, \bolda_{i_r}</m> of <m>A</m> form a basis for <m>\CS A</m>.
            </p>
          </li>
          <li>
            <p>
              Prove: the columns of <m>U</m> with leading 1's form a basis for <m>\CS U</m>.
            </p>
          </li>
          <li>
            <p>
              Prove: <m>\dim \NS U=\#(\text{ free variables } )</m> in linear system <m>U\boldx=\boldzero</m>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>x_{j_1}=t_{j_1}, x_{j_2}=t_{j_2}, \dots,
              x_{j_s}=t_{j_s}</m> are the free variables of the system <m>U\boldx=\boldzero</m>.
              Let <m>\boldv_{j_k}</m> be the element of <m>\NS U</m> obtained by setting
              <m>t_{j_k}=1</m> and <m>t_{j_\ell}=0</m> for
              <m>\ell\ne k</m> in our parametric description of solutions to <m>U\boldx=\boldzero</m>.
              Prove: <m>\{\boldv_{j_1}, \boldv_{j_2}, \dots, \boldv_{j_s}\}</m> is a basis for <m>\NS U</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Recall that <m>A</m> being row equivalent to <m>U</m> is equivalent to there being an invertible matrix <m>Q</m> such that
          <me>
            QA=U\tag{\(*\)}
          </me>
        </p>
        <p>
          We will this fact repeatedly.
        </p>
        <p>
          (a) By the column method of matrix multiplication we have <m>\boldu_i=Q\bolda_i</m>:
          i.e., the <m>i</m>-th column of <m>U</m> is obtained by multiplying the <m>i</m>-th column of <m>A</m> on the left by <m>Q</m>.
          First observe that
          <md>
            <mrow>\boldzero=c_1\bolda_{i_1}+\cdots +c_r\bolda_{i_r}\amp \Leftrightarrow Q\boldzero=Q(c_1\bolda_{i_1}+\cdots +c_r\bolda_{i_r}) \amp \text{ (since \(Q\) is invertible) }</mrow>
            <mrow>\amp \Leftrightarrow \boldzero=c_1Q\bolda_{i_1}+\cdots +c_rQ\bolda_{i_r} \amp \text{ (matrix arithmetic) }</mrow>
            <mrow>\amp \Leftrightarrow \boldzero=c_1\boldu_{i_1}+\cdots +c_r\boldu_{i_r}</mrow>
          </md>
        </p>
        <p>
          In particular,
          there a nontrivial linear combination of the <m>\bolda_{i_k}</m> equal to
          <m>\boldzero</m> if and only if there is a nontrivial linear combination of the
          <m>\boldu_{i_k}</m> equal to <m>\boldzero</m>.
          Thus the <m>\bolda_{i_k}</m> are independent if and only if the <m>\boldu_{i_k}</m> are independent.
        </p>
        <p>
          Next, I claim the <m>\bolda_{i_k}</m> span <m>\CS A</m> if and only if the <m>\boldu_{i_k}</m> span <m>\CS U</m>.
          Indeed, suppose the <m>\bolda_{i_k}</m> span <m>\CS A</m>.
          Take <m>\boldy\in \CS U</m>.
          I will show that <m>\boldy</m> is a linear combination of the <m>\boldu_{i_k}</m>.
        </p>
        <p>
          Since <m>\boldy\in \CS U</m>,
          there is an <m>\boldx\in\R^n</m> such that <m>\boldy=U\boldx</m>
          (since <m>\CS U=\range U</m>).
          We have <m>U=QA</m>, and thus <m>\boldy=QA(\boldx)=Q(\boldw)</m>,
          where <m>\boldw=A\boldx</m>.
          Then <m>\boldw\in \CS A</m>,
          and we can write <m>\boldw=c_1\bolda_{i_1}+\cdots +c_r\bolda_{i_r}</m>.
          It then follows that
          <me>
            \boldy=Q\boldw=Q(c_1\bolda_{i_1}+\cdots +c_r\bolda_{i_r})=c_1Q\bolda_{i_1}+\cdots +c_rQ\bolda_{i_r}=c_1\boldu_{i_1}+\cdots +c_r\boldu_{i_r}
          </me>.
        </p>
        <p>
          Since <m>\boldy</m> was any element of <m>\CS U</m>,
          we see that the <m>\boldu_{i_k}</m> span <m>\CS U</m>,
          as desired.
        </p>
        <p>
          To go the other way (i.e., that if the <m>\boldu_{i_k}</m> span <m>\CS U</m>,
          the <m>\bolda_{i_k}</m> span <m>\CS A</m>),
          note that <m>(*)</m> implies <m>A=Q^{-1}U</m>,
          and we can use the same argument above with the roles of
          <m>\bolda_{i_k}</m> and <m>\boldu_{i_k}</m> swapped!
        </p>
        <p>
          We have shown that the <m>\bolda_{i_k}</m> are independent if and only if the <m>\boldu_{i_k}</m>,
          and that they span <m>\CS A</m> if and only if the <m>\boldu_{i_k}</m> span <m>\CS U</m>.
          It follows that the <m>\bolda_{i_k}</m> form a basis for <m>\CS A</m> if and only if the
          <m>\boldu_{i_k}</m> form a basis for <m>\CS U</m>.
        </p>
        <p>
          (b) Now let <m>\boldu_{i_1},\dots, \boldu_{i_r}</m> be the columns of <m>U</m> with leading 1's, and let
          <m>\boldu_{j_1}, \boldu_{j_2}, \dots, \boldu_{j_s}</m> be the columns without leading 1's.
          To prove the <m>\boldu_{i_k}</m> form a basis for <m>\CS U</m>, I will show that given any
          <m>\boldy\in \CS U</m> there is a <em>unique</em>
          choice of scalars <m>c_1, c_2,\dots,
          c_r</m> such that <m>c_1\boldu_{i_1}+\cdots +c_r\boldu_{i_r}=\boldy</m>.
          (Recall that the uniqueness of this choice implies linear independence.)
        </p>
        <p>
          So assume <m>\boldy\in \CS U</m>.
          Then we can find <m>\boldx\in\R^n</m> such that <m>U\boldx=\boldy</m>,
          which means the linear system with augmented matrix <m>[\ U\ \vert \ \boldy]</m> is consistent.
          Using our Gaussian elimination theory,
          we know that the solutions
          <m>\boldx=(x_1,x_2,\dots,
          x_n)</m> to this system are in 1-1 correspondence with choices for the free variables <m>x_{j_1}=t_{j_1}, x_{j_2}=t_{j_2}, \dots,
          x_{j_s}=t_{j_s}</m>.
          (Remember that the columns
          <m>\boldu_{j_k}</m> without leading 1's correspond to the free variables.)
          In particular, there is a unique solution to
          <m>U\boldx=\boldy</m> where we set all the free variables equal to 0.
          By the column method,
          this gives us a unique linear combination of only the columns
          <m>\boldu_{i_k}</m> with leading 1's equal to <m>\boldy</m>.
          This proves the claim.
        </p>
        <p>
          (c) Using the notation and result from (b) we see that <m>\rank U=\dim\range U=\dim\CS U=r</m>,
          the number of columns of <m>U</m> with leading 1's.
          By the rank-nullity theorem,
          <m>\dim\NS U=n-r=s</m>, the number of columns without leading 1's.
          This is also equal to the number of free variables in the corresponding system of equations.
        </p>
        <p>
          (d) The given recipe produces a list of <m>s</m> distinct vectors in <m>\NS U</m>.
          Since <m>s=\dim\NS U</m>, by part (c),
          it suffice to show the <m>\boldv_i</m> are linearly independent.
          This is easy to see.
          Indeed suppose we have <m>c_1\boldv_1+c_2\boldv_2+\cdots +c_s\boldv_s=\boldzero</m>.
          Since for each <m>1\leq i\leq s</m>,
          <m>\boldv_i</m> is the <em>only</em>
          vector with a nonzero entry for the <m>i</m>-th component,
          we must have <m>c_i=0</m> for all <m>1\leq i\leq s</m>.
          Thus we see that the set is linearly independent.
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{4.3: coordinate vectors}
  </p>
  <ol>
    <li>
      <p>
        Find the coordinate vector
        <m>[\boldv]_B</m> relative to the basis <m>B= {\boldv_1,\boldv_2,\boldv_3}</m> for <m>\R^3</m>.
      </p>
      <ol>
        <li>
          <p>
            <m>\boldv =(2,-1,3)</m>;
            <m>\boldv_1 = (1,0,0)</m>,
            <m>\boldv_2=(2,2,0)</m>, <m>\boldv_3 = (3,3,3)</m>
          </p>
        </li>
        <li>
          <p>
            <m>\boldv =(5,-12,3)</m>; <m>\boldv_1 = (1,2,3)</m>,
            <m>\boldv_2=(-4,5,6)</m>, <m>\boldv_3 = (7,-8,9)</m>
          </p>
        </li>
      </ol>
      <solution>
        <p>
          In both parts, the coordinate vector
          <m>[\boldv]_S</m> is computed by finding
          <m>c_1,c_2,c_3</m> such that <m>\boldv=c_1\boldv_1+c_2\boldv_2+c_3\boldv_3</m>.
          We do so using GE. (a)
          <me>
            \begin{bmatrix}1\amp 2\amp 3\amp 2\\ 0\amp 2\amp 3\amp -1\\ 0\amp 0\amp 3\amp 3 \end{bmatrix}
          </me>
          <md>
            <mrow>c_3 \amp =\amp  1</mrow>
            <mrow>c_2 \amp =\amp  -1/2 -3/2 = -2</mrow>
            <mrow>c_1 \amp =\amp  2-3+4 = 3</mrow>
          </md>
        </p>
        <p>
          Thus <m>[\boldv]_B=(3,-2,1)</m>.
        </p>
        <p>
          (b)
          <md>
            <mrow>\begin{bmatrix}1\amp -4\amp 7\amp 5\\ 2\amp 5\amp -8\amp -12\\ 3\amp 6\amp 9\amp 3 \end{bmatrix} \amp \xrightarrow[]{2r_1-r_2}\amp \begin{bmatrix}1\amp -4\amp 7\amp 5\\ 0\amp -13\amp 22\amp 22\\ 3\amp 6\amp 9\amp 3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{3r_1-r_3}\amp \begin{bmatrix}1\amp -4\amp 7\amp 5\\ 0\amp -13\amp 22\amp 22\\ 0\amp -18\amp 12\amp 12 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-1/13r_2}\amp \begin{bmatrix}1\amp -4\amp 7\amp 5\\ 0\amp 1\amp -22/13\amp -22/13\\ 0\amp -18\amp 12\amp 12 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{18r_2+r_3}\amp \begin{bmatrix}1\amp -4\amp 7\amp 5\\ 0\amp 1\amp -22/13\amp -22/13\\ 0\amp 0\amp -240/13\amp -240/13 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{-13/240r_3}\amp \begin{bmatrix}1\amp -4\amp 7\amp 5\\ 0\amp 1\amp -22/13\amp -22/13\\ 0\amp 0\amp 1\amp 1 \end{bmatrix}</mrow>
          </md>
          <md>
            <mrow>c_3 \amp =\amp  1</mrow>
            <mrow>c_2 \amp =\amp  -22/13 + 22/13 = 0</mrow>
            <mrow>c_1 \amp =\amp  5 - 7 = -2</mrow>
          </md>
        </p>
        <p>
          Thus the coordinate vector is <m>[\boldv]_B=(-2,0,1)</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Find the coordinate vector <m>[p]_B</m> relative to the basis <m>B= \{p_1,p_2,p_3\}</m> for <m>P_2</m>.
        <ol>
          <li>
            <p>
              <m>p = 4 -3x +x^2</m>, <m>p_1 = 1</m>,
              <m>p_2 = x</m>, <m>p_3 = x^2</m>
            </p>
          </li>
          <li>
            <p>
              <m>p = 2 -x +x^2</m>, <m>p_1+x = 1</m>,
              <m>p_2 = 1+x^2</m>, <m>p_3 = x+x^2</m>
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          In both parts, the coordinate vector
          <m>[\boldv]_B</m> is computed by finding
          <m>c_1,c_2,c_3</m> such that <m>p=c_1p_1+c_2p_2+c_3p_3</m>.
          As usual this boils down
          (after grouping like terms and equating like coefficients)
          to solving a certain linear system,
          which we do either by inspection, or by using GE.
        </p>
        <p>
          (a) By inspection, we see that <m>p=4p_1+(-3)p_2+p_3</m>.
          Thus <m>[p]_B=(4,-3,1)</m>.
        </p>
        <p>
          (b) We solve the corresponding linear system in this case using GE:
          <md>
            <mrow>\begin{bmatrix}1\amp 1\amp 0\amp 2\\ 1\amp 0\amp 1\amp -1\\ 0\amp 1\amp 1\amp 1 \end{bmatrix} \amp \xrightarrow[]{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 1\amp 0\amp 2\\ 0\amp 1\amp 1\amp 1\\ 1\amp 0\amp 1\amp -1 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_1 -r_3}\amp \begin{bmatrix}1\amp 1\amp 0\amp 2\\ 0\amp 1\amp 1\amp 1\\ 0\amp 1\amp -1\amp 3 \end{bmatrix}</mrow>
            <mrow>\amp \xrightarrow[]{r_2 -r_3}\amp \begin{bmatrix}1\amp 1\amp 0\amp 2\\ 0\amp 1\amp 1\amp 1\\ 0\amp 0\amp 2\amp -2 \end{bmatrix}</mrow>
          </md>
          <md>
            <mrow>c_3 \amp =\amp  -1</mrow>
            <mrow>c_2 \amp =\amp  1 + 1 = 2</mrow>
            <mrow>c_1 \amp =\amp  2-2 = 0</mrow>
          </md>
        </p>
        <p>
          Thus the coordinate vector is <m>[p]_B=(0,2,-1)</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        The set <m>B = \{A_1,A_2,A_3,A_4\}</m> is a basis for <m>M_{22}</m>, where
        <me>
          A_1 = \begin{bmatrix}1\amp 0\\ 1\amp 0 \end{bmatrix} , A_2= \begin{bmatrix}1\amp 1\\ 0\amp 0 \end{bmatrix} , A_3 = \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} , A_4 = \begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix}
        </me>
        (a) Compute <m>[A]_B</m>, where
        <me>
          A = \begin{bmatrix}6\amp 2\\ 5\amp 3 \end{bmatrix}
        </me>.
        (b) Now compute <m>[A]_B</m> for the general matrix
        <me>
          A= \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}
        </me>.
        <solution>
          <p>
            (a) Finding <m>c_1, c_2, c_3, c_4</m> such that
            <m>A=c_1A_1+c_2A_2+c_3A_3+c_4A_4</m> amounts to solving the system corresponding to the augmented matrix below:
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp 0\amp 6\\ 0\amp 1\amp 0\amp 0\amp 2\\ 1\amp 0\amp 0\amp 1\amp 5\\ 0\amp 0\amp 1\amp 0\amp 3 \end{bmatrix}
            </me>
          </p>
          <p>,
            or equivalently, the matrix equation
            <me>
              \underset{A}{\underbrace{\begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 1\amp 0\amp 0\\ 1\amp 0\amp 0\amp 1\\ 0\amp 0\amp 1\amp 0 \end{bmatrix} }} \begin{bmatrix}c_1\\ c_2\\ c_3\\ c_4 \end{bmatrix} = \begin{bmatrix}6\\ 2\\ 5\\ 3 \end{bmatrix}
            </me>
          </p>
          <p>
            After GE this reduces to
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp 0\amp 6\\ 0\amp 1\amp 0\amp 0\amp 2\\ 0\amp 0\amp 1\amp -1\amp -1\\ 0\amp 0\amp 0\amp 1\amp 4 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus
            <md>
              <mrow>c_4 \amp =\amp  4</mrow>
              <mrow>c_3 \amp =\amp  3</mrow>
              <mrow>c_2 \amp =\amp  2</mrow>
              <mrow>x_1 \amp =\amp  1</mrow>
            </md>
          </p>
          <p>
            Then <m>A = 1A_1 + 2A_2 +3A_3 +4A_4</m> and the coordinate vector is:
          </p>
          <p>
            <m>[A]_B = (1,2,3,4)</m>.
          </p>
          <p>
            (b) For the general matrix <m>\begin{bmatrix}a\amp b\\c\amp d \end{bmatrix}</m>,
            the same reasoning shows that we need to solve the matrix equation
            <me>
              \underset{A}{\underbrace{\begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 1\amp 0\amp 0\\ 1\amp 0\amp 0\amp 1\\ 0\amp 0\amp 1\amp 0 \end{bmatrix} }} \begin{bmatrix}c_1\\ c_2\\ c_3\\ c_4 \end{bmatrix} = \begin{bmatrix}a\\ b\\ c\\ d \end{bmatrix}
            </me>
          </p>
          <p>
            We could proceed exactly as above,
            using Gaussian elimination.
            Instead, I observe that <m>A</m> is invertible,
            and solve (after computing <m>A^{-1}</m>)
            <md>
              <mrow>\begin{bmatrix} c_1</mrow>
              <mrow>c_2</mrow>
              <mrow>c_3</mrow>
              <mrow>c_4 \end{bmatrix}\amp =A^{-1}\begin{bmatrix} a</mrow>
              <mrow>b</mrow>
              <mrow>c</mrow>
              <mrow>d \end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix}[rrrr] 1\amp  -1\amp  0\amp  -1</mrow>
              <mrow>0\amp  1\amp  0\amp  0</mrow>
              <mrow>0\amp  0\amp  0\amp  1</mrow>
              <mrow>-1\amp  1\amp  1\amp  1 \end{bmatrix}\begin{bmatrix} a</mrow>
              <mrow>b</mrow>
              <mrow>c</mrow>
              <mrow>d \end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix} a - b - d</mrow>
              <mrow>b</mrow>
              <mrow>d</mrow>
              <mrow>-a + b + c + d \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Thus for general <m>A=\begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}</m> we see that
            <me>
              [A]_B=(a-b-d, b, d, -a+b+c+d)
            </me>
          </p>
        </solution>
        \begin{samepage}
      </p>
    </li>
    <li>
      <p>
        Suppose <m>\dim(V)=n</m>,
        and let <m>B=\{\boldv_1,\dots ,\boldv_n\}</m>  be a basis for <m>V</m>.
        Define
        <md>
          <mrow>T\colon V\amp \rightarrow \R^n</mrow>
          <mrow>\boldv\amp \mapsto [\boldv]_B</mrow>
        </md>.
        Prove all statements of the coordinate vector map theorem,
        as listed below.
        <ol>
          <li>
            <p>
              <m>T</m> is a linear transformation.
            </p>
          </li>
          <li>
            <p>
              <m>T(\boldv_1)=T(\boldv_2)</m> if and only if <m>\boldv_1=\boldv_2</m>:
              i.e., <m>T</m> is <em>one-to-one</em>.
              In particular,
              <m>T(\boldv)=\boldzero</m> if and only if <m>\boldv=\boldzero_V</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\range T=\R^n</m>: i.e.
              <m>T</m> is <em>onto</em>.
            </p>
          </li>
          <li>
            <p>
              A set <m>S=\{w_1,w_2,\dots,
              w_r\}\subseteq V</m> is linear independent if and only if
              <m>T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}\subseteq \R^n</m> is linearly independent.
            </p>
          </li>
          <li>
            <p>
              A set <m>S=\{w_1,w_2,\dots,
              w_r\}\subseteq V</m> spans <m>V</m> if and only if <m>T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}</m> spans <m>\R^n</m>.
            </p>
          </li>
        </ol>
      </p>
      \end{samepage}
      <solution>
        <p>
          (a) We must show <m>[c\boldv+d\boldw]_B=c[\boldv]_B+d[\boldw]_B</m> for all
          <m>\boldv,\boldw\in V</m> and all <m>c,d\in\R</m>.
          To this end, given <m>\boldv, \boldw</m>, write
          <md>
            <mrow>\boldv\amp =a_1\boldv_1+a_2\boldv_2+\cdots +a_n\boldv_n</mrow>
            <mrow>\boldw\amp =b_1\boldv_1+b_2\boldv_2+\cdots +b_n\boldv_n</mrow>
          </md>
        </p>
        <p>
          We can do this since <m>B</m> spans <m>V</m>.
          It follows that for any <m>c,d\in\R</m>
          <me>
            c\boldv+d\boldw=(ca_1+db_1)\boldv_1+(ca_2+db_2)\boldv_2+\cdots +(ca_n+db_n)\boldv_n
          </me>
        </p>
        <p>
          Now, using the definition of coordinate vectors we have
          <md>
            <mrow>_B\amp =\colvec{ca_1+db_1</mrow>
            <mrow>ca_2+db_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>ca_n+db_n}</mrow>
            <mrow>\amp =\colvec{ca_1</mrow>
            <mrow>ca_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>ca_n}+\colvec{db_1</mrow>
            <mrow>db_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>db_n}</mrow>
            <mrow>\amp =c\colvec{a_1</mrow>
            <mrow>a_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>a_n}+d\colvec{b_1</mrow>
            <mrow>b_2</mrow>
            <mrow>\vdots</mrow>
            <mrow>b_n}</mrow>
            <mrow>\amp =c[\boldv]_B+d[\boldw]_B</mrow>
          </md>.
        </p>
        <p>
          (b) I begin, somewhat ponderously,
          by first proving that <m>T(\boldv)=[\boldv]_B=(0,0,\dots, 0)</m> if and only if <m>\boldv=\boldzero</m>.
          This is fairily obvious.
          Indeed, given an arbitrary <m>\boldv\in V</m>,
          we have <m>\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n</m> for some <m>c_i\in \R</m>,
          and <m>[\boldv]_B=(c_1,c_2,\dots, c_n)</m>.
          Clearly <m>[\boldv]_B=(0,0,\dots, 0)</m> if and only if
          <m>c_1=c_2=\cdots =c_n=0</m> if and only if <m>\boldv=\boldzero</m>.
        </p>
        <p>
          We have proved a special case of the general claim:
          namely <m>T(\boldv)=T(\boldzero)</m> iff <m>\boldv=\boldzero</m>.
          The general case follows by using the fact that <m>T</m> is linear.
          We have
          <md>
            <mrow>T(\boldv_1)=T(\boldv_2)\amp \Leftrightarrow T(\boldv_1)-T(\boldv_2)=(0,0,\dots, 0)</mrow>
            <mrow>\amp \Leftrightarrow T(\boldv_1-\boldv_2)=(0,0,\dots, 0) \amp \text{ (\(T\) is linear) }</mrow>
            <mrow>\amp \Leftrightarrow \boldv_1-\boldv_2=\boldzero \amp \text{ (by our special case!) }</mrow>
            <mrow>\amp \Leftrightarrow \boldv_1=\boldv_2</mrow>
          </md>
        </p>
        <p>
          This same trick can be used to show, in general,
          that a linear transformation <m>T</m> is one-to-one if and only if <m>\NS T=\{\boldzero\}</m>.
        </p>
        <p>
          (c) This is obvious.
          Given any <m>(c_1,c_2,\dots, c_n)\in\R^n</m>,
          set <m>\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n</m>.
          Then by definition <m>[\boldv]_B=(c_1,c_2,\dots, c_n)</m>.
        </p>
        <p>
          (d) We can now use our results above to prove the remaining statements.
          Note that
          <md>
            <mrow>c_1\boldw_1+c_2\boldw_2+\cdots +c_r\boldw_r=\boldzero \amp \Leftrightarrow T(c_1\boldw_1+c_2\boldw_2+\cdots +c_r\boldw_r)=(0,0,\dots, 0) \amp \text{ (special case of (b)!) }</mrow>
            <mrow>\amp \Leftrightarrow c_1T(\boldw_1)+c_2T(\boldw_2)+\cdots +c_rT(\boldw_r)=(0,0,\dots, 0) \amp \text{ (\(T\) is linear) }</mrow>
          </md>
        </p>
        <p>
          From this we see that the <m>\boldw_i</m> are linearly independent if and only if the
          <m>T(\boldw_i)</m> are linearly independent.
        </p>
        <p>
          (e) Suppose the <m>\boldw_i</m> span.
          Given any <m>\boldy\in \R</m>,
          there is a <m>\boldv\in V</m> such that <m>T(\boldv)=\boldy</m>,
          since <m>\range T=\R^n</m> (part (c)).
          Since the <m>\boldw_i</m> span,
          we can write <m>\boldv=d_1\boldw_1+d_2\boldw_2+\cdots +d_r\boldw_r</m>.
          Then
          <me>
            \boldy=T(\boldv)=T(d_1\boldw_1+d_2\boldw_2+\cdots +d_r\boldw_r)=d_1T(\boldw_1)+d_2T(\boldw_2)+\cdots +d_rT(\boldw_r)
          </me>,
          showing that the <m>T(\boldw_i)</m> span <m>\R^n</m>.
        </p>
        <p>
          Conversely, suppose the <m>T(\boldw_i)</m> span <m>\R^n</m>.
          Given any <m>\boldv\in V</m>,
          we have <m>T(\boldv)=d_1T(\boldw_1)+d_2T(\boldw_2)+\cdots +d_rT(\boldw_r)</m>,
          for some <m>d_i\in\R</m>,
          since <m>T(\boldv)\in\R^n</m> and the
          <m>T(\boldw_i)</m> span <m>\R^n</m> by assumption.
          But then
          <me>
            T(\boldv)=T(d_1\boldw_1+d_2\boldw_2+\cdots +d_r\boldw_r)
          </me>,
          since <m>T</m> is linear.
          It follows that
          <me>
            \boldv=d_1\boldw_1+d_2\boldw_2+\cdots +d_r\boldw_r
          </me>,
          since <m>T</m> is 1-1 by (b) !! This shows that the
          <m>\boldw_i</m> span <m>V</m>, as desired.
        </p>
        <p>
          Note: properties (d) and (e) can be summarized by saying <m>T</m> preserves linear independence and <m>T</m> preserves spanning sets.
          This property is enjoyed more generally by any isomorphism between vector spaces.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>S=\{p_1=1+x+2x^2,p_2=1-x,p_3=1+x^2,p_4=1+x-x^2\}</m> and let <m>W=\Span(S)\subset P</m>.
        (Recall <m>P</m> is the space of all polynomials.)
        <ol>
          <li>
            <p>
              Use
              <q>street smarts</q>
              to decide whether <m>S</m> is linearly independent.
            </p>
          </li>
          <li>
            <p>
              Use coordinate vectors and an appropriate fundamental space algorithm to choose a basis of <m>W=\Span(S)</m>
              <em>from among the elements of <m>S</m></em>.
            </p>
          </li>
          <li>
            <p>
              Give a satisfying description of <m>W</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) In fact we see that <m>S</m> lies within the smaller subspace <m>P_2</m>,
          which has dimension <m>3</m>.
          Since <m>S</m> has 4 elements and lives in a 3-dimensional space,
          it is guaranteed to be dependent.
          Street smarts!
        </p>
        <p>
          (b) Let <m>B</m> be the standard basis of <m>P_2</m>:
          i.e., <m>B=\{1,x,x^2\}</m>.
          Then we can translate this whole problem into <m>\R^3</m> using coordinate vectors.
          Namely we let:
          <me>
            \boldv_1=(p_1)_B=\begin{bmatrix}1\\ 1\\ 2 \end{bmatrix} , \boldv_2=(p_2)_B=\begin{bmatrix}1\\ -1\\ 0 \end{bmatrix} , \boldv_3=(p_3)_B=\begin{bmatrix}1\\0 \\ 1 \end{bmatrix} , \boldv_4=(p_4)_B=\begin{bmatrix}1\\ 1\\ -1 \end{bmatrix}
          </me>,
          and set <m>W'=\Span(\{\boldv_1,\boldv_2,\boldv_3,\boldv_4\}</m>.
        </p>
        <p>
          Putting the <m>\boldv_i</m> into the columns of a matrix <m>A</m> and applying our column space algorithm tells us that
          <m>\{\boldv_1,\boldv_2,\boldv_4\}</m> form a basis for <m>W'</m>.
          (You do the work.)
        </p>
        <p>
          Translating back to <m>P_3</m> we conclude that the corresponding polynomials
          <m>\{p_1,p_2,p_4\}</m> form a basis for <m>W</m>.
        </p>
        <p>
          (c) Since <m>W</m> has a basis containing three elements,
          we see that <m>\dim(W)=3</m>.
          Since <m>W\subset P_2</m> and <m>\dim(P_2)=\dim(W)=3</m>,
          we see by the dimension theorem compendium
          (subspace part)
          that <m>W</m> is in fact all of <m>P_2</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=P_2</m>, and let <m>S=\{x^2+2x+1, 3x^2+6x \}</m>.
        Extend <m>S</m> to a basis of <m>P_2</m> by first translating the problem to <m>\R^3</m> using coordinate vectors and applying the relevant algorithm there.
        <solution>
          <p>
            Let <m>B=\{1,x,x^2\}</m> be the standard basis of <m>P_2</m>.
            Applying coordinate vectors to <m>S</m> yields the set <m>\{ (1,2,1), (0,6,3)\}</m>.
            The usual
            <q>extend to a basis</q>
            algorithm tells us that <m>\{(1,2,1), (0,6,3), (0,1,0)\}</m> is a basis for <m>\R^3</m>.
            Now translate back to our original vector space to conclude that
            <m>\{ x^2+2x+1, 3x^2+6x,
            x\}</m> is a basis for <m>P_2</m> extending <m>S</m>. (Note that the vector <m>(0,1,0)</m> corresponds to the polynomial
            <m>p(x)=0+1x+0=x</m> via the coordinate vector map. )
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=P_3</m> and consider the polynomials
        <me>
          p_1=x^3+1, p_2=2x^3+x+1, p_3=3x^3+2x+1, p_4=2x^3+x^2+x+1
        </me>,
        Let <m>S=\{p_1, p_2, p_3, p_4\}</m> and define <m>W=\Span(S)</m>.
        Find a subset of <m>S</m> that is a basis of <m>W</m> and compute <m>\dim(W)</m>.
        Use coordinate vectors!
        <solution>
          <p>
            <term>General technique:</term> translate problem to <m>\R^4</m> using coordinate vectors,
            answer the problem there using one of the fundamental space algorithms,
            then translate the results back into the original <m>P_3</m> context.
            Let <m>B=\{1,x,x^2,x^3\}</m> be the standard basis of <m>P_3</m>,
            and translate the whole problem to <m>\R^4</m> using coordinate vectors.
            That is, we let
            <md>
              <mrow>\boldv_1\amp =[p_1]_B=\begin{bmatrix}[r] 1</mrow>
              <mrow>0</mrow>
              <mrow>0</mrow>
              <mrow>1 \end{bmatrix} \amp  \boldv_2\amp =[p_2]_B=\begin{bmatrix}[r] 1</mrow>
              <mrow>1</mrow>
              <mrow>0</mrow>
              <mrow>2 \end{bmatrix}</mrow>
              <mrow>\boldv_3\amp =[p_3]_B=\begin{bmatrix}[r] 1</mrow>
              <mrow>2</mrow>
              <mrow>0</mrow>
              <mrow>3 \end{bmatrix} \amp \boldv_4\amp =[p_4]_B=\begin{bmatrix}[r] 1</mrow>
              <mrow>1</mrow>
              <mrow>1</mrow>
              <mrow>2 \end{bmatrix}</mrow>
            </md>
            and let <m>W'=\Span\{\boldv_1,\boldv_2,\boldv_3,\boldv_4\}\subseteq \R^4</m>.
            We use the <em>column space</em>
            algorithm to determine a basis of <m>W'</m> from among the <m>\boldv_i</m>.
            That is <m>W'=\CS(A)</m> where
            <me>
              A=\begin{bmatrix}1\amp 1\amp 1\amp 1\\ 0\amp 1\amp 2\amp 1\\ 0\amp 0\amp 0\amp 1\\ 1\amp 2\amp 3\amp 2 \end{bmatrix} \xrightarrow{\text{ row reduce } } U=\begin{bmatrix}1\amp 1\amp 1\amp 1\\0\amp 1\amp 2\amp 1\\0\amp 0\amp 0\amp 1\\0\amp 0\amp 0\amp 0 \end{bmatrix}
            </me>.
          </p>
          <p>
            Since the first,
            second and fourth columns of <m>U</m> form a basis for <m>\CS(U)</m>,
            the same is true of <m>A</m>,
            which means <m>\{\boldv_1,\boldv_2,\boldv_4\}</m> is a basis for <m>W'</m>.
          </p>
          <p>
            Lastly, returning back to <m>P_3</m> we conclude that
            <m>\{p_1,p_2,p_4\}</m> is a basis for <m>W</m> in <m>P_3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_fundspace">
      <p>
        Let
        <me>
          S=\left\{A_1=\begin{bmatrix}1\amp 2\\1\amp 1 \end{bmatrix} , \ A_2=\begin{bmatrix}1\amp 1\\2\amp 1 \end{bmatrix} , \ A_3=\begin{bmatrix}-1\amp 1\\ -4\amp -1 \end{bmatrix}  , \ A_4=\begin{bmatrix}0\amp 1\\2\amp 0 \end{bmatrix} \right\}
        </me>
        and define <m>W=\Span(S)\subseteq M_{22}</m>.
        <ol>
          <li>
            <p>
              Compute a basis <m>B</m> of <m>W</m> from among the elements of <m>S</m>.
              You should first
              <q>translate</q>
              the problem into <m>\R^4</m> using coordinate vectors and apply an appropriate fundamental space algorithm.
            </p>
          </li>
          <li>
            <p>
              Show that in fact <m>W</m> is equal to the subspace <m>W'=\left\{\begin{bmatrix}a\amp b\\ c\amp a \end{bmatrix} \colon a,b,c\in\R\right\}</m>.
              Make your life easier by using a dimension argument,
              but make sure all your claims are fully justified.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Translate the problem to <m>\R^4</m> using the coordinate vector map with respect to the standard basis,
          and identify in this way the given vector space with <m>\CS(A)</m>, where
          <me>
            A=\begin{bmatrix}1\amp 1\amp -1\amp 0\\ 2\amp 1\amp 1\amp 1\\ 1\amp 2\amp -4\amp 2\\ 1\amp 1\amp -1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The GE procedure for <m>\CS(A)</m> tells us that the first,
          second and fourth columns form a basis for <m>\CS(A)</m>.
          Translating back to <m>M_{22}</m>,
          we conclude that <m>B=\{A_1,A_2,A_4\}</m> is a basis for <m>W</m>.
        </p>
        <p>
          (b) We see easily that <m>W'</m> has basis
          <me>
            B'=\left\{\begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix} , \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix} \right\}
          </me>.
        </p>
        <p>
          Thus <m>\dim W'=3</m>.
          Since <m>B\subseteq W'</m>, it follows that <m>W=\Span(B)\subseteq W'</m>.
          Since <m>\dim W=\dim W'=3</m>,
          we conclude by the dimension theorem compendium
          (subspace part)
          that <m>W=W'</m>.
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{4.4: matrix representations}
  </p>
  <ol>
    <li>
      <p>
        Define <m>T\colon \R^3\rightarrow \R^3</m> to be reflection through the plane <m>\mathcal{P}\colon x+y+z=0</m>.
        In other words,
        <m>T</m> takes an input <m>(x,y,z)</m> and sends it to its reflection
        <m>(x',y',z')</m> on the other side of <m>\mathcal{P}</m>.
        You may take it on faith that <m>T</m> is a linear transformation.
      </p>
      <ol>
        <li>
          <p>
            Draw a picture of <m>\mathcal{P}</m>,
            an arbitrary point <m>(x,y,z)</m>,
            and its reflection <m>(x',y',z')</m>.
            Add some right triangles to your picture to indicate the relation between <m>(x,y,z)</m> and <m>(x',y',z')</m>.
          </p>
        </li>
        <li>
          <p>
            Pick a <em>nonstandard</em> basis <m>B'</m> of <m>\R^3</m>,
            for which the behavior of <m>T</m> is easy to determine:
            i.e., pick a basis that reflects the geometry of this picture.
            Compute <m>[T]_{B'}</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) In general to draw the reflection of a point
          <m>P=(x,y,z)</m> through a plane <m>\mathcal{P}</m>,
          draw the line <m>\ell</m> passing through <m>(x,y,z)</m> that is perpendicular to <m>\mathcal{P}</m>
          (so parallel to its normal vector),
          and call its intersection with the plane <m>Q</m>. (If you recall,
          <m>Q</m> is just the projection of <m>P</m> onto <m>\mathcal{P}</m>.
          The reflection of <m>P</m> is then the point <m>P'</m> on <m>\ell</m> on the other side of <m>\mathcal{P}</m>,
          whose distance to <m>Q</m> is equal to the distance from <m>P</m> to <m>Q</m>.
          If <m>P</m> is in the plane to begin with,
          then it is equal to its own reflection.
          <me>
            <image width="75%" source="../Lectures/Images/ReflectionThroughPlane"/>
          </me>
        </p>
        <p>
          (b) The description above helps us pick a basis <m>B</m> of <m>\R^3</m> that makes <m>[T]_B</m> particularly simple.
          Start of by choosing a basis
          <m>\{\boldv_1, \boldv_2\}</m> for the given plane <m>\mathcal{P}</m>:
          in our case,
          the vectors <m>\boldv_1=(1,-1,0), \boldv_2=(0,1,-1)</m> will do.
          To extend this to a full basis of <m>\R^3</m>,
          we add the normal vector <m>\boldn</m> to the plane.
          In our case,
          the basis is thus <m>B=\{\boldv_1=(1,-1,0), \boldv_2=(0,1,-1), \boldn=(1,1,1)\}</m>.
        </p>
        <p>
          Now, we have <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=\boldv_2</m>,
          since <m>\boldv_1</m> and <m>\boldv_2</m> lie in the plane already.
          Furthermore,
          from the picture we describe in (a) it is clear that <m>T(\boldn)=-\boldn</m>.
          This is because the projection of <m>\boldn</m> onto the plane is just <m>\boldzero</m>;
          so its reflection is just <m>-\boldn</m>.
        </p>
        <p>
          Lastly we now easily compute the coordinate vectors of these outputs:
          <m>[T(\boldv_1)]_B=[\boldv_1]_B=(1,0,0), [T(\boldv_2)]_B=[\boldv_2]_B=(0,1,0)</m>,
          and <m>[T(\boldn)]_B=[-\boldn]_B=(0,0,-1)</m>.
          We conclude that
          <me>
            [T]_B=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}
          </me>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Consider the differential equation
        <me>
          f''(x)+f'(x)=x^2+x+1 \tag{\(*\)}
        </me>
        The theory of ODE's tells us that there are infinitely many solutions <m>f(x)</m> to <m>(*)</m>.
        We will use linear algebra to find infinitely many polynomial solutions.
        <ol>
          <li>
            <p>
              First define <m>T\colon P_{3}\rightarrow P_{2}</m> as <m>T(f)=f''(x)+f'(x)</m>.
              Explain why <m>T(f)</m> indeed lies in <m>P_2</m> and show that <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Compute <m>A=[T]_{B}^{B'}</m> where <m>B</m> and <m>B'</m> are the standard bases of <m>P_{3}</m> and <m>P_{2}</m>,
              respectively.
            </p>
          </li>
          <li>
            <p>
              Determine <m>\NS(A)</m> and <m>\CS(A)</m>.
            </p>
          </li>
          <li>
            <p>
              Find all solutions in <m>P_3</m> to the differential equation <m>(*)</m>. (First solve the relevant matrix equation involving <m>A</m>, then
              <q>lift up</q>
              to <m>P_3</m>. )
            </p>
          </li>
        </ol>
      </p>
      { Note: though we have found all
      <em>polynomial solutions</em>
      to the differential equation <m>(*)</m>,
      we have not found <em>all</em> solutions.
      Observe that <m>g(x)=e^{-x}</m> is a solution to the corresponding homogeneous equation
      <m>f''+f'=0</m>, It follows from basic ODE theory that we can add <m>ce^{-x}</m> to any of our solutions in (d) to get a new solution.
      Our analysis didn't catch these solutions as we restricted our attention to <m>P_3</m>. }
      <solution>
        <p>
          (a) The usual proof shows <m>T</m> is linear.
          Since taking the derivative knocks down the degree of a polynomial by 1, it's clear that if <m>p(x)</m> has degree at most <m>3</m>,
          then <m>T(p)=p''+p'</m> will have degree at most <m>2</m>.
        </p>
        <p>
          (b) Let <m>B=\{1,x,x^2,x^3\}</m> and <m>B'=\{1, x, x^2\}</m>.
          The usual recipe for <m>[T]_{B',B}</m> begins by computing
          <me>
            T(1)=0, T(x)=1, T(x^2)=2+2x, T(x^3)=6x+3x^2
          </me>.
        </p>
        <p>
          Taking coordinate vectors with respect to <m>B</m> and placing these in columns yields
          <me>
            A=\begin{bmatrix}0\amp 1\amp 2\amp 0\\ 0\amp 0\amp 2\amp 6\\ 0\amp 0\amp 0\amp 3 \end{bmatrix}
          </me>.
        </p>
        <p>
          (c) The rank of <m>A</m> is clearly 3
          (look at rows),
          and by inspection we see that
          <m>\{(1,0,0), (2,2,0), (0,6,3)\}</m> is a basis for <m>\CS(A)</m>.
          Since <m>\dim\CS(A)=3</m> and <m>\CS(A)\subset\R^3</m>,
          it follows from the Subspace Dimension Theorem that <m>\CS(A)=\R^3</m>.
        </p>
        <p>
          Since <m>\rank(A)=3</m>, it follows from R-N that <m>\nullity(A)=4-3=1</m>.
          To come up with a basis for <m>\NS(A)</m> we simply need to find one nonzero element of <m>\NS(A)</m>.
          We see <m>\{(1,0,0)\}</m> does the trick.
        </p>
        <p>
          What does all this mean about <m>T</m>?
          Since <m>\CS(A)=\R^3</m>, it follows that <m>\range(T)=P_2</m>.
          Thus for <em>any</em> <m>q(x)\in P_2</m>,
          we can find a <m>p(x)\in P_3</m> with <m>T(p)=p''(x)+p'(x)=q(x)</m>.
          This means not only can we solve the given differential equation,
          we can solve any similar equation of the form <m>f''+f'=q</m>,
          where <m>q</m> is a polynomial in <m>P_2</m>.
        </p>
        <p>
          Furthermore, as we will see,
          the fact that <m>\NS(A)</m> is nontrivial will mean that we can always find infinitely many solutions to <m>(*)</m>.
        </p>
        <p>
          (d) The polynomial <m>x^2+x+1</m> corresponds via
          <m>[\hspace{8pt}]_{B'}</m> to the vector <m>\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix}</m>.
          All solutions <m>\boldx</m> to
          <me>
            A\boldx=\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix}
          </me>
          will lift up to the solutions <m>p</m> to <m>T(p)=x^2+x+1</m>.
        </p>
        <p>
          Using GE, we see the general solution to the matrix equation is
          <me>
            \boldx=c\begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} +\begin{bmatrix}1\\-\frac{1}{2}\\ \frac{1}{3} \end{bmatrix}
          </me>
        </p>
        <p>
          Thus the general <em>polynomial</em>
          solution to the original differential equation is
          <me>
            p(x)=c+(1-\frac{1}{2}x^2+\frac{1}{3}x^3)=(c+1)-\frac{1}{2}x^2+\frac{1}{3}x^3
          </me>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>T\colon M_{22}\rightarrow M_{22}</m> by <m>T(A)=A^T-A</m>.
        <ol>
          <li>
            <p>
              Compute <m>A=[T]_B</m> where <m>B</m> is the standard basis for <m>M_{22}</m>.
            </p>
          </li>
          <li>
            <p>
              Compute bases for <m>\NS(A)</m> and <m>\CS(A)</m>.
            </p>
          </li>
          <li>
            <p>
              Use your result in (b) to give bases for <m>\NS(T)</m> and <m>\range(T)</m>.
            </p>
          </li>
          <li>
            <p>
              Identify <m>\NS(T)</m> and
              <m>\range(T)</m> as familiar subspaces of matrices.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The usual recipe yields
          <me>
            A=\begin{bmatrix}0\amp 0\amp 0\amp 0\\ 0\amp -1\amp 1\amp 0\\ 0\amp 1\amp -1\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>.
        </p>
        <p>
          We have <m>\NS(A)=\Span(\{(1,0,0,0), (0,0,0,1), (0,1,1,0)\}</m> and <m>\CS(A)=\Span(\{(0,-1,1,0)\}</m>.
        </p>
        <p>
          By lifting we get that
          <me>
            \NS(T)=\Span\left(\left\{\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\\ 0\amp 1 \end{bmatrix} , \begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix} \right\}\right), \ \range(T)=\Span\left(\left\{\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} \right\}\right)
          </me>.
        </p>
        <p>
          From these descriptions it follows easily that <m>\NS(T)</m> is the subspace of all symmetric matrices,
          and <m>\range(T)</m> is the subspace of all skew-symmetric matrices.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T:P_2\rightarrow P_3</m> be the linear transformation defined by <m>T(p)=x\cdot p(x-3)</m>.
        <ol>
          <li>
            <p>
              Find <m>[T]_{B}^{B'}</m> relative to the bases
              <m>B=\{1,x,x^2\}</m> and <m>B' = \{1,x,x^2,x^3\}</m>.
            </p>
          </li>
          <li>
            <p>
              Use <m>[T]_{B}^{B'}</m> to compute bases for <m>\NS T</m> and <m>\range T</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          By the formula for <m>T</m>
          <md>
            <mrow>T(1) \amp =\amp  x(1) = x</mrow>
            <mrow>T(x) \amp =\amp  x(x-3) = x^2 - 3x</mrow>
            <mrow>T(x^2) \amp =\amp  x^3-6x^2+9x</mrow>
          </md>
        </p>
        <p>
          Taking these vectors relative the the basis <m>B'</m>,
          we can write
          <me>
            [T]_{B}^{B'} = \begin{bmatrix}0\amp 0\amp 0\\ 1\amp -3\amp 9\\ 0\amp 1\amp -6\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T:P_2 \rightarrow M_{22}</m> be the linear transformation defined by
        <me>
          T(p) = \begin{bmatrix}p(0)\amp p(1)\\ p(-1)\amp p(0) \end{bmatrix}
        </me>
        let <m>B</m> be the standard basis for <m>M_{22}</m>,
        let <m>B'=\{1,x,x^2\}</m> and
        <m>B''=\{1,1+x,1+x^2\}</m> be bases for <m>P_2</m>.
        <ol>
          <li>
            <p>
              Find <m>[T]_{B'}^{B}</m> and <m>[T]_{B''}^{B}</m>.
            </p>
          </li>
          <li>
            <p>
              Use either one of the matrix representations from (a) to compute bases for <m>\NS T</m> and <m>\range T</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Lets work on <m>[T]_{B}^{B'}</m> first.
          By the formula for <m>T</m>
          <md>
            <mrow>T(1) \amp =\amp \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</mrow>
            <mrow>T(x) \amp =\amp \begin{bmatrix}0\amp 1\\ -1\amp 0 \end{bmatrix}</mrow>
            <mrow>T(x^2) \amp =\amp \begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Taking these vectors relative the the basis <m>B</m>, we can write
          <me>
            [T]_{B}^{B'} = \begin{bmatrix}1\amp 0\amp 0\\ 1\amp 1\amp 1\\ 1\amp -1\amp 1\\ 1\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Now for <m>[T]_{B}^{B''}</m>.
          By the formula for <m>T</m>
          <md>
            <mrow>T(1) \amp =\amp \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</mrow>
            <mrow>T(1+x) \amp =\amp \begin{bmatrix}1\amp 2\\ 0\amp 1 \end{bmatrix}</mrow>
            <mrow>T(1+x^2) \amp =\amp \begin{bmatrix}1\amp 2\\ 2\amp 1 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Taking these vectors relative the the basis <m>B</m>, we can write
          <me>
            [T]_{B}^{B''} = \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 2\amp 2\\ 1\amp 0\amp 2\\ 1\amp 1\amp 1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T:M_{22} \rightarrow R^2</m> be the linear transformation given by
        <me>
          T\left( \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix} \right) = \begin{bmatrix}a+b+c\\ d \end{bmatrix}
        </me>
        and let <m>B</m> be the standard basis for <m>M_{22}</m>,
        <m>B'</m> be the standard basis for <m>R^2</m> and let
        <me>
          B'' = \left\{ \begin{bmatrix}1\\ 1 \end{bmatrix} , \begin{bmatrix}-1\\ 0 \end{bmatrix} \right\}
        </me>
        be another basis for <m>R^2</m>.
        Find <m>[T]_{B}^{B'}</m> and <m>[T]_{B}^{B''}</m>.
        <solution>
          <p>
            By the formula for <m>T</m>,
            we can find <m>T(\boldu_i)</m> where
            <m>\boldu_i</m> are the vectors in the basis <m>B</m>.
            <md>
              <mrow>T(\boldu_1) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_2) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_3) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_4) \amp =\amp \begin{bmatrix}0\\ 1 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B'</m>,
            we can write
            <me>
              [T]_{B}^{B'} = \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B''</m>,
            we can write
            <me>
              [T]_{B}^{B''} = \begin{bmatrix}0\amp 0\amp 0\amp 1\\ -1\amp -1\amp -1\amp 1 \end{bmatrix}
            </me>.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{4.5: change of basis}
  </p>
  <ol>
    <li>
      <p>
        Let <m>B=\{\boldv_1,\boldv_2,\dots ,\boldv_n\}</m>.
        Compute <m>\underset{B\rightarrow B'}{P}</m> for each of the following bases <m>B'</m>,
        and explain what happens to coordinate vectors as we change from <m>B</m> to <m>B'</m>. (a)
        <m>B'=\{\boldv_2,\boldv_1,\boldv_3,\dots, \boldv_n\}</m>. (b) <m>B'=\{3\boldv_1, 3\boldv_2,\dots , 3\boldv_n\}</m>.
        <solution>
          <p>
            In each case we will describe the change of basis matrix column by column,
            making use <m>\bolde_j</m>, the elements of the standard basis
            (or equivalently, the columns of the identity matrix).
          </p>
          <p>
            The computation of each column is done by inspection,
            since the new basis is closely related to the old one.
          </p>
          <p>
            (a) We have <m>\underset{B\rightarrow B'}{P}=\begin{bmatrix}\vert \amp \vert\amp \amp \vert\\ \bolde_2\amp \bolde_1\amp \cdots \amp \bolde_n\\ \vert \amp \vert\amp \amp \vert \end{bmatrix}</m>.
          </p>
          <p>
            When changing from <m>B</m> to <m>B'</m>,
            simply swap the first two entries of the coordinate vector.
          </p>
          <p>
            (b) We have <m>\underset{B\rightarrow B'}{P}=\begin{bmatrix}\vert \amp \vert\amp \amp \vert\\ \frac{1}{3}\bolde_1\amp \frac{1}{3}\bolde_2\amp \cdots \amp \frac{1}{3}\bolde_n\\ \vert \amp \vert\amp \amp \vert \end{bmatrix}</m>.
          </p>
          <p>
            When changing from <m>B</m> to <m>B'</m>,
            scale each entry of the coordinate vector by <m>1/3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=P_1</m> with basis <m>B'=\{2x+1, 3x+2\}</m>.
      </p>
      <ol>
        <li>
          <p>
            Compute <m>[p(x)]_{B'}</m>, where <m>p(x)=ax+b</m>, <m>a,b\in\R</m>.
            Your answer will be in terms of <m>a</m> and <m>b</m>.
          </p>
        </li>
        <li>
          <p>
            Now do the same computation using a change of basis matrix involving the standard basis <m>B</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) We seek the <m>c_1, c_2</m> such that <m>c_1(2x+1)+c_2(3x+2)=ax+b</m>.
          This boils down to solving the matrix equation
          <me>
            \begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix} \begin{bmatrix}c_1\\ c_2 \end{bmatrix} =\begin{bmatrix}a\\ b \end{bmatrix} \Longleftrightarrow \begin{bmatrix}c_1\\ c_2 \end{bmatrix} =\left(\begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix} \right)^{-1}\begin{bmatrix}a\\ b \end{bmatrix} =\begin{bmatrix}2a-3b\\ -a+2b \end{bmatrix}
          </me>
        </p>
        <p>
          (b) To do the same computation with a change of basis matrix,
          we let <m>B=\{x, 1\}</m>.
          Then <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix}</m>,
          and hence
          <me>
            \underset{B\rightarrow B'}{P}=\left(\begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix} \right)^{-1}=\begin{bmatrix}2\amp -3\\ -1\amp 2 \end{bmatrix}
          </me>.
        </p>
        <p>
          Thus
          <me>
            [ax+b]_{B'}= \underset{B\rightarrow B'}{P}[ax+b]_B=\begin{bmatrix}2\amp -3\\ -1\amp 2 \end{bmatrix} \begin{bmatrix}a\\ b \end{bmatrix} = \begin{bmatrix}2a-3b\\ -a+2b \end{bmatrix}
          </me>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V</m> be a finite-dimensional vector space, and let <m>B</m>,
        <m>B'</m>, <m>B''</m> be three different bases of <m>V</m>.
        Let <m>P=\underset{B\rightarrow B'}{P}</m> and <m>Q=\underset{B\rightarrow B''}{P}</m>.
        <ol>
          <li>
            <p>
              Express <m>\underset{B'\rightarrow B''}{P}</m> in terms of <m>P</m>,
              <m>Q</m>, and/or their inverses.
              Verify that your matrix satisfies the defining property of the change of basis matrix.
            </p>
          </li>
          <li>
            <p>
              Utilize the formula in (a) to compute <m>\underset{B'\rightarrow B''}{P}</m>,
              where <m>B'=\{2x+1, 3x+2\}</m> and <m>B''=\{x-1,x+1\}</m>:
              two nonstandard bases of <m>P_1</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) I claim <m>\underset{B'\rightarrow B''}{P}=QP^{-1}</m> To verify the claim, I need only show that <m>QP^{-1}</m> satisfies the defining property:
          i.e., <m>QP^{-1}[\boldv]_{B'}=[\boldv]_{B''}</m> for all <m>\boldv\in V</m>.
          We compute
          <md>
            <mrow>QP^{-1}[\boldv]_{B'}\amp =Q\underset{B'\rightarrow B}{P}[\boldv]_{B'} \amp \left(\underset{B'\rightarrow B}{P}=(\underset{B\rightarrow B'}{P})^{-1}\right)</mrow>
            <mrow>\amp =Q[\boldv]_{B}</mrow>
            <mrow>\amp =[\boldv]_{B''} \amp (Q=\underset{B\rightarrow B''}{P})</mrow>
          </md>
        </p>
        <p>
          This proves that <m>QP^{-1}=\underset{B'\rightarrow B''}{P}</m>.
        </p>
        <p>
          (b) Let <m>B=\{x, 1\}</m>.
          Then we have <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix}</m>,
          and <m>\underset{B\rightarrow B''}{P}=\begin{bmatrix}1\amp 1\\ -1\amp 1 \end{bmatrix} ^{-1}=\frac{1}{2}\begin{bmatrix}1\amp -1\\ 1\amp 1 \end{bmatrix}</m>.
        </p>
        <p>
          We have
          <me>
            \underset{B'\rightarrow B''}{P}=\underset{B\rightarrow B''}{P}\underset{B'\rightarrow B}{P}=\frac{1}{2}\begin{bmatrix}1\amp -1\\ 1\amp 1 \end{bmatrix} \begin{bmatrix}2\amp 3\\ 1\amp 2 \end{bmatrix} =\frac{1}{2}\begin{bmatrix}1\amp 1\\ 3\amp 5 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>B</m>, <m>B'</m>,
        and <m>B''</m> be three different bases for a finite dimensional vector space <m>V</m>.
        Show that
        <me>
          \underset{B\rightarrow B''}{P}=\underset{B'\rightarrow B''}{P}\cdot \underset{B\rightarrow B'}{P}
        </me>
        using only the defining property and uniqueness of change of basis matrices.
        <solution>
          <p>
            Let <m>C=\underset{B'\rightarrow B''}{P}\cdot \underset{B\rightarrow B'}{P}</m>.
            Then we have
            <md>
              <mrow>C[\boldv]_B\amp =\underset{B'\rightarrow B''}{P}\cdot \underset{B\rightarrow B'}{P} [\boldv]_B</mrow>
              <mrow>\amp =\underset{B'\rightarrow B''}{P}(\underset{B\rightarrow B'}{P} [\boldv]_B)</mrow>
              <mrow>\amp =\underset{B'\rightarrow B''}{P}[\boldv]_{B'} \amp \text{(prop. of \(\underset{B\rightarrow B'}{P}\)) }</mrow>
              <mrow>\amp =[\boldv]_{B''} \amp \text{(prop. of \(\underset{B'\rightarrow B''}{P}\)) } </mrow>
            </md>.
          </p>
          <p>
            Thus <m>C[\boldv]_B=[\boldv]_{B''}</m> for all <m>\boldv\in V</m>,
            which is the defining property of <m>\underset{B\rightarrow B''}{P}</m>.
            By <em>uniqueness</em> it follows that <m>C=\underset{B\rightarrow B''}{P}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        True or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>B</m> is a basis for an <m>n</m>-dimensional space <m>V</m>,
              then <m>\underset{B\rightarrow B}{P}=I_n</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\underset{B\rightarrow B'}{P}</m> is diagonal,
              then each basis element of <m>B</m> is a scalar multiple of some basis element of <m>B'</m>.
            </p>
          </li>
          <li>
            <p>
              If each basis element of <m>B'</m> is a scalar multiple of <em>some</em>
              basis element of <m>B</m>,
              then <m>\underset{B\rightarrow B'}{P}</m> is diagonal.
            </p>
          </li>
          <li>
            <p>
              The coordinate vector of a vector <m>\boldx</m> in <m>\R^n</m> relative to the standard basis for <m>\R^n</m> is <m>\boldx</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) True.
          We show this in the lecture notes.
        </p>
        <p>
          (b) True.
          Since <m>\underset{B\rightarrow B'}{P}</m> is diagonal,
          so is its inverse <m>\underset{B\rightarrow B'}{P}^{-1}=\underset{B'\rightarrow B}{P}</m>.
          The <m>j</m>-th column of this matrix is,
          using the change of basis formula,
          just <m>[\boldv_j']_B</m>,
          where <m>\boldv_j'</m> is the <m>j</m>-th element of <m>B'</m>.
          Since the matrix is diagonal,
          this column vector is of the form
          <m>c_j\bolde_j</m> for some <m>c_j\in\R</m>.
          Unpacking the definition of the coordinate vector,
          this means simply that <m>\boldv_j=c_j\bolde_j</m> for some <m>c_j\in \R</m>.
          So in fact the <m>j</m>-th element of <m>B'</m> is a scalar multiple {of the <m>j</m>-th element of <m>B</m>}
          (not just any element).
        </p>
        <p>
          (c) False.
          As the color in the previous solution emphasizes,
          to be diagonal we need the <m>j</m>-th element of <m>B'</m> to be a scalar multiple of the <m>j</m>-th element of <m>B</m>.
          Consider the bases <m>B=\{(1,0), (0,1)\}</m> and <m>B'=\{(0,1), (1,0)\}</m> of <m>\R^2</m>.
          Then the elements of <m>B'</m> are scalar multiples of elements of <m>B</m>,
          but <m>\underset{B\rightarrow B'}{P}=\begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix}</m> is not diagonal.
        </p>
        <p>
          (d) True.
          Let <m>B</m> be the standard basis,
          and let <m>\bolde_i</m> be the <m>i</m>-th element <m>B</m>:
          i.e., <m>\bolde_i</m> has a 1 in the <m>i</m>-th entry,
          and 0's elsewhere.
          Given any <m>\boldv=(a_1,a_2,\dots, a_n)</m>,
          we have <m>\boldv=a_1\bolde_1+a_2\bolde_2+\cdots +a_n\bolde_n</m>,
          as we have remarked before.
          By definition,
          we then have <m>[\boldv]_B=(a_1,a_2,\dots, a_n)=\boldv</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Consider the bases <m>B = \{p_1 = 6 +3x,
        p_2 = 10+2x\}</m> and <m>B'=\{q_1=2, q_2=3+2x\}</m> for <m>P_1</m>.
        <ol>
          <li>
            <p>
              Compute the transition matrix from <m>B'</m> to <m>B</m>.
            </p>
          </li>
          <li>
            <p>
              Compute the transition matrix from <m>B</m> to <m>B'</m>.
            </p>
          </li>
          <li>
            <p>
              Compute the coordinate vector <m>[p]_B</m>,
              where <m>p = -4 +x</m> and use the change of basis formula theorem to compute <m>[p]_{B'}</m>.
            </p>
          </li>
          <li>
            <p>
              Now compute <m>[p]_{B'}</m> directly and verify that your answer agrees with part (c).
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          To compute the transition matrix from <m>B'</m> to <m>B</m> place the bases in a matrix,
          with the new basis on the left and the old basis on the right,
          and use row operations to reduce the left side to the identity.
          <me>
            \begin{bmatrix}6\amp 10\amp 2\amp 3\\ 3\amp 2\amp 0\amp 2 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp -2/9\amp 7/9\\ 0\amp 1\amp 1/3\amp -1/6 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus transition matrix from <m>B'</m> to <m>B</m> is
          <me>
            \begin{bmatrix}-2/9\amp 7/9\\ 1/3\amp -1/6 \end{bmatrix}
          </me>
        </p>
        <p>
          Similarly, we can find the transition matrix from <m>B</m> to <m>B'</m>
          <me>
            \begin{bmatrix}2\amp 3\amp 6\amp 10\\ 0\amp 2\amp 3\amp 2 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp 3/4\amp 7/2\\ 0\amp 1\amp 3/2\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus transition matrix from <m>B</m> to <m>B'</m> is
          <me>
            \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          Computing <m>[p]_B</m> directly we can see
          <me>
            \begin{bmatrix}6\amp 10\amp -4\\ 3\amp 2\amp 1 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp 1\\ 0\amp 1\amp -1 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus
          <me>
            [p]_B = \begin{bmatrix}1\\ -1 \end{bmatrix}
          </me>
        </p>
        <p>
          Using formula <m>(12)</m>, we can find <m>[p]_{B'}</m>
          <me>
            [p]_{B'} = \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix} \begin{bmatrix}1\\ -1 \end{bmatrix} = \begin{bmatrix}-11/4\\ 1/2 \end{bmatrix}
          </me>
        </p>
        <p>
          Finally if we compute <m>[p]_{B'}</m> directly we will arrive at the same answer.
          <me>
            \begin{bmatrix}2\amp 3\amp -4\\ 0\amp 2\amp 1 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp -11/4\\ 0\amp 1\amp 1/2 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus
          <me>
            [p]_{B'} = \begin{bmatrix}-11/4\\ 1/2 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>B</m> be the standard basis for <m>\R^3</m>,
        and let <m>B' = \{\boldv_1,\boldv_2,\boldv_3\}</m> where <m>\boldv_1 = (1,2,1)</m>,
        <m>\boldv_2 = (2,5,0)</m>, and <m>\boldv_3 = (3,3,8)</m>.
        <ol>
          <li>
            <p>
              Compute <m>\underset{B'\rightarrow B}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Now compute <m>\underset{B\rightarrow B'}{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Confirm these two matrices are inverses of one another.
            </p>
          </li>
          <li>
            <p>
              Let <m>\boldw = (5,-3,1)</m>.
              Compute <m>[\boldw]_{B'}</m> directly,
              then compute <m>[\boldw]_{B'}</m> using
              <m>[\boldw]_{B}</m> and the relevant change of basis matrix.
            </p>
          </li>
          <li>
            <p>
              Let <m>\boldw = (3,-5,0)</m>.
              Compute <m>[\boldw]_{B}</m> by inspection,
              then compute <m>[\boldw]_{B'}</m> using
              <m>[\boldw]_{B}</m> and the relevant transition matrix.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) By inspection
          <me>
            \underset{B'\rightarrow B}{P} = \begin{bmatrix}1\amp 2\amp 3\\ 2\amp 5\amp 3\\ 1\amp 0\amp 8 \end{bmatrix}
          </me>
        </p>
        <p>
          (b) We now compute <m>\underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}</m>.
          Using formula (14)
          <me>
            \begin{bmatrix}1\amp 2\amp 3\amp 1\amp 0\amp 0\\ 2\amp 5\amp 3\amp 0\amp 1\amp 0\\ 1\amp 0\amp 8\amp 0\amp 0\amp 1 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp 0\amp -40\amp 16\amp 9\\ 0\amp 1\amp 0\amp 13\amp -5\amp -3\\ 0\amp 0\amp 1\amp 5\amp -2\amp -1 \end{bmatrix}
          </me>
          we can see that
          <me>
            \underset{B\rightarrow B'}{P}= \begin{bmatrix}-40\amp 16\amp 9\\ 13\amp -5\amp -3\\ 5\amp -2\amp -1 \end{bmatrix}
          </me>
        </p>
        <p>
          (c) It is easy to see that these are inverses
          <me>
            \begin{bmatrix}1\amp 2\amp 3\\ 2\amp 5\amp 3\\ 1\amp 0\amp 8 \end{bmatrix} \begin{bmatrix}-40\amp 16\amp 9\\ 13\amp -5\amp -3\\ 5\amp -2\amp -1 \end{bmatrix} = \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          (d) Calculating <m>[\boldw]_{B'}</m> directly
          <me>
            \begin{bmatrix}1\amp 2\amp 3\amp 5\\ 2\amp 5\amp 3\amp -3\\ 1\amp 0\amp 8\amp 1 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp 0\amp -239\\ 0\amp 1\amp 0\amp 77\\ 0\amp 0\amp 1\amp 30 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus
          <me>
            [\boldw]_{B'} = \begin{bmatrix}-239\\ 77\\ 30 \end{bmatrix}
          </me>
        </p>
        <p>
          Now using change of basis
          <me>
            [\boldw]_{B'} =\underset{B\rightarrow B'}{P}[\boldw]_B= \begin{bmatrix}-40\amp 16\amp 9\\ 13\amp -5\amp -3\\ 5\amp -2\amp -1 \end{bmatrix} \begin{bmatrix}5\\ -3\\ 1 \end{bmatrix} = \begin{bmatrix}-239\\ 77\\ 30 \end{bmatrix}
          </me>
        </p>
        <p>
          (e) By inspection we have <m>[(-3,5,0)]_B=(-3,5,0)</m>.
          Then, using our change of basis matrix, we have
          <me>
            [\boldw]_{B'} = \begin{bmatrix}-40\amp 16\amp 9\\ 13\amp -5\amp -3\\ 5\amp -2\amp -1 \end{bmatrix} \begin{bmatrix}3\\ -5\\ 0 \end{bmatrix} = \begin{bmatrix}-200\\ 64\\ 25 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        The matrix
        <me>
          \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 3\amp 2\\ 0\amp 1\amp 1 \end{bmatrix}
        </me>
        is the change of basis matrix from what basis <m>B</m> to the basis <m>\{(1,1,1),(1,1,0),(1,0,0)\}</m> in <m>\R^3</m>?
        <solution>
          <p>
            Let <m>B = \{(a,b,c),(d,e,f),(h,g,i)\}</m> Then
            <md>
              <mrow>(a,b,c) \amp =\amp  1(1,1,1) + 0(1,1,0) + 0(1,0,0) = (1,1,1)</mrow>
              <mrow>(d,e,f) \amp =\amp  0(1,1,1) + 3(1,1,0) + 1(1,0,0) = (4,3,0)</mrow>
              <mrow>(g,h,i) \amp =\amp  0(1,1,1) + 2(1,1,0) + 1(1,0,0) = (3,2,0)</mrow>
            </md>
          </p>
          <p>
            Thus <m>B = \{(1,1,1),(4,3,0),(3,2,0)\}</m>.
            We can even double check this by using formula (14)
            <me>
              \begin{bmatrix}1\amp 1\amp 1\amp 1\amp 4\amp 3\\ 1\amp 1\amp 0\amp 1\amp 3\amp 2\\ 1\amp 0\amp 0\amp 1\amp 0\amp 0 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 0\amp 3\amp 2\\ 0\amp 0\amp 1\amp 0\amp 1\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T_{\alpha}\colon\R^2\rightarrow\R^2</m> be reflection through the line <m>\ell_{\alpha}</m>,
        as defined previously in Recall also that
        <m>T_{\alpha}</m> is an origin-fixing isometry,
        hence a linear transformation.
      </p>
    </li>
    <li>
      <p>
        Define <m>T\colon \R^3\rightarrow\R^3</m> as
        <me>
          T(x_1,x_2,x_3) = (x_1+2x_2-x_3,-x_2,x_1+7x_3)
        </me>
        Let <m>B</m> be the standard basis of <m>\R^3</m>,
        and let <m>B'</m> be the nonstandard basis
        <me>
          B' = \{(1,0,0),(1,1,0),(1,1,1)\}
        </me>.
        First compute <m>[T]_B</m>,
        then compute <m>[T]_{B'}</m> using the change of basis formula for transformations.
        <solution>
          <p>
            Start by using <m>T</m> on each vector in the standard basis
            <md>
              <mrow>T((1,0,0)) \amp =\amp  (1,0,1)</mrow>
              <mrow>T((0,1,0)) \amp =\amp  (2,-1,0)</mrow>
              <mrow>T((0,0,1)) \amp =\amp  (-1,0,7)</mrow>
            </md>
          </p>
          <p>
            Then we can take these as the columns of <m>[T]_B</m>.
            <me>
              [T]_B = \begin{bmatrix}1\amp 2\amp -1\\ 0\amp -1\amp 0\\ 1\amp 0\amp 7 \end{bmatrix}
            </me>
          </p>
          <p>
            According to the change of basis formula,
            we have <m>[T]_{B'} = P_{B\rightarrow B'}[T]_BP_{B'\rightarrow B}</m>.
            Recall that in this very specific context (<m>V=\R^n</m>,
            <m>B</m> the standard basis) the change of basis matrix
            <m>\underset{B'\rightarrow B}{P}</m> is obtained by simply placing the elements of <m>B'</m> as the columns of a vector:
            <me>
              P_{B'\rightarrow B} = \begin{bmatrix}1\amp 1\amp 1\\ 0\amp 1\amp 1\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Then we have
            <me>
              \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}= \begin{bmatrix}1\amp -1\amp 0\\ 0\amp 1\amp -1\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Now we can use the change of basis formula to compute
            <me>
              [T]_{B'} = \begin{bmatrix}1\amp -1\amp 0\\ 0\amp 1\amp -1\\ 0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 2\amp -1\\ 0\amp -1\amp 0\\ 1\amp 0\amp 7 \end{bmatrix} \begin{bmatrix}1\amp 1\amp 1\\ 0\amp 1\amp 1\\ 0\amp 0\amp 1 \end{bmatrix}  = \begin{bmatrix}1\amp 4\amp 3\\ -1\amp -2\amp -9\\ 1\amp 1\amp 8 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Define <m>T\colon P_1\rightarrow P_1</m> as
        <me>
          T(a_0 +a_1x) = a_0 +a_1(x+1) = (a_0 +a_1) +a_1x
        </me>.
        The following are two different bases for <m>P_1</m>:
        <m>B = \{6+3x, 10+2x\}</m>; <m>B' = \{2,3+2x\}</m>.
        First compute <m>[T]_B</m>,
        then use the change of basis formula for transformations to compute <m>[T]_{B'}</m>.
        <solution>
          <p>
            In a similar fashion to the last problem
            <md>
              <mrow>_B \amp =\amp  [9+3x]_B=(2/3, 1/2)</mrow>
              <mrow>\left[T(10+2x)\right]_B \amp =\amp  [12 +2x]_B=(-2/9,4/3)</mrow>
            </md>
          </p>
          <p>
            Thus
            <me>
              [T]_B = \begin{bmatrix}2/3\amp -2/9\\ 1/2\amp 4/3 \end{bmatrix}
            </me>
          </p>
          <p>
            We compute <m>P_{B\rightarrow B'}</m> using the recipe
            <me>
              P_{B\rightarrow B'} =\begin{bmatrix}\vert\amp \vert \\ [6+3x]_{B'}\amp [10+2x]_{B'}\\ \vert\amp \vert \end{bmatrix} = \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Then
            <me>
              P_{B'\rightarrow B}=(P_{B\rightarrow B'})^{-1}= \begin{bmatrix}-2/9\amp 7/9\\ 1/3\amp -1/6 \end{bmatrix}
            </me>
          </p>
          <p>
            Then we have
            <me>
              [T]_{B'} = \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix} \begin{bmatrix}2/3\amp -2/9\\ 1/2\amp 4/3 \end{bmatrix} \begin{bmatrix}-2/9\amp 7/9\\ 1/3\amp -1/6 \end{bmatrix} = \begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation between two finite-dimensional vector spaces.
        Suppose <m>B_1, B_2</m> are two different bases of <m>V</m>,
        and <m>B_1', B_2'</m> are two different bases of <m>W</m>.
        State and prove a matrix equation relating
        <m>[T]_{B_2}^{B_2'}</m> in terms of <m>[T]_{B_1}^{B_1'}</m>,
        <m>\underset{B_1\rightarrow B_2}{P}</m>,
        <m>\underset{B_1'\rightarrow B_2'}{P}</m>,
        and/or the inverses of these change of basis matrices.
        To prove your equality,
        you should show that the LHS and RHS matrices both satisfy the defining property of <m>[T]_{B_2}^{B_2'}</m>.
        <solution>
          <p>
            I claim
            <me>
              [T]_{B_2}^{B_2'}=\underset{B_1'\rightarrow B_2'}{P}\ [T]_{B_1}^{B_1'}\ \underset{B_2\rightarrow B_1}{P}=\underset{B_1'\rightarrow B_2'}{P}\ [T]_{B_1}^{B_1'}\ (\underset{B_1\rightarrow B_2}{P})^{-1}
            </me>
          </p>
          <p>
            To prove the claim, I need only show the matrix on the RHS, call it <m>C</m>,
            satisfies the defining property of <m>[T]_{B_2}^{B_2'}</m>: namely,
            <me>
              C[\boldv]_{B_2}=[T(\boldv)]_{B_2'} \text{ for all \(\boldv\in V\) }
            </me>.
          </p>
          <p>
            To this end we compute:
            <md>
              <mrow>\underset{B_1'\rightarrow B_2'}{P}\ [T]_{B_1}^{B_1'}\ \underset{B_2\rightarrow B_1}{P}[\boldv]_{B_2}\amp =\underset{B_1'\rightarrow B_2'}{P}\ [T]_{B_1}^{B_1'}[\boldv]_{B_1} \amp \text{ (convert to \(B_1\) coordinates) }</mrow>
              <mrow>\amp =\underset{B_1'\rightarrow B_2'}{P}[T(\boldv)]_{B_1'} \amp \text{(defining property of \([T]_{B_1}^{B_1'}\))}</mrow>
              <mrow>\amp =[T(\boldv)]_{B_2'} \amp \text{ (convert to \(B_2'\)-coordinates) }</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Suppose <m>T\colon \mathbb{R}^2\rightarrow\mathbb{R}^2</m> is a linear transformation,
        and that for any vector <m>\mathbf{v}</m> of the form <m>\mathbf{v}=c(1,1)+d(-1,1)</m>, we have
        <me>
          T(\mathbf{v})=T\left(c(1,1)+d(-1,1)\right)=(c+d)(1,1)+d(-1,1)
        </me>.
        Note: this is an example of a <em>shearing transformation</em>
        along the vector <m>(1,1)</m>.
        <ol>
          <li>
            <p>
              Let <m>B'=\{(1,1), (-1,1)\}</m>.
              Compute <m>[T]_{B'}</m>.
            </p>
          </li>
          <li>
            <p>
              Use the change of basis formula to compute <m>A=[T]_B</m>,
              where <m>B</m> is the standard basis.
            </p>
          </li>
          <li>
            <p>
              Compute <m>T(1,2)</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) We compute <m>T(1,1)=T\left(1(1,1)+0(-1,1)\right)=(1+0)(1,1)+0(-1,1)=(1,1)</m>.
          Thus <m>[T(1,1)]_{B'}=(1,0)</m>.
        </p>
        <p>
          Similarly, <m>T(-1,1)=T\left( 0(1,1)+1(-1,1)\right)=1(1,1)+1(-1,1)</m>.
          Thus <m>[T(-1,1)]_{B'}=(1,1)</m>.
        </p>
        <p>
          It follows that <m>[T]_{B'}=\begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}</m>.
        </p>
        <p>
          (b) As usual we see by inspection that <m>\underset{B'\rightarrow B}{P}=\begin{bmatrix}1\amp -1\\ 1\amp 1 \end{bmatrix}</m>.
        </p>
        <p>
          Using inverses we then compute <m>\underset{B\rightarrow B'}{P}=\frac{1}{2}\begin{bmatrix}1\amp 1\\ -1\amp 1 \end{bmatrix}</m>.
        </p>
        <p>
          Thus
          <me>
            [T]_B=\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}=\frac{1}{2}\begin{bmatrix}1\amp -1\\ 1\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 1\\ -1\amp 1 \end{bmatrix} =\frac{1}{2}\begin{bmatrix}1\amp 1\\ -1\amp 3 \end{bmatrix}
          </me>.
        </p>
        <p>
          (c) We have
          <me>
            T(1,2)=A\begin{bmatrix}1\\2 \end{bmatrix} =\frac{1}{2}\begin{bmatrix}1\amp 1\\ -1\amp 3 \end{bmatrix} \begin{bmatrix}1\\ 2 \end{bmatrix} = \frac{1}{2}\begin{bmatrix}3\\ 5 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        For fixed direction vector <m>\boldd=(a,b)\ne \boldzero</m>,
        let <m>\ell</m> be the line in <m>\R^2</m> passing through the origin that it defines.
        Define <m>T\colon \R^2\rightarrow\R^2</m> to be reflection through <m>\ell</m>,
        as defined in <xref ref="ex_linereflection">Exercise</xref> of Section 3.2.
        Recall that <m>T</m> is a linear transformation.
        <ol>
          <li>
            <p>
              Construct a basis <m>B'</m> of <m>\R^2</m> which pays attention to the geometry involved in the definition of <m>T</m>,
              and compute <m>[T]_{B'}</m>.
              (If you picked your basis appropriately,
              <m>[T]_{B'}</m> should be diagonal.)
            </p>
          </li>
          <li>
            <p>
              Now use the change of basis formula to compute the <m>A</m> such that <m>T=T_A</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) I pick my basis to be <m>B'=\{\boldv_1, \boldv_2\}</m>,
          where <m>\boldv_1=(a,b)=\boldd</m>,
          the direction vector of <m>\ell</m>,
          and <m>\boldv_2=(-b,a)</m>, a vector pointing orthogonally to <m>\ell</m>.
        </p>
        <p>
          First observe that <m>T(\boldv_1)=\boldv_1</m>.
          This is because <m>\boldv_1</m> lies on <m>\ell</m>,
          and the reflection of a point that lies on <m>\ell</m> is equal to itself.
        </p>
        <p>
          Next, observe that <m>T(\boldv_2)=-\boldv_2</m>.
          This is because <m>\boldv_2</m> points orthogonally to <m>\ell</m>,
          so its orthogonal projection onto <m>\ell</m> is just <m>\boldzero</m>,
          and its reflection is thus <m>-\boldv_2</m>.
          (As usual, I conflate vectors with points in this discussion.)
          See picture below.
          <me>
            <image width="75%" source="../Lectures/Images/LineReflection"/>
          </me>
        </p>
        <p>
          Then we have
          <me>
            [T]_{B'}=\begin{bmatrix}\vert \amp \vert \\ [T(\boldv_1)]_{B'} \amp  [T(\boldv_2)]_{B'}\\ \vert \amp \vert \end{bmatrix} =\begin{bmatrix}\vert \amp \vert \\ [\boldv_1]_{B'} \amp  [-\boldv_2]_{B'}\\ \vert \amp \vert \end{bmatrix} =\begin{bmatrix}1\amp 0\\ 0\amp -1 \end{bmatrix}
          </me>.
        </p>
        <p>
          (b) To compute the <m>A</m> such that <m>T=T_A</m>,
          note that <m>A=[T]_B</m> where <m>B</m> is the standard basis of <m>\R^2</m>.
          We can then use the change of basis formula:
          <me>
            A=[T]_B=\underset{B'\rightarrow B}{P}\ [T]_{B'}\ \underset{B\rightarrow B'}{P}
          </me>
        </p>
        <p>
          We compute
          <me>
            \underset{B'\rightarrow B}{P}=\begin{bmatrix}a\amp -b\\ b\amp a \end{bmatrix} , \ \underset{B\rightarrow B'}{P}=\begin{bmatrix}a\amp -b\\ b\amp a \end{bmatrix} ^{-1}=\frac{1}{a^2+b^2}\begin{bmatrix}a\amp b\\ -b\amp a \end{bmatrix}
          </me>.
        </p>
        <p>
          Thus
          <me>
            A=[T]_B=\frac{1}{a^2+b^2}\begin{bmatrix}a\amp -b\\ b\amp a \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp -1 \end{bmatrix} \begin{bmatrix}a\amp b\\ -b\amp a \end{bmatrix} =\frac{1}{a^2+b^2}\begin{bmatrix}a^2-b^2\amp 2ab\\ 2ab\amp b^2-a^2 \end{bmatrix}
          </me>.
        </p>
        <p>
          Recall that earlier we derived the formula
          <me>
            A=\begin{bmatrix}\cos(2\theta) \amp \sin(2\theta)\\ \sin(2\theta)\amp -\cos(2\theta) \end{bmatrix}
          </me>,
          where <m>\theta</m> is the angle that the line <m>\ell</m> makes with the <m>x</m>-axis.
          Is our new formula consistent with this?
          Observe that the vector <m>(a,b)</m> has polar coordinates <m>(r,\theta)</m> where
          <me>
            r=\sqrt{a^2+b^2}, \ \cos\theta=\frac{a}{\sqrt{a^2+b^2}},\  \sin\theta=\frac{b}{\sqrt{a^2+b^2}}
          </me>.
        </p>
        <p>
          It follows that
          <me>
            \frac{1}{a^2+b^2}\begin{bmatrix}a^2-b^2\amp 2ab\\ 2ab\amp b^2-a^2 \end{bmatrix} = \begin{bmatrix}\cos^2\theta-\sin^2\theta\amp 2\cos\theta\sin\theta\\ 2\cos\theta\sin\theta\amp \sin^2\theta-\cos^2\theta \end{bmatrix} =\begin{bmatrix}\cos(2\theta) \amp \sin(2\theta)\\ \sin(2\theta)\amp -\cos(2\theta) \end{bmatrix}  \ \checkmark !
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        <em>Reflection through the plane <m>\mathcal{P}\colon x+y+z=0</m></em>.
        Define <m>T\colon \R^3\rightarrow \R^3</m> to be reflection through the plane <m>\mathcal{P}\colon x+y+z=0</m>.
        See the next exercise for a precise definition of this operation.
        <ol>
          <li>
            <p>
              Construct an explicit basis of the form <m>B'=\{\boldv_1,\boldv_2, (1,1,1)\}</m> where
              <m>\boldv_1,\boldv_2</m> are elements of <m>\mathcal{P}</m>.
            </p>
          </li>
          <li>
            <p>
              Compute <m>[T]_{B'}</m>.
            </p>
          </li>
          <li>
            <p>
              Find the <m>A</m> such that <m>T=T_A</m> using your result in (b) and a change of basis formula.
            </p>
          </li>
          <li>
            <p>
              Compute <m>T(1,2,3)</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          This exercise is really just a special case of the next one.
          I give more details in the solution there.
        </p>
        <p>
          (a) Set <m>B'=\{\boldv_1, \boldv_2, \boldn\}</m>,
          where <m>\boldv_1=(1,-1,0), \boldv_2=(0,1,-1), \boldn=(1,1,1)</m>.
          As is explained in more detail in the next exercise,
          we have <m>T(\boldv_1)=\boldv_1, T(\boldv_2)=\boldv_2</m>,
          and <m>T(\boldn)=-\boldn</m>.
          It follows that
          <me>
            [T]_{B'}=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}
          </me>.
        </p>
        <p>
          (b) To compute <m>A=[T]_B</m>,
          we use the change of basis formula
          <md>
            <mrow>A=[T]_B\amp =\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}</mrow>
            <mrow>\amp =\begin{bmatrix}[rrr] 1\amp 0\amp 1</mrow>
            <mrow>-1\amp 1\amp 1</mrow>
            <mrow>0\amp -1\amp 1 \end{bmatrix} \begin{bmatrix}[rrr] 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp -1 \end{bmatrix} \begin{bmatrix}[rrr] 1\amp 0\amp 1</mrow>
            <mrow>-1\amp 1\amp 1</mrow>
            <mrow>0\amp -1\amp 1 \end{bmatrix}^{-1}</mrow>
            <mrow>\amp =\frac{1}{3} \begin{bmatrix}[rrr] 1\amp -2\amp -2</mrow>
            <mrow>-2\amp 1\amp -2</mrow>
            <mrow>-2\amp -2\amp 1 \end{bmatrix}</mrow>
          </md>.
        </p>
        <p>
          (c) We have
          <me>
            T(1,2,3)=A\begin{bmatrix}1\\ 2\\ 3 \end{bmatrix} =\frac{1}{3} \begin{bmatrix}1\amp -2\amp -2\\ -2\amp 1\amp -2\\ -2\amp -2\amp 1 \end{bmatrix} \begin{bmatrix}1\\ 2\\ 3 \end{bmatrix} =\begin{bmatrix}-3\\ -2\\ -1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        <em>Reflection through a general plane in <m>\R^3</m></em>.
        Fix <m>\boldn=(a,b,c)\ne \boldzero</m>.
        Let <m>W</m> be the plane passing through the origin with normal vector <m>\boldn=(a,b,c)</m>:
        i.e., <m>W: ax+by+cz=0</m>.
        Reflection through <m>W</m> is the map <m>T\colon\R^3\rightarrow\R^3</m> defined as follows:
        <blockquote>
          given <m>P=(x,y,z)</m> let <m>\ell</m> be the line passing through <m>P</m> and perpendicular to <m>W</m>; let <m>Q</m> be the intersection of this line with <m>W</m>; then the reflection <m>T(P)</m> is the unique point <m>P'=(x',y',z')</m> on <m>\ell</m> such that the distance from <m>P</m> to <m>P'</m> is twice the distance from <m>P</m> to <m>Q</m>.
        </blockquote>
        Intuitively, <m>T(P)=P'</m> is the point
        <q>on the other side of the plane</q>
        from <m>P</m> and an equal distance away.
        You may take for granted that <m>T</m> is a linear transformation.
        Also, observe that if <m>P</m> lies in <m>W</m>,
        then the definition implies <m>T(P)=P</m>:
        <m>P</m> is its own reflection.
        Compute the matrix <m>A</m> such that <m>T=T_A</m> as follows:
        <ol>
          <li>
            <p>
              First compute <m>A'=[T]_{B'}</m>,
              where <m>B'=\{\boldv_1,\boldv_2,\boldv_3\}</m> is a basis for <m>\R^3</m> satisfying
              <m>\boldv_1, \boldv_2\in W</m>, <m>\boldv_3=(a,b,c)</m>.
            </p>
          </li>
          <li>
            <p>
              Now use the change of basis formula to compute <m>A=[T]_B</m>,
              where <m>B</m> is the standard basis.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) If we have a basis <m>B'</m> as described,
          then <m>[T]_{B'}=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}</m>.
          This is because <m>T</m> fixes any vector in <m>W</m>
          (reflection of point in plane is itself)
          and maps <m>\boldn</m> to <m>-\boldn</m>.
          Once we have found the basis <m>B'</m>,
          letting <m>B</m> be the standard basis, we have
          <me>
            A=[T]_B=\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}
          </me>.
        </p>
        <p>
          Furthermore, we know
          <me>
            \underset{B'\rightarrow B}{P}=\begin{bmatrix}\vert\amp \vert\amp \vert\\ \boldv_1\amp \boldv_2\amp \boldv_3\\ \vert\amp \vert\amp \vert \end{bmatrix} , \text{ and }  \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}
          </me>.
        </p>
        <p>
          So we are essentially done,
          except for some slightly unpleasant matrix arithmetic,
          after we find the basis <m>B'</m>.
          Let's do this now.
        </p>
        <p>
          To make our life easier, I assume that <em>at least two</em>
          of the elements <m>a, b, c</m> are nonzero.
          The excluded case, where exactly one of the elements <m>a</m>,
          <m>b</m>, and <m>c</m> is nonzero,
          would just be reflection through one of the coordinate planes,
          which is very easy to compute.
          Furthermore,
          the matrix formula we derive is valid even in these excluded cases,
          as you can check below!
        </p>
        <p>
          So assume that at least two of the elements <m>a,b,c</m> are nonzero.
          Then <m>\boldv_1=(b,-a,0)</m> and
          <m>\boldv_2=(0,c,-b)</m> make up a basis of <m>W</m>, as is easily checked.
          Setting <m>\boldv_3=\boldn</m>, we then have
          <me>
            \underset{B'\rightarrow B}{P}=\begin{bmatrix}b\amp 0\amp a\\ -a\amp c\amp b\\ 0\amp -b\amp c \end{bmatrix}
          </me>
          and hence
          <md>
            <mrow>A\amp =\begin{bmatrix} b\amp 0\amp a</mrow>
            <mrow>-a\amp c\amp b</mrow>
            <mrow>0\amp -b\amp c \end{bmatrix} \begin{bmatrix} 1\amp 0\amp 0</mrow>
            <mrow>0\amp 1\amp 0</mrow>
            <mrow>0\amp 0\amp -1 \end{bmatrix} \left(\begin{bmatrix} b\amp 0\amp a</mrow>
            <mrow>-a\amp c\amp b</mrow>
            <mrow>0\amp -b\amp c \end{bmatrix} \right)^{-1}</mrow>
            <mrow>\amp =\frac{1}{a^2+b^2+c^2}\begin{bmatrix} -a^2+b^2+c^2\amp -2ab\amp -2ac</mrow>
            <mrow>-2ab\amp a^2-b^2+c^2\amp -2bc</mrow>
            <mrow>-2ac\amp -2bc\amp a^2+b^2-c^2 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Admittedly, I used technology to compute the inverse and the matrix multiplication in the last line.
        </p>
      </solution>
    </li>
    <li>
      <p>
        <em>General rotation in <m>\R^3</m></em>.
        Suppose <m>W</m> is the plane passing through the origin perpendicular to a given <em>unit</em>
        normal vector <m>\boldn=(a,b,c)</m>:
        i.e., <m>W: ax+by+cz=0</m> and <m>\norm{\boldn}=1</m>.
        Fix an angle <m>\theta</m>.
        We define <m>T</m> to be rotation about <m>\boldn</m> by <m>\theta</m>,
        where the positive rotational direction is taken to be counterclockwise with respect to <m>\boldn</m>.
      </p>
    </li>
  </ol>
  <p>
    \chapter*{4.6: eigenvectors}
  </p>
  <ol type="itemsep5pt">
    <li>
      <p>
        Compute the characteristic polynomial, eigenvalues,
        and eigenspace bases for each of the following matrices.
      </p>
      <ol>
        <li>
          <p>
            <m>A= \begin{bmatrix}2\amp 1\\ 1\amp 2 \end{bmatrix}</m>
          </p>
        </li>
        <li>
          <p>
            <m>A= \begin{bmatrix}2\amp -3\\ 0\amp 2 \end{bmatrix}</m>
          </p>
        </li>
        <li>
          <p>
            <m>A= \begin{bmatrix}2\amp 0\\ 0\amp 2 \end{bmatrix}</m>
          </p>
        </li>
        <li>
          <p>
            <m>A= \begin{bmatrix}1\amp 2\\ -2\amp -1 \end{bmatrix}</m>
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) The characteristic polynomial is
          <md>
            \det(t I - A) = \begin{vmatrix}[cc] t - 2\amp -1\\ -1\amp t-2 \end{vmatrix} = (t - 2)^2 - 1 = t^2 -4t +3 = (t - 3)(t - 1) =0
          </md>
        </p>
        <p>
          Thus the eigenvalues are <m>\lambda = 3</m> and <m>\lambda = 1</m>.
        </p>
        <p>
          For <m>W_3</m> we see that
          <m>3I-A= \begin{bmatrix}1\amp -1\\ -1\amp 1 \end{bmatrix}</m> has rank 1, and hence nullity 1.
          By inspection we see that <m>\boldv_1=(1,1)</m> comprises a basis for <m>W_3</m>.
        </p>
        <p>
          For <m>\lambda = 1</m>,
          we have <m>I-A= \begin{bmatrix}-1\amp -1\\ -1\amp -1 \end{bmatrix}</m>.
          Again, by inspection we see that <m>I-A</m> has rank 1, hence nullity 1, and that <m>\boldv_2=(1,-1)</m> forms a basis.
        </p>
        <p>
          (b)
          <md>
            \det(t I- A) = \begin{vmatrix}[cc] t - 2\amp  3\\ 0\amp t - 2 \end{vmatrix} = t^2 - 4t + 4 = 0
          </md>
        </p>
        <p>
          Which factors as <m>(t -2)^2 = 0</m>.
          Thus we have only one eigenvalue, <m>\lambda = 2</m>.
        </p>
        <p>
          We have <m>2I-A=\begin{bmatrix}0\amp 3\\ 0\amp 0 \end{bmatrix}</m>,
          a matrix of rank 1, and hence nullity 1.
          We see that <m>\boldv_1=(1,0)</m> forms a basis for <m>W_2</m>.
        </p>
        <p>
          (c)
          <md>
            \det(t I- A) = \begin{vmatrix}[cc] t - 2\amp  0\\ 0\amp t - 2 \end{vmatrix} = t^2 - 4t + 4 = 0
          </md>
        </p>
        <p>
          Which factors as <m>(t -2)^2 = 0</m>.
          Thus we have only one eigenvalue, <m>t = 2</m>.
        </p>
        <p>
          We have <m>2I-A=\begin{bmatrix}0\amp 0\\ 0\amp 0 \end{bmatrix}</m> Then clearly <m>W_2=\NS(2I-A)=\R^2</m>,
          and <m>\{(1,0), (0,1)\}</m> is a basis of <m>W_2</m>:
          i.e., all nonzero vectors of <m>\R^2</m> are eigenvectors of <m>A</m>!
        </p>
        <p>
          (d) The characteristic polynomial is
          <md>
            \det(t I- A) = \begin{vmatrix}[cc] t - 1\amp  -\\ 2\amp t + 1 \end{vmatrix} = t^2 + 3 = 0
          </md>
        </p>
        <p>
          Which does not have any real eigenvalues.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Compute the characteristic polynomial,
        eigenvalues, and eigenspace bases for
        <me>
          A= \begin{bmatrix}1\amp 0\amp -2\\ 0\amp 0\amp 0\\ -2\amp 0\amp 4 \end{bmatrix}
        </me>
        <solution>
          <p>
            The characteristic polynomial is
            <md>
              \det(t I - A) = \begin{vmatrix}[ccc] t - 1\amp 0\amp 2\\ 0\amp t \amp 0\\ 2\amp 0\amp t -4 \end{vmatrix} = t^3-5t = 0
            </md>
          </p>
          <p>
            This factors as <m>t^2(t - 5)=0</m>.
            So the eigenvalues are <m>\lambda =0</m> and <m>\lambda = 5</m>.
            To find the basis for <m>\lambda = 0</m> we use
            <me>
              \begin{bmatrix}-1\amp 0\amp 2\\ 0\amp 0\amp 0\\ 2\amp 0\amp -4 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = 2x_3</m>,
            with the free variables <m>x_2=t</m> and <m>x_3=r</m>.
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}2r\\ t\\ r \end{bmatrix} = r \begin{bmatrix}2\\ 0\\ 1 \end{bmatrix}  +t \begin{bmatrix}0\\ 1\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = 0</m> is
            <me>
              \left\{ \begin{bmatrix}2\\ 0\\ 1 \end{bmatrix} , \begin{bmatrix}0\\ 1\\ 0 \end{bmatrix} \right\}
            </me>
          </p>
          <p>
            Now for <m>\lambda = 5</m> we use
            <me>
              \begin{bmatrix}4\amp 0\amp 2\\ 0\amp 5\amp 0\\ 2\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = -1/2x_3</m>,
            <m>x_2 = 0</m> and the free variable <m>x_3 = r</m>
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}-1/2r\\ 0\\ r \end{bmatrix} = r \begin{bmatrix}-1/2\\ 0\\ 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = 5</m> is
            <me>
              \left\{ \begin{bmatrix}-1/2\\ 0\\ 1 \end{bmatrix} \right\}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute the characteristic polynomial,
        eigenvalues, and eigenspace bases for
        <me>
          A= \begin{bmatrix}0\amp 1\amp 1\\ 1\amp 0\amp 1\\ 1\amp 1\amp 0 \end{bmatrix}
        </me>
        <solution>
          <p>
            The characteristic polynomial is
            <md>
              \det(t I - A) = \begin{vmatrix}[ccc] t\amp -1\amp -1\\ -1\amp t \amp -1\\ -1\amp -1\amp t \end{vmatrix} = t^3-3t - 2 = 0
            </md>
          </p>
          <p>
            This factors as <m>(t+1)^2(t - 2)=0</m>.
            So the eigenvalues are <m>\lambda =-1</m> and <m>\lambda = 2</m>.
            To find the basis for <m>\lambda = -1</m> we use
            <me>
              \begin{bmatrix}-1\amp -1\amp -1\\ -1\amp -1\amp -1\\ -1\amp -1\amp -1 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = -x_2-x_3</m>,
            with the free variables <m>x_2=t</m> and <m>x_3=r</m>.
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}-t-r\\ t\\ r \end{bmatrix} = t \begin{bmatrix}-1\\ 1\\ 0 \end{bmatrix}  +r \begin{bmatrix}-1\\ 0\\ 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = -1</m> is
            <me>
              \left\{ \begin{bmatrix}-1\\ 1\\ 0 \end{bmatrix} , \begin{bmatrix}-1\\ 0\\ -1 \end{bmatrix} \right\}
            </me>
          </p>
          <p>
            Now for <m>\lambda = 2</m> we use
            <me>
              \begin{bmatrix}2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = x_3</m>,
            <m>x_2 = x_3</m> and the free variable <m>x_3 = r</m>
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}r\\ r\\ r \end{bmatrix} = r \begin{bmatrix}1\\ 0\\ 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = 2</m> is
            <me>
              \left\{ \begin{bmatrix}1\\ 1\\ 1 \end{bmatrix} \right\}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute the characteristic polynomial,
        eigenvalues, and eigenspace bases for
        <me>
          A= \begin{bmatrix}1\amp -3\amp 3\\ 3\amp -5\amp 3\\ 6\amp -6\amp 4 \end{bmatrix}
        </me>
        <solution>
          <p>
            The characteristic polynomial is
            <md>
              \det(t I - A) = \begin{vmatrix}[ccc] t-1\amp 3\amp -3\\ -3\amp t + 5\amp -3\\ -6\amp 6\amp t-4 \end{vmatrix} = t^3-12t - 16 = 0
            </md>
          </p>
          <p>
            This factors as <m>(t+2)^2(t - 4)=0</m>.
            So the eigenvalues are <m>\lambda =-2</m> and <m>\lambda = 4</m>.
            To find the basis for <m>\lambda = -2</m> we use
            <me>
              \begin{bmatrix}-3\amp 3\amp -3\\ -3\amp 3\amp -3\\ -6\amp 6\amp -6 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = x_2-x_3</m>,
            with the free variables <m>x_2=t</m> and <m>x_3=r</m>.
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}t-r\\ t\\ r \end{bmatrix} = t \begin{bmatrix}1\\ 1\\ 0 \end{bmatrix}  +r \begin{bmatrix}-1\\ 0\\ 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = -2</m> is
            <me>
              \left\{ \begin{bmatrix}1\\ 1\\ 0 \end{bmatrix} , \begin{bmatrix}-1\\ 0\\ 1 \end{bmatrix} \right\}
            </me>
          </p>
          <p>
            Now for <m>\lambda = 4</m> we use
            <me>
              \begin{bmatrix}3\amp 3\amp -3\\ -3\amp 9\amp -3\\ -6\amp 6\amp 0 \end{bmatrix} \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}0\\ 0\\ 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus <m>x_1 = 1/2x_3</m>,
            <m>x_2 = 1/2x_3</m> and the free variable <m>x_3 = r</m>
            <me>
              \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix}1/2r\\ 1/2r\\ r \end{bmatrix} = r \begin{bmatrix}1/2\\ 1/2\\ 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus the basis when <m>\lambda = 4</m> is
            <me>
              \left\{ \begin{bmatrix}1/2\\ 1/2\\ 1 \end{bmatrix} \right\}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Find the characteristic polynomial by inspection.
        <me>
          \begin{bmatrix}9\amp -8\amp 6\amp 3\\ 0\amp -1\amp 0\amp 0\\ 0\amp 0\amp 3\amp 0\\ 0\amp 0\amp 0\amp 7 \end{bmatrix}
        </me>
        <solution>
          <p>
            Since the given matrix is upper triangular, so is <m>tI - A</m>.
            Since determinants of triangular matrices are just the product of their diagonal entries, we have
            <me>
              p(t)=\det(tI-A)=(t-9)(t+1)(t-3)(t-7)
            </me>.
          </p>
          <p>
            Similar reasoning shows,
            in general, that the eigenvalues of a triangular matrix
            (upper or lower or diagonal)
            are simply the entries along the diagonal.
          </p>
        </solution>
      </p>
    </li>
    <li xml:id="ex_last">
      <p>
        Let <m>A=\begin{bmatrix}-5\amp 0\amp 3\\ -6\amp 1\amp 3\\ -6\amp 0\amp 4 \end{bmatrix}</m>.
        Compute <m>p(t)</m> and find bases for all eigenspaces of <m>A</m>.
        <solution>
          <p>
            The characteristic polynomial of <m>A</m> is <m>p(t)=t^3-3t+2=(t-1)^2(t+2)</m>.
          </p>
          <p>
            We compute:
            <md>
              <mrow>W_{1}\amp =\NS(I-A)=\Span(\{(1,0,2),(0,1,0) \})</mrow>
              <mrow>W_{-2}\amp =\NS(-2I-A)=\Span(\{(1,1,1)\})</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        The matrix <m>B=\begin{bmatrix}-2\amp -3\amp 3\\ -3\amp -3\amp 4\\ -3\amp -4\amp 5 \end{bmatrix}</m> has the same characteristic polynomial of <m>A</m> in <xref ref="ex_last">Exercise</xref>.
        Find bases for all eigenspaces of <m>B</m>.
        <solution>
          <p>
            Since <m>B</m> also has characteristic polynomial <m>p(t)=(t-1)^2(t+2)</m>,
            it has only two eigenspaces.
          </p>
          <p>
            We compute:
            <md>
              <mrow>W_{1}\amp =\NS(I-B)=\Span(\{ (0,1,1) \})</mrow>
              <mrow>W_{-2}\amp =\NS(-2I-B)=\Span(\{(1,1,1)\})</mrow>
            </md>
          </p>
          <p>
            The two problems taken together show that the characteristic polynomial is not the whole story!
            The matrix <m>A</m> has more linearly independent eigenvectors than <m>B</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Use induction to prove that if <m>\lambda</m> is an eigenvalue of <m>A</m>,
        then <m>\lambda^k</m> is an eigenvalue of <m>A^k</m> for all positive integers <m>k</m>.
        Make sure to invoke the definition of eigenvalue in your proof.
        <solution>
          <p>
            Take <m>\boldv\ne \boldzero</m> with <m>A\boldv=\lambda\boldv</m>.
            We show by induction that <m>A^k\boldv=\lambda^k\boldv</m>,
            and thus that <m>\lambda^k</m> is an eigenvalue of <m>A^k</m>.
          </p>
          <p>
            The base case, <m>k=1</m>,
            is just the statement that <m>\boldv</m> is an eigenvector.
          </p>
          <p>
            Induction step: assume <m>A^{k}\boldv=\lambda^k\boldv</m>.
            Then
            <md>
              <mrow>A^{k+1}\boldv \amp =AA^{k}\boldv</mrow>
              <mrow>\amp =A(\lambda^k\boldv) \amp \text{ (induction hypo.) }</mrow>
              <mrow>\amp =\lambda^kA\boldv \amp \text{ (\(\lambda\) is a scalar) }</mrow>
              <mrow>\amp =\lambda^k\lambda\boldv\amp \text{ (\(\boldv\) is an eigenvector) }</mrow>
              <mrow>\amp =\lambda^{k+1}\boldv</mrow>
            </md>.
          </p>
          <p>
            If we didn't know proof by induction, our
            <q>proof</q>
            would look something like
            <md>
              <mrow>A^k\boldv \amp =\underbrace{A\cdot A\cdots A}_{\text{ \(k\) times } }\boldv</mrow>
              <mrow>\amp =\underbrace{A\cdot A\cdots A}_{\text{ \(k-1\) times } }(A\boldv)</mrow>
              <mrow>\amp =\underbrace{A\cdot A\cdots A}_{\text{ \(k-1\) times } }(\lambda\boldv) \amp \text{ (since \(A\boldv=\lambda\boldv\)) }</mrow>
              <mrow>\amp =\lambda\underbrace{A\cdot A\cdots A}_{\text{ \(k-1\) times } }(\boldv) \amp \text{ (since \(\lambda\) is a scalar) }</mrow>
              <mrow>\amp =\lambda^2\underbrace{A\cdot A\cdots A}_{\text{ \(k-2\) times } }(\boldv) \amp \text{ (by same logic) }</mrow>
              <mrow>\amp \vdots</mrow>
              <mrow>\amp =\lambda^{k-1}A\boldv</mrow>
              <mrow>\amp =\lambda^k\boldv</mrow>
            </md>
          </p>
          <p>
            You can think of proof by induction as a way of replacing all these <m>\dots</m> and
            <m>\ \vdots \</m> with a rigorous argument!
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove: <m>A</m> is invertible if and only if <m>\lambda=0</m> is <em>not</em>
        an eigenvalue of <m>A</m>.
        <solution>
          <p>
            There are many different ways of proving this.
          </p>
          <p>
            Let <m>\lambda_1, \dots , \lambda_n</m> be the eigenvalues of <m>A</m>.
            Recall that <m>\det(A)=\lambda_1\lambda_2\cdots \lambda_n</m>.
          </p>
          <p>
            Then we have
            <md>
              <mrow>A \text{ invertible }  \amp \Leftrightarrow \det(A)\ne 0</mrow>
              <mrow>\amp \Leftrightarrow \lambda_1\lambda_2\cdots \lambda_n\ne 0</mrow>
              <mrow>\amp \Leftrightarrow \lambda_i\ne 0 \text{ for all \(i\) }</mrow>
              <mrow>\amp \Leftrightarrow 0 \text{ is not an eigenvalue of \(A\) } </mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>B=\{\cos(x), \sin(x), \cosh(x), \sinh(x)\}</m>,
        let <m>V=\Span(B)</m>,
        and define <m>T\colon V\rightarrow V</m> by <m>T(f)=f'</m>.
        You may take as a fact that the set <m>B</m> is linearly independent,
        hence a basis.
        Recall: <m>\ds\cosh(x)=\frac{e^x+e^{-x}}{2}</m>,
        <m>\ds\sinh(x)=\frac{e^x-e^{-x}}{2}</m>.
        <ol>
          <li>
            <p>
              Compute <m>A=[T]_B</m>.
              What does <m>A</m> tell you about the invertibility of <m>T</m>?
            </p>
          </li>
          <li>
            <p>
              Find all eigenvalues of <m>T</m>.
            </p>
          </li>
          <li>
            <p>
              For each eigenvalue <m>\lambda</m> of <m>T</m>,
              find a basis for <m>W_\lambda</m>.
            </p>
          </li>
          <li>
            <p>
              Use your work in (c) to find a new basis <m>B'</m> of <m>V</m> that contains two eigenvectors of <m>T</m>.
            </p>
          </li>
          <li>
            <p>
              Compute <m>[T]_{B'}</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          We find that <m>A=\begin{bmatrix}0\amp 1\amp 0\amp 0\\ -1\amp 0\amp 0\amp 0\\ 0\amp 0\amp 0\amp 1\\ 0\amp 0\amp 1\amp 0 \end{bmatrix}</m>.
        </p>
        <p>
          Since <m>A</m> is invertible
          (its rank is clearly 4),
          it follows that <m>T</m> is invertible.
        </p>
        <p>
          The characteristic polynomial of <m>A</m>
          (in factored form)
          is <m>p(t)=(t-1)(t+1)(t^2+1)</m>.
          Thus the eigenvalues of <m>A</m>
          (and hence also <m>T</m>)
          are <m>\lambda=1</m> and <m>\lambda=-1</m>.
        </p>
        <p>
          We compute bases for the eigenspaces of <m>A</m>:
          <md>
            <mrow>W_1\amp =\NS(I-A)=\Span(\{ (0,0,1,1)\}</mrow>
            <mrow>W_{-1}\amp =\NS(-I-A)=\Span(\{(0,0,1,-1)\}</mrow>
          </md>
        </p>
        <p>
          Lifting these vectors back to <m>V</m> via the
          <m>[\hspace{5pt}]_B</m> map yields bases for the same eigenspaces of <m>T</m>:
          <md>
            <mrow>W_1\amp =\Span(\{\cosh x+\sinh x\}=\Span(\{e^x\})</mrow>
            <mrow>W_{-1}\amp =\Span(\{\cosh x-\sinh x\}=\Span(\{e^{-x}\})</mrow>
          </md>
        </p>
        <p>
          We construct the new basis <m>B'=\{\cos(x),\sin(x),e^x, e^{-x}\}</m>
          (it is easy to see that these four functions are independent),
          and compute
          <me>
            A'=[T]_{B'}=\begin{bmatrix}0\amp 1\amp 0\amp 0\\ -1\amp 0\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0\\ 0\amp 0\amp 0\amp -1 \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Suppose <m>A</m> is a <m>2\times 2</m> matrix satisfying <m>\tr A=2</m> and <m>\det A=-15</m>.
        Find the eigenvalues of <m>A</m>.
        <solution>
          <p>
            Let <m>\lambda, \lambda'</m> be the eigenvalues of <m>A</m>
            (possibly complex, possibly repeated).
            We have
            <md>
              <mrow>\lambda+\lambda'\amp =\tr A=2 \amp \text{ (trace is the sum of the eigenvalues) }</mrow>
              <mrow>\lambda\lambda'\amp =\det A=-15\amp \text{ (det. is the product of the eigenvalues) }</mrow>
            </md>
          </p>
          <p>
            The system implies <m>\lambda'=2-\lambda</m>,
            and hence that <m>\lambda^2-2\lambda-15=0</m>.
            Thus <m>\lambda=5</m> or <m>\lambda=-3</m>.
            Thus the eigenvalues of <m>A</m> are <m>5</m> and <m>-3</m>.
          </p>
          <p>
            Alternatively, the characteristic polynomial of <m>A</m> is
            <me>
              p(x)=x^2-\tr A x+\det A=x^2-2x-15=(x-5)(x+3)
            </me>.
          </p>
          <p>
            Thus the eigenvalues of <m>A</m> are <m>5</m> and <m>-3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T\colon V\rightarrow V</m> be a linear transformation,
        and suppose <m>B=\{\boldv_1, \boldv_2,\dots, \boldv_n\}</m>.
        Prove: <m>[T]_B</m> is a diagonal matrix if and only if
        <m>\boldv_j</m> is an eigenvector of <m>T</m> for all <m>1\leq i\leq n</m>.
        <em>Moral</em>.
        We can represent <m>T</m> by a diagonal matrix if and only if <m>T</m> has
        <q>enough</q>
        linearly independent eigenvectors to build a basis.
        <solution>
          <p>
            We prove the two implications separately.
          </p>
          <p>
            (<m>\Rightarrow</m>) Suppose the vectors
            <m>\boldv_i</m> are all eigenvectors with <m>T(\boldv_i)=\lambda_i\boldv_i</m>.
            Using the recipe for <m>[T]_{B}</m> we compute
            <md>
              <mrow>_B\amp =\begin{bmatrix} \vert \amp \vert \amp  \amp \vert</mrow>
              <mrow>[T(\boldv_1)]_B\amp [T(\boldv_2)]\amp \cdots \amp [T(\boldv_n)]_B</mrow>
              <mrow>\vert \amp \vert \amp  \amp \vert \end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix} \vert \amp \vert \amp  \amp \vert</mrow>
              <mrow>[\lambda_1\boldv_1]_B\amp [\lambda_2\boldv_2]\amp \cdots\amp  [\lambda_n\boldv_n]_B</mrow>
              <mrow>\vert \amp \vert \amp  \amp \vert \end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix}\lambda_1\amp 0\amp \cdots \amp 0</mrow>
              <mrow>0\amp \lambda_2\amp \cdots\amp 0</mrow>
              <mrow>\vdots \amp  \amp  \amp</mrow>
              <mrow>0\amp 0\amp \cdots\amp \lambda_n \end{bmatrix}</mrow>
            </md>,
            where the last equality holds since <m>\lambda_1\boldv_1=\lambda_1\boldv_1+0\boldv_2+\dots, \lambda_2\boldv_2=0\boldv_1+\lambda_2\boldv_2+0\boldv_3+\dots</m>, etc.
          </p>
          <p>
            (<m>\Leftarrow</m>) Suppose
            <me>
              [T]_B=D=\begin{bmatrix}d_1\amp 0\amp \cdots \amp 0\\ 0\amp d_2\amp \cdots\amp 0\\ \vdots \amp  \amp  \amp \\ 0\amp 0\amp \cdots\amp d_n \end{bmatrix}
            </me>.
          </p>
          <p>
            I claim <m>T(\boldv_i)=d_i\boldv_i</m> for all <m>i</m>,
            which proves <m>B</m> consists of eigenvectors of <m>T</m>.
            To do so, first observe that
            <md>
              <mrow>_B\amp =[T]_B[\boldv_i]_B \amp \text{ (defining property of \([T]_B\)) }</mrow>
              <mrow>\amp =D\bolde_i \amp \text{ (since \([\boldv_i]_B=(0,0,\dots, 1,0,\dots, 0)=\bolde_i\)) }</mrow>
              <mrow>\amp =\begin{bmatrix} 0</mrow>
              <mrow>\vdots</mrow>
              <mrow>d_i</mrow>
              <mrow>\vdots</mrow>
              <mrow>0 \end{bmatrix}</mrow>
            </md>.
          </p>
          <p>
            But we have <m>[T(\boldv_i)]_B=(0,0,\dots,
            d_i, 0,\dots, 0)</m> if and only if <m>T(\boldv_i)=d_i\boldv_i</m>,
            by definition of the coordinate vector map.
            This proves each <m>\boldv_i</m> is an eigenvector with eigenvalue <m>d_i</m>.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{4.7: diagonalization}
  </p>
  <ol>
    <li>
      <p>
        Find the eigenvalues of <m>A</m>.
        For each eigenvalue <m>\lambda</m>,
        find the rank of the matrix <m>\lambda I - A</m>.
        Is <m>A</m> diagonalizable?
        Justify.
        <me>
          A= \begin{bmatrix}3\amp 0\amp 0\\ 0\amp 2\amp 0\\ 0\amp 1\amp 2 \end{bmatrix}
        </me>
        <solution>
          <p>
            We compute <m>p(t)</m>:
            <md>
              \det(t I - A) = \begin{vmatrix}[rrr] t - 3\amp 0\amp 0\\ 0\amp t - 2\amp 0\\ 0\amp -1\amp t - 2 \end{vmatrix} = (t - 3)(t - 2)^2 = 0
            </md>
          </p>
          <p>
            Thus the eigenvalues are <m>\lambda =3</m> and <m>\lambda = 2</m>.
          </p>
          <p>
            We compute <m>\dim W_3=\NS(3I-A)</m>:
            <me>
              3I-A= \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp -1\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Since the rank of this matrix is 2, its nullity is 3-2=1.
            This means <m>\dim(W_{3})=1</m>.
          </p>
          <p>
            Next, we have
            <me>
              2I-A= \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp 0\amp 0\\ 0\amp -1\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Once again the nullity is 1, which means <m>\dim(W_2)=1</m>.
            Since the sum of the dimensions of the eigenspaces is <m>1+1\ne 3</m>,
            we conclude that <m>A</m> is not diagonalizable.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each matrix <m>A</m> below,
        find the geometric and algebraic multiplicity of each eigenvalue of the matrix <m>A</m>,
        and determine whether <m>A</m> is diagonalizable.
        If <m>A</m> is diagonalizable,
        then find the matrix <m>P</m> that diagonalizes <m>A</m>,
        and find <m>P^{-1}AP</m>.
      </p>
      <ol>
        <li>
          <p>
            <me>
              A = \begin{bmatrix}-1\amp 4\amp -2\\ -3\amp 4\amp 0\\ -3\amp 1\amp 3 \end{bmatrix}
            </me>
          </p>
        </li>
        <li>
          <p>
            <me>
              A= \begin{bmatrix}19\amp -9\amp -6\\ 25\amp -11\amp -9\\ 17\amp -9\amp -4 \end{bmatrix}
            </me>
          </p>
        </li>
        <li>
          <p>
            <me>
              A= \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 0\\ 3\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
        </li>
        <li>
          <p>
            <me>
              A = \begin{bmatrix}5\amp 0\amp 0\\ 1\amp 5\amp 0\\ 0\amp 1\amp 5 \end{bmatrix}
            </me>
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) Start by finding the eigenvalues of matrix <m>A</m>.
          <md>
            \det(t I - A) = \begin{vmatrix}[rrr] t +1\amp -4\amp 2\\ 3\amp t - 4\amp 0\\ 3\amp -1\amp t - 3 \end{vmatrix} = (t - 1)(t - 2)(t - 3) = 0
          </md>
        </p>
        <p>
          We have a theorem that says a matrix
          <m>A\in M_{nn}</m> with <m>n</m> distinct eigenvalues is diagonalizable.
          Thus <m>A</m> is diagonalizable.
          To construct the matrix <m>P</m>,
          we need to come up with bases for each space <m>W_\lambda=\NS(\lambda I-A)</m>.
          Each of these has dimension 1, and we easily come up with bases by inspection.
          This yields the basis <m>\boldv_1=(1,1,1)</m>,
          <m>\boldv_2=(2,3,3)</m> and <m>\boldv_3=(1,3,4)</m>.
        </p>
        <p>
          According to the recipe, we then have
          <me>
            P = \begin{bmatrix}1\amp 2\amp 1\\ 1\amp 3\amp 3\\ 1\amp 3\amp 4 \end{bmatrix} ,  D= \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 2\amp 0\\ 0\amp 0\amp 3 \end{bmatrix} , \text{ and }  D=P^{-1}AP
          </me>.
        </p>
        <p>
          (b) Start by finding the eigenvectors in the usual way
          <md>
            \det(t I - A) = \begin{vmatrix}[ccc] t-19 \amp 9\amp 6\\ -25\amp t+11\amp 9\\ -17\amp 9\amp t +4 \end{vmatrix} = t^3 - 4t^2+5t-2 = (t -1)^2(t-2) = 0
          </md>
        </p>
        <p>
          For <m>\lambda = 1</m>, we can see that the algebraic multiplicity is 2.
          Consider the matrix
          <me>
            \begin{bmatrix}-18 \amp 9\amp 6\\ -25\amp 12\amp 9\\ -17\amp 9\amp 5 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp -1\\ 0\amp 1\amp -4/3\\ 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus the rank is 2 and the geometric multiplicity is 1.
          Since the geometric multiplicity is less than the algebraic multiplicity for one of the eigenspaces,
          we know immediately that <m>A</m> is not diagonalizable.
        </p>
        <p>
          However, we continue on nonetheless.
          For <m>\lambda =2</m>, we can see that the algebraic multiplicity is 1.
          We know that <m>1\leq \dim W_2\leq 1</m>,
          hence we must have <m>\dim W_2=1</m>.
          Let's verify this:
          <me>
            2I-A= \begin{bmatrix}-17 \amp 9\amp 6\\ -25\amp 13\amp 9\\ -17\amp 9\amp 6 \end{bmatrix} \rightarrow \begin{bmatrix}1\amp 0\amp -3/4\\ 0\amp 1\amp -3/4\\ 0\amp 0\amp 0 \end{bmatrix}
          </me>,
          a matrix of rank 2, and hence nullity 1.
        </p>
        <p>
          Lastly, we see that <m>\dim W_1+\dim W_2=1+1=2\ne 3</m>.
          Hence <m>A</m> is not diagonalizable, as we claimed above.
        </p>
        <p>
          (c)
          <md>
            \det(t I - A) = \begin{vmatrix}[ccc] t \amp 0\amp 0\\ 0\amp t\amp 0\\ -3\amp 0\amp t -1 \end{vmatrix} = t^2(t -1) = 0
          </md>
        </p>
        <p>
          Thus the eigenvalues are <m>\lambda =0</m> and <m>\lambda = 1</m>.
          For <m>\lambda =0</m>, notice that the algebraic multiplicity is 2.
          Now consider the matrix
          <me>
            \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 0\\ -3\amp 0\amp -1 \end{bmatrix}
          </me>
        </p>
        <p>
          The rank is 1, thus the geometric multiplicity is 2.
          For <m>\lambda = 1</m>, notice that the algebraic multiplicity is 1.
          Now consider the matrix
          <me>
            \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ -3\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The rank is 2, thus the geometric multiplicity is 1.
          Since for all <m>\lambda</m> the geometric multiplicity is equal to the algebraic multiplicity,
          the initial matrix is diagonalizable.
          As usual, we build <m>P</m> by first computing bases for <m>W_0</m> and <m>W_1</m>,
          and then putting these in as the columns of <m>P</m>.
          One possible answer is
          <me>
            P = \begin{bmatrix}-1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 3\amp 0\amp 1 \end{bmatrix}
          </me>
          with corresponding
          <me>
            D= \begin{bmatrix}0\amp 0\amp 0\\ 0\amp 0\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}
          </me>
        </p>
        <p>
          (d)
          <md>
            \det(t I - A) = \begin{vmatrix}[ccc] t - 5\amp 0\amp 0\\ -1\amp t-5\amp 0\\ 0\amp -1\amp t -5 \end{vmatrix} = (t -5)^3 = 0
          </md>
        </p>
        <p>
          Thus <m>\lambda = 5</m> is the only eigenvalue and its algebraic multiplicity is 3.
          Now considering when <m>\lambda</m> is 5, we look at the matrix
          <me>
            \begin{bmatrix}0\amp 0\amp 0\\ -1\amp 0\amp 0\\ 0\amp -1\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          The rank of this matrix is 2, thus the geometric multiplicity is 1.
          Since algebraic multiplicity is not equal to the geometric multiplicity,
          the initial matrix is not diagonalizable.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A=\begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}</m>.
        Derive necessary and sufficient conditions for <m>A</m> to be diagonalizable expressed in terms of equalities and inequalities involving <m>a,b,c,d</m>.
        <solution>
          <p>
            First analyze the diagonalizability of <m>A</m> based on the number of distinct real eigenvalues it has: 0, 1, or 2.
          </p>
          <p>
            If <m>A</m> has 0 eigenvalues,
            then it is clearly not diagonalizable,
            as it has no eigenvectors.
          </p>
          <p>
            If <m>A</m> has 2 distinct eigenvalues,
            then it is automatically diagonalizable,
            by the corollary to the linear independence of eigenvectors theorem.
          </p>
          <p>
            If <m>A</m> has exactly one eigenvalue <m>\lambda</m>,
            then <m>A</m> is diagonalizable if and only if <m>\dim W_\lambda =2</m>
            (by the diagonalization theorem)
            if and only if <m>\dim\NS \lambda I -A=2</m> if and only if <m>\rank \lambda I-A=0</m> if and only if
            <m>\lambda I-A=\underset{2\times 2}{\boldzero}</m> if and only if <m>A=\lambda I</m> is diagonal !!
          </p>
          <p>
            We conclude that <m>A</m> is diagonalizable if and only if (a) it has two distinct eigenvalues or (b) it has one eigenvalue and is diagonal.
            We can relate these conditions to <m>a,b,c,d</m> by looking at the characteristic polynomial <m>p(x)</m>.
          </p>
          <p>
            The characteristic polynomial of
            <m>A=\begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix}</m> is <m>p(x)=x^2-(a+d)x+(ad-bc)</m>.
            Its roots are given by
            <me>
              x=\frac{1}{2}(a+d\pm \sqrt{a^2+2ad+d^2-4ad+4bc}\ )=\frac{1}{2}(a+d\pm \sqrt{(a-d)^2+4bc}\ )
            </me>
          </p>
          <p>
            We see that (a) <m>A</m> has two real eigenvalues iff <m>(a-d)^2>-4bc</m>,
            and that (b) <m>A</m> is diagonal with one distinct eigenvalue iff <m>a=d</m> and <m>b=c=0</m>.
          </p>
          <p>
            We conclude:
            <me>
              \text{ \(A\) is diagonalizable }  \Longleftrightarrow (a-d)^2>-4bc\text{ or }  a-d=b=c=0
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove all statements of the properties of similarity theorem,
        stated below.
      </p>
      <theorem>
        <title>Properties of similarity theorem</title>
        <statement>
          <p>
            Suppose <m>A</m> is similar to <m>B</m>:
            i.e., there is an invertible matrix <m>P</m> such that <m>B=P^{-1}AP</m>.
            Then:
            <ol>
              <li>
                <p>
                  <m>B</m> is similar to <m>A</m>.
                  (<em>Similarity is symmetric</em>.)
                </p>
              </li>
              <li>
                <p>
                  <m>A</m> and <m>B</m> have the same trace and determinant.
                </p>
              </li>
              <li>
                <p>
                  <m>A</m> and <m>B</m> have the same rank nullity and rank.
                </p>
              </li>
              <li>
                <p>
                  <m>A</m> and <m>B</m> have the same characteristic polynomial.
                </p>
              </li>
              <li>
                <p>
                  <m>A</m> and <m>B</m> have the same eigenvalues.
                </p>
              </li>
              <li>
                <p>
                  Given any <m>\lambda\in\R</m>,
                  let <m>W_\lambda</m> be the corresponding eigenspace for <m>A</m>,
                  and <m>W_\lambda'</m> the corresponding eigenspace for <m>B</m>.
                  Then <m>\dim W_{\lambda}=\dim W_{\lambda}'</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>
      <solution>
        <p>
          (a) If <m>B=P^{-1}AP</m>,
          then <m>A=PBP^{-1}=Q^{-1}BQ</m>, where <m>Q=P^{-1}</m>.
        </p>
        <p>
          (d) This was proven in the lecture notes.
        </p>
        <p>
          (b)&amp; (e) Follow directly from (d) and our theorem about characteristic polynomials.
          Indeed, let <m>p(x)=x^n+a_{n-1}x^{n-1}+\cdots +a_1x+a_0</m> be the characteristic polynomial of <m>A</m> and <m>B</m>.
          Then <m>\tr A=\tr B=-a_{n-1}</m>,
          <m>\det A=\det B=(-1)^na_0</m>,
          and the eigenvalues of both <m>A</m> and <m>B</m> are precisely the roots of <m>p(x)</m>. (c) I claim <m>\boldx'\in \NS B</m> if and only if
          <m>\boldx'=P^{-1}\boldx</m> for some <m>\boldx\in \NS B</m>.
          To see this, first observe that if <m>\boldx\in\NS A</m>, then
          <me>
            B\boldx'=B(P^{-1}\boldx)=P^{-1}AP(P^{-1}\boldx)=P^{-1}A\boldx=\boldzero
          </me>.
        </p>
        <p>
          Now suppose <m>\boldx'\in \NS B</m>.
          A similar argument
          (using <m>A=PBP^{-1}</m>)
          shows that <m>\boldx=P\boldx'\in \NS A</m>;
          and we have <m>\boldx'=P\boldx</m>.
        </p>
        <p>
          From our claim it follows that <m>P^{-1}</m> defines an invertible linear transformation
          (with inverse <m>P</m>)
          from <m>\NS A</m> to <m>\NS B</m>.
          In fancy terms this means <m>\NS A</m> and <m>\NS B</m> are isomorphic,
          and hence have the same dimension.
          More directly,
          if <m>B=\{\boldv_1,\dots, \boldv_r\}</m> is a basis for <m>\NS A</m>,
          then you can show that <m>\{P^{-1}\boldv_1, P^{-1}\boldv_2, \dots , P^{-1}\boldv_r\}</m> is a basis for <m>\NS B</m>.
        </p>
        <p>
          Either way, we see that <m>\nullity A=\dim \NS A=\dim\NS B=\nullity B</m>.
          The rank-nullity theorem then implies that <m>\rank A=\rank B</m>.
        </p>
        <p>
          (f) An argument exactly like the the one above shows that the map
          <m>\boldx\mapsto P^{-1}\boldx</m> defines an invertible linear transformation from
          <m>W_\lambda</m> to <m>W_\lambda'</m>,
          and that <m>P^{-1}</m> maps any basis of
          <m>W_{\lambda}</m> to a basis of <m>W_{\lambda}'</m>.
          Thus <m>\dim W_{\lambda}=\dim W_{\lambda}'</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Similar matrices have the same rank.
        Show the converse is false by showing the matrices
        <me>
          A = \begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} , B = \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}
        </me>
        have the same rank, but are not similar.
        <solution>
          <p>
            Notice that <m>\rank(A) = \rank(B) = 1</m>.
            Also notice that <m>\tr(A) = 1 \neq 0 = \tr(B)</m>.
            Since the traces are different,
            the matrices are not similar.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Similar matrices have the same eigenvalues.
        Show the converse is false by showing the matrices
        <me>
          A = \begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix} , B = \begin{bmatrix}1\amp 0\\ 0\amp 1 \end{bmatrix}
        </me>
        have the same eigenvalues, but are not similar.
        <solution>
          <p>
            Notice both <m>A</m> and <m>B</m> are triangular,
            thus <m>\det(t I - A) = (t - 1)^2 = \det(t I -B)</m>.
            Thus <m>\lambda = 1</m> is the only eigenvalue for both <m>A</m> and <m>B</m>.
          </p>
          <p>
            However, as one can easily compute,
            for <m>A</m> the eigenspace <m>W_1</m> has dimension 1, whereas for <m>B</m> the eigenspace <m>W_1</m> has dimension 2.
            Thus the two cannot be similar!
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove that if <m>B = P^{-1}AP</m>,
        and <m>\boldv</m> is an eigenvector of <m>B</m> corresponding to the eigenvalue <m>\lambda</m>,
        the <m>P\boldv</m> is the eigenvector of <m>A</m> corresponding to <m>\lambda</m>.
      </p>
      <solution>
        <p>
          We assume that <m>\boldv</m> is an eigenvector of <m>B</m>,
          and must show that <m>P\boldv</m> is an eigenvector of <m>A</m>.
        </p>
        <p>
          Thus by definition of eigenvector we assume
          <ol>
            <li>
              <p>
                <m>B\boldv=\lambda\bold v</m>, and
              </p>
            </li>
            <li>
              <p>
                <m>\boldv\ne\boldzero</m>.
              </p>
            </li>
          </ol>
        </p>
        <p>
          But then
          <md>
            <mrow>A(P\boldv)\amp =AP\boldv</mrow>
            <mrow>\amp =PB\boldv \amp \text{(since \(B=P^{-1}AP\))}</mrow>
            <mrow>\amp =P(\lambda\boldv) \amp \text{ (since \(B\boldv=\lambda\boldv\)) }</mrow>
            <mrow>\amp =\lambda P\boldv) \amp \text{ (since \(\lambda\) is a scalar) } </mrow>
          </md>.
        </p>
        <p>
          Thus <m>A(P\boldv)=\lambda P(\boldv)</m>.
        </p>
        <p>
          To show <m>P\boldv</m> is an eigenvector,
          it remains only to show that <m>P\boldv\ne\boldzero</m>.
          But since <m>P</m> is invertible,
          and since <m>\boldv\ne\boldzero</m>,
          it follows that <m>P\boldv\ne\boldzero</m>.
        </p>
      </solution>
    </li>
    <li xml:id="ex_conjpoly">
      <p>
        Let <m>A</m> be an <m>n</m> by <m>n</m> matrix,
        and let <m>q(A)</m> be the matrix
        <me>
          q(A) = a_nA^n +a_{n-1}A^{n-1}+\cdots+a_1A+a_0I_n
        </me>
        Prove that if <m>B = P^{-1}AP</m>,
        then <m>q(B) = P^{-1}q(A)P</m>.
        <solution>
          <p>
            First we need to show
            (by induction)
            that <m>B^n =P^{-1}A^nP</m> for all <m>n</m>.
            The base case, when <m>n=1</m> is give by the hypothesis.
            Assume that equality holds when <m>n \leq k</m>.
            We need to show that equality holds when <m>n = k+1</m>.
            <md>
              <mrow>B^{k+1} \amp =\amp  BB^k</mrow>
              <mrow>\amp =\amp (P^{-1}AP)B^k</mrow>
              <mrow>\amp =\amp  (P^{-1}AP)(P^{-1}A^kP)</mrow>
              <mrow>\amp =\amp  P^{-1}A(PP^{-1})A^kP</mrow>
              <mrow>\amp =\amp P^{-1}A^{k+1}P</mrow>
            </md>
          </p>
          <p>
            Thus <m>B^n =P^{-1}A^nP</m> holds for all <m>n</m>.
            Now we can show the desired result.
            <md>
              <mrow>q(B) \amp =\amp  a_nB^n + a_{n-1}B^{n-1}+\cdots+a_1B+a_0I_n</mrow>
              <mrow>\amp =\amp  a_n(P^{-1}A^nP) + a_{n-1}(P^{-1}A^{n-1}P)+\cdots+a_1(P^{-1}AP)+a_0I_n</mrow>
              <mrow>\amp =\amp  P^{-1}(a_nA^nP) + (a_{n-1}A^{n-1}P)+\cdots+(a_1AP)+a_0I_n)</mrow>
              <mrow>\amp =\amp P^{-1}(a_nA^n) + (a_{n-1}A^{n-1})+\cdots+(a_1A)+a_0I_n)P</mrow>
              <mrow>\amp =\amp  P^{-1}(q(A))P</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> be a <m>3\times 3</m> matrix with eigenvalues <m>1, -1, 0</m>.
        <ol>
          <li>
            <p>
              Prove that <m>A</m> is diagonalizable.
              What is <m>D</m> in this case?
            </p>
          </li>
          <li>
            <p>
              Show that <m>A^n=A</m> for any <term>odd</term> <m>n\geq 0</m>:
              e.g., <m>A^{13}=A</m>, <m>A^{77}=A</m>, etc.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          The matrix is diagonalizable as it has three distinct eigenvalues.
          We have <m>D=P^{-1}AP</m> where <m>D=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp -1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}</m>.
        </p>
        <p>
          It is easy to see that
          <me>
            D^n=\begin{bmatrix}1^n\amp 0\amp 0\\ 0\amp (-1)^n\amp 0\\ 0\amp 0\amp 0^n \end{bmatrix} =\begin{bmatrix}1\amp 0\amp 0\\ 0\amp (-1)^n\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}
          </me>
        </p>
        <p>
          Thus we see clearly that <m>D^n=D</m> if and only if <m>n</m> is odd.
        </p>
        <p>
          Next, write <m>A=PDP^{-1}</m>.
          Then <m>A^n=(PDP^{-1})^n=PD^nP^{-1}</m>,
          where this last step is the extremely useful fact that
          <q>conjugation respects powers of matrices</q>. If <m>n</m> is odd,
          we thus have <m>A^n=PD^nP^{-1}=PDP^{-1}=A</m>, as claimed.
        </p>
      </solution>
    </li>
    <li>
      <p>
        The <em>Fibonacci sequence</em>
        <m>F_0, F_1, F_2, \dots</m> is defined recursively as follows:
        <md>
          <mrow>F_0\amp =0</mrow>
          <mrow>F_1\amp =1</mrow>
          <mrow>F_n\amp =F_{n-2}+F_{n-1} \amp \text{ for \(n\geq 2\) }</mrow>
        </md>
        Let <m>A=\begin{bmatrix}0\amp 1\\ 1\amp 1 \end{bmatrix}</m>.
        <ol>
          <li>
            <p>
              Show that
              <me>
                A^n\begin{bmatrix}0 \\ 1 \end{bmatrix} =\begin{bmatrix}F_n\\ F_{n+1} \end{bmatrix}
              </me>
              for all <m>n\geq 0</m>.
            </p>
          </li>
          <li>
            <p>
              Diagonalize <m>A</m>.
              Write its two eigenvalues as
              <m>\lambda_1</m> and <m>\lambda_2</m>.
            </p>
          </li>
          <li>
            <p>
              Derive a closed formula for <m>A^n</m>.
            </p>
          </li>
          <li>
            <p>
              Derive a closed formula for <m>F_n</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Proof by induction.
          Base case (<m>n=0</m>) is trivial.
          For induction step,
          suppose <m>A^n\begin{bmatrix}0 \\ 1 \end{bmatrix} =\begin{bmatrix}F_n\\ F_{n+1} \end{bmatrix}</m>, then
          <md>
            <mrow>A^{n+1}\begin{bmatrix} 0</mrow>
            <mrow>1 \end{bmatrix}\amp =AA^n\begin{bmatrix} 0</mrow>
            <mrow>1 \end{bmatrix}</mrow>
            <mrow>\amp =A\begin{bmatrix} F_n</mrow>
            <mrow>F_{n+1} \end{bmatrix} \amp \text{ (induction hypo.) }</mrow>
            <mrow>\amp =\begin{bmatrix} F_{n+1}</mrow>
            <mrow>F_n+F_{n+1} \end{bmatrix}</mrow>
            <mrow>\amp =\begin{bmatrix} F_{n+1}</mrow>
            <mrow>F_{n+2}. \end{bmatrix}</mrow>
          </md>
          as desired.
        </p>
        <p>
          (b) The eigenvalues of <m>A</m> are
          <m>\lambda_1=\frac{1}{2}(1+\sqrt{5})</m> and <m>\lambda_2=\frac{1}{2}(1-\sqrt{5})</m>.
        </p>
        <p>
          Note that the <m>\lambda_i</m> satisfy
        </p>
        <p>
          <m>\lambda_1+\lambda_2=1</m>
        </p>
        <p>
          <m>\lambda_1\lambda_2=-1</m>
        </p>
        <p>
          <m>\lambda_1^2=\lambda_1+1</m>,
          <m>\lambda_2^2+\lambda_2=1</m>.
        </p>
        <p>
          (These follow from the fact that they are roots of <m>p(x)=x^2-x-1=(x-\lambda_1)(x-\lambda_2)</m>. )
        </p>
        <p>
          From these relations it is easy to verify that
          <m>\boldv_1=(1,\lambda_1)</m> and <m>\boldv_2=(1,\lambda_2)</m> form bases of
          <m>W_{\lambda_1}</m> and <m>W_{\lambda_2}</m>, respectively.
        </p>
        <p>
          Then we have <m>D=P^{-1}AP</m> where
          <me>
            D=\begin{bmatrix}\lambda_1\amp 0\\ 0\amp \lambda_2 \end{bmatrix} , \ P=\begin{bmatrix}1\amp 1\\ \lambda_1\amp \lambda_2 \end{bmatrix} , \ P^{-1}=-\frac{1}{\sqrt{5}}\begin{bmatrix}\lambda_2\amp -1\\ -\lambda_1\amp 1 \end{bmatrix}
          </me>.
        </p>
        <p>
          (c) Now we compute
          <me>
            A^n=(PDP^{-1})^n=PD^nP^{-1}=-\frac{1}{\sqrt{5}}\begin{bmatrix}1\amp 1\\ \lambda_1\amp \lambda_2 \end{bmatrix} \begin{bmatrix}\lambda_1^n\amp 0\\ 0\amp \lambda_2^n \end{bmatrix} \begin{bmatrix}\lambda_2\amp -1\\ -\lambda_1\amp 1 \end{bmatrix} =\frac{1}{\sqrt{5}}\begin{bmatrix}\lambda_1^{n-1}-\lambda_2^{n-1}\amp \lambda_1^n-\lambda_2^n\\ \lambda_1^n-\lambda_2^n\amp \lambda_1^{n+1}-\lambda_2^{n+1} \end{bmatrix}
          </me>
        </p>
        <p>
          (d) Lastly, using our formula above for <m>A^n</m> we see that
          <me>
            \begin{bmatrix}F_n\\ F_{n+1} \end{bmatrix} =A^n\begin{bmatrix}0\\1 \end{bmatrix} =\frac{1}{\sqrt{5}}\begin{bmatrix}\lambda_1^n-\lambda_2^n\\ \lambda_1^{n+1}-\lambda_2^{n+1} \end{bmatrix}
          </me>.
        </p>
        <p>
          It follows that
          <me>
            F_n=\frac{1}{\sqrt{5}}(\lambda_1^{n}-\lambda_2^n)
          </me>,
          where <m>\lambda_1=\frac{1}{2}(1+\sqrt{5}), \lambda_2=\frac{1}{2}(1-\sqrt{5})</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A\in M_{nn}</m> be defined as the matrix of all 1's:
        i.e., <m>a_{ij}=1</m> for all <m>1\leq i, j\leq n</m>.
        Show that <m>A</m> has exactly 2 eigenvalues and that <m>A</m> is diagonalizable.
        Hint: do NOT compute the characteristic polynomial of <m>A</m>.
        Instead, begin by first considering <m>\NS(A)</m>.
        <solution>
          <p>
            Since <m>\rank(A)=1</m>, we have <m>\nullity(A)=\dim W_0=n-1</m>.
            Thus <m>0</m> is an eigenvalue of <m>A</m> with geometric multiplicity <m>n-1</m>.
            Since geometric multiplicity is bounded by algebraic multiplicity,
            we must have the characteristic polynomial <m>p(t)</m> factoring as
            <m>p(t)=t^{n-1}(t-c)</m> for some <m>c\in \R</m>. (Remember that
            <m>\deg p(t)=n</m>.) Now use the fact that
            <me>
              n=\tr A=(\text{ sum of eigenvalues } )=0+0+\cdots +0+c
            </me>.
          </p>
          <p>
            It follows that <m>c=n</m>.
            Thus the distinct eigenvalues of <m>A</m> are <m>0</m> and <m>c</m>.
            Since <m>\dim W_0=n-1</m> and <m>1\leq \dim W_n\leq 1</m>, we have
            <me>
              \dim W_0+\dim W_n=n-1+1=n
            </me>.
          </p>
          <p>
            Thus <m>A</m> is diagonalizable.
          </p>
          <p>
            By the way, try diagonalizing <m>A</m> yourself:
            that is, produce a basis of eigenvectors.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        The <em>Cayley-Hamilton theorem</em>
        states that any <m>A\in M_{nn}</m> satisfies its own characteristic polynomial <m>p(t)</m>:
        i.e., <m>p(A)=\boldzero_{n\times n}</m>.
        Prove the Cayley-Hamilton theorem for
        <em>diagonalizable</em> matrices.
        <solution>
          <p>
            First, since <m>A</m> is diagonalizable,
            we write <m>D=P^{-1}AP</m> where
            <me>
              D=\begin{bmatrix}d_1\amp 0\amp \dots \\ 0\amp d_2\amp 0\amp \dots \amp 0\\ \vdots \\ 0\amp 0\amp \dots \amp 0\amp d_n \end{bmatrix}
            </me>
          </p>
          <p>
            Next, observe that for any polynomial <m>q(t)</m>,
            we easily compute <m>q(D)</m> as follows:
            <me>
              q(D)=\begin{bmatrix}q(d_1)\amp 0\amp \dots \\ 0\amp q(d_2)\amp 0\amp \dots \amp 0\\ \vdots \\ 0\amp 0\amp \dots \amp 0\amp q(d_n) \end{bmatrix}
            </me>
          </p>
          <p>
            That is, when evaluating <m>q(D)</m> we simply get the diagonal matrix whose <m>j</m>-th diagonal entry is <m>q(d_j)</m>.
          </p>
          <p>
            Now let <m>p(t)</m> be the characteristic polynomial of <m>A</m>.
            Since <m>A</m> and <m>D</m> are similar,
            <m>p(t)</m> is also the characteristic polynomial of <m>D</m>,
            which is clearly <m>p(t)=(t-d_1)(t-d_2)\cdots (t-d_n)</m>.
            From the observation above, we see that
            <me>
              p(D)=\begin{bmatrix}p(d_1)\amp 0\amp \dots \\ 0\amp p(d_2)\amp 0\amp \dots \amp 0\\ \vdots \\ 0\amp 0\amp \dots \amp 0\amp p(d_n) \end{bmatrix} =0_{n\times n}
            </me>,
            since clearly <m>p(d_j)=0</m> for all <m>j</m>.
          </p>
          <p>
            Lastly, we compute
            <me>
              p(A)=p(PDP^{-1})=Pp(D)P^{-1}=P0_{n\times n}P^{-1}=0_{n\times n}
            </me>.
          </p>
          <p>
            (Note that the equality <m>p(PDP^{-1})=Pp(D)P^{-1}</m> follows from <xref ref="ex_conjpoly">Exercise</xref>.)
          </p>
          <p>
            We have thus shown that <m>A</m> satisfies its own characteristic polynomial,
            as claimed.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Information about three matrices is given below.
        Find all eigenvalues of each matrix and decide whether it is diagonalizable
        (you are given enough information in each case).
        <ol>
          <li>
            <p>
              <m>A_1\in M_{33}</m>, <m>p(t)=\det(tI-A_1)=t^3-t^2</m>,
              <m>\NS(A_1)=</m>the plane perpendicular to <m>(1,0,1)</m>.
            </p>
          </li>
          <li>
            <p>
              <m>A_2\in M_{33}</m>, <m>p(t)=\det(tI-A_2)=t^3+t^2-t</m>.
            </p>
          </li>
          <li>
            <p>
              <m>A_3\in M_{22}</m>, <m>\tr A=4</m>, <m>\det A=3</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The eigenvalues of <m>A</m> are <m>0, 1</m>.
          We are given that <m>\NS(A_1)=W_0</m> is a plane, hence 2-dimensional.
          Since <m>W_1</m> must be 1-dimensional
          (looking at multiplicity),
          we have <m>\dim W_0+\dim W_1=3</m>.
          Hence <m>A</m> is diagonalizable.
        </p>
        <p>
          (b) The eigenvalues of <m>A</m> are <m>0, (-1+\sqrt{5})/2, (-1-\sqrt{5})/2</m>.
          Since <m>A</m> has three distinct eigenvalues, it is diagonalizable.
        </p>
        <p>
          (c) Let <m>\lambda_1, \lambda_2</m> be the roots of <m>p(t)</m>.
          Then we have
          <md>
            <mrow>\tr A=\lambda_1+\lambda_2\amp =4</mrow>
            <mrow>\det A=\lambda_1\lambda_2\amp 3</mrow>
          </md>
        </p>
        <p>
          We easily solve this system of two equations in the two unknown
          <m>\lambda_i</m> to conclude <m>\lambda_1=1</m>, <m>\lambda_2=3</m>.
          Since <m>A</m> has two distinct eigenvalues, it is diagonalizable.
        </p>
      </solution>
    </li>
    <li>
      <p>
        A matrix <m>A\in M_{nn}</m> is called <term>nilpotent</term>
        if <m>A^k=0</m> for some <m>k\geq 1</m>.
        In what follows, assume <m>A</m> is nilpotent.
        <ol>
          <li>
            <p>
              Prove that <m>0</m> is the only
              <em>possible</em> eigenvalue of <m>A</m>.
            </p>
          </li>
          <li>
            <p>
              Prove that <m>0</m> is indeed an eigenvalue of <m>A</m>.
              Thus <m>0</m> is the only eigenvalue of <m>A</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Suppose <m>\lambda</m> is an eigenvalue of <m>A</m>.
          Then we have <m>A\boldv=\lambda \boldv</m> for some <m>\boldv\ne 0</m>.
          Then <m>A^k\boldv=\lambda^k\boldv</m>, as shown earlier.
          Since <m>A^k=0_{n\times n}</m>,
          we thus have <m>\boldzero=\lambda^k\boldv</m>.
          Since <m>\boldv</m> is <em>nonzero (!!)</em>,
          we conclude that <m>\lambda^k=0</m>,
          and hence that <m>\lambda=0</m>.
        </p>
        <p>
          (b) Now we show that <m>0</m> actually is an eigenvalue.
          Since <m>A^k=0_{n\times n}</m>,
          we have <m>\det(A^k)=(\det A)^k=0</m>.
          Hence <m>A</m> is not invertible.
          From the invertibility theorem we conclude that <m>0</m> is an eigenvalue of <m>A</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        A matrix <m>A\in M_{nn}</m> is called
        <term>idempotent</term> if <m>A^2=A</m>.
        In what follows, assume <m>A</m> is idempotent.
        <ol>
          <li>
            <p>
              Prove that the only <em>possible</em>
              eigenvalues of <m>A</m> are <m>0</m> and <m>1</m>.
            </p>
          </li>
          <li>
            <p>
              Prove that either <m>0</m> is an eigenvalue of <m>A</m> or else <m>A=I_n</m>.
            </p>
          </li>
          <li>
            <p>
              Give an example of an idempotent matrix for each of the following three possibilities:
              only 0 is an eigenvalue;
              only 1 is an eigenvalue; 0 and 1 are both eigenvalues.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Suppose <m>\lambda</m> is an eigenvalue of <m>A</m>.
          Then we have <m>A\boldv=\lambda \boldv</m> for some <m>\boldv\ne 0</m>.
          Then <m>A^2\boldv=\lambda^2\boldv</m> as shown earlier.
          Since <m>A^2=A</m>, we have <m>A\boldv=\lambda^2\boldv</m>,
          and hence that <m>\lambda^2\boldv=\lambda\boldv</m>.
          Since <m>\boldv</m> is <em>nonzero (!!)</em>,
          we conclude that <m>\lambda^2=\lambda</m>,
          and hence that <m>\lambda=0</m> or <m>\lambda=1</m>.
        </p>
        <p>
          (b) Suppose <m>0</m> is not an eigenvalue.
          Then <m>A</m> is invertible.
        </p>
        <p>
          Then <m>A^2=A\Rightarrow A^{-1}A^2=A^{-1}A\Rightarrow A=I</m>.
        </p>
        <p>
          (c) Easy.
          You do it.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A\in M_{nn}</m> and suppose the characteristic polynomial of <m>A</m> factors as
        <me>
          p(t)=(t-\lambda)^{m}g(t)
        </me>,
        where the <m>\lambda\in\R</m> and <m>\lambda</m> is not a root of <m>g(t)</m>.
        Recall that we define the algebraic multiplicity of <m>\lambda</m> to be <m>m</m>;
        and we define the geometric multiplicity of <m>\lambda</m> to be <m>\dim W_\lambda</m>.
        In this exercise we will show <m>\dim W_{\lambda}\leq m</m>.
        To do so, set <m>\dim W_{\lambda}=r</m>.
        Show that <m>A</m> is similar to a matrix <m>B</m> whose characteristic polynomial is divisible by <m>(t-\lambda)^r</m>.
        Conclude that <m>(t-\lambda)^r</m> divides <m>p(t)</m>,
        and hence that <m>r\leq m</m>.
        <solution>
          <p>
            Set <m>T=T_A</m>.
            Suppose <m>\dim W_\lambda=r</m>.
            Let <m>\boldv_1,\boldv_2,\dots, \boldv_r</m> be a basis for
            <m>W_\lambda</m> and extend to a full basis <m>B'</m> of <m>\R^n</m>.
            Representing <m>T</m> with respect to <m>B'</m> yields
            <me>
              A'=[T]_{B'}=\begin{bmatrix}\lambda I_{r}\amp C\\ 0_{(n-r)\times r}\amp D \end{bmatrix}
            </me>
          </p>
          <p>
            Here I've represented <m>A'</m> in terms of matrix blocks.
            Now, since <m>A</m> and <m>A'</m> are just two matrix representations of <m>T</m>,
            they are similar, hence have the same characteristic polynomial.
            From the block matrix description above it is easy to see that
            <m>p(t)=\det(tI-A')</m> factors as <m>p(t)=(t-\lambda)^rq(t)</m>.
            Thus the algebraic multiplicity of <m>\lambda</m> is <em>at least</em>
            <m>r=\dim W_\lambda</m>, as claimed.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Each of the matrices below has characteristic polynomial <m>p(x)=(x-1)^2(x+2)</m>.
        For each decide whether it is diagonalizable.
        If yes, provide explicit <m>P</m> and <m>D</m> witnessing this fact.
        <ol>
          <li>
            <p>
              <m>A=\begin{bmatrix}-5\amp 0\amp 3\\ -6\amp 1\amp 3\\ -6\amp 0\amp 4 \end{bmatrix}</m>.
            </p>
          </li>
          <li>
            <p>
              <m>B=\begin{bmatrix}-2\amp -3\amp 3\\ -3\amp -3\amp 4\\ -3\amp -4\amp 5 \end{bmatrix}</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          These are the matrices that showed up in a previous exercise,
          where we did essentially all the necessary work.
        </p>
        <p>
          For <m>A</m> we have <m>\dim W_1+\dim W_{-2}=2+1=3</m>,
          so <m>A</m> is diagonalizable.
        </p>
        <p>
          For <m>B</m> we have <m>\dim W_1+\dim W_{-2}=1+1=2\lt 3</m> so <m>B</m> is not diagonalizable.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>A=\begin{bmatrix}-5\amp 0\amp 3\\ -6\amp 1\amp 3\\ -6\amp 0\amp 4 \end{bmatrix}</m>.
        Use your work above to do the following to find a matrix <m>C</m> such that <m>C^3=A</m>.
        <solution>
          <p>
            From a previous exercise,
            we know that <m>A</m> has eigenspaces <m>W_1</m> and <m>W_-2</m> with corresponding bases
            <m>\{(1,0,2),(0,1,0)\}</m> and <m>\{(1,1,1)\}</m>.
            It follows that <m>D=P^{-1}AP</m>, where
            <me>
              D=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -2 \end{bmatrix} , \ P=\begin{bmatrix}1\amp 0\amp 1\\ 0\amp 1\amp 1\\ 2\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            We first find a cube-root of <m>D</m>.
            This is easy!
            Take
            <me>
              E=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -\sqrt[3]{2} \end{bmatrix}
            </me>
            for example.
            Then <m>C=PEP^{-1}</m> satsifies <m>C^3=(PEP^{-1})^3=PE^3P^{-1}=PDP^{-1}=A</m>.
            Thus <m>C</m> is a a cube-root of <m>A</m>.
            Numerically, we have
            <me>
              C=\begin{bmatrix}1\amp 0\amp 1\\ 0\amp 1\amp 1\\ 2\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -\sqrt[3]{2} \end{bmatrix} \begin{bmatrix}1\amp 0\amp 1\\ 0\amp 1\amp 1\\ 2\amp 0\amp 1 \end{bmatrix} ^{-1}=\begin{bmatrix}-1-2\sqrt[3]{2}\amp 0\amp 1+\sqrt[3]{2}\\ -2-2\sqrt[3]{2}\amp 1\amp 1+\sqrt[3]{2}\\ -2-2\sqrt[3]{2}\amp 0\amp 2+\sqrt[3]{2} \end{bmatrix}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Suppose <m>A</m> is <m>3\times 3</m>,
        has determinant 0, and has among its eigenvalues
        <m>\lambda=2</m> with eigenspace <m>W_2</m> a plane in <m>\R^3</m>.
        <ol>
          <li>
            <p>
              Find all eigenvalues of <m>A</m>.
            </p>
          </li>
          <li>
            <p>
              Find <m>\tr(A)</m>.
            </p>
          </li>
          <li>
            <p>
              Decide whether <m>A</m> is diagonalizable.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Consider the characteristic polynomial <m>p(x)</m> of <m>A</m>.
          Observe that <m>p(x)</m> has degree 3.
        </p>
        <p>
          Since <m>W_2</m> is has dimension 2, we must have <m>p(x)=(x-2)^2(x-a)</m>,
          where <m>a</m> is possibly a second distinct eigenvalue,
          though it is also possible that <m>a=2</m>.
          However <m>\det(A)=0</m> implies 0 is an eigenvalue of <m>A</m>
          (see previous classwork).
          Thus we must have <m>a=0</m> and <m>p(x)=x(x-2)^2</m>.
        </p>
        <p>
          The trace of <m>A</m> is the sum of the eigenvalues.
          Thus <m>\tr(A)=2+2+0=4</m>.
        </p>
        <p>
          We have <m>\dim W_2=2</m>.
          We have automatically that <m>\dim W_0\geq 1</m>
          (if <m>\lambda</m> is an eigenvalue,
          then by definition the eigenspace <m>W_\lambda</m> is nonzero).
          Since the algebraic multiplicity of <m>0</m> is 1, it follows that <m>\dim W_0=1</m>,
          and <m>\dim W_2+\dim W_0=3</m>.
          This shows <m>A</m> is diagonalizable.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Give an example of a <m>3\times 3</m> matrix <m>A</m> satisfying the following conditions:
        <ol>
          <li>
            <p>
              <m>\lambda=2</m> is an eigenvalue and <m>(1,0,1), (1,1,1)</m> are 2-eigenvectors.
            </p>
          </li>
          <li>
            <p>
              <m>\lambda=-1</m> is an eigenvalue and
              <m>(1,0,-1)</m> is a <m>-1</m>-eigenvector.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Such an <m>A</m> will be diagonalizable
          (since we have a basis of eigenvectors).
          Using this fact to reverse engineer <m>A</m>,
          we see that the corresponding <m>P</m> and <m>D</m> would be
          <me>
            P=\begin{bmatrix}1\amp 1\amp 1\\ 0\amp 1\amp 0\\ 1\amp 1\amp -1 \end{bmatrix} , D=\begin{bmatrix}2\amp 0\amp 0\\ 0\amp 2\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}
          </me>.
        </p>
        <p>
          Since <m>D=P^{-1}AP</m>, it follows that
          <me>
            A=PDP^{-1}=\begin{bmatrix}1/2\amp 0\amp 3/2\\ 0\amp 2\amp 0\\ 3/2\amp 0\amp 1/2 \end{bmatrix}
          </me>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        True or false.
        If true, provide a proof; if false,
        give an explicit counterexample.
        <ol>
          <li>
            <p>
              If <m>A\in M_{nn}</m> has less than <m>n</m> distinct eigenvalues,
              then <m>A</m> is not diagonalizable.
            </p>
          </li>
          <li>
            <p>
              If <m>A\in M_{nn}</m> has fewer than <m>n</m> linearly independent eigenvectors,
              then <m>A</m> is not diagonalizable.
            </p>
          </li>
          <li>
            <p>
              If <m>A\in M_{nn}</m> is diagonalizable,
              then there is a <em>unique</em>
              matrix <m>P</m> such that <m>P^{-1}AP</m> is diagonal.
            </p>
          </li>
          <li>
            <p>
              If <m>A\in M_{nn}</m> is diagonalizable,
              then <m>A^{-1}</m> is diagonalizable.
            </p>
          </li>
          <li>
            <p>
              If <m>A\in M_{nn}</m> is diagonalizable,
              then <m>A^T</m> is diagonalizable. s
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) False.
          Take the identity matrix <m>I_2</m>.
          It is diagonal, hence diagonalizable!
        </p>
        <p>
          (b) True.
          This is the content of our main diagonalization theorem.
        </p>
        <p>
          (c) False.
          Take <m>A=I_2</m>.
          Then we have <m>D=I_2</m>,
          and <m>D=P^{-1}AP</m> for <em>any</em>
          invertible matrix <m>P</m> whatsoever!
        </p>
        <p>
          (d) True.
          Suppose <m>D=P^{-1}AP</m> is diagonal.
          Then so is <m>D^{-1}</m>,
          and we have <m>D^{-1}=P^{-1}A^{-1}(P^{-1})^{-1}=P^{-1}A^{-1}P</m>.
        </p>
        <p>
          (e) True.
          Suppose <m>D= P^{-1}AP</m> is diagonal.
          Then so is <m>D^T</m> and we have
          <me>
            D^T=P^TA^T(P^{-1})^T=P^TA^T(P^T)^{-1}=Q^{-1}AQ
          </me>,
          where <m>Q=(P^T)^{-1}</m>.
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{5.1: inner products} In what follows <m>V</m> is always a vector space along with an inner product <m>\langle \ , \rangle</m>.
    Notions of norm and orthogonality always refer to this fixed choice of inner product.
  </p>
  <ol>
    <li>
      <p>
        Show that <m>\frac{\boldv}{\norm{\boldv}}:=\frac{1}{\norm{\boldv}}\cdot\boldv</m> is a unit vector for any <m>\boldv\in V</m>.
        <solution>
          <p>
            In any inner product space the norm operation satisfies
            <m>\norm{c\boldv}=\val{c}\norm{\boldv}</m>. (Observe the subtle notational difference:
            <m>\val{c}</m> is absolute value of the real number <m>c</m>;
            <m>\norm{\boldv}</m> is norm of the vector <m>\boldv</m>. )
          </p>
          <p>
            Now let <m>\ds\boldw=\frac{1}{\norm{\boldv}}\boldv</m>.
            Then
            <md>
              <mrow>\norm{\boldw}\amp =\norm{\frac{1}{\norm{\boldv}}\boldv}</mrow>
              <mrow>\amp =\val{\frac{1}{\norm{\boldv}}}\norm{\boldv}</mrow>
              <mrow>\amp =\frac{1}{\norm{\boldv}}\norm{\boldv} \amp \text{(since \(\ds\frac{1}{\norm{\boldv}}\geq 0\))}</mrow>
              <mrow>\amp =1</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>(V, \angvec{ \ , })</m> be an inner product space.
        Prove: <m>\norm{c\boldv}=\val{c}\norm{\boldv}</m> for all <m>c\in \R</m>.
        (Here <m>\val{c}</m> is the absolute value of <m>c</m>.)
        Your proof must be valid for a general inner product space;
        i.e., you can only invoke the definition of
        <m>\norm{ \underline{\hspace{5pt}}}</m> in terms of <m>\angvec{\ , }</m>,
        the axioms of an inner product,
        and properties of real number arithmetic.
        <solution>
          <md>
            <mrow>\norm{c\boldv}^2\amp =\angvec{c\boldv, c\boldv}</mrow>
            <mrow>\amp =c^2\angvec{\boldv, \boldv}</mrow>
            <mrow>\amp =c^2\norm{\boldv}^2</mrow>
          </md>
          <p>
            This implies
            <me>
              \norm{c\boldv}=\sqrt{c^2}\norm{\boldv}=\val{c}\norm{\boldv}
            </me>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute the angle between <m>\boldv=(1,1,1,1)</m> and <m>\boldw=(1,-1,1,1)</m>.
        Do NOT use a calculator.
        <solution>
          <p>
            The equation for the angle <m>\theta</m> ends up being <m>\cos(\theta)=\frac{1}{2}</m>.
            The unique <m>\theta</m> in the range <m>[0,\pi]</m> satisfying this is <m>\theta=\pi/3=60^\circ</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=C([0,1])</m> with inner product <m>\ds \angvec{ f, g}=\int_0^1f(x)g(x) \ dx</m>.
        Let <m>f(x)=1</m> and <m>g(x)=x</m>.
        Compute the angle <m>\theta</m> between <m>f</m> and <m>g</m> with respect to this inner product.
        Your answer cannot be expressed in terms of inverse trig functions:
        i.e., the angle is a familiar one that you can solve for by hand.
        <solution>
          <p>
            The angle <m>\theta</m> is the unique angle between 0 and <m>\pi</m> satisfying
            <me>
              \cos(\theta)=\frac{\angvec{f,g}}{\norm{f}\norm{g}}=\frac{\int_0^1x \ dx}{\sqrt{\int_0^11\ dx}\sqrt{\int_0^1 x^2 \ dx}}=\frac{1/2}{1/\sqrt{3}}=\frac{\sqrt{3}}{2}
            </me>.
          </p>
          <p>
            Thus <m>\theta=\pi/6</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove that if <m>\boldw</m> is orthogonal to <m>\boldv_1,\boldv_2,\dots, \boldv_n</m>,
        then it is orthogonal to any linear combination of the <m>\boldv_i</m>.
        In other words prove the implication:
        <me>
          (\boldw\perp\boldv_i \text{ for all \(i\) } )\Rightarrow (\boldw\perp(c_1\boldv_1+c_2\boldv_2+\cdots+c_n\boldv_n) \text{ for any \(c_i\) } )
        </me>
        <solution>
          <p>
            We prove both directions of the if and only if separately.
          </p>
          <p>
            <m>(\Leftarrow)</m>: this is the easy direction.
            If <m>\boldw</m> is orthogonal to <em>any</em>
            linear combination of the <m>\boldv_i</m>,
            then in particular it is orthogonal to each of the <m>\boldv_i</m>.
          </p>
          <p>
            (<m>\Rightarrow)</m>:
            assume <m>\boldw\perp\boldv_i</m> for all <m>i</m>.
            Then <m>\boldw\cdot\boldv_i=0</m> for all <m>i</m>.
            But then
            <md>
              <mrow>\boldw\cdot(c_1\boldv_1+\cdots +c_n\boldv_n)\amp =\boldw\cdot(c_1\boldv_1)+\cdots +\boldw\cdot(c_n\boldv_n) \amp \text{ (add. prop. of dot) }</mrow>
              <mrow>\amp =c_1(\boldw\cdot\boldv_1)+\cdots +c_n\boldw\cdot\boldv_n) \amp \text{ (scalar mult. prop. of dot) }</mrow>
              <mrow>\amp =0+0+\cdots +0=0</mrow>
            </md>.
          </p>
          <p>
            Thus <m>\boldw\perp(c_1\boldv_1+\cdots +c_n\boldv_n)</m> for any <m>c_i</m>,
            as desired.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        For each of the following operations on <m>\R^2</m>,
        determine whether it defines an inner product on <m>\R^2</m>.
        If it fails to be an inner product,
        identify which of the three inner product axioms
        (if any)
        it does satisfy,
        and provide explicit counterexamples for any axiom that fails.
      </p>
      <ol>
        <li>
          <p>
            <m>\angvec{(x_1,x_2),\ (y_1,y_2)}=x_1y_2+x_2y_1</m>.
          </p>
        </li>
        <li>
          <p>
            <m>\angvec{(x_1,x_2),\ (y_1,y_2)}=2x_1y_1+x_1y_2+x_2y_1+3x_2y_2</m>.
          </p>
        </li>
        <li>
          <p>
            <m>\angvec{(x_1,x_2), \ (y_1,y_2)}=x_1^2y_1^2+x_2^2y_2^2</m>.
          </p>
        </li>
      </ol>
      <solution>
        <p>
          In what follows we let <m>\boldx=(x_1,x_2)</m>,
          <m>\boldy=(y_1,y_2)</m>, <m>\boldz=(z_1,z_2)</m>.
        </p>
        <p>
          (a) The proposed operation satisfies all axioms except positivity.
          Let's see why.
        </p>
        <p>
          Symmetry.
          We have
          <md>
            <mrow>\angvec{\boldy,\ \boldx}\amp =y_1x_2+y_2x_1</mrow>
            <mrow>\amp =x_1y_2+x_2y_1</mrow>
            <mrow>\amp =\angvec{\boldx, \boldy}</mrow>
          </md>
        </p>
        <p>
          Linearity in first variable.
          We have
          <md>
            <mrow>\angvec{c\boldx+d\boldy,\boldz}\amp =\angvec{(cx_1+dy_1,cx_2+dy_2),\ (z_1,z_2)}</mrow>
            <mrow>\amp =(cx_1+dy_1)z_2+(cx_2+dy_2)z_1</mrow>
            <mrow>\amp =c(x_1z_2+x_2z_1)+d(y_1z_2+y_2z_1)</mrow>
            <mrow>\amp =c\angvec{\boldx,\ \boldy}+d\angvec{\boldx, \ \boldz}</mrow>
          </md>
        </p>
        <p>
          Positivity.
          This fails.
          Take <m>\boldx=(1,-1)</m>.
          Then
          <me>
            \angvec{\boldx,\boldx}=1\cdot (-1)+(-1)\cdot 1=-2\lt 0
          </me>.
        </p>
        <p>
          Furthermore, taking <m>\boldx=(0,1)</m>,
          we see that <m>\angvec{\boldx,\boldx}=0</m>.
          Thus both parts of positivity fail.
        </p>
        <p>
          (b) This is in fact an inner product!
          The proofs that properties (i) and (ii) hold proceed exactly as the argument above.
          Let's focus on positivity.
          We have
          <md>
            <mrow>\angvec{\boldx,\boldx}\amp =2x_1^2+2x_1x_2+3x_2^2</mrow>
            <mrow>\amp =2(x_1+\frac{1}{2}x_2)^2+\frac{11}{4}x_2^2 \amp \text{ (complete the square) }</mrow>
            <mrow>\amp =2u^2+\frac{11}{4}v^2 \amp (u=x_1+\frac{1}{2}x_2, \ v=x_2)</mrow>
          </md>
        </p>
        <p>
          Clearly the expression <m>2u^2+\frac{11}{4}v^2</m> is nonnegative.
          Furthermore this will be equal to zero if and only if <m>u=v=0</m> if and only if
          <m>x_1+\frac{1}{2}{x_2}=x_2=0</m> if and only if <m>x_1=x_2=0</m>.
          This proves <m>\angvec{\boldx,\boldx}\geq 0</m> and
          <m>\angvec{\boldx, \boldx}=0</m> if and only if <m>\boldx=\boldzero</m>.
        </p>
        <p>
          (c) It is clear that the operation is symmetric:
          that is <m>\angvec{\boldx, \boldy}=\angvec{\boldy,\boldx}</m>.
          Thus axiom (i) is satsified.
        </p>
        <p>
          The operation does not satisfy axiom
          (ii).
          For example, Let <m>\boldx=(1,1)</m>, <m>\boldy=(0,1)</m>.
          Then
          <me>
            \angvec{2\boldx,\boldy}=\angvec{(2,2),(0,1)}=4\ne 2\angvec{\boldx,\boldy}=1
          </me>.
        </p>
        <p>
          The operation does satisfy positivity, as is easily shown.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Equip <m>P_2</m> with the <em>evaluation inner product</em>
        <m>\angvec{p,q}=p(-1)q(-1)+p(0)q(0)+p(1)q(1)</m>.
        Find all polynomials <m>p(x)</m> orthogonal to <m>q(x)=x</m> with respect to this inner product.
        Note: this is an infinite set of polynomials.
        Describe it by giving a parameter description of its elements.
        <solution>
          <p>
            A polynomial <m>p</m> is orthogonal to <m>q</m> if and only if
            <m>p(-1)q(-1)+p(0)q(0)+p(1)q(1)=0</m> if and only if <m>-p(-1)+p(1)=0</m>.
            If <m>p(x)=ax^2+bx+c</m>,
            this means <m>-(a-b+c)+(a+b+c)=0</m>, or <m>2b=0</m>.
            Thus <m>p(x)=ax^2+c</m>, for <m>a,c\in\R</m> any real numbers.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Equip <m>V=C([-\pi,\pi])</m> with the
        <em>integral inner product</em>
        <m>\angvec{f, g}=\int_{-\pi}^\pi f(x)g(x) \ dx</m>.
        <ol>
          <li>
            <p>
              Show that <m>f(x)=\cos(x)</m> and
              <m>g(x)=\sin(x)</m> are orthogonal with respect to this inner product.
            </p>
          </li>
          <li>
            <p>
              Compute <m>\norm{\cos(x)}</m> with respect to this inner product.
            </p>
          </li>
          <li>
            <p>
              Show that if <m>f(x)</m> is an odd function (i.e.,
              <m>f(x)=-f(-x)</m> for all <m>x</m>) and <m>g(x)</m> is an even function (<m>g(-x)=g(x)</m> for all <m>x</m>),
              then <m>f</m> and <m>g</m> are orthogonal with respect to this inner product.
              <term>Hint</term>: use the area interpretation of the integral,
              as well as graphical properties of even/odd functions.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          Orthogonality follows from
          <m>\int_{-\pi}^\pi \cos x\sin x\ dx=\int_{\pi}^{\pi}\frac{1}{2}\sin(2x)\ dx=0</m>,
          as one easily shows.
        </p>
        <p>
          We have
          <me>
            \norm{\cos x}=\sqrt{\angvec{\cos x, \cos x}}=\sqrt{\int_{-\pi}^\pi \cos^2(x) \ dx}=\sqrt{\pi}
          </me>.
        </p>
        <p>
          If <m>f</m> is odd and <m>g</m> is even,
          then their product <m>fg</m> is odd.
          The integral of an odd function over a symmetric interval like <m>[-\pi, \pi]</m> is always 0.
          You can see this using the area interpretation
          (any region to the right of the <m>y</m>-axis has a flipped region to the left that cancels out its contribution to the integral),
          or by using integral properties:
          <md>
            <mrow>\int_{-\pi}^\pi f(x)g(x) \ dx\amp =\int_{\pi}^{-\pi} -f(-u)g(-u) \ du \amp \text{ (change of variables \(u=-x, du=-dx\)) }</mrow>
            <mrow>\amp = \int_{\pi}^{-\pi} f(u)g(u) \ du\amp \text{ (since \(fg\) is odd) }</mrow>
            <mrow>\amp =-\int_{-\pi}^\pi f(x)g(x) \ dx \amp \text{ (flip the enpoints, change variable name) }</mrow>
          </md>
        </p>
        <p>
          Since <m>\int_{-\pi}^\pi f(x)g(x) \ dx=-\int_{-\pi}^\pi f(x)g(x) \ dx</m>,
          we conclude <m>\angvec{f, g}=\int_{-\pi}^\pi f(x)g(x) \ dx=0</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>\boldv, \boldw\in V</m>,
        and let <m>\theta</m> be the angle between them.
        Prove the following equivalence:
        <me>
          \norm{\boldv+\boldw}=\norm{\boldv}+\norm{\boldw}\text{ if and only if }  \theta=0
        </me>.
        Your proof should be a <em>chain of equivalences</em>
        with each step justified.
        <solution>
          <md>
            <mrow>\norm{\boldv+\boldw}=\norm{\boldv}+\norm{\boldw}\amp \Leftrightarrow \norm{\boldv+\boldw}^2=\left(\norm{\boldv}+\norm{\boldw}\right)^2 \amp \text{ ( square both sides) }</mrow>
            <mrow>\amp \Leftrightarrow (\boldv+\boldw)\cdot(\boldv+\boldw)=\norm{\boldv}^2+2\norm{\boldv}\norm{\boldw}+\norm{\boldw}^2</mrow>
            <mrow>\amp \Leftrightarrow \boldv\cdot\boldv+2\boldv\cdot\boldw+\boldw\cdot\boldw=\boldv\cdot\boldv+2\norm{\boldv}\norm{\boldw}+\boldw\cdot\boldw</mrow>
            <mrow>\amp \Leftrightarrow \boldv\cdot\boldw=\norm{\boldv}\norm{\boldw}</mrow>
            <mrow>\amp \Leftrightarrow \frac{\boldv\cdot\boldw}{\norm{\boldv}\norm{\boldw}}=1</mrow>
            <mrow>\amp \Leftrightarrow \cos(\theta)=1</mrow>
            <mrow>\amp \Leftrightarrow \theta=0</mrow>
          </md>.
        </solution>
      </p>
    </li>
    <li>
      <p>
        Take <m>\boldv,\boldw\in V</m>.
        Suppose <m>\norm{\boldv}=2</m> and <m>\norm{\boldw}=3</m>.
        Find the maximum and minimum possible values of <m>\norm{\boldv-\boldw}</m>,
        and give explicit examples where those values occur.
        <solution>
          <p>
            We we have
            <md>
              <mrow>\norm{\boldv-\boldw}^2\amp =(\boldv-\boldw)\cdot(\boldv-\boldw)</mrow>
              <mrow>\amp =\norm{\boldv}^2-2\boldv\cdot\boldw+\norm{\boldw}^2</mrow>
            </md>.
          </p>
          <p>
            By Cauchy-Schwarz
            <me>
              - \norm{\boldv}\norm{\boldw}\leq\boldv\cdot\boldw\leq \norm{\boldv}\norm{\boldw}
            </me>.
          </p>
          <p>
            Thus
            <me>
              \norm{\boldv}^2-2\norm{\boldv}\norm{\boldw}+\norm{\boldw}^2\leq\norm{\boldv-\boldw}^2\leq \norm{\boldv}^2+2\norm{\boldv}\norm{\boldw}+\norm{\boldw}^2
            </me>
            which implies
            <me>
              (\norm{\boldv}-\norm{\boldw})^2\leq\norm{\boldv-\boldw}^2\leq(\norm{\boldv}+\norm{\boldw})^2
            </me>
            and thus
            <me>
              \norm{\boldv}-\norm{\boldw}\leq\norm{\boldv-\boldw}\leq\norm{\boldv}+\norm{\boldw}
            </me>
          </p>
          <p>
            So in our example
            <me>
              1\leq \norm{\boldv-\boldw}\leq 5
            </me>.
          </p>
          <p>
            In the special case when <m>V=\R^2</m> with the dot product,
            the picture of this situation consists of two circles centered about the origin:
            one of radius 2, the other of radius 3.
            The vector <m>\boldv</m> represents a point on the small circle;
            the vector <m>\boldw</m> represents a point on the large one.
            These are closest when they lie along the same radius vector from the origin;
            they are furthest when lying on diametrically opposed radii.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove the <em>Pythagorean theorem for general inner product spaces</em>:
        if <m>\boldv\perp\boldw</m>, then
        <me>
          \norm{\boldv+\boldw}^2=\norm{\boldv}^2+\norm{\boldw}^2
        </me>.
        <solution>
          <p>
            We have
            <md>
              <mrow>\norm{\boldv+\boldw}^2\amp =\langle\boldv+\boldw,\boldv+\boldw\rangle</mrow>
              <mrow>\amp =\langle\boldv,\boldv\rangle+2\cancelto{0}{\langle\boldv,\boldw\rangle}+\langle\boldw,\boldw\rangle \amp \text{ (since \(\boldv\perp\boldw\)) }</mrow>
              <mrow>\amp =\norm{\boldv}^2+\norm{\boldw}^2</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Prove each inequality below using a judicious choice of inner product in conjunction with the Cauchy-Schwarz inequality
        (and possibly a judicious choice of one of the vectors in the Cauchy-Schwarz inequality).
        <ol>
          <li>
            <p>
              For all <m>f, g\in C([a,b])</m>
              <me>
                \left(\int_a^b f(x)g(x) \ dx\right)^2\leq \int_a^b f^2(x)\ dx\int_a^b g^2(x) \ dx
              </me>.
            </p>
          </li>
          <li>
            <p>
              For all <m>(x_1,x_2,\dots, x_n)\in\R^n</m>,
              <me>
                (x_1+x_2+\cdots +x_n)\leq\sqrt{x_1^2+x_2^2+\cdots +x_n^2}\sqrt{n}
              </me>.
            </p>
          </li>
          <li>
            <p>
              For all <m>a,b,\theta\in\R</m>
              <me>
                (a\cos(\theta)+b\sin(\theta))^2\leq a^2+b^2
              </me>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Once we translate the integrals as inner products,
          this inequality is obtained by squaring both sides of the Cauchy-Schwarz (C-S) inequality.
        </p>
        <p>
          (b) This is an instance of C-S, where <m>V=\R^n</m> with dot product,
          <m>\boldx</m> is arbitrary, and <m>\boldy=(1,1,\dots, 1)</m>.
        </p>
        <p>
          (c) This is an instance of C-S, where <m>V=\R^2</m> with dot product,
          <m>\boldx=(a,b)</m> and <m>\boldy=(\cos(\theta), \sin(\theta))</m>.
          Note that <m>\norm{\boldy}=1</m> for this choice of <m>\boldy</m>.
        </p>
      </solution>
      \begin{samepage}
    </li>
    <li>
      <p>
        Let <m>(V,\angvec{ \ , })</m> be an inner product space.
        Recall in this context that we define <m>\norm{\boldv}=\sqrt{\angvec{\boldv, \boldv}}</m>,
        and <m>d(\boldv, \boldw)=\norm{\boldv-\boldw}</m>.
        An <term>isometry</term>  of <m>V</m> is a function
        <m>f\colon V\rightarrow V</m> that preserves distance: i.e.,
        <me>
          d(f(\boldv), f(\boldw))=d(\boldv, \boldw) \text{ for all \(\boldv, \boldw\in V\) }
        </me>.
        In this exercise we will show that any isometry that maps <m>\boldzero</m> to
        <m>\boldzero</m> is a linear transformation.  (This is a handy fact to know.
        For example,
        reflections through lines in <m>\R^2</m> and planes in <m>\R^3</m> clearly preserve distances and map <m>\boldzero</m> to itself;
        it follows immediately that these operations are linear. )   In what follows,
        assume that <m>f</m> is an isometry of <m>V</m> satisfying <m>f(\boldzero)=\boldzero</m>.
        <ol>
          <li>
            <p>
              Prove that <m>\norm{f(\boldv)}=\norm{\boldv}</m>:
              i.e., <m>f</m> preserves norms.
            </p>
          </li>
          <li>
            <p>
              Prove <m>\angvec{f(\boldv), f(\boldw)}=\angvec{\boldv, \boldw}</m>:
              i.e., <m>f</m> preserves inner products.
              Hint: first prove that <m>\angvec{\boldv, \boldw}=\frac{1}{2}(\norm{\boldv}^2+\norm{\boldw}^2-\norm{\boldv-\boldw}^2)</m>.
            </p>
          </li>
          <li>
            <p>
              To prove <m>f</m> is linear it is enough to show
              <m>f(\boldv+c\boldw)=f(\boldv)+cf(\boldw)</m> for all <m>\boldv, \boldw\in V</m>,
              <m>c\in \R</m>.
              To do so, use the above parts to show that
              <me>
                \norm{f(\boldv+c\boldw)-(f(\boldv)+cf(\boldw))}=0
              </me>.
              Hint: regroup this difference in a suitable manner so that you can use parts (a)-(b).
              You may also want to use the identity
              <me>
                \norm{\boldv-\boldw}^2=\norm{\boldv}^2-2\angvec{\boldv,\boldw}+\norm{\boldw}^2
              </me>.
            </p>
          </li>
        </ol>
      </p>
      \end{samepage}
      <solution>
        <p>
          (a) We have
          <md>
            <mrow>\norm{f(\boldv)}\amp =\norm{f(\boldv)-\boldzero}</mrow>
            <mrow>\amp =\norm{f(\boldv)-f(\boldzero)} \amp \text{ (since \(f(\boldzero)=\boldzero\)) }</mrow>
            <mrow>\amp =d(f(\boldv),f(\boldzero) \amp \text{ (by def.) }</mrow>
            <mrow>\amp =d(\boldv, \boldzero) \amp \text{ (\(f\) is an isometry) }</mrow>
            <mrow>\amp =\norm{\boldv}</mrow>
          </md>
        </p>
        <p>
          (b) The hint here is proved by starting with the RHS and substituting <m>\norm{\boldv}^2=\angvec{\boldv, \boldv}, \norm{\boldw}^2=\angvec{\boldw,\boldw}, \norm{\boldv-\boldw}^2=\angvec{\boldv-\boldw,\boldv-\boldw}</m>,
          and then using algebraic properties of the inner product to simplify the resulting expression until you get the LHS.
        </p>
        <p>
          Once we have shown the equality mentioned in the hint,
          the rest is easy:
          <md>
            <mrow>\angvec{f(\boldv),f(\boldw)}\amp =\frac{1}{2}(\norm{f(\boldv)}^2+\norm{f(\boldw)}^2-\norm{f(\boldv)-f(\boldw)}^2) \amp \text{ (hint equality) }</mrow>
            <mrow>\amp =\frac{1}{2}(\norm{f(\boldv)}^2+\norm{f(\boldw)}^2-d(f(\boldv), f\boldw))^2) \amp \text{ (def. of dist.) }</mrow>
            <mrow>\amp =\frac{1}{2}(\norm{\boldv}^2+\norm{\boldw}^2-d(\boldv, \boldw)^2) \amp \text{ (\(f\) preserves norm and dist.) }</mrow>
            <mrow>\amp =\frac{1}{2}(\norm{\boldv}^2+\norm{\boldw}^2-\norm{\boldv-\boldw}^2) \amp \text{ (def. of dist.) }</mrow>
            <mrow>\amp =\angvec{\boldv,\boldw} \amp \text{ (hint equality) }</mrow>
          </md>
        </p>
        <p>
          (c) Note that proving
          <me>
            \norm{f(\boldv+c\boldw)-(f(\boldv)+cf(\boldw))}=0
          </me>
          implies that <m>f(\boldv+c\boldw)-(f(\boldv)+cf(\boldw))=\boldzero</m>,
          and hence that <m>f(\boldv+c\boldw)=f(\boldv)+cf(\boldw)</m>,
          as desired.
          We will prove the square of this norm is 0, and we group the expression as <m>(f(\boldv+c\boldw)-f(\boldv))-cf(\boldw)</m>.
        </p>
        <p>
          We have
          <md>
            <mrow>\amp \norm{\left(f(\boldv+c\boldw)-f(\boldv)\right)-cf(\boldw))}^2</mrow>
            <mrow>\amp =\norm{f(\boldv+c\boldw)-f(\boldv)}^2-2\angvec{f(\boldv+c\boldw)-f(\boldv),cf(\boldw)}+\norm{cf(\boldw)}^2 \amp \text{ (suggested identity) }</mrow>
            <mrow>\amp =\norm{f(\boldv+c\boldw)-f(\boldv)}^2-2c\angvec{f(\boldv+c\boldw)-f(\boldv),f(\boldw)}+c^2\norm{f(\boldw)}^2</mrow>
            <mrow>\amp =\norm{\boldv+c\boldw- \boldv}^2-2c\angvec{f(\boldv+c\boldw)-f(\boldv),f(\boldw)}+c^2\norm{\boldw}^2 \amp \text{ (\(f\) preserves distance and norms) }</mrow>
            <mrow>\amp =c^2\norm{\boldw}^2-2c\angvec{f(\boldv+c\boldw),f(\boldw)}+2c\angvec{f(\boldv),f(\boldw)}+c^2\norm{\boldw}^2</mrow>
            <mrow>\amp =c^2\norm{\boldw}^2-2c\angvec{\boldv+c\boldw,\boldw}+2c\angvec{\boldv, \boldw} +c^2\norm{\boldw}^2 \amp \text{ \(f\) preserves inner product) }</mrow>
            <mrow>\amp \vdots \amp \text{ (inner product simplification) }</mrow>
            <mrow>\amp =0</mrow>
          </md>
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{5.2: orthogonal bases and projection} All questions related to orthogonality, norm,
    distance,
    etc. in the following exercises are always understood to be with regard to the given inner product.
  </p>
  <ol>
    <li>
      <p>
        Let <m>(V, \angvec{ , })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a subspace.
        Show that <m>W^\perp</m> is a subspace.
        <solution>
          <p>
            (i) Since <m>\angvec{\boldv,\boldzero}=0</m> for
            <em>any <m>\boldv</m> whatsoever</em>,
            we have <m>\boldzero\in W^\perp</m>.
          </p>
          <p>
            (ii)+(iii) Suppose <m>\boldv_1,\boldv_2\in W^\perp</m>.
            We must show <m>c_1\boldv_1+c_2\boldv_2\in W^\perp</m> for any <m>c_1,c_2\in\R</m>.
            Given any <m>\boldw\in W</m>, we compute
            <md>
              <mrow>\angvec{\boldw, c_1\boldv_1+c_2\boldv_2}\amp =c_1\angvec{\boldw,\boldv_1}+c_2\angvec{\boldw,\boldv_2} \amp \text{ (bilinearity }</mrow>
              <mrow>\amp =c_1\cdot 0+c_2\cdot 0 \amp \text{ (since \(\boldv_i\in W^\perp\)) }</mrow>
              <mrow>\amp =0</mrow>
            </md>.
          </p>
          <p>
            This proves <m>c_1\boldv_1+c_2\boldv_2\in W^\perp</m>,
            as desired.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>(V, \angvec{ , })</m> be an inner product space,
        and let <m>W\subseteq V</m> be a subspace.
        Show that <m>W\cap W^\perp=\{\boldzero\}</m>.
        <solution>
          <p>
            First observe that <m>\boldzero_V\in W</m> and
            <m>\boldzero_V\in W^\perp</m>, since both are subspaces.
            It follows that <m>\boldzero_V\in W\cap W^\perp</m>
            (by def. of <m>\cap</m>),
            and thus that <m>\{\boldzero_V\}\subseteq W\cap W^\perp</m>.
          </p>
          <p>
            To show the other subset relation,
            suppose <m>\boldv\in W\cap W^\perp</m>.
            Recall that by definition of <m>\cap</m> this means
            <m>\boldv\in W</m> and <m>\boldv\in W^\perp</m>.
            Since an element of <m>W^\perp</m> is orthogonal to everything in <m>W</m> we must have <m>\boldv</m> orthogonal to itself.
            This means <m>\langle\boldv,\boldv\rangle=0</m>.
            By the positivity axiom it now follows that <m>\boldv=\boldzero_V</m>.
            Thus the only element of <m>W\cap W^\perp</m> is <m>\boldzero_V</m>,
            and hence <m>W\cap W^\perp=\{\boldzero_V\}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Consider the vector space <m>V=C([0,1])</m> with standard
        <em>integral inner product</em>.
        Apply Gram-Schmidt to the basis <m>B=\{1,2^x, 3^x\}</m> of
        <m>W=\Span(B)</m> to obtain an orthogonal basis of <m>W</m>.
        <solution>
          <p>
            The resulting orthogonal basis is <m>B'=\{f_1, f_2,f_3\}</m>, where
            <md>
              <mrow>f_1\amp =1</mrow>
              <mrow>f_2\amp =2^x-(\angvec{2^x,1}/\angvec{1,1})1</mrow>
              <mrow>\amp =2^x-(\int_{0}^12^x \ dx)/(\int_0^1 1 \ dx)=2^x-\frac{1}{\ln 2}</mrow>
              <mrow>f_3\amp =3^x-(\angvec{3^x,2^x-\frac{1}{\ln 2}}/\angvec{2^x-\frac{1}{\ln 2}, 2^x-\frac{1}{\ln 2}})(2^x-\frac{1}{\ln 2})-(\angvec{3^x,1}/\angvec{1,1})1</mrow>
              <mrow>\amp =3^x-\frac{\frac{2}{\ln 2\ln 3}+\frac{5}{\ln 6}}{\frac{1}{(\ln 2)^2}+\frac{3}{\ln 4}}(2^x-\frac{1}{\ln 2})-\frac{1}{\ln 3}</mrow>
            </md>
          </p>
          <p>
            OK, I admit, I used technology to compute those integrals.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Consider the vector space <m>V=P_2</m> with the
        <em>evaluation inner product</em>
        <me>
          \angvec{p(x),q(x)}=p(-1)q(-1)+p(0)q(0)+p(1)q(1)
        </me>.
        Apply Gram-Schmidt to the standard basis of <m>P_2</m> to obtain an orthogonal basis of <m>P_2</m>.
        <solution>
          <p>
            Applying Gram-Schmidt to the basis
            <m>B=\{1,x,x^2\}</m> yields the orthogonal basis
            <me>
              B'=\left\{p_1=1, p_2=x, p_3=x^2-\frac{2}{3}\right\}
            </me>.
          </p>
          <p>
            Here are the steps:
            <md>
              <mrow>p_1\amp =1</mrow>
              <mrow>p_2\amp =x-\frac{\angvec{x,1}}{\angvec{1,1}}1</mrow>
              <mrow>\amp =x \amp \text{(since \(\angvec{x,1}=-1+0+1=0\))}</mrow>
              <mrow>p_3\amp =x^2-\frac{\angvec{x^2,p_2}}{\angvec{p_2,p_2}}p_2-\frac{\angvec{x^2,1}}{\angvec{1,1}}</mrow>
              <mrow>\amp =x^2-\frac{0}{2}x-\frac{2}{3}1\amp \left(\begin{array}{c} \angvec{x^2,x}=-1+0+1=0</mrow>
              <mrow>\angvec{x^2,1}=1+0+1=2</mrow>
              <mrow>\angvec{1,1}=1+1+1=3\end{array}\right)</mrow>
              <mrow>\amp =x^2-\frac{2}{3}</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>V=M_{22}</m> with inner product <m>\angvec{A,B}=\tr(A^TB)</m>,
        and let <m>W\subseteq V</m> be the subspace of matrices whose trace is 0.
      </p>
      <ol>
        <li>
          <p>
            Compute an orthogonal basis for <m>W</m>.
          </p>
        </li>
        <li>
          <p>
            Compute <m>\proj{A}{W}</m>,
            where <m>A</m> is the matrix <m>A=\begin{bmatrix}1\amp 2\\ 1\amp 1 \end{bmatrix}</m>
          </p>
        </li>
      </ol>
      <solution>
        <p>
          (a) The general element of <m>W</m> looks like <m>A=\begin{bmatrix}a\amp b\\ c\amp -a \end{bmatrix}</m>.
          From this description we see easily that
          <me>
            B=\left\{ A_1=\begin{bmatrix}1\amp 0\\ 0\amp -1 \end{bmatrix} , A_2=\begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} , A_3=\begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix} \right\}
          </me>
          is a basis for <m>W</m>.
          Furthermore,
          a simple computation shows this basis is already orthogonal.
          What luck!
        </p>
        <p>
          (b) With the orthogonal basis <m>B</m> in hand,
          we use the orthogonal projection formula to compute
          <md>
            <mrow>\proj{A}{W}\amp =\angvec{A,A_1}/\angvec{A_1,A_1} A_1+\angvec{A,A_2}/\angvec{A_2,A_2} A_2+\angvec{A,A_3}/\angvec{A_3,A_3} A_3</mrow>
            <mrow>\amp =0A_1+2A_2+A_3</mrow>
            <mrow>\amp =\begin{bmatrix} 0\amp 2</mrow>
            <mrow>1\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=M_{nn}</m> with inner product <m>\angvec{A,B}=\tr(A^TB)</m>.
        Let <m>W_1</m> be the subspace of all symmetric matrices (<m>A^T=A</m>),
        and let <m>W_2</m> be the subspace of all skew-symmetric matrices
        (<m>A^T=-A</m>).
        Recall (from a previous exercise) that
        <m>\dim W_1=\frac{n(n+1)}{2}</m> and <m>\dim W_2=\frac{n(n-1)}{2}</m>.
        <ol>
          <li>
            <p>
              Show that <m>W_2\subseteq (W_1)^\perp</m>:
              i.e., skew-symmetric matrices are orthogonal to symmetric matrices!
            </p>
          </li>
          <li>
            <p>
              Use the result of <xref ref="ex_orthcomp">Exercise</xref>
              and a dimension argument to show that in fact <m>W_2=W_1^\perp</m>.
            </p>
          </li>
          <li>
            <p>
              Use parts (a)-(b) and the orthogonal projection theorem to show that any matrix
              <m>A\in M_{nn}</m> can be written uniquely in the form <m>A=A_1+A_2</m>,
              where <m>A_1</m> is a symmetric matrix and <m>A_2</m> is a skew-symmetric matrix.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          First I reveal a dirty little secret.
          This
          <q>exotic</q>
          inner product is really nothing more than the dot product in disguise,
          once we take these rectangular arrays and stretch them out into column vectors!
          More rigorously,
          given <m>A=(a_{ij})</m> and <m>B=(b_{ij})</m>, we have
          <md>
            <mrow>\angvec{A,B}\amp =\tr(A^TB)</mrow>
            <mrow>\amp =\sum_{k=1}^n \left(A^TB\right)_{kk}</mrow>
            <mrow>\amp =\sum_{k=1}^n \sum_{\ell=1}^n (A^T)_{k\ell}(B)_{\ell k}</mrow>
            <mrow>\amp =\sum_{k=1}^n \sum_{\ell=1}^n (A)_{\ell k}(B)_{\ell k}</mrow>
            <mrow>\amp =\sum_{k=1}^n \sum_{\ell=1}^n a_{\ell k}b_{\ell k}</mrow>
            <mrow>\amp =\sum_{\substack{1\leq i\leq n\\ 1\leq j\leq n} } a_{ij}b_{ij}</mrow>
          </md>
        </p>
        <p>
          In other words,
          the inner product of two matrices is just the sum of the products of their corresponding entries:
          aka, the dot product!
        </p>
        <p>
          Let's use this last formula to compute
          <m>\angvec{A,B}</m> when <m>A^T=-A</m> and <m>B^T=B</m>.
          First, since skew-symmetric matrices must have 0's along the diagonal,
          we have <m>a_{ii}=0</m> for all <m>i</m>.
          For <m>i\ne j</m>, we have
          <m>a_{ji}=-a_{ij}</m> and <m>b_{ji}=b_{ij}</m>.
          Then
          <md>
            <mrow>\angvec{A, B}\amp =\sum_{\substack{1\leq i\leq n\\ 1\leq j\leq n} } a_{ij}b_{ij}</mrow>
            <mrow>\amp =\sum_{1\leq i \leq n}a_{ii}b_{ii}+\sum_{1\leq i\lt j\leq n} a_{ij}b_{ij}+\sum_{1\leq i\lt j \leq n}a_{ji}b_{ji} \amp \text{ (grouping the terms of the sum any way we like) }</mrow>
            <mrow>\amp =0+\sum_{1\leq i\lt j\leq n} a_{ij}b_{ij}+\sum_{1\leq i\lt j \leq n}-a_{ij}b_{ij} \amp \text{(\(a_{ii}=0\), \(a_{ji}=-a_{ij}\), \(b_{ij}=b_{ji}\))}</mrow>
            <mrow>\amp =\sum_{1\leq i\lt j\leq n} a_{ij}b_{ij}-\sum_{1\leq i\lt j\leq n} a_{ij}b_{ij}</mrow>
            <mrow>\amp =0</mrow>
          </md>.
        </p>
        <p>
          Here's an alternative, slightly slicker approach.
          First observe that in general <m>\tr(AB)=\tr(BA)</m>.
          This is a somewhat surprising property,
          but follows easily from an argument similar to above: namely,
          <me>
            \tr(AB)=\sum_{1\leq i, j\leq n} a_{ji}b_{ij}=\sum_{1\leq i\leq j\leq n}b_{ij}a_{ji}=\tr(BA)
          </me>.
        </p>
        <p>
          Now assume <m>A^T=-A</m> and <m>B^T=B</m>.
          Then
          <md>
            <mrow>\tr(A^TB)\amp =\tr\left( (A^TB)^T)\right) \amp \text{ (since transpose doesn't change diagonal) }</mrow>
            <mrow>\amp =\tr(B^TA) \amp \text{ (prop. of transpose) }</mrow>
            <mrow>\amp =\tr(AB^T) \amp \text{ (prop. of trace mentioned above) }</mrow>
            <mrow>\amp =\tr(-A^TB) \amp \text{ (since \(A^T=-A\), \(B^T=B\)) }</mrow>
            <mrow>\amp =-\tr(A^TB) \amp \text{ (prop. of trace) }</mrow>
          </md>
        </p>
        <p>
          We've shown <m>\tr(A^TB)=-\tr(A^TB)</m>;
          it follows that <m>\tr(A^TB)=0</m>.
        </p>
        <p>
          (b) We have <m>W_2\subseteq W_1^\perp</m>.
          By <xref ref="ex_orthcomp">Exercise</xref>,
          we know <m>\dim W_1^\perp=\dim M_{nn}-\dim W_1=n^2-\frac{n(n+1)}{2}=\frac{n(n-1)}{2}</m>.
          But we've seen in a previous exercise that <m>\dim W_2=\frac{n(n-1)}{2}</m>.
          It follows from the dimension theorem compendium
          (subspace part)
          that <m>W_2=W_1^\perp</m>.
        </p>
        <p>
          (c) By the orthogonal projection theorem,
          any <m>A\in M_{nn}</m> can be expressed as <m>A=B+B^\perp</m> where
          <m>B\in W_1</m> and <m>B^\perp\in W_1^\perp</m>.
          We have <m>W_1</m> the space of symmetric matrices,
          and we just showed that <m>W_1^\perp</m> is the space of skew-symmetric matrices.
          Thus <m>B</m> is symmetric and <m>B^\perp</m> is skew-symmetric,
          as desired.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=M_{22}</m> with inner product <m>\angvec{A,B}=\tr(A^TB)</m>,
        let <m>W_1</m> be the subspace of symmetric matrices,
        and let <m>W_2</m> be the subspace of skew-symmetric matrices. \begin{samepage}
      </p>
      <ol>
        <li>
          <p>
            Verify that <m>B_1=\left\{ A_1=\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix} , A_2=\begin{bmatrix}0\amp 0\\ 0\amp 1 \end{bmatrix} \right\}</m> is an orthogonal basis of <m>W_1</m>.
            Verify that <m>B_2=\left\{ A_3=\begin{bmatrix}0\amp 1\\ -1\amp 0 \end{bmatrix} \right\}</m> is an orthogonal basis of <m>W_2</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>A=\begin{bmatrix}1\amp 2\\ 1\amp 1 \end{bmatrix}</m>.
          </p>
          <ol>
            <li>
              <p>
                Use the previous exercise and the orthogonal projection formula to write <m>A=A_1+A_2</m>,
                where <m>A_1</m> is symmetric and <m>A_2</m> is skew-symmetric.
              </p>
            </li>
            <li>
              <p>
                Compute <m>d(A,W_1)</m>,
                the distance from <m>A</m> to the subspace <m>W_1</m>.
              </p>
            </li>
          </ol>
        </li>
      </ol>
      \end{samepage}
      <solution>
        <p>
          (a) It is easy to see that the given bases are orthogonal.
        </p>
        <p>
          (b) From the previous exercise,
          we know that the orthogonal decomposition of
          <m>A=B+B^\perp</m> yields the desired decomposition of <m>A</m> into a sum of a symmetric matrix and skew-symmetric matrix.
          We use the projection formulas to compute the projections <m>B</m> and <m>B^\perp</m>:
          <md>
            <mrow>B\amp =\proj{A}{W_1}</mrow>
            <mrow>\amp =\angvec{A,A_1}/\angvec{A_1,A_1} A_1+\angvec{A,A_2}/\angvec{A_2,A_2} A_2+\angvec{A,A_3}/\angvec{A_3,A_3} A_3</mrow>
            <mrow>\amp =\frac{1}{1}A_1+\frac{3}{2}A_2+\frac{1}{1}A_3</mrow>
            <mrow>\amp =\begin{bmatrix} 1\amp \frac{3}{2}</mrow>
            <mrow>\frac{3}{2}\amp 1 \end{bmatrix}</mrow>
            <mrow>B^\perp\amp =A-\proj{A}{W_1}</mrow>
            <mrow>\amp =\begin{bmatrix} 0\amp \frac{1}{2}</mrow>
            <mrow>-\frac{1}{2}\amp 0 \end{bmatrix}</mrow>
          </md>
        </p>
        <p>
          Observe that <m>B</m> and <m>B^\perp</m> are indeed symmetric and skew-symmetric,
          resp., and that <m>A=B+B^\perp</m>.
        </p>
        <p>
          By definition <m>d(A, W_1)=\norm{A-B}=\norm{B^\perp}=\sqrt{1/2}=\frac{\sqrt{2}}{2}</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>V=C([0,1])</m> with the standard
        <em>integral inner product</em>, and let <m>f(x)=x</m>.
        Find the function of the form
        <m>g(x)=a+b\cos(2\pi x)+c\sin(2\pi x)</m> that best approximates <m>f(x)</m>
        (with respect to this inner product).
        <solution>
          <p>
            The given inner product is <m>\ds \angvec{f,g}=\int_0^1f(x)g(x) \ dx</m>.
            The desired function <m>g</m> is simply <m>\proj{f}{W}</m>,
            where <m>W=\Span\{f_1=1, f_2=\cos(2\pi x), f_3=\sin(2\pi x)\}</m>.
            The given functions form an orthogonal basis for <m>W</m> with respect to the given inner product,
            allowing us to use the orthogonal projection formula:
            <md>
              <mrow>g\amp =\proj{f}{W}</mrow>
              <mrow>\amp =\frac{\angvec{f,f_1}}{\angvec{f_1,f_1}}f_1+\frac{\angvec{f,f_2}}{\angvec{f_2,f_2}}f_2+\frac{\angvec{f,f_3}}{\angvec{f_3,f_3}}f_3</mrow>
              <mrow>\amp =\frac{1}{2}+\frac{0}{1/2}\cos(2\pi x)+\frac{1/2\pi}{1/2}\sin(2\pi x) \amp \text{ (I've omitted all the integral details) }</mrow>
              <mrow>\amp =\frac{1}{2}+\frac{1}{\pi}\sin(2\pi x)</mrow>
            </md>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Compute an orthogonal basis of <m>W</m>,
        where <m>W</m> is the plane <m>x+2y-z=0</m>.
        Then extend this orthgonal basis to an orthogonal basis of <m>\R^3</m>.
        <solution>
          <p>
            The plane <m>W</m> is the null space of the matrix <m>[1 \ 2 \ -1]</m>.
            The usual nulls space algorithm produces the basis <m>\{ (1,0,1), (-2,1,0)\}</m> for <m>W</m>.
            Use Gram-Schmidt to convert this to an orthogonal basis <m>B'=\{ (1,0,1), (-1,1,1)\}</m>.
          </p>
          <p>
            To extend <m>B'</m> to a basis of <m>\R^3</m>,
            we need only add another nonzero vector that is orthogonal to both <m>(1,0,1), (-1,1,1)</m>.
            That's easy!
            Since both vectors are elements of <m>W</m>,
            they are both orthogonal to its defining normal vector <m>\boldn=(1,2,-1)</m>.
            Thus <m>B''=\{(1,0,1),(-1,1,1),(1,2,-1)\}</m> is an orthogonal basis of <m>\R^3</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix.
        Prove: <m>A^T=A^{-1}</m> if and only if the columns of <m>A</m> are an orthonormal basis of <m>\R^n</m>.
        Matrices satisfying <m>A^T=A^{-1}</m> are called
        <term>orthogonal matrices</term>.
      </p>
    </li>
    <li xml:id="ex_orthproj">
      <p>
        Let <m>V</m> an inner product space,
        and let <m>W\subseteq V</m> be a finite-dimensional subspace.
        Recall that <m>\proj{\boldv}{W}</m> is defined as the unique
        <m>\boldw\in W</m> satisfying <m>\boldv=\boldw+\boldw^\perp</m>,
        where <m>\boldw^\perp\in W^\perp</m>.
        Use this definition
        (including the uniqueness claim)
        to prove the following properties:
        <ol>
          <li>
            <p>
              <m>\proj{c_1\boldv_1+c_2\boldv_2}{W}=c_1\proj{\boldv_1}{W}+c_2\proj{\boldv_2}{W}</m> for all
              <m>\boldv_1,\boldv_2\in V</m> and all <m>c_1,c_2\in \R</m>;
            </p>
          </li>
          <li>
            <p>
              <m>\boldv\in W\Longleftrightarrow \proj{\boldv}{W}=\boldv</m>;
            </p>
          </li>
          <li>
            <p>
              <m>\boldv\in W^\perp\Longleftrightarrow \proj{\boldv}{W}=\boldzero</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) By the orthogonal projection theorem we can write
          <md>
            <mrow>\boldv_1\amp =\boldw_1+\boldw_1^\perp \hspace{5pt} \text{ where }  \boldw_1\in W, \boldw_1^\perp\in W^\perp</mrow>
            <mrow>\boldv_2\amp =\boldw_2+\boldw_2^\perp \hspace{5pt} \text{ where }  \boldw_2\in W, \boldw_2^\perp\in W^\perp</mrow>
          </md>.
        </p>
        <p>
          By definition,
          this means <m>\proj{\boldv_i}{W}=\boldw_i</m> for <m>i=1,2</m>.
          Then we have
          <md>
            <mrow>c_1\boldv_1+c_2\boldv_2\amp =c_1(\boldw_1+\boldw_1^\perp)+ c_2(\boldw_2+\boldw_2^\perp)</mrow>
            <mrow>\amp =(c_1\boldw_1+c_2\boldw_2)+(c_1\boldw_1^\perp+c_2\boldw^\perp)</mrow>
          </md>
        </p>
        <p>
          Since <m>c_1\boldw_1+c_2\boldw_2\in W</m> and <m>c_1\boldw_1^\perp+c_2\boldw^\perp\in W^\perp</m>,
          we recognize
          <me>
            c_1\boldv_1+c_2\boldv_2=(c_1\boldw_1+c_2\boldw_2)+(c_1\boldw_1^\perp+c_2\boldw^\perp)
          </me>
          as an orthogonal decomposition.
          It follows, again by definition, that
          <me>
            \proj{\boldv}{W}=c_1\boldw_1+c_2\boldw_2=c_1\proj{\boldv_1}{W}+c_2\proj{\boldv_2}{W}
          </me>.
        </p>
        <p>
          (b) We have <m>\boldv\in W</m> if and only if
          <m>\boldv=\boldv+\boldzero</m> (<m>\boldv\in W, \boldzero\in W^\perp</m>) is the orthogonal decomposition of <m>\boldv</m> with respect to <m>W</m> if and only if <m>\proj{\boldv}{W}=\boldv</m>.
        </p>
        <p>
          (c) We have <m>\boldv\in W^\perp</m> if and only if
          <m>\boldv=\boldzero+\boldv</m> (<m>\boldzero\in W, \boldv\in W^\perp</m>) is the orthogonal decomposition of <m>\boldv</m> with respect to <m>W</m> if and only if <m>\proj{\boldv}{W}=\boldzero</m>.
        </p>
      </solution>
    </li>
    <li xml:id="ex_orthcomp">
      <p>
        Let <m>(V, \ \angvec{\ , \ })</m> be an inner product space of dimension <m>n</m>,
        and suppose  <m>W\subseteq V</m> is a subspace of dimension <m>r</m>.
        Prove: <m>\dim W^\perp=n-r</m>.
        <term>Hint</term>: begin by picking an <em>orthogonal</em>
        basis <m>B=\{\boldv_1,\dots ,\boldv_r\}</m> of <m>W</m> and extend to an <em>orthogonal</em>
        basis <m>B'=\{\boldv_1,\boldv_2, \dots, \boldv_r, \boldu_1,\dots , \boldu_{n-r}\}</m> of all of <m>V</m>.
        Show the <m>\boldu_i</m> form a basis for <m>W^\perp</m>.
        <solution>
          <p>
            Observe first that if we can show all the steps in the hint,
            then <m>W^\perp</m> has as a basis <m>B''=\{\boldu_1,\dots, \boldu_{n-r}\}</m>,
            and hence <m>\dim(W^\perp)=n-r</m>, as desired.
          </p>
          <p>
            To show <m>B''</m> is a basis of <m>W^\perp</m> we must show that (1) it is independent,
            and (2) it spans <m>W^\perp</m>.
          </p>
          <p>
            Independence is easy:
            this follows from the fact that the big set <m>B'</m> is independent (in fact orthogonal);
            any subset of a linearly independent set is itself linearly independent.
          </p>
          <p>
            Now we show that <m>B''</m> spans <m>W^\perp</m>.
            First note that each <m>\boldu_i</m> is indeed an element of <m>W^\perp</m> since by assumption the set <m>B'</m> is orthogonal.
            This means that <m>\langle\boldu_i,\boldv_j\rangle=0</m> for all <m>j</m>;
            since the <m>\boldv_j</m> are a basis for <m>W</m>,
            it follows that <m>\langle\boldu_i,\boldw\rangle=0</m> for all <m>\boldw\in W</m>.
          </p>
          <p>
            Next take any element <m>\boldu\in W^\perp</m>.
            Since <m>B'</m> is a basis of the whole space <m>V</m>,
            we can write <m>\boldu</m> as a linear combination of the <m>\boldv_i</m> and <m>\boldu_i</m>:
            <me>
              \boldu=c_1\boldv_1+\cdots +c_r\boldv_r+d_1\boldu_1+\cdots d_{n-r}\boldu_{n-r}
            </me>.
          </p>
          <p>
            We will show that each <m>c_i=0</m>,
            and hence that in fact we have
            <me>
              \boldu=d_1\boldu_1+\cdots d_{n-r}\boldu_{n-r}\in\Span(\{\boldu_1,\dots, \boldu_{n-r}\})
            </me>.
          </p>
          <p>
            To show <m>c_i=0</m> we simply take the inner product of <m>\boldu</m> with <m>\boldv_i</m>:
            <md>
              <mrow>0\amp =\langle\boldu,\boldv_i\rangle \amp \text{ (since \(\boldu\in W^\perp\)) }</mrow>
              <mrow>\amp =\langle c_1\boldv_1+\cdots +c_r\boldv_r+d_1\boldu_1+\cdots d_{n-r}\boldu_{n-r},\boldv_i\rangle</mrow>
              <mrow>\amp =c_1\cancelto{0}{\langle\boldv_1,\boldv_i\rangle}+\cdots c_i\langle\boldv_i,\boldv_i\rangle+\cdots c_r\cancelto{0}{\langle\boldv_r,\boldv_i\rangle}+d_1\cancelto{0}{\langle\boldu_1,\boldv_i\rangle}+\cdots +d_{n-r}\cancelto{0}{\langle\boldu_{n-r},\boldv_i\rangle}</mrow>
              <mrow>\amp =c_i\langle\boldv_i,\boldv_i\rangle</mrow>
            </md>.
          </p>
          <p>
            Since <m>0=c_i\langle\boldv_i,\boldv_i\rangle</m> and <m>\langle\boldv_i,\boldv_i\rangle\ne 0</m>
            (by positivity),
            it follows that <m>c_i=0</m>, as desired.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>(V, \ \angvec{\ , \ })</m> be an inner product space of dimension <m>n</m>.
        Let <m>T\colon V\rightarrow V</m> be defined as <m>T(\boldv)=\proj{\boldv}{W}</m>:
        i.e., <m>T</m> is orthogonal projection onto <m>W</m>.
        Let <m>B_1</m> be a basis for <m>W</m> and <m>B_2</m> be a basis for <m>W^\perp</m>.
        <ol>
          <li>
            <p>
              Using <xref ref="ex_orthproj">Exercises</xref>
              and <xref ref="ex_orthcomp"></xref>,
              as well as the orthogonal projection theorem,
              show that <m>B=B_1\cup B_2</m> is a basis for <m>V</m> consisting of eigenvectors of <m>T</m>.
            </p>
          </li>
          <li>
            <p>
              Compute the corresponding diagonal matrix <m>D=[T]_B</m>.
              Make sure to indicate how many times a given eigenvalue is repeated.
            </p>
          </li>
        </ol>
      </p>
      Note: this shows that any orthogonal projection of a finite-dimensional inner product space is  diagonalizable.
      <solution>
        <p>
          (a) Let <m>B_1=\{\boldv_1,\dots, \boldv_r\}</m> be a basis of <m>W</m>.
          By <xref ref="ex_orthocomp">Exercise</xref> <m>\dim W^\perp=n-r</m>,
          and hence we can find a basis <m>B_2=\{\boldu_1,\dots, \boldu_{n-r}</m>.
        </p>
        <p>
          By <xref ref="ex_orthproj">Exercise</xref>.b,
          <m>T(\boldv)=\boldv</m> if and only if <m>\boldv\in W</m>.
          Thus <m>W=W_1</m>, the <m>(\lambda=1)</m>-eigenspace of <m>T</m>.
        </p>
        <p>
          By <xref ref="ex_orthproj">Exercise</xref>.c, we have have
          <m>T(\boldv)=\boldzero=0\boldv</m> if and only if <m>\boldv\in W^\perp</m>.
          Thus <m>W^\perp=W_0</m>, the
          <m>(\lambda=0)</m>-eigenspace of <m>T</m>.
        </p>
        <p>
          Since the <m>\boldv_i</m> are linearly independent elements of <m>W_1</m>,
          and the <m>\boldu_j</m> are linearly independent vectors of <m>W_0</m>,
          it follows that <m>B=B_1\cup B_2=\{\boldv_1,\dots, \boldv_r,\boldu_1,\dots, \boldu_{n-r}\}</m> is linearly independent.
          (This is a consequence of the fact that eigenvectors with distinct eigenvalues are linearly independent.)
        </p>
        <p>
          Since <m>\#B=n+(n-r)=n</m> we see that <m>B</m> is a basis of <m>V</m> consisting of eigenvectors of <m>T</m>.
        </p>
        <p>
          (b) Having shown that our <m>B</m> is a basis of eigenvectors,
          we know that <m>[T]_B</m>,
          the matrix representing <m>T</m> with respect to <m>B</m>,
          is diagonal.
          More specifically, following the recipe for computing <m>[T]_B</m>,
          we see that for <m>1\leq j\leq r</m>, the <m>j</m>-th column is
          <me>
            [T(\boldv_j)]_B=[\boldv_j]_B=\underset{\text{ \(j\)th-entry } }{\underbrace{(0,0,\dots, 1,0,\dots, 0)}}=\bolde_j;
          </me>
          and for <m>r+1\leq j\leq n</m> the <m>j</m>-th column is
          <me>
            [T(\boldu_j)]_B=[\boldzero]_B=(0,0,\dots, 0)
          </me>.
        </p>
        <p>
          It follows that <m>[T]_B</m> is a diagonal matrix whose first <m>r</m> diagonal entries are 1, and whose remaining <m>n-r</m> diagonal entries are <m>0</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        We consider the problem of fitting a collection of data points <m>(x,y)</m> with a quadratic curve of the form <m>y=f(x)=ax^2+bx+c</m>.
        Thus we are <em>given</em> some collection of points <m>(x,y)</m>,
        and we <em>seek</em> parameters
        <m>a,
        b, c</m> for which the graph of <m>f(x)=ax^2+bx+c</m>
        <q>best fits</q>
        the points in some way.
      </p>
      <ol>
        <li>
          <p>
            Show, using linear algebra,
            that if we are given any three points <m>(x,y)=(r_1,s_1), (r_2,s_2), (r_3,s_3)</m>,
            where the <m>x</m>-coordinates <m>r_i</m> are all distinct,
            then there is a <em>unique</em>
            choice of <m>a,b,c</m> such that the corresponding quadratic function agrees
            <em>precisely</em> with the data.
            In other words, given just about any three points in the plane,
            there is a unique quadratic curve connecting them.
          </p>
        </li>
        <li>
          <p>
            Now suppose we are given the four data points
            <me>
              P_1=(0,2), P_2=(1,0), P_3=(2,2), P_4=(3,6)
            </me>.
          </p>
          <ol>
            <li>
              <p>
                Use the least-squares method described in the lecture notes to come up with a quadratic function <m>y=f(x)</m> that
                <q>best fits</q>
                the data.
              </p>
            </li>
            <li>
              <p>
                Graph the function <m>f</m> you found,
                along with the points <m>P_i</m>.
                (You may want to use technology.)
                Use your graph to explain precisely in what sense <m>f</m>
                <q>best fits</q>
                the data.
              </p>
            </li>
          </ol>
        </li>
      </ol>
      <solution>
        <p>
          (a) Each data point <m>(r,s)</m> yields a
          <em>linear equation</em>
          in the unknowns <m>a,b,c</m>:
          namely, <m>r^2a+rb+c=s</m>.
          Thus given three data points we get a linear system of three equations in the three unknowns <m>a,b,c</m> that is represented as
          <me>
            \begin{bmatrix}r_1^2\amp r_1\amp 1\\[2ex] r_2^2\amp r_2\amp 1\\[2ex] r_3^2\amp r_3\amp 1 \end{bmatrix} \begin{bmatrix}a\\ b\\ c \end{bmatrix} =\colvec{s_1\\s_2 \\s_3}
          </me>
        </p>
        <p>
          The matrix on the left has determinant <m>(r_1-r_2)(r_1-r_3)(r_2-r_3)</m>.
          Since the <m>r_i</m> are distinct, this is nonzero!
          It follows that the matrix is invertible,
          and hence there is a unique solution <m>(a,b,c)</m>.
        </p>
        <p>
          (b) Using the reasoning above,
          the matrix equation we would like to solve for <m>a,b,c</m> is
          <me>
            \begin{bmatrix}0\amp 0\amp 1\\ 1\amp 1\amp 1\\ 4\amp 2\amp 1\\ 9\amp 3\amp 1 \end{bmatrix} \colvec{a\\ b\\ c} =\colvec{2\\ 0\\ 2\\ 6}
          </me>
        </p>
        <p>
          To find a least-squares solution,
          we solve the corresponding equation <m>A^TA\boldx=A^T\boldy</m>,
          which boils down to
          <me>
            \begin{bmatrix}98\amp 36\amp 14\\ 36\amp 14\amp 6\\ 14\amp 6\amp 4 \end{bmatrix} \colvec{a\\ b\\ c} = \colvec{62\\ 22\\ 10}
          </me>
        </p>
        <p>
          The least-squares solution is <m>(a,b,c)=(3/2,-31/10, 19,10)</m>.
          Thus the quadratic curve that
          <q>best fits</q>
          the data is
          <me>
            f(x)=\frac{3}{2}x^2-\frac{31}{10}x+\frac{19}{10}
          </me>.
        </p>
        <p>
          It is a
          <q>best fit</q>
          in the sense that it minimizes the following sum of squared errors:
          <me>
            (f(0)-2)^2+(f(1)-0)^2+(f(2)-2)^2+(f(3)-6)^2
          </me>.
        </p>
        <p>
          You can think of this sum as a cumulative measure of how far off the function is from the desired outputs for the inputs <m>x=0,1,2,3</m>.
          In terms of the graph below,
          <m>f</m> is the quadratic function that minimizes the sum of squares of the vertical distances between the green points and the blue curve.
          <me>
            <image width="93%" source="images/leastsquares.png"/>
          </me>
        </p>
      </solution>
    </li>
  </ol>
  <p>
    \chapter*{5.1-5.2: linear transformations and isomorphisms}
  </p>
  <ol>
    \begin{samepage}
    <li>
      <p>
        For each of the following functions <m>T</m>,
        decide whether <m>T</m> is linear.
        If yes, provide a proof; if no,
        given an explicit example that violates one of the axioms.
      </p>
      <ol>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow M_{nn}</m>, <m>T(A)=A^2</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow \R</m>, <m>T(A)=\tr A</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon M_{nn}\rightarrow \R</m>, <m>T(A)=\det A</m>.
          </p>
        </li>
        <li>
          <p>
            <m>T\colon F((-\infty, \infty))\rightarrow F((-\infty, \infty))</m>,
            <m>T(f)=1+f</m>
          </p>
        </li>
        <li>
          <p>
            <m>T\colon F((-\infty, \infty))\rightarrow F((-\infty, \infty))</m>,
            <m>T(f(x))=f(x+3)</m>
          </p>
        </li>
      </ol>
      \end{samepage}
      <solution>
        <p>
          (a) Nonlinear.
          Take <m>A=I_2</m>.
          Then <m>T(2A)=4I\ne 2T(A)=2I</m>.
        </p>
        <p>
          (b) Linear.
          Easy proof.
        </p>
        <p>
          (c) Nonlinear.
          Let <m>A=I_2</m>.
          Then <m>\det(2A)=4\ne 2\det(A)=2</m>.
        </p>
        <p>
          (d) Nonlinear.
          Notice that <m>T(\boldzero)=1\ne\boldzero</m>;
          yet any linear transformation sends the zero vector to the zero vector.
        </p>
        <p>
          (e) Linear.
          <md>
            <mrow>T(cf+dg)\amp =(cf+g)(x+3)\amp \text{ (by def) }</mrow>
            <mrow>\amp =cf(x+3)+dg(x+3)\amp \text{ (function arith.) }</mrow>
            <mrow>\amp =cT(f)+dT(g) \amp \text{ (by def) }</mrow>
          </md>
        </p>
      </solution>
      \begin{samepage}
    </li>
    <li>
      <p>
        Let <m>(V,\angvec{ \ , })</m> be an inner product space.
        Recall in this context that we define <m>\norm{\boldv}=\sqrt{\angvec{\boldv, \boldv}}</m>,
        and <m>d(\boldv, \boldw)=\norm{\boldv-\boldw}</m>.
        An <term>isometry</term>  of <m>V</m> is a function
        <m>f\colon V\rightarrow V</m> that preserves distance: i.e.,
        <me>
          d(f(\boldv), f(\boldw))=d(\boldv, \boldw) \text{ for all \(\boldv, \boldw\in V\) }
        </me>.
        In this exercise we will show that any isometry that maps <m>\boldzero</m> to
        <m>\boldzero</m> is a linear transformation.
        To that end,
        assume from now on that <m>f</m> is an isometry of <m>V</m> satisfying <m>f(\boldzero)=\boldzero</m>.
        <ol>
          <li>
            <p>
              Prove that <m>\norm{f(\boldv)}=\norm{\boldv}</m>:
              i.e., <m>f</m> preserves norms.
            </p>
          </li>
          <li>
            <p>
              Prove <m>\angvec{f(\boldv), f(\boldw)}=\angvec{\boldv, \boldw}</m>:
              i.e., <m>f</m> preserves inner products.
              Hint: first prove that <m>\angvec{\boldv, \boldw}=\frac{1}{2}(\norm{\boldv}^2+\norm{\boldw}^2-\norm{\boldv-\boldw}^2)</m>.
            </p>
          </li>
          <li>
            <p>
              To prove <m>f</m> is linear it is enough to show
              <m>f(\boldv+c\boldw)=f(\boldv)+cf(\boldw)</m> for all <m>\boldv, \boldw\in V</m>,
              <m>c\in \R</m>.
              To do so, use the above parts to show that
              <me>
                \norm{f(\boldv+c\boldw)-(f(\boldv)+cf(\boldw))}=0
              </me>.
              Hint: regroup this difference in a suitable manner so that you can use parts (a)-(b) once you expand out <m>\norm{\hspace{5pt}}^2</m>.
            </p>
          </li>
        </ol>
      </p>
      \end{samepage}
      <solution>
        <p>
          Done in class!
        </p>
      </solution>
      \begin{samepage}
    </li>
    <li xml:id="Ex_refl">
      <p>
        <em>Reflections in <m>\R^2</m></em>.
        Given a fixed angle <m>\alpha</m>,
        <m>0\leq \alpha\lt \pi</m>,
        let <m>\ell_{\alpha}</m> be the line through the origin that makes an angle of <m>\alpha</m> with the positive <m>x</m>-axis.
        We define <m>T_{\alpha}\colon\R^2\rightarrow \R^2</m> as
        <me>
          T_\alpha(\boldx)=\text{ the reflection of \(\boldx\) through \(\ell_\alpha\) }
        </me>
        <ol>
          <li>
            <p>
              Use the previous exercise to show that <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Use the recipe to compute the matrix
              <m>A_{\alpha}</m> such that <m>T_\alpha=T_{A_\alpha}</m>.
              You will want to draw a picture to see what the action of
              <m>T_\alpha</m> on <m>\bolde_1, \bolde_2</m> is.
            </p>
          </li>
        </ol>
      </p>
      \end{samepage}
      <solution>
        <p>
          It is easiest to see what reflection does by thinking in terms of polar coordinates.
          Recall: a vector <m>\boldv</m> is completely determined by its length
          <m>r=\norm{\boldv}</m> and the angle <m>\theta</m> it makes with respect to the positive <m>x</m>-axis.
          (The angle <m>\theta</m> is well-defined only up to multiples of <m>2\pi</m>.)
          The values <m>r</m> and <m>\theta</m> are called polar coordinates of the vector.
        </p>
        <p>
          The reflection of <m>\boldv</m> through
          <m>\ell_{\alpha}</m> is obtained by first rotating it so that it points along <m>\ell_{\alpha}</m>,
          and then rotating again in the same direction by the same amount.
          As described,
          it is clear that reflection is an isometry that fixes the origin.
          It follows from the previous exercise that reflection is thus a linear transformation.
        </p>
        <p>
          In more quantitative terms,
          if <m>\boldv</m> makes an angle of <m>\theta</m> with the <m>x</m>-axis,
          then to rotate <m>\boldv</m> to point along
          <m>\ell_{\alpha}</m> we rotate by an angle of <m>(\alpha-\theta)</m>.
          To get to the reflection we add the same angle <m>(\alpha-\theta)</m> again.
          In all we have changed the angle <m>\theta</m> to <m>\theta+2(\alpha-\theta)=2\alpha-\theta</m>.
        </p>
        <p>
          Thus <m>\bolde_1</m>, which has
          <m>\theta=0</m> gets mapped to <m>T_{\alpha}(\bolde_1)</m>,
          a unit vector with angle <m>2\alpha</m>;
          and <m>\bolde_2</m>, which
          <m>\theta=\pi/2</m> gets mapped to <m>T_{\alpha}(\bolde_2)</m>,
          which angle <m>2\alpha-\pi/2</m>.
        </p>
        <p>
          Using the recipe
        </p>
        <p>
          The matrix we get is
          <me>
            A_\alpha=\begin{bmatrix}\cos(2\alpha)\amp \cos(2\alpha-\pi/2))\\ \sin(2\alpha)\amp \sin(2\alpha-\pi/2) \end{bmatrix} = \begin{bmatrix}\cos(2\alpha)\amp \sin(2\alpha)\\ \sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix}
          </me>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T\colon \R^3\rightarrow \R^3</m> be the transformation defined as
        <m>T(\boldv)=\boldw</m> where the <m>\boldw</m> is the orthogonal projection of <m>\boldv</m> onto the plane <m>W\colon x+y+z=0</m>.
        You can take as a fact that <m>T</m> is a
        <em>linear transformation</em>.   (a) Find the matrix <m>A</m> such that <m>T=T_A</m>.   (b) Use <m>A</m> to compute the orthogonal projection of
        <m>(1,-5,2)</m> on the plane <m>x+y+z=0</m>.   (c) Compute <m>\NS(T)</m> and <m>\range(T)</m>.
        <solution>
          <p>
            As usual, we have
            <me>
              A=\begin{bmatrix}\vert \amp \vert\amp \vert \\ T(\bolde_1) \amp T(\bolde_2)\amp T(\bolde_3)\\ \vert \amp \vert \amp \vert \end{bmatrix}
            </me>
            where <m>\bolde_1=(1,0,0)</m>,
            <m>\bolde_2=(0,1,0)</m>, <m>\bolde_3=(0,0,1)</m>.
          </p>
          <p>
            Now to compute <m>T(\bolde_i)</m> for each <m>i</m> we use the method from Chapter 3 for projecting a vector onto a plane:
            namely subtract the projection of
            <m>\bolde_i</m> onto the normal vector
            <m>\boldn=(1,1,1)</m> from <m>\bolde_i</m>.
            After some computation we get
            <md>
              <mrow>T(\bolde_1)\amp =(2/3,-1/3,-1/3)</mrow>
              <mrow>T(\bolde_2)\amp =(-1/3,2/3,-1/3)</mrow>
              <mrow>T(\bolde_3)\amp =(-1/3,-1/3,2/3)</mrow>
            </md>
          </p>
          <p>
            Thus
            <me>
              A=\frac{1}{3}\begin{bmatrix}2\amp -1\amp -1\\ -1\amp 2\amp -1\\ -1\amp -1\amp 2 \end{bmatrix}
            </me>.
          </p>
          <p>
            Nifty!
            This means to compute the orthogonal projection of <m>(x,y,z)</m> onto the plane <m>x+y+z=0</m> we just compute <m>A\begin{bmatrix}x\\ y\\ z \end{bmatrix}</m>.
          </p>
          <p>
            Thus, for example,
            <me>
              T(1,-5,2)=A\begin{bmatrix}1\\ -5 \\ 2 \end{bmatrix} =\begin{bmatrix}5/3\\ -13/3 \\ 8/3 \end{bmatrix}
            </me>.
          </p>
          <p>
            To determine <m>\NS(T)</m> and
            <m>\range(T)</m> we can proceed either (1) conceptually,
            or (2) algebraically.
          </p>
          <p>
            Conceptually,
            we have shown previously that for any orthogonal projection <m>\text{ proj } _W</m> we have
            <md>
              <mrow>\proj{\boldv}{W}=\boldzero\amp \Longleftrightarrow \boldv\in W^\perp, \text{ and }</mrow>
              <mrow>\proj{\boldw}{W}=\boldw\amp \text{ for all }  \boldw\in W</mrow>
            </md>
          </p>
          <p>
            From these two facts it follows easily that
            <m>\NS(T)=W^\perp=\Span(\{(1,1,1)\})</m> and <m>\range(T)=W</m>.
          </p>
          <p>
            Algebraically,
            since <m>T=T_A</m> where <m>A</m> is the matrix above,
            it follows that
            <md>
              <mrow>\NS(T)\amp =\NS(A)</mrow>
              <mrow>\range(T)\amp =\CS(A)</mrow>
            </md>
            and we can compute these using the usual fundamental space algorithms.
            We get <m>\NS(A)=\Span(\{(1,1,1)\}</m> and <m>\CS(A)=\Span(\{(2/3,-1/3,-1/3),(-1/3,2/3,-1/3)\})</m>.
            These are of course the line through <m>(1,1,1)</m> and the plane <m>x+y+z=0</m>,
            respectively.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation.
        <ol>
          <li>
            <p>
              Prove that <m>\NS(T)</m> is a subspace of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              Prove that <m>\range(T)</m> is a subspace of <m>W</m>.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          We proved (b) in lecture.
          Part (a) is easy.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Use the rank-nullity theorem to compute the rank of the linear transformation <m>T</m> described.
        <ol>
          <li>
            <p>
              <m>T\colon\R^7\rightarrow M_{32}</m> has nullity 2.
            </p>
          </li>
          <li>
            <p>
              <m>T\colon P_3\rightarrow \R</m> has nullity 1.
            </p>
          </li>
          <li>
            <p>
              The null space of <m>T\colon P_5 \rightarrow P_5</m> is <m>P_4</m>.
            </p>
          </li>
          <li>
            <p>
              <m>T\colon P_n\rightarrow M_{mn}</m> has nullity 3.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) <m>\rank(T) = \dim(R^7) - 2 = 7-2=5</m>
        </p>
        <p>
          (b) <m>\rank(T) = \dim(P_3) - 1 = 4-1=3</m>
        </p>
        <p>
          (c)<m>\rank(T) = \dim(P_5) - \nullity(T) = \dim(P_5)-\dim(P_4) = 1</m>
        </p>
        <p>
          (d) <m>\rank(T) = \dim(P_n)-3 = (n+1) - 3 = n-2</m>
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T\colon V\rightarrow W</m> be a linear transformation,
        where <m>V</m> is finite dimensional.
        Let <m>S=\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> be a basis of <m>\NS(T)</m> and this basis to a full basis <m>B=\{\boldv_1,\boldv_2,\dots, \boldv_r, \boldu_{1},\dots \boldu_{n-r}\}</m> of <m>V</m>.
        Prove the claim made in my lecture notes:
        namely, that  <m>S'=\{T(\boldu_{1}),\dots, T(\boldu_{n-r})\}</m> is a basis of <m>\range(T)</m>.
        Treat the linear independence and span questions separately.
        For the spanning question, begin as follows:
        given <m>\boldw\in \range(T)</m>,
        by definition there is a <m>\boldv\in V</m> such that <m>T(\boldv)=\boldw</m>;
        write out <m>\boldv</m> in terms of the basis <m>B</m> and go from there.
        <solution>
          <p>
            We prove separately that (a) <m>S'</m> is linearly independent,
            and (b) <m>S'</m> spans <m>\range(T)</m>.
          </p>
          <p>
            (a) Suppose we have <m>\sum_{i=1}^{n-r}c_iT(\boldu_i)=\boldzero</m>.
            Then <m>T(\sum_{i=1}^{n-r}c_i\boldu_i)=\boldzero</m>,
            and we see that the vector <m>\boldu=\sum_{i=1}^{n-r}c_i\boldu_i\in\NS(T)</m>.
            Since the <m>\boldv_i</m> span <m>\NS(T)</m> we then also have
            <me>
              \boldu=\sum_{i=1}^rd_i\boldv_i \text{ for some \(d_i\in\R\) }
            </me>.
          </p>
          <p>
            It would stand to reason,
            since <m>S</m> is a basis, that the only way can have
            <me>
              \boldu=\sum_{i=1}^rd_i\boldv_i=\sum_{i=1}^{n-r}c_i\boldu_i
            </me>
            is if <m>d_i=c_j=0</m> for all <m>i, j</m>.
            But how can we prove this?
            The equality above implies
            <me>
              d_1\boldv_1+d_2\boldv_2+\cdots +d_r\boldv_r-c_1\boldu_1-c_2\boldu_2-\cdots -c_{n-r}\boldu_{n-r}=\boldzero
            </me>.
          </p>
          <p>
            By linear independence of <m>S</m> it now follows that <m>d_i=c_j=0</m> for all <m>i, j</m>.
          </p>
          <p>
            (b) First, since <m>S'\subseteq \range(T)</m>,
            we have <m>\Span(S')\subseteq \range(T)</m>.
            To show the other direction, take any <m>\boldw\in \range(T)</m>.
            Then <m>\boldw=T(\boldv)</m> for some <m>\boldv\in V</m>.
            Since <m>S</m> is a basis for <m>V</m>, we can write
            <me>
              \boldv=c_1\boldv_1+c_2\boldv_2+\cdots c_r\boldv_r+d_1\boldu_1+d_2\boldu_2+\cdots +d_{n-r}\boldu_{n-r}
            </me>
            for some <m>c_i, d_j\in\R</m>.
            Then we have
            <md>
              <mrow>\boldw\amp =T(\boldv)</mrow>
              <mrow>\amp =T(c_1\boldv_1+c_2\boldv_2+\cdots c_r\boldv_r+d_1\boldu_1+d_2\boldu_2+\cdots +d_{n-r}\boldu_{n-r})</mrow>
              <mrow>\amp =c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots c_rT(\boldv_r)+d_1T(\boldu_1)+d_2T(\boldu_2)+\cdots +d_{n-r}T(\boldu_{n-r})</mrow>
              <mrow>\amp =\boldzero+d_1T(\boldu_1)+d_2T(\boldu_2)+\cdots +d_{n-r}T(\boldu_{n-r})  \amp \text{ (since \(T(\boldv_i)=\boldzero\) for all \(i\)) }</mrow>
              <mrow>\amp =d_1T(\boldu_1)+d_2T(\boldu_2)+\cdots +d_{n-r}T(\boldu_{n-r})</mrow>
            </md>
          </p>
          <p>
            This shows <m>\boldw\in \Span(S')</m>, as desired.
          </p>
        </solution>
        \begin{samepage}
      </p>
    </li>
    <li>
      <p>
        Let <m>V=\R^\infty=\{(a_1,a_2,\dots, )\colon a_i\in\R\}</m>,
        the space of all infinite sequences.
        Define the
        <q>shift left</q>
        and
        <q>shift right</q>
        functions as follows:
        <md>
          <mrow>T_L\colon \R^\infty\amp \rightarrow \R^\infty</mrow>
          <mrow>s=(a_1,a_2, a_3,\dots )\amp \longmapsto T_L(s)=(a_2, a_3,\dots)</mrow>
          <mrow>T_R\colon \R^\infty\amp \rightarrow \R^\infty</mrow>
          <mrow>s=(a_1,a_2, a_3,\dots )\amp \longmapsto T_R(s)=(0,a_1,a_2,\dots)</mrow>
        </md>
        <ol>
          <li>
            <p>
              Prove that <m>T_L</m> and <m>T_R</m> are linear.
            </p>
          </li>
          <li>
            <p>
              Show that <m>T_L</m> is onto,
              but not one-to-one. (Compute
              <m>\NS(T_L)</m> and <m>\range(T_L)</m>.)
            </p>
          </li>
          <li>
            <p>
              Show that <m>T_R</m> is one-to-one,
              but not onto. (Compute <m>\NS(T_R)</m> and <m>\range(T_R)</m>. )
            </p>
          </li>
          <li>
            <p>
              Show that <m>T_L\circ T_R=I_{\R^\infty}</m> but <m>T_R\circ T_L\ne I_{\R^\infty}</m>.
            </p>
          </li>
        </ol>
      </p>
      \end{samepage}
      <solution>
        <p>
          <em>Moral of this exercise</em>:
          when <m>V</m> is NOT finite-dimensional,
          we do NOT necessarily have the equivalence
          <me>
            \text{ \(T\) invertible iff \(T\) onto iff \(T\) 1-1 }
          </me>.
          for linear transformations <m>T\colon V\rightarrow V</m>.
        </p>
        <p>
          (a) Easy.
        </p>
        <p>
          (b) Given any sequence <m>t=(b_1,b_2,\dots )</m>,
          we have <m>b=T(s)</m>, where <m>s=(0,b_1,b_2,\dots)</m>.
          Thus <m>T_L</m> is onto: i.e., <m>\range(T_L)=\R^\infty</m>.
          It is east to see that <m>T_L</m> is not 1-1:
          in fact, we compute easily
          <m>\NS(T_L)</m> is the subspace of infinite sequences of the form
          <m>(c,0,0,\dots)</m>, which is nontrivial.
        </p>
        <p>
          (c) We have <m>T_R((a_1,a_2,\dots))=(0,0,\dots)</m> iff
          <m>(0,a_1,a_2,\dots)=(0,0,\dots)</m> if and only if <m>a_1=a_2=\cdots =0</m>.
          This shows <m>\NS(T_R)=\{\boldzero\}</m>.
          It is also clear that <m>\range(T_R)</m> is the space of all infinite sequences of the form <m>(0,b_1,b_2,\dots)</m>.
          As this is not all of <m>\R^\infty</m>,
          we see that <m>T_R</m> is not onto.
        </p>
        <p>
          (d) Also easy.
          This is an example of a function (<m>T_R)</m> that has a left-inverse (namely,
          <m>T_L</m>) which is not a right-inverse
          (since <m>T_R\circ T_L\ne I_V</m>)!!
          Funny things can happen when <m>V</m> is not finite-dimensional.
        </p>
      </solution>
    </li>
    <li>
      <p>
        (Coordinate vectors) Let <m>V</m> be finite-dimensional.
        Choose any basis <m>B=\{\boldv_1,\boldv_2,\dots , \boldv_n\}</m>.
        Show that taking coordinate vectors defines an isomorphism from <m>V</m> to <m>\R^n</m>.
        That is, define <m>T\colon V\rightarrow \R^n</m> by <m>T(\boldv)=[\boldv]_B</m>,
        and show <m>T</m> is an isomorphism. (Note:
        you must first show <m>T</m> is a linear transformation!)
        <solution>
          <p>
            We showed <m>T=[\hspace{8pt}]_B</m> was linear in an earlier homework exercise.
          </p>
          <p>
            Since <m>\dim(V)=\dim(\R^n)=n</m>,
            to show <m>T</m> is invertible
            (or an isomorphism),
            it suffices to show either that it is onto, or that it is 1-1.
            I'll prove the former.
            Given any vector <m>(a_1,a_2,\dots,
            a_n)\in\R^n</m> we need to show there is a vector <m>\boldv</m> such that <m>[\boldv]_B=(a_1,a_2,\dots,
            a_n)</m>.
            That's easy:
            take <m>\boldv=a_1\boldv_1+\cdots +a_n\boldv_n</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        (Transpose) Define <m>S\colon M_{mn}\rightarrow M_{nm}</m> by <m>S(A)=A^T</m>.
        Show that <m>S</m> is an isomorphism.
        <solution>
          <p>
            Define the linear transformation
            <m>\hat{S}\colon M_{nm}\Rightarrow M_{mn}</m> as <m>\hat{S}(B)=B^T</m>.
            Then
            <md>
              <mrow>\hat{S}(S(A))\amp =\hat{S}(A^T)=(A^T)^T=A, \text{ and }</mrow>
              <mrow>S(\hat{S}(B))\amp =S(B^T)=(B^T)^T=B</mrow>
            </md>,
            showing <m>\hat{S}</m> is the inverse of <m>S</m>,
            and hence that <m>S</m> is invertible.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        (Conjugation).
        Fix an invertible matrix <m>Q\in M_{nn}</m>.
        Define <m>T\colon M_{nn}\rightarrow M_{nn}</m> as <m>T(A)=QAQ^{-1}</m>.
        Show <m>T</m> is an isomorphism.
        (This operation is called <term>conjugation by <m>Q</m></term>.)
      </p>
    </li>
    <li>
      <p>
        (Evaluation).
        Let <m>V=P_n</m>.
        Let <m>c_1,c_2,\dots, c_{n+1}</m> be any distinct constants.
        Show that the map
        <md>
          <mrow>T\colon P_n\amp \rightarrow \R^{n+1}</mrow>
          <mrow>p(x)\amp \mapsto (p(c_1), p(c_2), \dots, p(c_{n+1})</mrow>
        </md>
        is an isomorphism.  (Note:
        this isomorphism tells us that a degree <m>n</m> polynomial <m>p(x)</m> is uniquely determined by its values at any choice of <m>n+1</m> distinct inputs. )
        <solution>
          <p>
            Note: it is easy to show that <m>T</m> is actually a linear transformation.
          </p>
          <p>
            To show <m>T</m> is invertible
            (or an isomorphism),
            it is enough to show that <m>\NS(T)</m> is trivial
            (since <m>\dim P_n=\dim\R^{n+1}=n+1</m>).
          </p>
          <p>
            Suppose <m>T(p)=\boldzero</m>.
            Then <m>p(c_1)=p(c_2)=\cdots =p(c_{n+1})=0</m>.
            But then the polynomial <m>p(x)</m> has <m>n+1</m> distinct roots.
            But a nonzero polynomial of degree <m>r\leq n</m> can have at most <m>n</m> distinct roots.
            It follows that <m>p(x)</m> must be the zero polynomial,
            and hence that <m>\NS(T)</m> is trivial.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Fix any <m>c\in\R</m> and define
        <m>T\colon F((-\infty, \infty))\rightarrow F((-\infty, \infty))</m> as <m>T(f)=g</m>,
        where <m>g(x)=f(x+c)</m>.
        (Thus <m>T</m> is the
        <q>shift by <m>c</m></q>
        operator on function space.)
        <ol>
          <li>
            <p>
              Show that <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Prove <m>T</m> is invertible by providing its inverse.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) Easy.
        </p>
        <p>
          (b) The inverse is defined as
          <m>S(g)=h(x)</m>, where <m>h(x)=g(x-c)</m>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Show that a matrix transformation <m>T_A\colon\R^n\rightarrow \R^n</m> is invertible
        (as a linear transformation)
        if and only if <m>A</m> is invertible
        (as a matrix).
        Thus invertibility in the matrix sense is the same thing as invertibility in the linear transformation sense.
        <solution>
          <p>
            Observe that <m>\NS(T_A)=\NS(A)</m>.
            Then by our various invertibility theorems
            <md>
              <mrow>T_A \text{ invertible } \amp \Leftrightarrow \NS(T_A)=\{\boldzero\} \amp \text{ (invertibility theorem for linear trans.) }</mrow>
              <mrow>\amp \Leftrightarrow \NS(A)=\{\boldzero\}</mrow>
              <mrow>\amp \Leftrightarrow A \text{ invertible }  \amp \text{ (IV Theorem for matrices) }</mrow>
            </md>
          </p>
          <p>
            Note: we can further say that the inverse of <m>T_A</m> in this case is <m>T_{A^{-1}}</m>.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Show that if <m>T\colon V\rightarrow W</m> is an isomorphism,
        and <m>B=\{\boldv_1,\boldv_2,\dots,\boldv_n\}\subseteq V</m> is a basis for <m>V</m>,
        then <m>T(B)\subseteq W</m> is a basis for <m>W</m>.
        <solution>
          <p>
            The notation <m>T(B)</m> denotes the set <m>B'=\{T(\boldv_1),T(\boldv_2),\dots,T(\boldv_n)\}</m>.
            To show <m>B'</m> is a basis,
            we must show the vectors <m>\boldw_i=T(\boldv_i)</m> both span <m>W</m> and are linearly independent.
          </p>
          <p>
            We can make one shortcut.
            Since <m>\#B=n</m> we have <m>\dim(V)=n</m>.
            Since <m>T</m> is an <em>isomorphism</em> and <m>\dim(V)=n</m>,
            we must also have <m>\dim(W)=n</m>.
            Lastly, since <m>\# B'=n=\dim(W)</m> we need only show either that <m>B'</m> spans <em>or</em>
            that it is linearly independent.
          </p>
          <p>
            I will show that <m>T(B)</m> spans <m>W</m>.
            To this end take any <m>\boldw\in W</m>.
            We must show that we can write <m>\boldw</m> as a linear combination of the <m>\boldw_i</m>.
          </p>
          <p>
            Since <m>T</m> is invertible, it is onto.
            Thus there is a <m>\boldv\in V</m> such that <m>T(\boldv)=\boldw</m>.
            Since <m>B</m> is a basis of <m>V</m>,
            we can write <m>\boldv=a_1\boldv_1+a_2\boldv_2+\cdots +a_n\boldv_n</m>.
            But then we have
            <md>
              <mrow>\boldw\amp =T(\boldv)</mrow>
              <mrow>\amp =T(a_1\boldv_1+a_2\boldb_2+\cdots +a_n\boldv_n)</mrow>
              <mrow>\amp =a_1T(\boldv_1)+a_2T(\boldb_2)+\cdots +a_nT(\boldv_n) \amp \text{ (since \(T\) is linear) }</mrow>
              <mrow>\amp =a_1\boldw_1+a_2\boldw_2+\cdots +a_n\boldw_n \amp \text{ (by def. \(\boldw_i=T(\boldv_i)\) } </mrow>
            </md>,
            and this shows <m>w</m> is a linear combination of the <m>\boldw_i</m>,
            as desired.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{5.3: matrix representations}
  </p>
  <ol>
    <li>
      <p>
        Consider the differential equation
        <me>
          f''(x)+f'(x)=x^2+x+1 \tag{\(*\)}
        </me>
        The theory of ODE's tells us that there are infinitely many solutions <m>f(x)</m> to <m>(*)</m>.
        We will use linear algebra to find infinitely many polynomial solutions.
      </p>
      <ol>
        <li>
          <p>
            First define <m>T\colon P_{3}\rightarrow P_{2}</m> as <m>T(f)=f''(x)+f'(x)</m>.
            Explain why <m>T(f)</m> indeed lies in <m>P_2</m> and show that <m>T</m> is linear.
          </p>
        </li>
        <li>
          <p>
            Compute <m>A=[T]_{B}^{B'}</m> where <m>B</m> and <m>B'</m> are the standard bases of <m>P_{3}</m> and <m>P_{2}</m>,
            respectively.
          </p>
        </li>
        <li>
          <p>
            Determine <m>\NS(A)</m> and <m>\CS(A)</m>.
          </p>
        </li>
        <li>
          <p>
            Find all solutions in <m>P_3</m> to the differential equation <m>(*)</m>. (First solve the relevant matrix equation involving <m>A</m>, then
            <q>lift up</q>
            to <m>P_3</m>. )
          </p>
        </li>
      </ol>
      { Note: though we have found all
      <em>polynomial solutions</em>
      to the differential equation <m>(*)</m>,
      we have not found <em>all</em> solutions.
      Observe that <m>g(x)=e^{-x}</m> is a solution to the corresponding homogeneous equation
      <m>f''+f'=0</m>, It follows from basic ODE theory that we can add <m>ce^{-x}</m> to any of our solutions in (d) to get a new solution.
      Our analysis didn't catch these solutions as we restricted our attention to <m>P_3</m>. }
      <solution>
        <p>
          (a) The usual proof shows <m>T</m> is linear.
          Since taking the derivative knocks down the degree of a polynomial by 1, it's clear that if <m>p(x)</m> has degree at most <m>3</m>,
          then <m>T(p)=p''+p'</m> will have degree at most <m>2</m>.
        </p>
        <p>
          (b) Let <m>B=\{1,x,x^2,x^3\}</m> and <m>B'=\{1, x, x^2\}</m>.
          The usual recipe for <m>[T]_{B',B}</m> begins by computing
          <me>
            T(1)=0, T(x)=1, T(x^2)=2+2x, T(x^3)=6x+3x^2
          </me>.
        </p>
        <p>
          Taking coordinate vectors with respect to <m>B</m> and placing these in columns yields
          <me>
            A=\begin{bmatrix}0\amp 1\amp 2\amp 0\\ 0\amp 0\amp 2\amp 6\\ 0\amp 0\amp 0\amp 3 \end{bmatrix}
          </me>.
        </p>
        <p>
          (c) The rank of <m>A</m> is clearly 3
          (look at rows),
          and by inspection we see that
          <m>\{(1,0,0), (2,2,0), (0,6,3)\}</m> is a basis for <m>\CS(A)</m>.
          Since <m>\dim\CS(A)=3</m> and <m>\CS(A)\subset\R^3</m>,
          it follows from the Subspace Dimension Theorem that <m>\CS(A)=\R^3</m>.
        </p>
        <p>
          Since <m>\rank(A)=3</m>, it follows from R-N that <m>\nullity(A)=4-3=1</m>.
          To come up with a basis for <m>\NS(A)</m> we simply need to find one nonzero element of <m>\NS(A)</m>.
          We see <m>\{(1,0,0)\}</m> does the trick.
        </p>
        <p>
          What does all this mean about <m>T</m>?
          Since <m>\CS(A)=\R^3</m>, it follows that <m>\range(T)=P_2</m>.
          Thus for <em>any</em> <m>q(x)\in P_2</m>,
          we can find a <m>p(x)\in P_3</m> with <m>T(p)=p''(x)+p'(x)=q(x)</m>.
          This means not only can we solve the given differential equation,
          we can solve any similar equation of the form <m>f''+f'=q</m>,
          where <m>q</m> is a polynomial in <m>P_2</m>.
        </p>
        <p>
          Furthermore, as we will see,
          the fact that <m>\NS(A)</m> is nontrivial will mean that we can always find infinitely many solutions to <m>(*)</m>.
        </p>
        <p>
          (d) The polynomial <m>x^2+x+1</m> corresponds via
          <m>[\hspace{8pt}]_{B'}</m> to the vector <m>\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix}</m>.
          All solutions <m>\boldx</m> to
          <me>
            A\boldx=\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix}
          </me>
          will lift up to the solutions <m>p</m> to <m>T(p)=x^2+x+1</m>.
        </p>
        <p>
          Using GE, we see the general solution to the matrix equation is
          <me>
            \boldx=c\begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} +\begin{bmatrix}1\\-\frac{1}{2}\\ \frac{1}{3} \end{bmatrix}
          </me>
        </p>
        <p>
          Thus the general <em>polynomial</em>
          solution to the original differential equation is
          <me>
            p(x)=c+(1-\frac{1}{2}x^2+\frac{1}{3}x^3)=(c+1)-\frac{1}{2}x^2+\frac{1}{3}x^3
          </me>.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Define <m>T\colon M_{22}\rightarrow M_{22}</m> by <m>T(A)=A^T-A</m>.
        <ol>
          <li>
            <p>
              Compute <m>A=[T]_B</m> where <m>B</m> is the standard basis for <m>M_{22}</m>.
            </p>
          </li>
          <li>
            <p>
              Compute bases for <m>\NS(A)</m> and <m>\CS(A)</m>.
            </p>
          </li>
          <li>
            <p>
              Use your result in (b) to give bases for <m>\NS(T)</m> and <m>\range(T)</m>.
            </p>
          </li>
          <li>
            <p>
              Identify <m>\NS(T)</m> and
              <m>\range(T)</m> as familiar subspaces of matrices.
            </p>
          </li>
        </ol>
      </p>
      <solution>
        <p>
          (a) The usual recipe yields
          <me>
            A=\begin{bmatrix}0\amp 0\amp 0\amp 0\\ 0\amp -1\amp 1\amp 0\\ 0\amp 1\amp -1\amp 0\\ 0\amp 0\amp 0\amp 0 \end{bmatrix}
          </me>.
        </p>
        <p>
          We have <m>\NS(A)=\Span(\{(1,0,0,0), (0,0,0,1), (0,1,1,0)\}</m> and <m>\CS(A)=\Span(\{(0,-1,1,0)\}</m>.
        </p>
        <p>
          By lifting we get that
          <me>
            \NS(T)=\Span\left(\left\{\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} , \begin{bmatrix}0\amp 0\\ 0\amp 1 \end{bmatrix} , \begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix} \right\}\right), \ \range(T)=\Span\left(\left\{\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} \right\}\right)
          </me>.
        </p>
        <p>
          From these descriptions it follows easily that <m>\NS(T)</m> is the subspace of all symmetric matrices,
          and <m>\range(T)</m> is the subspace of all skew-symmetric matrices.
        </p>
      </solution>
    </li>
    <li>
      <p>
        Let <m>T_{\alpha}\colon\R^2\rightarrow\R^2</m> be reflection through the line <m>\ell_{\alpha}</m>,
        as defined previously in <xref ref="Ex_refl">Exercise</xref> in Section 5.1-5.2.
        Recall also that <m>T_{\alpha}</m> is an origin-fixing isometry,
        hence a linear transformation.
        Compute the matrix <m>A</m> such that <m>T_{\alpha}=T_A</m> as follows:
        first compute <m>A'=[T_{\alpha}]_{B'}</m> where
        <m>B'=\{\boldv_1,\boldv_2\}</m> is a basis for <m>\R^2</m> satisfying <m>\boldv_1\in \ell_\alpha</m>,
        <m>\boldv_2\in \ell_\alpha^\perp</m>;
        then compute <m>A=[T_{\alpha}]_B</m> from <m>A'</m> using the change of basis formula for transformations.
        Note: your vectors <m>\boldv_1, \boldv_2</m> will be expressed in terms of <m>\alpha</m>
        (and some trig functions).
        Check that you get the same answer from before!
        <solution>
          <p>
            Since <m>B</m> is the standard basis,
            it turns out that The matrix <m>A=[T]_B</m> is just the matrix that defines the given linear transformation.
            We computed this in an earlier exercise.
            We got
            <me>
              A=\begin{bmatrix}\cos(2\alpha)\amp \sin(2\alpha)\\ \sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix} =\begin{bmatrix}-1/2\amp \sqrt{3}/2\\ \sqrt{3}/2\amp 1/2 \end{bmatrix}
            </me>.
          </p>
          <p>
            Given a basis as specified,
            the matrix <m>A'=[T]_{B'}</m> is simpler because the action of <m>T</m> on this basis is simpler:
            <m>T(\boldv_1)=\boldv_1</m> and <m>T(\boldv_2)=-\boldv_2</m>
            (draw a picture!).
            Thus
            <me>
              A'=\begin{bmatrix}\vert\amp \vert\\ [T(\boldv_1)]_{B'}\amp [T(\boldv_2)]_{B'}\\ \vert\amp \vert \end{bmatrix} =\begin{bmatrix}\vert\amp \vert\\ [\boldv_1]_{B'}\amp [-\boldv_2]_{B'}\\ \vert\amp \vert \end{bmatrix} = \begin{bmatrix}1\amp 0\\ 0\amp -1 \end{bmatrix}
            </me>.
          </p>
          <p>
            Let's actually pin down a basis <m>B'</m> as specified.
            The basis elements are necessarily orthogonal
            (from the description);
            I will make them orthonormal to make my life as easy as possible!
            The vector <m>\boldv_1</m> points along <m>\ell_{\alpha}</m>:
            a unit vector fitting this description is <m>\boldv_1=(\cos(\alpha), \sin(\alpha))</m>.
            The vector <m>\boldv_2</m> should be a unit vector orthogonal to
            <m>\boldv_1</m>: I pick <m>\boldv_2=(-\sin(\alpha), \cos(\alpha))</m>.
          </p>
          <p>
            We need to compute change of bases matrices between <m>B'</m> and the standard basis <m>B</m>.
            Again we find ourselves in the special setting where
            <m>\underset{B'\rightarrow B}{P}</m> is built by simply putting the elements of <m>B'</m> as its columns.
            Thus we have
            <md>
              <mrow>\underset{B'\rightarrow B}{P}\amp =\begin{bmatrix} [rr] \cos(\alpha) \amp -\sin(\alpha)</mrow>
              <mrow>\sin(\alpha)\amp \cos(\alpha) \end{bmatrix}</mrow>
              <mrow>\underset{B\rightarrow B'}{P}\amp =(\underset{B'\rightarrow B}{P})^{-1}</mrow>
              <mrow>\amp =(\underset{B'\rightarrow B}{P})^{T} \amp \text{(since columns of \(\underset{B'\rightarrow B}{P}\) are orthonormal!}</mrow>
              <mrow>\amp =\begin{bmatrix} [rr] \cos(\alpha) \amp \sin(\alpha)</mrow>
              <mrow>-\sin(\alpha)\amp \cos(\alpha) \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Lastly, we compute
            <md>
              <mrow>A=[T]_B\amp =(\underset{B\rightarrow B'}{P})^{-1}[T]_{B'}\underset{B\rightarrow B'}{P}</mrow>
              <mrow>\amp =\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}</mrow>
              <mrow>\amp =\begin{bmatrix} [rr] \cos(\alpha) \amp -\sin(\alpha)</mrow>
              <mrow>\sin(\alpha)\amp \cos(\alpha) \end{bmatrix} \begin{bmatrix}[rr] 1\amp 0</mrow>
              <mrow>0\amp -1 \end{bmatrix} \begin{bmatrix} [rr] \cos(\alpha) \amp \sin(\alpha)</mrow>
              <mrow>-\sin(\alpha)\amp \cos(\alpha) \end{bmatrix}</mrow>
              <mrow>\amp = \begin{bmatrix} \cos^2(\alpha)-\sin^2(\alpha) \amp  2\sin(\alpha)\cos(\alpha)</mrow>
              <mrow>2\sin(\alpha)\cos(\alpha)\amp \sin^2(\alpha)-\cos^2(\alpha) \end{bmatrix}</mrow>
              <mrow>\amp =\begin{bmatrix}[rr] \cos(2\alpha)\amp \sin(2\alpha)</mrow>
              <mrow>\sin(2\alpha)\amp -\cos(2\alpha) \end{bmatrix}. \ \checkmark</mrow>
            </md>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T:P_2\rightarrow P_3</m> be the linear transformation defined by <m>T(p)=x\cdot p(x-3)</m>.
        Find <m>[T]_{B}^{B'}</m> relative to the bases
        <m>B=\{1,x,x^2\}</m> and <m>B' = \{1,x,x^2,x^3\}</m>.
        <solution>
          <p>
            By the formula for <m>T</m>
            <md>
              <mrow>T(1) \amp =\amp  x(1) = x</mrow>
              <mrow>T(x) \amp =\amp  x(x-3) = x^2 - 3x</mrow>
              <mrow>T(x^2) \amp =\amp  x^3-6x^2+9x</mrow>
            </md>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B'</m>,
            we can write
            <me>
              [T]_{B}^{B'} = \begin{bmatrix}0\amp 0\amp 0\\ 1\amp -3\amp 9\\ 0\amp 1\amp -6\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T:P_2 \rightarrow M_{22}</m> be the linear transformation defined by
        <me>
          T(p) = \begin{bmatrix}p(0)\amp p(1)\\ p(-1)\amp p(0) \end{bmatrix}
        </me>
        let <m>B</m> be the standard basis for <m>M_{22}</m>,
        let <m>B'=\{1,x,x^2\}</m> and
        <m>B''=\{1,1+x,1+x^2\}</m> be bases for <m>P_2</m>.
        Find <m>[T]_{B'}^{B}</m> and <m>[T]_{B''}^{B}</m>.
        <solution>
          <p>
            Lets work on <m>[T]_{B}^{B'}</m> first.
            By the formula for <m>T</m>
            <md>
              <mrow>T(1) \amp =\amp \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</mrow>
              <mrow>T(x) \amp =\amp \begin{bmatrix}0\amp 1\\ -1\amp 0 \end{bmatrix}</mrow>
              <mrow>T(x^2) \amp =\amp \begin{bmatrix}0\amp 1\\ 1\amp 0 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B</m>, we can write
            <me>
              [T]_{B}^{B'} = \begin{bmatrix}1\amp 0\amp 0\\ 1\amp 1\amp 1\\ 1\amp -1\amp 1\\ 1\amp 0\amp 0 \end{bmatrix}
            </me>
          </p>
          <p>
            Now for <m>[T]_{B}^{B''}</m>.
            By the formula for <m>T</m>
            <md>
              <mrow>T(1) \amp =\amp \begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</mrow>
              <mrow>T(1+x) \amp =\amp \begin{bmatrix}1\amp 2\\ 0\amp 1 \end{bmatrix}</mrow>
              <mrow>T(1+x^2) \amp =\amp \begin{bmatrix}1\amp 2\\ 2\amp 1 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B</m>, we can write
            <me>
              [T]_{B}^{B''} = \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 2\amp 2\\ 1\amp 0\amp 2\\ 1\amp 1\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Let <m>T:M_{22} \rightarrow R^2</m> be the linear transformation given by
        <me>
          T\left( \begin{bmatrix}a\amp b\\ c\amp d \end{bmatrix} \right) = \begin{bmatrix}a+b+c\\ d \end{bmatrix}
        </me>
        and let <m>B</m> be the standard basis for <m>M_{22}</m>,
        <m>B'</m> be the standard basis for <m>R^2</m> and let
        <me>
          B'' = \left\{ \begin{bmatrix}1\\ 1 \end{bmatrix} , \begin{bmatrix}-1\\ 0 \end{bmatrix} \right\}
        </me>
        be another basis for <m>R^2</m>.
        Find <m>[T]_{B}^{B'}</m> and <m>[T]_{B}^{B''}</m>.
        <solution>
          <p>
            By the formula for <m>T</m>,
            we can find <m>T(\boldu_i)</m> where
            <m>\boldu_i</m> are the vectors in the basis <m>B</m>.
            <md>
              <mrow>T(\boldu_1) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_2) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_3) \amp =\amp \begin{bmatrix}1\\ 0 \end{bmatrix}</mrow>
              <mrow>T(\boldu_4) \amp =\amp \begin{bmatrix}0\\ 1 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B'</m>,
            we can write
            <me>
              [T]_{B}^{B'} = \begin{bmatrix}1\amp 1\amp 1\amp 0\\ 0\amp 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Taking these vectors relative the the basis <m>B''</m>,
            we can write
            <me>
              [T]_{B}^{B''} = \begin{bmatrix}0\amp 0\amp 0\amp 1\\ -1\amp -1\amp -1\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Define <m>T\colon \R^3\rightarrow\R^3</m> as
        <me>
          T(x_1,x_2,x_3) = (x_1+2x_2-x_3,-x_2,x_1+7x_3)
        </me>
        Let <m>B</m> be the standard basis of <m>\R^3</m>,
        and let <m>B'</m> be the nonstandard basis
        <me>
          B' = \{(1,0,0),(1,1,0),(1,1,1)\}
        </me>.
        First compute <m>[T]_B</m>,
        then compute <m>[T]_{B'}</m> using the change of basis formula for transformations.
        <solution>
          <p>
            Start by using <m>T</m> on each vector in the standard basis
            <md>
              <mrow>T((1,0,0)) \amp =\amp  (1,0,1)</mrow>
              <mrow>T((0,1,0)) \amp =\amp  (2,-1,0)</mrow>
              <mrow>T((0,0,1)) \amp =\amp  (-1,0,7)</mrow>
            </md>
          </p>
          <p>
            Then we can take these as the columns of <m>[T]_B</m>.
            <me>
              [T]_B = \begin{bmatrix}1\amp 2\amp -1\\ 0\amp -1\amp 0\\ 1\amp 0\amp 7 \end{bmatrix}
            </me>
          </p>
          <p>
            According to the change of basis formula,
            we have <m>[T]_{B'} = P_{B\rightarrow B'}[T]_BP_{B'\rightarrow B}</m>.
            Recall that in this very specific context (<m>V=\R^n</m>,
            <m>B</m> the standard basis) the change of basis matrix
            <m>\underset{B'\rightarrow B}{P}</m> is obtained by simply placing the elements of <m>B'</m> as the columns of a vector:
            <me>
              P_{B'\rightarrow B} = \begin{bmatrix}1\amp 1\amp 1\\ 0\amp 1\amp 1\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Then we have
            <me>
              \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}= \begin{bmatrix}1\amp -1\amp 0\\ 0\amp 1\amp -1\\ 0\amp 0\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Now we can use the change of basis formula to compute
            <me>
              [T]_{B'} = \begin{bmatrix}1\amp -1\amp 0\\ 0\amp 1\amp -1\\ 0\amp 0\amp 1 \end{bmatrix} \begin{bmatrix}1\amp 2\amp -1\\ 0\amp -1\amp 0\\ 1\amp 0\amp 7 \end{bmatrix} \begin{bmatrix}1\amp 1\amp 1\\ 0\amp 1\amp 1\\ 0\amp 0\amp 1 \end{bmatrix}  = \begin{bmatrix}1\amp 4\amp 3\\ -1\amp -2\amp -9\\ 1\amp 1\amp 8 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        Define <m>T\colon P_1\rightarrow P_1</m> as
        <me>
          T(a_0 +a_1x) = a_0 +a_1(x+1) = (a_0 +a_1) +a_1x
        </me>.
        The following are two different bases for <m>P_1</m>:
        <m>B = \{6+3x, 10+2x\}</m>; <m>B' = \{2,3+2x\}</m>.
        First compute <m>[T]_B</m>,
        then use the change of basis formula for transformations to compute <m>[T]_{B'}</m>.
        <solution>
          <p>
            In a similar fashion to the last problem
            <md>
              <mrow>_B \amp =\amp  [9+3x]_B=(2/3, 1/2)</mrow>
              <mrow>\left[T(10+2x)\right]_B \amp =\amp  [12 +2x]_B=(-2/9,4/3)</mrow>
            </md>
          </p>
          <p>
            Thus
            <me>
              [T]_B = \begin{bmatrix}2/3\amp -2/9\\ 1/2\amp 4/3 \end{bmatrix}
            </me>
          </p>
          <p>
            We compute <m>P_{B\rightarrow B'}</m> using the recipe
            <me>
              P_{B\rightarrow B'} =\begin{bmatrix}\vert\amp \vert \\ [6+3x]_{B'}\amp [10+2x]_{B'}\\ \vert\amp \vert \end{bmatrix} = \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix}
            </me>
          </p>
          <p>
            Then
            <me>
              P_{B'\rightarrow B}=(P_{B\rightarrow B'})^{-1}= \begin{bmatrix}-2/9\amp 7/9\\ 1/3\amp -1/6 \end{bmatrix}
            </me>
          </p>
          <p>
            Then we have
            <me>
              [T]_{B'} = \begin{bmatrix}3/4\amp 7/2\\ 3/2\amp 1 \end{bmatrix} \begin{bmatrix}2/3\amp -2/9\\ 1/2\amp 4/3 \end{bmatrix} \begin{bmatrix}-2/9\amp 7/9\\ 1/3\amp -1/6 \end{bmatrix} = \begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}
            </me>
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        <em>Reflection through a plane in <m>\R^3</m></em>.
        Suppose <m>W</m> is the plane passing through the origin with normal vector <m>\boldn=(a,b,c)</m>:
        i.e., <m>W: ax+by+cz=0</m>.
        Reflection through <m>W</m> is the map defined rigorously as follows:
        <blockquote>
          given <m>\boldv=\overrightarrow{OP}\in\R^3</m> define <m>T(\boldv)=\overrightarrow{OQ}</m>, where <m>Q</m> is the unique point satisfying (i) the line segment <m>\overline{PQ}</m> is perpendicular to <m>W</m>, and (ii) <m>d(P,Q)=2\cdot d(P,W)</m>.
        </blockquote>
        Intuitively, <m>Q</m> is the point
        <q>on the other side of the plane</q>
        from <m>P</m> and an equal distance away.
        It is clear that reflection is an isometry mapping <m>\boldzero</m> to <m>\boldzero</m>,
        hence a linear transformation.
        We will compute the matrix <m>A</m> such that <m>T=T_A</m> in two different ways below.   (a) <em>Method 1</em>.
        First compute <m>A'=[T]_{B'}</m>,
        where <m>B'=\{\boldv_1,\boldv_2,\boldv_3\}</m> is a basis for <m>\R^3</m> satisfying
        <m>\boldv_1, \boldv_2\in W</m>, <m>\boldv_3=(a,b,c)</m>;
        then use the change of basis formula for transformations to compute <m>A=[T]_B</m>,
        where <m>B</m> is the standard basis.
        Note: you have to give explicit vectors <m>\boldv_1, \boldv_2</m> yourself;
        they will be expressed in terms of <m>a,b,c</m>.
        Feel free to use technology
        (or a determinant formula for inverses)
        to compute your change of basis matrices.   (b) <em>Method 2</em>.
        Let <m>\ell=\Span(\{\boldn\})</m>.
        Show geometrically that <m>T(\boldv)=\boldv-2\proj{\boldv}{\ell}</m>,
        and express this as a single matrix formula.
        (You may use the matrix formula for
        <m>\text{ proj } _{\ell}</m> derived in the notes.)
        Check that you get the same matrix with both methods!
        <solution>
          <p>
            <em>Method 2</em> ends up being the computationally less heavy one,
            so we begin with it.
            It is intuitively clear why the formula
            <me>
              \boldv-2\proj{\boldv}{\ell}
            </me>
            computes the reflection of <m>\boldv</m> through <m>W</m>;
            to get the projection of <m>\boldn</m> onto <m>W</m>,
            we subtract <m>\proj{\boldv}{\ell}</m> from <m>\boldv</m>;
            subtracting this again brings us to the reflection of <m>\boldv</m> on the other side of <m>W</m>.
            Alternatively,
            set <m>S(\boldv)=\boldv-\proj{\boldv}{\ell}=(I-2A_\ell)\boldv</m>,
            where <m>A_\ell</m> is the projection onto <m>\ell</m> matrix.
            Since <m>S</m> is clearly a linear transformation,
            to show <m>S=T</m> we need only show they agree on a basis for <m>\R^3</m>.
            Clearly <m>S(\boldw)=\boldw</m> for all <m>\boldw\in W</m>,
            and <m>S(\boldn)=-\boldn</m>.
            From this it follows that <m>S</m> and <m>T</m> agree on the basis <m>B'</m> of the form described in Method 1.
            Thus <m>S=T</m>.
          </p>
          <p>
            Recall that projection onto <m>\ell</m> is given by the matrix
            <me>
              A_\ell=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix}
            </me>
          </p>
          <p>
            Thus
            <me>
              T=I-2A_{\ell}= \frac{1}{a^2+b^2+c^2}\begin{bmatrix}-a^2+b^2+c^2\amp -2ab\amp -2ac\\ -2ab\amp a^2-b^2+c^2\amp -2bc\\ -2ac\amp -2bc\amp a^2+b^2-c^2 \end{bmatrix}
            </me>
          </p>
          <p>
            As for <em>Method 1</em>, if we have a basis <m>B'</m> as described,
            then <m>[T]_{B'}=\begin{bmatrix}1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp -1 \end{bmatrix}</m>,
            since <m>T</m> clearly fixes any vector in <m>W</m> and maps <m>\boldn</m> to <m>-\boldn</m>.
            Once we have found the basis <m>B'</m>,
            letting <m>B</m> be the standard basis, we have
            <me>
              A=[T]_B=\underset{B'\rightarrow B}{P}[T]_{B'}\underset{B\rightarrow B'}{P}
            </me>.
          </p>
          <p>
            Furthermore, we know
            <me>
              \underset{B'\rightarrow B}{P}=\begin{bmatrix}\vert\amp \vert\amp \vert\\ \boldv_1\amp \boldv_2\amp \boldv_3\\ \vert\amp \vert\amp \vert \end{bmatrix} , \text{ and }  \underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}
            </me>.
          </p>
          <p>
            So we are essentially done,
            except for some slightly unpleasant matrix arithmetic,
            after we find the basis <m>B'</m>.
            Let's do this now.
          </p>
          <p>
            To make our life easier, I assume that it is not the case that all but one of <m>a,b,c</m> are zero.
            This excluded case would just be reflection through one of the coordinate planes,
            which is very easy to compute.
            Furthermore,
            the matrix formula we derive is valid even in these excluded cases,
            as you can check above!
          </p>
          <p>
            So assume it is not the case that all but one of <m>a,b,c</m> are zero.
            Then <m>\boldv_1=(b,-a,0)</m> and
            <m>\boldv_2=\boldn\times\boldv_1=(ac,bc,-a^2-b^2)</m> comprise an <em>orthogonal</em>
            basis for <m>W</m>. (Note that our assumption about <m>a,b,c</m> ensures <m>\boldv_1\ne 0</m>,
            and hence also that <m>\boldv_2\ne 0</m>.
            Setting <m>\boldv_3=\boldn</m>, we then have
            <me>
              \underset{B'\rightarrow B}{P}=\begin{bmatrix}b\amp ac\amp a\\ -a\amp bc\amp b\\ 0\amp -a^2-b^2\amp c \end{bmatrix}
            </me>
            and hence
            <md>
              <mrow>A\amp =\begin{bmatrix} b\amp ac\amp a</mrow>
              <mrow>-a\amp bc\amp b</mrow>
              <mrow>0\amp a^2+b^2\amp c \end{bmatrix} \begin{bmatrix} 1\amp 0\amp 0</mrow>
              <mrow>0\amp 1\amp 0</mrow>
              <mrow>0\amp 0\amp -1 \end{bmatrix} \left(\begin{bmatrix} b\amp ac\amp a</mrow>
              <mrow>-a\amp bc\amp b</mrow>
              <mrow>0\amp -a^2-b^2\amp c \end{bmatrix} \right)^{-1}</mrow>
              <mrow>\amp =\frac{1}{a^2+b^2+c^2}\begin{bmatrix} -a^2+b^2+c^2\amp -2ab\amp -2ac</mrow>
              <mrow>-2ab\amp a^2-b^2+c^2\amp -2bc</mrow>
              <mrow>-2ac\amp -2bc\amp a^2+b^2-c^2 \end{bmatrix}</mrow>
            </md>
          </p>
          <p>
            Admittedly, I used technology to compute the inverse and the matrix multiplication in the last line.
          </p>
        </solution>
      </p>
    </li>
    <li>
      <p>
        <em>General rotation in <m>\R^3</m></em>.
        Suppose <m>W</m> is the plane passing through the origin perpendicular to a given <em>unit</em>
        normal vector <m>\boldn=(a,b,c)</m>:
        i.e., <m>W: ax+by+cz=0</m> and <m>\norm{\boldn}=1</m>.
        Fix an angle <m>\theta</m>.
        We define <m>T</m> to be rotation about <m>\boldn</m> by <m>\theta</m>,
        where the positive rotational direction is taken to be counterclockwise with respect to <m>\boldn</m>.
        In more detail: given <m>\boldv</m>,
        to compute <m>T(\boldv)</m> you first write <m>\boldv=\boldw+\boldw^\perp</m>,
        where <m>\boldw=\proj{\boldv}{W}</m> and
        <m>\boldw^\perp=\boldv-\boldw</m> is the  component of <m>\boldv</m> orthogonal to <m>W</m>.
        Then define <m>T(\boldv)=\boldw'+\boldw^\perp</m>,
        where <m>\boldw'</m> is the result of rotating <m>\boldw</m> by <m>\theta</m> in the plane <m>W</m>.
        Thus <m>T</m> rotates the projection of <m>\boldv</m> onto <m>W</m> by <m>\theta</m> and leaves the orthogonal complement <m>\boldw^\perp</m> untouched.
        Here is the picture:
        <me>
          <image width="37%" source="images/GeneralRotation.png"/>
        </me>
        Since rotation is an isometry that fixes the origin, <m>T</m> is linear.
        As such we wish to come up with the matrix <m>A</m> such that <m>T=T_A</m>.
        Below I outline two different methods.   (a) <em>Method 1</em>.
        Create a basis <m>B'</m> of the form <m>\ds B'=\{ \boldw_1,\boldw_2,\boldn \}</m>,
        where <m>\boldw_1, \boldw_2</m> are two orthonormal vectors in <m>W</m>.
        Compute <m>[T]_{B'}</m> then convert everything to the standard basis <m>B</m> using the change of basis formula for transformations.
        The basis <m>B'</m> being orthonormal simplifies computations.   (b)
        <em>Method 2</em>. (Requires the cross product) Given <m>\boldv=(x,y,z)</m>,
        describe <m>T(\boldv)</m> in terms of the the three mutually orthogonal vectors  <m>\boldn</m>,
        <m>\boldw=\proj{\boldv}{W}</m>,
        and <m>\ds\boldw'=\boldn\times\proj{\boldv}{W}</m>.
        After some work you can derive the formula
        <me>
          T(\boldv)=\cos(\theta)\boldv+\sin(\theta)(\boldn\times\boldv)+((1-\cos \theta)\boldv\cdot\boldn)\boldn
        </me>.
        Writing <m>\boldn=(a,b,c)</m> and evaluating this formula for <m>T</m> at
        <m>\boldv=(1,0,0), (0,1,0)</m> and <m>(0,0,1)</m> will give us the three columns of our matrix,
        expressed in terms of <m>a, b, c</m> and <m>\theta</m>.
        <solution>
          <p>
            I'll let you think about this one some more.
          </p>
        </solution>
      </p>
    </li>
  </ol>
  <p>
    \chapter*{6.1: eigenvectors} \chapter*{6.2: diagonalization}
  </p>
</section>
