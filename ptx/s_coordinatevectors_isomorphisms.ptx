<section xml:id="s_coordinatevectors_isomorphisms">
  <title>Coordinate vectors and isomorphisms</title>
  <introduction>

  </introduction>
  <paragraphs xml:id="ss_coordinate_vectors">
    <title>Coordinate vectors</title>
    <p>
      Before we can define coordinate vectors we need to define an <em>ordered basis</em>. As the name suggests this is nothing more than a basis along with a particular choice of ordering of its elements: <ie /> first element, second element, <etc />. In other words, an ordered basis will be a <em>sequence</em> of vectors, as opposed to a  <em>set</em> of vectors.
    </p>
    <definition xml:id="d_ordered_basis">
      <title>Ordered bases</title>
      <idx><h>ordered basis</h></idx>
      <statement>
        <p>
        Let <m>V</m> be a finite-dimensional vector space. An <term>ordered basis</term> of <m>V</m> is a sequence of vectors <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> whose underlying set <m>\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> is a basis of <m>V</m>.
        </p>
        <p>
        Similarly, an ordered basis is <term>orthogonal</term> (resp. <term>orthonormal</term>) if its underlying set is orthogonal (resp. orthonormal).
        </p>
      </statement>
    </definition>
        <remark >
      <statement>
        <p>
        A single (unordered) basis <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}</m> of a vector space gives rise to <m>n!</m> different ordered bases: you have <m>n</m> choices for the first element of the ordered basis, <m>(n-1)</m> choices for the second element, <etc />.
        </p>
        <p>
          For example the standard basis <m>B=\{\bolde_1, \bolde_2, \bolde_3\}</m> of <m>\R^3</m> gives rise to <m>3!=6</m> different ordered bases of <m>\R^3</m>:
          <md>
            <mrow>B_1\amp =(\bolde_1, \bolde_2, \bolde_3) \amp B_2\amp =(\bolde_1, \bolde_3, \bolde_2) </mrow>
            <mrow> B_3\amp=(\bolde_2, \bolde_1, \bolde_3) \amp B_4\amp =(\bolde_2, \bolde_3, \bolde_1) </mrow>
            <mrow> B_5 \amp =(\bolde_3, \bolde_1, \bolde_2) \amp B_6\amp =(\bolde_3, \bolde_2, \bolde_1)</mrow>
          </md>.
        </p>
        <p>
        By a slight abuse of language we will use <sq>standard basis</sq> to describe both one of our standard unordered bases and the corresponding ordered basis obtained by choosing the implicit ordering of the set descriptions in <xref ref="rm_standard_bases"/>. For example, <m>\{x^2, x, 1\}</m> and <m>(x^2, x, 1)</m> will both be called the standard basis of <m>P_2</m>.
        </p>
      </statement>
    </remark>

    <definition xml:id="d_coordinatevector">
      <title>Coordinate vectors</title>
      <statement>
        <p>
          Let <m>B=(\boldv_1, \boldv_2,\dots , \boldv_n)</m> be an ordered basis for the vector space <m>V</m>. According to <xref ref="th_basis_equivalence"/>, for any <m>\boldv\in V</m> there is a <em>unique</em> choice of scalars <m>c_i\in \R</m> satisfying
          <me>
            \boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_n\boldv_n
          </me>.
          We call the corresponding <m>n</m>-tuple <m>(c_1,c_2,\dots, c_n)</m> the <term>coordinate vector of <m>\boldv</m> relative to the basis <m>B</m></term>, and denote it <m>[\boldv]_B</m>: <ie />,
          <me>
            [\boldv]_B=(c_1,c_2,\dots, c_n)
          </me>.
        </p>
      </statement>
    </definition>
          <p>
            To compute the coordinate vector of an element <m>\boldv\in V</m> relative to a given ordered basis
            <m>B=(\boldv_1,\boldv_2,\dots, \boldv_n)</m> we must <em>find</em> the scalars <m>c_1, c_2, \dots, c_n</m>
            that satisfy the vector equation
            <me>
            \boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n
            </me>.
            Usually this is accomplished by reducing this vector equation to an equivalent system of linear equations in the unknowns <m>c_i</m> and solving using the method of Gaussian elimination. However, there are some cases when we can easily produce the <m>c_i</m> by inspection: for example
            when computing with standard bases as the first example below illustrates.
          </p>
          <p>Furthermore, if the given basis happens to be orthogonal (or orthonormal) with respect to some inner product, <xref ref="th_coordinates_orthogonal"/> provides an inner product formula for computing the <m>c_i</m>.
          </p>
  <example xml:id="ex_coordinatevector_standard">
    <title>Standard bases</title>
    <statement>
      <p>
        Computing coordinate vectors relative to one of our standard bases for <m>\R^n</m>, <m>M_{mn}</m>, or <m>P_{n}</m> amounts to just listing the coefficients or entries used to specify the given vector. The examples below serve to illustrate the general method in this setting.
      </p>
      <ol>
        <li>
          <p>
          Let <m>V=\R^3</m> and <m>B=(\bolde_1, \bolde_2, \bolde_3)</m>. For any <m>\boldv=(a,b,c)\in \R^3</m> we have <m>[\boldv]_{B}=(a,b,c)</m>, since <m>(a,b,c)=a\bolde_1+b\bolde_2+c\bolde_3</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>V=M_{22}</m> and <m>B=(E_{11}, E_{12}, E_{21}, E_{22})</m>. For any <m>A=\begin{amatrix}[rr]a\amp b\\ c\amp d \end{amatrix}</m> we have <m>[A]_B=(a,b,c,d)</m> since
            <me>
              A=aE_{11}+bE_{12}+cE_{21}+dE_{22}
            </me>.
          </p>
        </li>
      </ol>
    </statement>

  </example>
  <example>
    <title>Reorderings of standard bases</title>
    <statement>
      <p>
      If we choose an alternate ordering of one of the standard bases, the entries of the coordinate vector are reordered accordingly, as illustrated by the examples below.
      </p>
      <ol>
        <li>
          <p>
            Let <m>V=\R^3</m> and <m>B=(\bolde_2, \bolde_1, \bolde_3)</m>. Given <m>\boldv=(a,b,c)\in \R^3</m> we have <m>[\boldv]_B=(b,a,c)</m>, since
            <me>
            \boldv=b\bolde_2+a\bolde_1+c\bolde_3
            </me>.
          </p>
        </li>
        <li>
          <p>
            Let <m>P=P_3</m> and <m>B=(1,x,x^2, x^3)</m>. Given <m>p(x)=ax^3+bx^2+cx+d</m> we have <m>[p(x)]_B=(d, c, b, a)</m>, since
            <me>
              p(x)=d(1)+cx+bx^2+ax^3
            </me>.
          </p>
        </li>
      </ol>
    </statement>
  </example>
  <example>
    <title>Nonstandard bases</title>

    <statement>
      <p>
        For a nonstandard ordered basis, we compute coordinate vectors by solving a relevant system of linear equations, as the examples below illustrate.
      </p>
      <ol>
        <li>
          <p>
            Let <m>V=\R^2</m>, <m>B=((1,2),(1,1))</m>, and <m>\boldv=(3,3)</m>. To compute <m>[\boldv]_B</m> we must find the unique pair <m>(c_1, c_2)</m> satisfying
            <me>
              (3,3)=c_1(1,2)+c_2(1,1)
            </me>.
            By inspection, we see that
            <me>
              (3,3)=3(1,1)=0(1,2)+3(1,1)
            </me>.
            We conclude that
            <me>
              [\boldv]_{B}=(0,3)
            </me>.
            More generally, to compute <m>[\boldv]_B</m> for an arbitrary <m>\boldv=(a,b)\in \R^2</m>, we must find the pair <m>(c_1,c_2)</m> satisfting <m>(a,b)=c_1(1,2)+c_2(1,1)</m>, or equivalelenty
            <me>
              \begin{linsys}{2}
                c_1\amp +\amp c_2 \amp =\amp a\\
                2c_1\amp +\amp c_2\amp =\amp b
              \end{linsys}
            </me>.
             The usual techniques yield the unique solution <m>(c_1,c_2)=(-a+b,2a-b)</m>, and thus
             <me>
               [\boldv]_B=(-a+b, 2a-b)
             </me>
             for <m>\boldv=(a,b)</m>.
          </p>
        </li>
        <li>
          <p>
          Let <m>V=P_2</m>, <m>B=(x^2+x+1, x^2-x, x^2-1)</m>, and <m>p(x)=x^2</m>. To compute <m>[p(x)]_B</m> we must find the unique triple <m>(c_1,c_2,c_3)</m> satisfying
          <me>
            x^2=c_1(x^2+x+1)+c_2(x^2-x)+c_3(x^2-1)
          </me>.
          The equivalent linear system once we combine like terms and equate coefficients is
          <me>
            \begin{linsys}{3}
              c_1\amp +\amp c_2\amp +\amp c_3\amp =\amp 1\\
              c_1\amp -\amp c_2\amp \amp \amp =\amp 0\\
              c_1\amp \amp \amp -\amp c_3\amp =\amp 0\\
            \end{linsys}
          </me>.
          The unique solution to this system is <m>(c_1,c_2,c_3)=(1/3, 1/3, 1/3)</m>. We conclude
          <me>
            [x^2]_B=\frac{1}{3}(1, 1, 1)
          </me>.
          The same reasoning shows that more generally, given polynomial <m>p(x)=ax^2+bx+c</m>, we have
          <me>
            [p(x)]_B=\frac{1}{3}(a+b+c, a-2b+c, a+b-2)
          </me>.

          </p>
        </li>
      </ol>
    </statement>
  </example>
  <p>
    When <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is an orthogonal basis with respect to some inner product <m>\langle , \rangle </m>, then by <xref ref="th_orthogonal_basis_formula"/> we have
    <me>
      \boldv=\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }\boldv_1+\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }\boldv_1+\cdots +\frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\boldv_n
    </me>,
    and thus
    <me>
      [\boldv]_B=\left(\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }, \frac{\langle \boldv,\boldv_2  \rangle }{\langle \boldv_2, \boldv_2\rangle },\dots, \frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\right)
    </me>.
    We have thus proved the following theorem.
  </p>
  <theorem xml:id="th_coordinates_orthogonal">
    <title>Coordinate vectors for orthogonal bases</title>
    <statement>
      <p>
      Let <m>(V,\langle , \rangle )</m> be an inner product space, and suppose the ordered basis <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> is orthogonal with respect to <m>\langle , \rangle </m>. For all <m>\boldv\in V</m> we have
      <me>
        [\boldv]_B=\left(\frac{\langle \boldv,\boldv_1  \rangle }{\langle \boldv_1, \boldv_1\rangle }, \frac{\langle \boldv,\boldv_2  \rangle }{\langle \boldv_2, \boldv_2\rangle },\dots, \frac{\langle \boldv,\boldv_n  \rangle }{\langle \boldv_n, \boldv_n\rangle }\right)
      </me>.
      </p>
    </statement>
  </theorem>
  <example>
    <title>Orthogonal bases</title>
    <statement>
      <p>
        Let <m>V=\R^2</m> and <m>B=((1,1),(-1,2))</m>. Find a general formula for <m>[(a,b)]_B</m>. Note: <m>B</m> is orthogonal with respect to the weighted dot product
        <me>
          \langle (x_1,x_2), (y_1,y_2)\rangle =2x_1y_1+x_2y_2
        </me>.
      </p>
    </statement>
    <solution>
      <p>
        Applying <xref ref="th_coordinates_orthogonal"/> to <m>B</m> and the dot product with weights <m>2, 1</m>, for any <m>\boldv=(a,b)</m> we compute
        <md>
          <mrow>[(a,b)]_B \amp =\left(\frac{\langle (a,b), (1,1)\rangle }{\langle (1,1),(1,1) \rangle }, \frac{\langle (a,b), (-1,2)\rangle }{\langle (-1,2),(-1,2) \rangle }\right)</mrow>
          <mrow> \amp=\left(\frac{1}{3}(2a+b),\frac{1}{3}(-a+b) \right) </mrow>
        </md>.
        Let's check our formula with <m>\boldv=(3,-3)</m>. The formula yields <m>[(3,-3)]_B=(1,-2)</m>, and indeed we see that
        <me>
          (3,-3)=1(1,1)-2(-1,2)
        </me>.

      </p>
    </solution>
  </example>
  <p>
    The next theorem is the key to understanding the tremendous computational value of coordinate vectors. The map
    <me>
      \boldv\in V \longmapsto [\boldv]_B\in \R^n
    </me>
    is a one-to-one correspondence, allowing us identify the vector space <m>V</m> with <m>\R^n</m>. Furthermore, since this map is linear, any question regarding the vector space structure of <m>V</m> can be translated to an equivalent question about the vector space <m>\R^n</m>. The great virtue of this is that we have a wealth of computational procedures that allow us to answer such questions for <m>\R^n</m>. As a result, given any <q>exotic</q> vector space <m>V</m> of finite dimension, once we choose an ordered basis <m>B</m> of <m>V</m>, questions about <m>V</m> can be answered by taking coordinate vectors with respect to <m>B</m> and answering the corresponding question in the familiar setting of <m>\R^n</m>.
  </p>
    <theorem xml:id="th_coordinates">
      <title>Coordinate vector transformation</title>

      <statement>
        <p>
          Let <m>B=(\boldv_1,\boldv_2,\dots, \boldv_n)</m> be an ordered basis for the vector space <m>V</m>.
          Define <m>T\colon V\rightarrow \R^n</m> by <m>T(\boldv)=[\boldv]_B</m>.
          <ol>
            <li>
              <p>
                <m>T</m> is a linear transformation.
                We will call <m>T</m> the <term>coordinate vector map with respect to <m>B</m></term>.
              </p>
            </li>
            <li>
              <p>
                <m>T(\boldv)=T(\boldw)</m> if and only if <m>\boldv=\boldw</m>.
                In particular,
                <m>T(\boldv)=(0,0,\dots, 0)</m> if and only if <m>\boldv=\boldzero_V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\im T=\R^n</m>
              </p>
            </li>
            <li>
              <p>
                Given a subset <m>S=\{\boldw_1,\boldw_2,\dots,
                \boldw_r\}\subseteq V</m>, let <m>S'=\{[\boldw_1]_B,[\boldw_2]_B,\dots,
                [\boldw_r]_B\}\subseteq \R^n</m>: <ie />, <m>S'</m> is the set of coordinate vectors of the elements of <m>S</m>. We have the following equivalences:
                <ol>
                  <li>
                    <p>
                      <m>S</m> spans <m>V</m> if and only if <m>S'</m> spans <m>\R^n</m>;
                    </p>
                  </li>
                  <li>
                    <p>
                      <m>S</m> is linearly independent if and only if <m>S'</m> is linearly independent.
                    </p>
                  </li>
                </ol>
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <proof>
        <ol>
          <li>
            <p>
              Suppose <m>T(\boldv)=[\boldv]_B=(a_1,a_2,\dots, a_n), T(\boldw)=[\boldw]_B=(b_1, b_2, \dots, b_n)</m>. By definition this means
              <md>
                <mrow>\boldv \amp =\sum_{i=1}^na_i\boldv_i \amp \boldw\amp =\sum_{i=1}^nb_i\boldv_i</mrow>
              </md>.
              Then
              <me>
                c\boldv+d\boldw=\sum_{i=1}^n(ca_i+db_i)\boldv_i
              </me>,
              and hence
              <md>
                <mrow>T(c\boldv+d\boldw)\amp =[c\boldv+d\boldw]_B </mrow>
                <mrow> \amp =(ca_1+db_1,ca_2+db_2,\dots, ca_n+db_n) </mrow>
                <mrow>  \amp =c(a_1,a_2,\dots, a_n)+d(b_1,b_2,\dots, b_n)</mrow>
                <mrow>  \amp =c[\boldv]_B+d[\boldw]_B</mrow>
                <mrow>  \amp =cT(\boldv)+dT(\boldw) </mrow>
              </md>.
              This proves <m>T</m> is linear.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>T(\boldv)=T(\boldw)=(c_1,c_2,\dots, c_n)</m>, then by definition of <m>[\hspace{.5pt}]_B</m> we have
              <me>
                \boldv=\boldw=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n
              </me>.
            </p>
          </li>
          <li>
            <p>
              Given any <m>\boldb=(b_1,b_2,\dots, b_n)\in \R^n</m>, we have <m>\boldb=T(\boldv)</m>, where
              <me>
                \boldv=b_1\boldv_1+b_2\boldv_2+\cdots +b_n\boldv_n
              </me>.
              This proves <m>\im T=\R^n</m>.
            </p>
          </li>
          <li>
            <p>
              This follows from (1)-(3) and <xref ref="th_isomorphisms"/>.
            </p>
          </li>
        </ol>

    </proof>
  <p>
    As an illustration of the comments before <xref ref="th_coordinates"/>, we describe a general method of contracting and extending sets to bases.
  </p>
  <algorithm xml:id="proc_contract_extend_general">
    <title>Contracting and extending to bases in general spaces</title>
    <statement>
      <p>
        Let <m>V</m> be a vector space of dimension <m>n</m>, and let
        <m>S=\{\boldv_1, \boldv_2,\dots, \boldv_r\}\subseteq V</m>.
      </p>
      <dl>
        <li>
          <title>Contracting to a basis</title>
          <p>
            Assume <m>S</m> spans <m>V</m>. To contract <m>S</m> to a basis <m>B\subseteq V</m>, proceed as follows.
          </p>
          <ol>
            <li>
              <p>
                Pick any ordered basis <m>C</m> of <m>V</m> and let <m>S'=\{[\boldv_1]_C, [\boldv_2]_C, \dots, [\boldv_r]_C\}\subseteq \R^n</m>.
              </p>
            </li>
            <li>
              <p>
                Use the relevant procedure of <xref ref="proc_contract_extend"/> to contract <m>S'</m> to basis <m>B'</m> of <m>\R^n</m>.
              </p>
            </li>
            <li>
              <p>
                The elements of <m>B'</m> correspond uniquely via the coordinate map <m>\boldv\mapsto [\boldv]_C</m> to a subset <m>B\subseteq S</m> .
              </p>
            </li>
            <li>
              <p>
                The subset <m>B\subseteq S</m> is a basis for <m>V</m>.
              </p>
            </li>
          </ol>
        </li>
        <li>
          <title>Extending to a basis</title>
          <p>
            Assume <m>S</m> is linearly independent. To extend <m>S</m> to a basis <m>B</m> of <m>V</m> proceed as follows.
          </p>
          <ol>
            <li>
              <p>
                Pick any ordered basis <m>C</m> of <m>V</m> and let <m>S'=\{[\boldv_1]_C, [\boldv_2]_C, \dots, [\boldv_r]_C\}\subseteq \R^n</m>.
              </p>
            </li>
            <li>
              <p>
                Use the relevant procedure of <xref ref="proc_contract_extend"/> to extend <m>S'</m> to a basis <m>B'</m> of <m>\R^n</m>.
              </p>
            </li>
            <li>
              <p>
                The elements of <m>B'</m> correspond uniquely via the coordinate map <m>\boldv\mapsto [\boldv]_C</m> to a set <m>B\subseteq V</m> containing <m>S</m>.
              </p>
            </li>
            <li>
              <p>
                The set <m>B</m> is a basis for <m>V</m> containing <m>S</m>.
              </p>
            </li>
          </ol>
        </li>
      </dl>
    </statement>
  </algorithm>
  <example>
    <statement>
      <p>
        The set
        <me>
          S=\left \{ A_1=\begin{bmatrix}2\amp 1\\ 0\amp -2 \end{bmatrix} , A_2=\begin{bmatrix}1\amp 1\\ 1\amp -1 \end{bmatrix} , A_3=\begin{bmatrix}0\amp 1\\ 2\amp 0 \end{bmatrix} , A_4=\begin{bmatrix}-1\amp 0\\ 1\amp 1 \end{bmatrix} \right\}
        </me>
        is a subset of the space <m>W=\{ A\in M_{22}\colon \tr A=0\}</m>.
        Let <m>W'=\Span S</m>. Contract <m>S</m> to a basis of <m>W'</m>
        and determine whether <m>W'=W</m>.
      </p>
    </statement>
    <hint>
      <p>
        Choose an ordered basis <m>C</m> of <m>M_{22}</m> and use the coordinate vector map to translate to a question about subspaces of <m>\R^4</m>. Answer this question and translate back to <m>M_{22}</m>.
      </p>
    </hint>
    <solution>
      <p>
        Let <m>C=(E_{11}, E_{12}, E_{21}, E_{22})</m> be the standard basis of <m>M_{22}</m>.
        Apply <m>[\hspace{10pt}]_C</m> to the elements of the given <m>S</m> to get a corresponding set <m>S'\subseteq\R^4</m>:
        <me>
          S'=\left\{ [A_1]_C=(2,1,0,-2), [A_2]_C=(1,1,1,-1), [A_3]_C=(0,1,2,0), [A_4]_C=(-1,0,1,1) \right\}
        </me>.
        Apply the column space procedure of <xref ref="proc_contract_extend"/> to contract <m>S'</m> to a basis <m>B'</m> of <m>\Span S'</m>. This produces the subset
        <me>
          B'=\{[A_1]_C=(2,1,0,-2), [A_2]_C=(1,1,1,-1)\}
        </me>
        Translating back to <m>V=M_{22}</m>, we conclude that the corresponding set
        <me>
          B=\{A_1, A_2\}
        </me>
        is a basis for <m>W'=\Span S</m>. We conclude that <m>\dim W'=2 </m>.
      </p>
      <p>
        Lastly the space <m>W</m> of all trace-zero matrices is easily seen to have basis
        <me>
        \left\{
        \begin{amatrix}[rr]1\amp 0\\ 0 \amp -1  \end{amatrix},
        \begin{amatrix}[rr]0\amp 1\\ 0\amp 0  \end{amatrix},
        \begin{amatrix}[rr]0 \amp 0\\ 1\amp 0  \end{amatrix}
        \right\}
        </me>,
        and hence <m>\dim W=3</m>. Since <m>\dim W'\lt\dim W</m>, we conclude that <m>W'\ne W</m>.
      </p>
    </solution>
  </example>
  </paragraphs>
  <paragraphs xml:id="ss_isomorphisms">
    <title>Isomorphisms</title>
    <p>
      We spoke of the coordinate vector map as a means of <q>translating</q> questions about an abstract vector space <m>V</m> to equivalent ones about the more familiar vector space <m>\R^n</m>.  Properties (1)-(3) of <xref ref="th_coordinates"/> are what guarantee that nothing is lost in this translation. Axiomitizing these properties, we obtain an important family of linear transformations called <em>isomorphisms</em>. Before getting to this definition, we recall some basic properties of general set functions.
    </p>
    <!-- <definition xml:id="d_injective_surjective_bijective">
      <title>Injective, surjective, bijective</title>
      <idx><h>invertible</h><h>function</h></idx>
      <idx><h>bijective</h><h>function</h></idx>
      <idx><h>injective</h><h>function</h></idx>
      <idx><h>surjective</h><h>function</h></idx>
      <idx><h>one-to-one</h><h>function</h></idx>
      <idx><h>onto</h><h>function</h></idx>
      <idx><h>one-to-one correspondence</h></idx>
      <statement>
        <p>
          Let <m>f\colon A\rightarrow B</m> be a function.
          <ul>
            <li>
              <p>
                We say <m>f</m> is <term>injective</term> (or <term>one-to-one</term>) if for all <m>a,a'\in A</m>, if <m>f(a)=f(a')</m>, then <m>a=a'</m>: equivalently, if <m>a\ne a'</m>, then <m>f(a)\ne f(a')</m>.
              </p>
            </li>
            <li>
              <p>
                We say <m>f</m> is <term>surjective</term> (or <term>onto</term>) if for all <m>b\in B</m>, there is an <m>a\in A</m> such that <m>f(a)=b</m>: equivalently, <m>\im f=B</m>.
              </p>
            </li>
            <li>
              <p>
                We say <m>f</m> is <term>bijective</term> (or a <term>one-to-one correspondence</term>) if it is injective and surjective.
              </p>
            </li>
            <li>
              <p>
                We say <m>f</m> is <term>invertible</term> if there exists a function <m>f^{-1}\colon B\rightarrow A</m>, called the <term>inverse of <m>f</m></term>, satisfying
                <md>
                  <mrow>f^{-1}\circ f\amp =\id_A \amp f\circ f^{-1}\amp =\id_B </mrow>
                </md>.
              </p>
            </li>
          </ul>
        </p>
      </statement>
    </definition>

        <remark xml:id="rm_invertible_functions">
      <statement>
        <p>
          It is an elementary fact from set theory that a function <m>f\colon A\rightarrow B</m> is invertible if and only if it bijective: <ie />, if and only if it is both one-to-one and onto. As such will use the two terms interchangeably.
        </p>
      </statement>
    </remark> -->
    <definition xml:id="d_isomorphism">
      <title>Isomorphism</title>
      <idx><h>isomorphism</h></idx>
      <idx><h>invertible</h><h>linear transformation</h></idx>
      <statement>
        <p>
          Let <m>V</m> and <m>W</m> be vector spaces. An <term>isomorphism</term> from <m>V</m> to <m>W</m> is a bijective linear transformation <m>T\colon V\rightarrow W</m>: <ie />, a function <m>T\colon V\rightarrow W</m> that is (i) linear, (ii) injective, and (iii) surjective.
        </p>
        <p>
          We say <m>V</m> and <m>W</m> are <term>isomorphic</term> if there exists an isomorphism between them.
        </p>
      </statement>
    </definition>
    <p>
      The following omnibus result is useful for deciding whether a linear transformation is an isomorphism, and lists a few of the properties of a vector space that are preserved by isomorphisms: namely, dimension, span, and linear independence.
    </p>
    <theorem xml:id="th_isomorphisms">
      <title>Isomorphism compendium</title>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow W</m> be a linear transformation.
        </p>
        <ol>
          <li>
            <p>
              <m>T</m> is injective if and only if <m>\NS T=\{\boldzero\}.</m>
            </p>
          </li>
          <li>
            <p>
              Assume <m>T</m> is an isomorphism, let <m>S\subseteq V</m>, and let <m>T(S)=\{T(\boldv)\colon \boldv\in S\}</m>, the set of images of <m>S</m> under <m>T</m>.
              <ol>
                <li>
                  <p>
                    The inverse function <m>T^{-1}\colon W\rightarrow V</m> is a linear transformation.
                  </p>
                </li>
                <li>
                  <p>
                    The set <m>S</m> spans <m>V</m> if and only if <m>T(S)</m> spans <m>W</m>.
                  </p>
                </li>
                <li>
                  <p>
                    The set <m>S</m> is linearly independent if and only if <m>T(S)</m> is linearly independent.
                  </p>
                </li>
              </ol>
            </p>
          </li>
          <li>
            <p>
              Assume <m>\dim V=n</m>. Then the following are equivalent:
              <ol label="i">
                <li>
                  <p>
                    <m>T</m> is an isomorphism;
                  </p>
                </li>
                <li>
                  <p>
                    <m>\dim W=n</m> and <m>\NS T=\{\boldzero\}</m>;
                  </p>
                </li>
                <li>
                  <p>
                    <m>\dim W=n</m> and <m>\im T=W</m>.
                  </p>
                </li>
              </ol>
            </p>
          </li>

        </ol>
      </statement>
      <proof>
          <ol>
            <li>
              <p>
                Assume <m>T</m> is injective. Then
                <md>
                  <mrow> T(\boldv)=\boldzero_W\amp \implies T(\boldv)=T(\boldzero_V) </mrow>
                  <mrow> \amp\implies \boldv=\boldzero_V </mrow>
                </md>.
                It follows that <m>\NS T=\{\boldzero_V\}</m>.
              </p>
              <p>
                Now assume <m>\NS T=\{\boldzero_V\}</m>. Then
                <md>
                  <mrow> T(\boldv)=T(\boldv') \amp\implies T(\boldv)-T(\boldv')=\boldzero_W </mrow>
                  <mrow> \amp\implies T(\boldv-\boldv')=\boldzero_W </mrow>
                  <mrow>  \amp \implies \boldv-\boldv'\in \NS T=\{\boldzero_V\}</mrow>
                  <mrow>  \amp\implies \boldv-\boldv'=\boldzero_V </mrow>
                  <mrow>  \amp \implies \boldv=\boldv'</mrow>
                </md>.
                Thus <m>T</m> is injective.
              </p>
            </li>
          </ol>

      </proof>

    </theorem>

  </paragraphs>

</section>
