<section xml:id="ss_elementary">
  <title>Invertible matrices</title>
  <introduction>
    <p>Picking up the thread of <xref ref="rm_cancel_failure"/>, we observe that the cancellation property enjoyed in real number algebra is a consequence of the fact that every nonzero real number <m>a\ne 0</m> has a <em>multiplicative inverse</em>, denoted <m>a^{-1}</m> or <m>1/a</m>, that satisfies <m>aa^{-1}=1</m>. Indeed, <q>canceling</q> the <m>a</m> in the equation <m>ab=ac</m> (assuming <m>a\ne 0</m>) is really the act of multiplying both sides of this equation by the multiplicative inverse <m>a^{-1}</m>.
  </p>
  <p>
  Ever on the lookout for connections between real number and matrix algebra, we ask whether the is a sensible analogue of multiplicative inverses for matrices. We have seen already that identity matrices <m>I_n</m> play the role of multiplicative identities for <m>n\times n</m> matrices, just as the number <m>1</m> does for real numbers. This suggests we should restrict our attention to <m>n\times n</m> matrices. The following definition is then the desired analogue of the multiplcative inverse of a nonzero real number.
  </p>
  </introduction>
  <definition>
    <title>Invertible matrix</title>
    <idx><h>invertible</h><h>matrix</h></idx>
    <idx><h>matrix</h><h>inverse</h></idx>
    <notation>
      <usage>A^{-1}</usage>
      <description>inverse of <m>A</m></description>
    </notation>

    <statement>
      <p>
        An <m>n\times n</m> matrix <m>A</m>  is <term>invertible</term> (or <term>nonsingular</term>)
        if there is a <m>n\times n</m> matrix <m>B</m> satisfying
        <me>
          AB=BA=I_n
        </me>.
        When this is the case we call <m>B</m> an
        <term>inverse</term> of <m>A</m>,
        and we say that <m>A</m> and <m>B</m> are
        <term>inverses</term> of one another.
      </p>
      <p>
        A square matrix that is not invertible is called <term>singular</term>.
      </p>
      <p>
      According to <xref ref="th_inverse_unique"/> the inverse of a matrix <m>A</m>, if it exists, is unique. We denote this unique inverse <m>A^{-1}</m>.
      </p>
    </statement>
  </definition>
<theorem xml:id="th_inverse_unique">
  <statement>
    <p>
      If <m>A</m> is an invertible matrix, then its inverse is unique: that is, there is only one matrix <m>B</m> satisfying <m>AB=BA=I</m>.
      </p>
  </statement>
</theorem>
  <proof>
    <p>
      Suppose matrices <m>B</m> and <m>C</m> both satisfy the properties of the multiplicative inverse: i.e.,
      <md>
        <mrow>AB\amp =BA=I  </mrow>
        <mrow> AC\amp=CA=I </mrow>
      </md>.
      Then
      <md>
        <mrow>AB=I\text{ and } AC=I \amp\implies  AB=AC</mrow>
        <mrow> \amp \implies BAB=BAC</mrow>
        <mrow>  \amp \implies I\,B=I\,C </mrow>
        <mrow>  \amp \implies B=C</mrow>
      </md>.
    Thus we see that <m>B=C</m>, showing that the inverse of <m>A</m>, if it exists, is unique.
    </p>
  </proof>
<p>
  Without any additional theory at our disposal, to show a matrix <m>A</m> is invertible we must exhibit an inverse. The onus is on us to find a matrix <m>B</m> satisfying both <m>AB=I</m> and <m>BA=I</m>. (Remember: since we cannot assume <m>BA=AB</m>, we really need to show both equations hold.)</p>
  <p>
  By the same token, to show <m>A</m> is <em>not</em> invertible we must show that an inverse does not exist: that is, we must prove that there is no <m>B</m> satisfying <m>AB=BA=I</m>. The next example illustrates this technique for a variety of matrices.
</p>
<example>
  <statement>
    <p>
      <ol>
        <li>
          <p>
            Identity matrices are invertible, and in fact we have <m>I^{-1}=I</m>, as witnessed by the fact that <m>II=I</m>.
          </p>
        </li>
        <li>
          <p>
            Square zero matrices <m>\boldzero</m> are never invertible, since for any square matrix <m>B</m> of the same dimension we have <me>B\,\boldzero=\boldzero B=\boldzero\ne I</me>.
            Thus there is no matrix satisfying the inverse property for <m>\boldzero</m>.
          </p>
        </li>
        <li>
          <p>
            Claim: <m>\begin{amatrix}[rr]2\amp 1\\ 3\amp 2
          \end{amatrix}^{-1}=\begin{amatrix}
          2\amp -1\\ -3\amp 2
          \end{amatrix}
          </m>. Indeed, we have
          <me>
          \begin{amatrix}[rr]2\amp 1\\ 3\amp 2
        \end{amatrix}\begin{amatrix}[rr]
        2\amp -1\\ -3\amp 2
        \end{amatrix}=\begin{amatrix}[rr]
        2\amp -1\\ -3\amp 2
        \end{amatrix}\begin{amatrix}[rr]2\amp 1\\ 3\amp 2
      \end{amatrix}=I_2
          </me>,
          as you can easily verify.

          </p>
        </li>
        <li>
          <p>
            The matrix <m>A= \begin{amatrix}[rrr]
              1\amp 1 \amp 1\\
              1\amp 1 \amp 1\\
              1\amp 1\amp 1
            \end{amatrix}</m> is not invertible. To see why we make use of the <xref ref="th_row_method" text="custom">row method of matrix multiplication</xref>.
          </p>
          <p>
            Indeed, given any matrix <m>B</m>, each row of <m>AB</m> is given by <m>\begin{amatrix}1\amp 1\amp 1
            \end{amatrix}\,B </m>. It follows that all the rows of <m>AB</m> are identical, and hence that we cannot have <m>AB=I_3</m>, since the rows of <m>I_3</m> are not identical.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</example>
<p>
  As the preceding example illustrates, deciding whether a matrix is invertible is not so straightforward, especially if the matrix is large. For the <m>2\times 2</m> case, however, we have a relatively simple test for invertibility. (We will generalize this to the <m>n\times n</m> case in <xref ref="c_det"/>.)
</p>
<theorem xml:id="th_2by2_inverse">
  <title>Inverses of <m>2\times 2</m> matrices</title>
  <statement>
    <p>
      A matrix <m>A=\abcdmatrix{a}{b}{c}{d}</m> is invertible if and only if <m>ad-bc\ne 0</m>.
    </p>
    <p>
      When this is the case, we have
      <me>
        \abcdmatrix{a}{b}{c}{d}=\frac{1}{ad-bc}\begin{amatrix}[rr]d\amp -b \\ -c\amp a
      \end{amatrix}
      </me>.
    </p>
  </statement>
</theorem>
<proof>
  <p>
    If <m>ad-bc\ne 0</m>, the proposed matrix is indeed an inverse of <m>A</m>, as one readily verifies.
  </p>
  <p>
    Assume <m>ad-bc=0</m>. If <m>A=\boldzero</m>, then <m>A</m> is not invertible, as we saw in the example above. Thus we can assume <m>A</m> is nonzero, in which case <m>B=\begin{amatrix}[rr]d\amp -b\\ -c\amp a
  \end{amatrix}</m> is also nonzero. An easy computation shows
  <me>
    AB=\abcdmatrix{ad-bc}{0}{0}{ad-bc}=\abcdmatrix{0}{0}{0}{0}.
  </me>
  This implies <m>A</m> is not invertible. Indeed if it were, then the inverse <m>A^{-1}</m> would exist, and we'd have
  <md>
    <mrow>AB=\boldzero \amp\implies A^{-1}AB=\boldzero </mrow>
    <mrow> \amp \implies IB=\boldzero </mrow>
    <mrow>  \amp \implies B=\boldzero </mrow>
  </md>,
  which is a contradiction. We have proved that if <m>ad-bc=0</m>, then <m>A</m> is not invertible.
  </p>
</proof>

  <theorem xml:id="th_invertible_prod">
    <title>Invertibility of products</title>
    <statement>
      <p>
        Let <m>A,B</m> be <m>n\times n</m> matrices. If <m>A</m> and <m>B</m> are both invertible, then so is their product <m>AB</m>. Using logical notation:
        <men xml:id="eq_inv_implies_prod">
          A \text{ and }  B  \text{ invertible } \implies AB \text{ invertible }
        </men>.
        In fact when this is the case we have
        <men xml:id="eq_inv_prod_form">(AB)^{-1}=B^{-1}A^{-1}</men>.
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Assume <m>A</m> and <m>B</m> are invertible. The statment of the theorem proposes a candidate for the inverse of <m>AB</m>: namely, <m>C=B^{-1}A^{-1}</m>.
      We need only show that <m>C</m> satisfies <m>C(AB)=(AB)C=I</m>. Here goes:
      <md>
        <mrow>C(AB)\amp =\amp (B^{-1}A^{-1})AB=B^{-1}A^{-1}AB=B^{-1}IB=B^{-1}B=I</mrow>
        <mrow>(AB)C\amp =\amp (AB)B^{-1}A^{-1}=ABB^{-1}A^{-1}=AIA^{-1}=AA^{-1}=I</mrow>
      </md>.
    </p>
  </proof>
  <corollary xml:id="c_invertible_prod">
    <statement>
      <p>
        More generally, if <m>A_1,A_2,\dots A_r</m> are invertible <m>n\times n</m> matrices, then their product <m>A_1A_2\cdots A_r</m> is invertible. Furthermore, we have in this case

        <men xml:id="eq_inv_prod_arb_form">
          (A_1A_2\cdots A_r)^{-1}=A_r^{-1}A_{r-1}^{-1}\cdots A_1^{-1}
        </men>.
      </p>
    </statement>
  </corollary>
  <proof>
    <p>
      We prove by induction on the number <m>r</m> of matrices, <m>r\geq 1</m>, that if the <m>A_i</m> are invertible, then the proposed inverse formula is valid.
      <case>
        <title>Base step: <m>r=1</m></title>
        <p>
          For <m>r=1</m>, the inverse formula reads <m>A_1^{-1}=A_1^{-1}</m>, which is clearly true.
        </p>
      </case>
      <case>
        <title>Induction step</title>
        <p>
          For the induction step we assume that the inverse formula is valid for any collection of <m>r-1</m> invertible matrices, and then show it is valid for any collection of <m>r</m> invertible matrices. Let <m>A_1,A_2,\dots, A_r</m> be invertible <m>n\times n</m> matrices. Define <m>A=A_1A_2\cdots A_{r-1}</m>. Then
          <md>
            <mrow>(A_1A_2\cdots A_{r-1})^{-1} \amp=\left((A_1A_2\cdots A_{r-1})A_r\right)^{-1} </mrow>
            <mrow> \amp=(AA_r)^{-1} </mrow>
            <mrow>  \amp= A_r^{-1}A^{-1} \amp (<xref ref="th_invertible_prod" />)</mrow>
            <mrow>  \amp =A_r^{-1}(A_{r-1}^{-1}A_{r-2}^{-1}\cdots A_1^{-1}) \amp (\text{induction})</mrow>
            <mrow>  \amp=A_r^{-1}A_{r-1}^{-1}\cdots A_1^{-1} \amp (\text{assoc.}) </mrow>
          </md>.
        </p>
      </case>
    </p>
  </proof>
  <remark>
    <p>
      Whenever confronted with a logical implication of the form <m>\mathcal{P}\implies\mathcal{Q}</m>, where <m>\mathcal{P}</m> and <m>\mathcal{Q}</m> denote arbitrary propositions, you should always ask whether the implication <q>goes the other way</q>. In other words, does the reverse implication  <m> \mathcal{Q}\implies \mathcal{P} </m> also hold?
    </p>
   <p>
     The answer with regard to the implication <xref ref="eq_inv_implies_prod"/> is yes, though the proof of this is more difficult then you think. (See <xref ref="cor_inv_prod_eq"/>.)
   </p>
  <p> The following argument is a common <em>invalid</em> proof of the reverse implication:
     <ol>
       <li>
         <p>
           Assume <m>AB</m> is invertible.
         </p>
       </li>
       <li>
         <p>
           Then <m>AB</m> is invertible.
         </p>
       </li>
       <li>
         <p>
           Then the inverse of <m>AB</m> is <m>B^{-1}A^{-1}</m>.
         </p>
       </li>
       <li>
         <p>
           Then <m>A^{-1}</m> and <m>B^{-1}</m> exist. Hence <m>A</m> and <m>B</m> are invertible.
         </p>
       </li>
     </ol>
    Where is the flaw in our logic here? The second statement only allows us to conclude that there is some mystery matrix <m>C</m> satisfying <m>(AB)C=C(AB)=I</m>. We cannot yet say that <m>C=B^{-1}A^{-1}</m>, as this formula from <xref ref="th_invertible_prod"/> only applies when we already know that <m>A</m> and <m>B</m> are both invertible. But this is exactly what we are trying to prove! As such we are guilty here of <q>begging the question</q>, or <em>petitio principii</em> in Latin.
   </p>
 </remark>
  <paragraphs>
  <title>Powers of matrices, matrix polynomials</title>
  <p>
    We end this section by exploring how the matrix inverse operation fits into our matrix algebra. First, we can now use the inverse operation to define matrix powers of the form <m>A^r</m> where <m>r</m> is an arbitary integer.
  </p>
  <definition xml:id="d_matrix_powers">
    <title>Matrix powers</title>
    <idx><h>matrix powers</h></idx>
    <notation>
      <usage>A^r</usage>
      <description>matrix power</description>
    </notation>
    <statement>
      <p>
      Let <m>A</m> be an <m>n\times n</m> matrix, and let <m>r\in\Z</m> be an integer. We define the power matrix <m>A^r</m> as follows:
      <me>
        A^r=\begin{cases} I\amp \text{if } r=0;\\[2ex] \underset{r \text{ times}}{\underbrace{AA\cdots A}}\amp \text{if } r>0; \\[2ex] (A^{-1})^s \amp \text{if } r=-s &lt; 0 \text{ and } A \text{ is invertible}.
       \end{cases}
      </me>.
      </p>
    </statement>
  </definition>
<theorem xml:id="th_power_rules">
  <title>Properties of matrix powers</title>

  <statement>
    <p>
      The following properties hold for all matrices <m>A, B</m>, all scalars <m>c\in \R</m>, and all integers <m>r,s\in\Z</m> for which the given expression makes sense.
      <ol>
        <li>
          <p>
            <m>A^{r+s}=A^rA^s</m>
          </p>
        </li>
        <li>
          <p>
            <m>(A^r)^s=A^{rs}</m>
          </p>
        </li>
        <li>
          <p>
            <m>(cA)^r=c^rA^r</m>
          </p>
        </li>
        <li>
          <p>
            <m>(A^{-1})^{-1}=A</m>
          </p>
        </li>
        <li>
          <p>
            <m>A^{-r}=(A^{-1})^r</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>
<proof>
  <p>
    The proofs of the first three statements are elementary, and closely resemble proofs of similar results in real number algebra. We leave these as an (unassigned) exercise.
  </p>
  <p>
    For the fourth statement to make sense, we must assume that <m>A</m> is invertible. The claim here is that <m>A^{-1}</m> is invertible, and that its inverse is <m>A</m> itself. To prove this we need only show <m>A^{-1}A=AA^{-1}=I</m>, which follows from the definition of the inverse.
  </p>
  <p>
    The fifth statement also tacitly assumes <m>A</m> is invertible. To prove it, we consider the three cases <m>r=0</m>, <m>r>0</m> and <m>r &lt; 0</m>.
  </p>
  <p>
    If <m>r=0</m>, then by definition  <m>A^{-r}=A^0=I=(A^{-1})^0=(A^{-1})^r</m>.
  </p>
  <p>
    If <m>r>0</m>, then by definition <m>A^{-r}=(A^{-1})^r</m>.
  </p>
  <p>
    Suppose <m>r=-s &lt; 0</m>. Then
    <md>
      <mrow>(A^{-1})^{r} \amp = ((A^{-1})^{-1})^s \amp (<xref ref="d_matrix_powers"/>) </mrow>
      <mrow> \amp =A^s \amp ((A^{-1})^{-1}=A)</mrow>
      <mrow>  \amp=A^{-r} \amp (r=-s) </mrow></md>.
  </p>
</proof>

  <theorem xml:id="th_inverse_trans">
    <statement>
      <p>
        Let <m>A</m> be invertible.
        Then <m>A^T</m> is invertible
        and <m>\left(A^T\right)^{-1}=\left(A^{-1}\right)^T</m>.
      </p>
    </statement>
    <proof>
      <p>
        Suppose <m>A</m> is invertible with inverse <m>A^{-1}</m>. The theorem claims <m>A^T</m> is invertible, and that in fact <m>(A^T)^{-1}=(A^{-1})^T</m>. To prove this, we need only show that
        <me>
          A^T(A^{-1})^T)=(A^{-1})^T)A^T=I
        </me>.
        We verify the two equalities separately:
        <md>
          <mrow>A^T(A^{-1})^T\amp =\left(A^{-1}A\right)^T \amp (<xref ref="th_trans_props"/>)</mrow>
          <mrow>\amp =I_n^T </mrow>
          <mrow> \amp =I_n  \checkmark</mrow>
        </md>
        <md>
          <mrow>(A^{-1})^TA^T \amp =(AA^{-1})^T \amp (<xref ref="th_trans_props"/>)</mrow>
          <mrow>\amp =I_n^T=I_n \ \checkmark</mrow>
        </md>.
        In both chains of equality we make use of the obvious claim <m>I^T=I</m>.
      </p>
    </proof>
  </theorem>
    <definition>
      <statement>
        <p>
          A square matrix <m>E_{m\times m}</m> is an
          <term>elementary matrix</term>
          if multiplying any matrix <m>A_{m\times n}</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
        </p>
      </statement>
    </definition>

  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Scale.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>c\cdot\boldr_i</m> for <m>c\ne 0</m>.
    Then <m>E</m> is the identity matrix except for the <m>i</m>-th row,
    where the 1 is replaced with <m>c</m>.
    <me>
      \underset{cr_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Swap.} Suppose <m>E</m> swaps rows <m>i</m> and <m>j</m> of <m>A</m>.
    Then <m>E</m> is the identity matrix with the <m>i</m>-th row and <m>j</m>-th rows swapped.
    <me>
      \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Row addition.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>\boldr_i+c\boldr_j</m>.
    Then <m>E</m> is the identity matrix except for its <m>i</m>-th row.
    <me>
      \underset{r_i+cr_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  </paragraphs>
  <paragraphs>
    <title>Row reduction as matrix multiplication</title>
    <p>
      Suppose we perform a sequence of row operations on a matrix <m>A</m>.
      Denote the <m>i</m>-th row operation <m>\rho_i</m>,
      and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
    </p>
    <p>
      Our sequence of operations <m>\rho_i</m> would produce the following sequence of matrices:
      <me>
        \begin{array}{c} A\\  \rho_1(A)\\  \rho_2(\rho_1(A))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A)))) \end{array}
      </me>
    </p>
    <p>
      Let <m>\rho_i</m> corresponds to the elementary matrix <m>E_i</m>.
      Then we represent this same sequence using matrix multiplication:
      <me>
        \begin{array}{c} A\\  E_1A\\  E_2E_1A\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A \end{array}
      </me>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Elementary matrices are invertible</title>
    <p>
      We will use our row operation notation to denote the different types of elementary matrices:
      <me>
        \underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+cr_j}E
      </me>.
    </p>
    <theorem>
      <title>Elementary matrix theorem</title>
      <statement>
        <p>
          Fix <m>n</m>.
          All elementary matrices are invertible,
          and their inverses are elementary matrices.
          In fact:
          <md>
            <mrow>\underset{cr_i}{E}^{-1}\amp =\amp \underset{\frac{1}{c}r_i}{E}</mrow>
            <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp =\amp \underset{r_i\leftrightarrow r_j}{E}</mrow>
            <mrow>\underset{r_i+cr_j}E^{-1}\amp =\amp \underset{r_i-cr_j}E</mrow>
          </md>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        These are formulas that you can easily check for each type of elementary matrix directly.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Examples</title>
    <p>
      Fix <m>n=3</m>.
      Verify that the pairs below are indeed inverses.
      We have <m>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> and thus <m>\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> We have
      <m>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</m> and thus <m>\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp \frac{1}{2} \end{bmatrix}</m> We have
      <m>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m> and thus <m>\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      We take a moment to make the following simple,
      somewhat overdo observation.
      Namely, we can represent a <em>system of linear equations</em>
      <me>
        \eqsys\tag{\(*\)}
      </me>
      as a <em>single matrix equation</em>
      <me>
        \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix} ,\tag{\(**\)}
      </me>
      or <m>A\boldx=\boldb</m>,where <m>A=[a_{ij}]</m>,
      <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
    </p>
    <p>
      Indeed if you expand out the left-hand side of <m>(**)</m> into an <m>m\times 1</m> column vector,
      using the definition of matrix multiplication,
      and then invoke the definition of matrix equality,
      then you obtain the linear system <m>(*)</m>.
    </p>
    <p>
      By the same token, an <m>n</m>-tuple
      <m>(s_1,s_2,\dots,
      s_n)</m> is a solution to the <em>system of equations</em>
      <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em> <m>A\boldx=\boldb</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      In particular, a homogeneous linear system
      <me>
        \homsys
      </me>
      can be represented as the single matrix equation
      <me>
        A\boldx=\underset{m\times 1}{\boldzero}
      </me>,
      where <m>A=[a_{ij}]</m> and <m>\boldx=[x_i]</m>;
      and <m>(s_1,s_2,\dots, s_n)</m> is a solution to the
      <em>homogenous system</em>
      if and only if its corresponding column vector
      <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em>
      <m>A\boldx=\underset{m\times 1}{\boldzero}</m>.
    </p>
  </paragraphs>
  <p>
    We are now ready to state a major theorem about invertibility.
  </p>
  <theorem>
    <title>Invertibility theorem</title>
    <statement>
      <p>
        Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
        The following statements are equivalent.
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              <m>A\boldx=\boldzero</m> has a unique solution
              (the trivial one).
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is row equivalent to <m>I_n</m>,
              the <m>n\times n</m> identity matrix.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a product of elementary matrices.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Recall that two show two statements <m>P</m> and <m>Q</m> are equivalent,
      we must show two implications:
      <m>P\Rightarrow Q</m>, and <m>Q\Rightarrow P</m>.
    </p>
    <p>
      We must show this for each pair of statements above.
      That would be 6 different pairs and a total of 12 different implications to show!
      We ease our work load by showing the following
      <em>cycle</em> of implications:
      <me>
        (a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
      </me>
      and using the fact that implication is transitive!
      We will complete the proof in class.
    </p>
  </proof>
  <paragraphs>
    <title>Algorithm for inverses</title>
    <p>
      The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether <m>A</m> is invertible,
      and (2) computing <m>A^{-1}</m> if it exists.
    </p>
    <p>
      The algorithm:
      <ol>
        <li>
          <p>
            Row reduce <m>A</m> to row echelon form <m>U</m>
            (keeping track of the elementary matrices you use).
            The theorem tells us that <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading 1's.
          </p>
        </li>
        <li>
          <p>
            If this is so, we can keep row reducing to the identity matrix.
            In terms of matrices we have
            <me>
              E_rE_{r-1}\cdots E_1A=I_n
            </me>.
            This means <m>A^{-1}=E_rE_{r-1}\cdots E_1</m>.
          </p>
        </li>
        <li>
          <p>
            As an added bonus we also have <m>A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}</m>,
            expressing <m>A</m> as a product of elementary matrices.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Inverses with augmented matrix</title>
    <p>
      It can be bothersome to keep track of those <m>E_i</m>,
      and the order of their multiplication.
      Instead, we use an augmented matrix method,
      beginning with the augmented matrix
      <me>
        \begin{bmatrix}A\amp I_n \end{bmatrix}
      </me>,
      and simply applying row reductions until <m>A</m> on the left becomes <m>I_n</m>,
      at which point <m>I_n</m> on the right has become <m>A^{-1}</m>.
    </p>
    <p>
      Why does this work?
      In terms of the <m>E_i</m>, we have the sequence:
      <me>
        \begin{array}{c} \begin{bmatrix}A\amp I_n \end{bmatrix} \begin{bmatrix}E_1A\amp E_1 \end{bmatrix} \begin{bmatrix}E_2E_1A\amp E_2E_1 \end{bmatrix} \\  \vdots \begin{bmatrix}E_rE_{r-1}\cdots E_1A\amp E_rE_{r-1}\cdots E_1 \end{bmatrix} =\begin{bmatrix}I_n\amp A^{-1} \end{bmatrix} \end{array}
      </me>
    </p>
    <p>
      Thus when we are done,
      the RHS of our augmented matrix magically becomes <m>A^{-1}</m>.
    </p>
  </paragraphs>
  <p>
    \alert{Example:} take <m>A=\begin{bmatrix}1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{bmatrix} </m>.
    <md>
      <mrow>\begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}  \amp \xrightarrow{r_2-r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_3+r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\alert{\text{ (3 leading 1's, thus invertible!) } }\amp \xrightarrow{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_2-3r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_1-2r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    </md>
  </p>
  <p>
    We conclude that <m>A^{-1}=\begin{bmatrix}-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{bmatrix}</m>.
    On the next slide I show how <m>A^{-1}</m> and <m>A</m> can be written as products of elementary matrices.
  </p>
  <paragraphs>
    <title>Example (contd):</title>
    <p>
      in terms of elementary matrices we have
    </p>
  </paragraphs>
  <me>
    E_5E_4E_3E_2E_1A=I
  </me>,
  <p>
    where fuck
    <me>\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      -1\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}
    </me>
    <md>
      <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        -1\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 1\amp 0\\
        1\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 0\amp 1\\
        0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        0\amp 1\amp -3\\
        0\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
        0\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}</mrow>
    </md>
  </p>
  <p>
    Thus we have <m>A^{-1}=E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</m>.
    This shows how both of these matrices can be written as products of elementary matrices
    (recall that each <m>E_i^{-1}</m> is also elementary).
  </p>
</section>
