<section xml:id="ss_elementary">
  <title>Invertible matrices</title>
  <introduction>
    <p>Picking up the thread of <xref ref="rm_cancel_failure"/>, we observe that the cancellation property enjoyed in real number algebra is a consequence of the fact that every nonzero real number <m>a\ne 0</m> has a <em>multiplicative inverse</em>, denoted <m>a^{-1}</m> or <m>1/a</m>, that satisfies <m>aa^{-1}=1</m>. Indeed, <q>canceling</q> the <m>a</m> in the equation <m>ab=ac</m> (assuming <m>a\ne 0</m>) is really the act of multiplying both sides of this equation by the multiplicative inverse <m>a^{-1}</m>.
  </p>
  <p>
  Ever on the lookout for connections between real number and matrix algebra, we ask whether the is a sensible analogue of multiplicative inverses for matrices. We have seen already that identity matrices <m>I_n</m> play the role multiplicative identites for <m>n\times n</m> matrices, just as the number <m>1</m> does for real numbers. This suggests we should restrict our attention to <m>n\times n</m> matrices. The following definition is then the desired analogue of the multiplcative inverse of a nonzero real number.
  </p>
  </introduction>
  <definition>
    <statement>
      <p>
        An <m>n\times n</m>matrix <m>A</m>  is <term>invertible</term> (or <term>nonsingular</term>)
        if there is a <m>n\times n</m> matrix <m>B</m> satisfying
        <me>
          AB=BA=I_n
        </me>.
        When this is the case we call <m>B</m> an
        <term>inverse</term> of <m>A</m>,
        and we say that <m>A</m> and <m>B</m> are
        <term>inverses</term> of one another.
      </p>
      <p>
        A square matrix that is not invertible is called <term>singular</term>.
      </p>
      <p>
      According to <xref ref="th_inverse_unique"/> the inverse of a matrix <m>A</m>, if it exists, is unique. We denote this unique inverse <m>A^{-1}</m>.
      </p>
    </statement>
  </definition>
<theorem xml:id="th_inverse_unique">
  <statement>
    <p>
      If <m>A</m> is an invertible matrix, then its inverse is unique: that is, there is only one matrix <m>B</m> satisfying <m>AB=BA=I</m>.
      </p>
  </statement>
</theorem>
  <proof>
    <p>
      Suppose matrices <m>B</m> and <m>C</m> both satisfy the properties of the multiplicative inverse: i.e.,
      <md>
        <mrow>AB\amp =BA=I  </mrow>
        <mrow> AC\amp=CA=I </mrow>
      </md>.
      Then
      <md>
        <mrow>AB=I\text{ and } AC=I \amp\implies  AB=AC</mrow>
        <mrow> \amp \implies BAB=BAC</mrow>
        <mrow>  \amp \implies I\,B=I\,C </mrow>
        <mrow>  \amp \implies B=C</mrow>
      </md>.
    </p>
  </proof>
<p>
  Without any additional theory at our disposal, to show a matrix <m>A</m> is invertible we must exhibit an inverse; the onus is on us to find a matrix <m>B</m> satisfying both <m>AB=I</m> and <m>BA=I</m>. (Remember: we cannot assume <m>BA=AB</m>.) By the same token, to show <m>A</m> is <em>not</em> invertible we must show that an inverse does not exist: that is, we must prove that there is no <m>B</m> satisfying <m>AB=BA=I</m>. The next example illustrates this technique for a variety of matrices.
</p>
<example>
  <statement>
    <p>
      <ol>
        <li>
          <p>
            Identity matrices are invertible, and in fact we have <m>I^{-1}=I</m>, as witnessed by the fact that <m>II=I</m>.
          </p>
        </li>
        <li>
          <p>
            Square zero matrices <m>\boldzero</m> are never invertible, since for any square matrix <m>B</m> of the same dimension we have <me>B\,\boldzero=\boldzero B=\boldzero\ne I</me>.
            Thus there is no matrix satisfying the inverse property for <m>\boldzero</m>.
          </p>
        </li>
        <li>
          <p>
            Claim: <m>\begin{amatrix}[rr]2\amp 1\\ 3\amp 2
          \end{amatrix}^{-1}=\begin{amatrix}
          2\amp -1\\ -3\amp 2
          \end{amatrix}
          </m>. Indeed, we have
          <me>
          \begin{amatrix}[rr]2\amp 1\\ 3\amp 2
        \end{amatrix}\begin{amatrix}[rr]
        2\amp -1\\ -3\amp 2
        \end{amatrix}=\begin{amatrix}[rr]
        2\amp -1\\ -3\amp 2
        \end{amatrix}\begin{amatrix}[rr]2\amp 1\\ 3\amp 2
      \end{amatrix}=I_2
          </me>,
          as you can easily verify.

          </p>
        </li>
        <li>
          <p>
            The matrix <m>A= \begin{amatrix}[rrr]
              1\amp 1 \amp 1\\
              1\amp 1 \amp 1\\
              1\amp 1\amp 1
            \end{amatrix}</m> is not invertible. To see why we make use of the <xref ref="th_row_method" text="custom">row method of matrix multiplication</xref>.
          </p>
          <p>
            Indeed, given any matrix <m>B</m>, each row of <m>AB</m> is given by <m>\begin{amatrix}1\amp 1\amp 1
            \end{amatrix}\,B </m>. It follows that all the rows of <m>AB</m> are identical, and hence that we cannot have <m>AB=I_3</m>, since the rows of <m>I_3</m> are not identical.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</example>
<p>
  As the preceding example illustrates, deciding whether a matrix is invertible is not so straightforward. However, there is a relatively simple test for <m>2\times 2</m> matrices.
</p>
<theorem xml:id="th_2by2_inverse">
  <title>Inverses of <m>2\times 2</m> matrices</title>
  <statement>
    <p>
      A matrix <m>A=\abcdmatrix{a}{b}{c}{d}</m> is invertible if and only if <m>ad-bc\ne 0</m>.
    </p>
    <p>
      When this is the case, we have
      <me>
        \abcdmatrix{a}{b}{c}{d}=\frac{1}{ad-bc}\begin{amatrix}[rr]d\amp -b \\ -c\amp a
      \end{amatrix}
      </me>.
    </p>
  </statement>
</theorem>
<proof>
  <p>
    If <m>ad-bc\ne 0</m>, the proposed matrix is indeed an inverse of <m>A</m>, as one readily verifies.
  </p>
  <p>
    Assume <m>ad-bc=0</m>. If <m>A=\boldzero</m>, then <m>A</m> is not invertible, as we saw in the example above. Thus we can assume <m>A</m> is nonzero, in which case <m>B=\begin{amatrix}[rr]d\amp -b\\ -c\amp a
  \end{amatrix}</m> is also nonzero. An easy computation shows
  <me>
    AB=\abcdmatrix{ad-bc}{0}{0}{ad-bc}=\abcdmatrix{0}{0}{0}{0}.
  </me>
  This implies <m>A</m> is not invertible. Indeed if it were, then the inverse <m>A^{-1}</m> would exist, and we'd have
  <md>
    <mrow>AB=\boldzero \amp\implies A^{-1}AB=\boldzero </mrow>
    <mrow> \amp \implies IB=\boldzero </mrow>
    <mrow>  \amp \implies B=\boldzero </mrow>
  </md>,
  which is a contradiction. We have proved that if <m>ad-bc=0</m>, then <m>A</m> is not invertible.
  </p>
</proof>

  <theorem>
    <statement>
      <p>
        Let <m>A,B</m> be <m>n\times n</m> matrices.
        <me>
          A \text{ and }  B  \text{ invertible } \Rightarrow AB \text{ invertible }
        </me>,
        and in fact <m>(AB)^{-1}=B^{-1}A^{-1}</m>.
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      In the absence of more theory,
      we can only prove invertibility by providing an inverse.
      The last statement offers up a candidate:
      viz., <m>C=B^{-1}A^{-1}</m>.
      We need only show that <m>C</m> satisfies the relevant properties:
      <md>
        <mrow>C(AB)\amp =\amp (B^{-1}A^{-1})AB=B^{-1}A^{-1}AB=B^{-1}IB=B^{-1}B=I</mrow>
        <mrow>(AB)C\amp =\amp (AB)B^{-1}A^{-1}=ABB^{-1}A^{-1}=AIA^{-1}=AA^{-1}=I</mrow>
      </md>.
    </p>
  </proof>
  <corollary>
    <statement>
      <p>
        More generally,
        <me>
          A_1,A_2,\dots A_r \text{ invertible } \Rightarrow (A_1A_2\cdots A_r)\text{ invertible }
        </me>,
        and in fact <m>(A_1A_2\cdots A_r)^{-1}=A_r^{-1}A_{r-1}^{-1}\cdots A_1^{-1}</m>.
      </p>
    </statement>
  </corollary>
  <p>
    <em>Question:</em> do the implication arrows above go the other way?
  </p>
  <p>
    <em>Answer:</em> we shall see.
  </p>
  <paragraphs>
  <title>Powers of matrices</title>


  <theorem>
    <statement>
      <p>
        Let <m>A_{n\times n}</m> be invertible.
        Then <m>A^T</m> is invertible,
        and <m>\left(A^T\right)^{-1}=\left(A^{-1}\right)^T</m>.
      </p>
    </statement>
    <proof>
      <p>
        Suppose <m>A</m> is invertible,
        and let <m>A^{-1}</m> be its inverse.
        The claim above is that <m>B=\left(A^{-1}\right)^T</m> is the inverse of <m>A^T</m>.
        To prove this claim we need only show that <m>BA^T=A^TB=I_n</m>:
        <md>
          <mrow>BA^T=\left(A^{-1}\right)^TA^T \amp =\amp \left(AA^{-1}\right)^T \ \text{ (since \((CD)^T=D^TC^T\)) }</mrow>
          <mrow>\amp =\amp I_n^T=I_n \ \checkmark</mrow>
        </md>
        <md>
          <mrow>A^TB=A^T\left(A^{-1}\right)^T \amp =\amp \left(A^{-1}A\right)^T \ \text{ (since \((CD)^T=D^TC^T\)) }</mrow>
          <mrow>\amp =\amp I_n^T=I_n \ \checkmark</mrow>
        </md>
      </p>
    </proof>
  </theorem>
    <definition>
      <statement>
        <p>
          A square matrix <m>E_{m\times m}</m> is an
          <term>elementary matrix</term>
          if multiplying any matrix <m>A_{m\times n}</m> on the left by <m>E</m> performs one of our row operations on <m>A</m>.
        </p>
      </statement>
    </definition>

  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Scale.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>c\cdot\boldr_i</m> for <m>c\ne 0</m>.
    Then <m>E</m> is the identity matrix except for the <m>i</m>-th row,
    where the 1 is replaced with <m>c</m>.
    <me>
      \underset{cr_i}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ 0\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 0\amp c\amp 0\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Swap.} Suppose <m>E</m> swaps rows <m>i</m> and <m>j</m> of <m>A</m>.
    Then <m>E</m> is the identity matrix with the <m>i</m>-th row and <m>j</m>-th rows swapped.
    <me>
      \underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp 0\amp 0\amp 1\amp \cdots\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp \cdots\amp 1\amp 0\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  <p>
    The <em>row method</em> of multiplication is the key to matching up a row operation with a particular elementary matrix.
  </p>
  <p>
    Suppose the multiplication
    <me>
      \underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
    </me>
    performs a single row operation on <m>A</m>.
    What must <m>E</m> be?
    In what follows let <m>\boldr_i</m> be the <m>i</m>-th row of <m>A</m> for
    <m>1\leq i\leq m</m>.    \alert{Row addition.} Suppose <m>E</m> replaces
    <m>\boldr_i</m> of <m>A</m> with <m>\boldr_i+c\boldr_j</m>.
    Then <m>E</m> is the identity matrix except for its <m>i</m>-th row.
    <me>
      \underset{r_i+cr_j}{E}=\begin{bmatrix}1\amp 0\amp 0\amp \cdots\amp  0\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ \cdots\amp c\amp \cdots\amp 1\amp \cdots\amp 0\\ \vdots\amp  \amp \vdots \amp  \amp  \vdots \\ 0\amp 0\amp \cdots\amp 0\amp 0\amp 1 \end{bmatrix}
    </me>
  </p>
  </paragraphs>
  <paragraphs>
    <title>Row reduction as matrix multiplication</title>
    <p>
      Suppose we perform a sequence of row operations on a matrix <m>A</m>.
      Denote the <m>i</m>-th row operation <m>\rho_i</m>,
      and denote by <m>\rho_i(B)</m> the result of applying <m>\rho_i</m> to a matrix <m>B</m>.
    </p>
    <p>
      Our sequence of operations <m>\rho_i</m> would produce the following sequence of matrices:
      <me>
        \begin{array}{c} A\\  \rho_1(A)\\  \rho_2(\rho_1(A))\\  \vdots \\ \rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A)))) \end{array}
      </me>
    </p>
    <p>
      Let <m>\rho_i</m> corresponds to the elementary matrix <m>E_i</m>.
      Then we represent this same sequence using matrix multiplication:
      <me>
        \begin{array}{c} A\\  E_1A\\  E_2E_1A\\  \vdots \\ E_rE_{r-1}\cdots E_2E_1A \end{array}
      </me>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Elementary matrices are invertible</title>
    <p>
      We will use our row operation notation to denote the different types of elementary matrices:
      <me>
        \underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+cr_j}E
      </me>.
    </p>
    <theorem>
      <title>Elementary matrix theorem</title>
      <statement>
        <p>
          Fix <m>n</m>.
          All elementary matrices are invertible,
          and their inverses are elementary matrices.
          In fact:
          <md>
            <mrow>\underset{cr_i}{E}^{-1}\amp =\amp \underset{\frac{1}{c}r_i}{E}</mrow>
            <mrow>\underset{r_i\leftrightarrow r_j}{E}^{-1}\amp =\amp \underset{r_i\leftrightarrow r_j}{E}</mrow>
            <mrow>\underset{r_i+cr_j}E^{-1}\amp =\amp \underset{r_i-cr_j}E</mrow>
          </md>
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        These are formulas that you can easily check for each type of elementary matrix directly.
      </p>
    </proof>
  </paragraphs>
  <paragraphs>
    <title>Examples</title>
    <p>
      Fix <m>n=3</m>.
      Verify that the pairs below are indeed inverses.
      We have <m>\underset{r_1+3r_3}{E}=\begin{bmatrix}1\amp 0\amp 3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> and thus <m>\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp -3\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{bmatrix}</m> We have
      <m>\underset{2r_3}{E}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 2 \end{bmatrix}</m> and thus <m>\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp \frac{1}{2} \end{bmatrix}</m> We have
      <m>\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m> and thus <m>\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}0\amp 0\amp 1\\0\amp 1\amp 0\\ 1\amp 0\amp 0 \end{bmatrix}</m>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      We take a moment to make the following simple,
      somewhat overdo observation.
      Namely, we can represent a <em>system of linear equations</em>
      <me>
        \eqsys\tag{\(*\)}
      </me>
      as a <em>single matrix equation</em>
      <me>
        \genmatrix\begin{bmatrix}x_1\\x_2\\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix}b_1\\ b_2 \\ \vdots \\ b_m \end{bmatrix} ,\tag{\(**\)}
      </me>
      or <m>A\boldx=\boldb</m>,where <m>A=[a_{ij}]</m>,
      <m>\boldx=[x_i]</m>, <m>\boldb=[b_i]</m>.
    </p>
    <p>
      Indeed if you expand out the left-hand side of <m>(**)</m> into an <m>m\times 1</m> column vector,
      using the definition of matrix multiplication,
      and then invoke the definition of matrix equality,
      then you obtain the linear system <m>(*)</m>.
    </p>
    <p>
      By the same token, an <m>n</m>-tuple
      <m>(s_1,s_2,\dots,
      s_n)</m> is a solution to the <em>system of equations</em>
      <m>(*)</m> if and only if its corresponding column vector <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em> <m>A\boldx=\boldb</m>.
    </p>
  </paragraphs>
  <paragraphs>
    <title>Interlude on matrix equations</title>
    <p>
      In particular, a homogeneous linear system
      <me>
        \homsys
      </me>
      can be represented as the single matrix equation
      <me>
        A\boldx=\underset{m\times 1}{\boldzero}
      </me>,
      where <m>A=[a_{ij}]</m> and <m>\boldx=[x_i]</m>;
      and <m>(s_1,s_2,\dots, s_n)</m> is a solution to the
      <em>homogenous system</em>
      if and only if its corresponding column vector
      <m>\underset{n\times 1}{\boldsymbol}=[s_i]</m> is a solution to the
      <em>matrix equation</em>
      <m>A\boldx=\underset{m\times 1}{\boldzero}</m>.
    </p>
  </paragraphs>
  <p>
    We are now ready to state a major theorem about invertibility.
  </p>
  <theorem>
    <title>Invertibility theorem</title>
    <statement>
      <p>
        Let <m>A=[a_{ij}]</m> be a square <m>n\times n</m> matrix.
        The following statements are equivalent.
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>
          <li>
            <p>
              <m>A\boldx=\boldzero</m> has a unique solution
              (the trivial one).
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is row equivalent to <m>I_n</m>,
              the <m>n\times n</m> identity matrix.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a product of elementary matrices.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  <proof>
    <p>
      Recall that two show two statements <m>P</m> and <m>Q</m> are equivalent,
      we must show two implications:
      <m>P\Rightarrow Q</m>, and <m>Q\Rightarrow P</m>.
    </p>
    <p>
      We must show this for each pair of statements above.
      That would be 6 different pairs and a total of 12 different implications to show!
      We ease our work load by showing the following
      <em>cycle</em> of implications:
      <me>
        (a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
      </me>
      and using the fact that implication is transitive!
      We will complete the proof in class.
    </p>
  </proof>
  <paragraphs>
    <title>Algorithm for inverses</title>
    <p>
      The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether <m>A</m> is invertible,
      and (2) computing <m>A^{-1}</m> if it exists.
    </p>
    <p>
      The algorithm:
      <ol>
        <li>
          <p>
            Row reduce <m>A</m> to row echelon form <m>U</m>
            (keeping track of the elementary matrices you use).
            The theorem tells us that <m>A</m> is invertible if and only if <m>U</m> has <m>n</m> leading 1's.
          </p>
        </li>
        <li>
          <p>
            If this is so, we can keep row reducing to the identity matrix.
            In terms of matrices we have
            <me>
              E_rE_{r-1}\cdots E_1A=I_n
            </me>.
            This means <m>A^{-1}=E_rE_{r-1}\cdots E_1</m>.
          </p>
        </li>
        <li>
          <p>
            As an added bonus we also have <m>A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}</m>,
            expressing <m>A</m> as a product of elementary matrices.
          </p>
        </li>
      </ol>
    </p>
  </paragraphs>
  <paragraphs>
    <title>Inverses with augmented matrix</title>
    <p>
      It can be bothersome to keep track of those <m>E_i</m>,
      and the order of their multiplication.
      Instead, we use an augmented matrix method,
      beginning with the augmented matrix
      <me>
        \begin{bmatrix}A\amp I_n \end{bmatrix}
      </me>,
      and simply applying row reductions until <m>A</m> on the left becomes <m>I_n</m>,
      at which point <m>I_n</m> on the right has become <m>A^{-1}</m>.
    </p>
    <p>
      Why does this work?
      In terms of the <m>E_i</m>, we have the sequence:
      <me>
        \begin{array}{c} \begin{bmatrix}A\amp I_n \end{bmatrix} \begin{bmatrix}E_1A\amp E_1 \end{bmatrix} \begin{bmatrix}E_2E_1A\amp E_2E_1 \end{bmatrix} \\  \vdots \begin{bmatrix}E_rE_{r-1}\cdots E_1A\amp E_rE_{r-1}\cdots E_1 \end{bmatrix} =\begin{bmatrix}I_n\amp A^{-1} \end{bmatrix} \end{array}
      </me>
    </p>
    <p>
      Thus when we are done,
      the RHS of our augmented matrix magically becomes <m>A^{-1}</m>.
    </p>
  </paragraphs>
  <p>
    \alert{Example:} take <m>A=\begin{bmatrix}1\amp 2\amp 0\\ 1\amp 2\amp 1\\ -1\amp -1\amp 3 \end{bmatrix} </m>.
    <md>
      <mrow>\begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 1\amp 2\amp 1\amp 0\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}  \amp \xrightarrow{r_2-r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ -1\amp -1\amp 3\amp 0\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_3+r_1}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1 \end{bmatrix}</mrow>
      <mrow>\alert{\text{ (3 leading 1's, thus invertible!) } }\amp \xrightarrow{r_2\leftrightarrow r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 3\amp 1\amp 0\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_2-3r_3}\amp \begin{bmatrix}1\amp 2\amp 0\amp 1\amp 0\amp 0\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
      <mrow>\amp \xrightarrow{r_1-2r_2}\amp \begin{bmatrix}1\amp 0\amp 0\amp -7\amp 6\amp -2\\ 0\amp 1\amp 0\amp 4\amp -3\amp 1\\ 0\amp 0\amp 1\amp -1\amp 1\amp 0 \end{bmatrix}</mrow>
    </md>
  </p>
  <p>
    We conclude that <m>A^{-1}=\begin{bmatrix}-7\amp 6\amp -2\\ 4\amp -3\amp 1\\ -1\amp 1\amp 0 \end{bmatrix}</m>.
    On the next slide I show how <m>A^{-1}</m> and <m>A</m> can be written as products of elementary matrices.
  </p>
  <paragraphs>
    <title>Example (contd):</title>
    <p>
      in terms of elementary matrices we have
    </p>
  </paragraphs>
  <me>
    E_5E_4E_3E_2E_1A=I
  </me>,
  <p>
    where fuck
    <me>\begin{amatrix}[rrr] 1\amp 0\amp 0\\
      -1\amp 1\amp 0\\
      0\amp 0\amp 1\end{amatrix}
    </me>
    <md>
      <mrow> E_1\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        -1\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}  \amp   E_2\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 1\amp 0\\
        1\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_3\amp =\begin{amatrix}[rrr]1\amp 0\amp 0\\
        0\amp 0\amp 1\\
        0\amp 1\amp 0 \end{amatrix} \amp   E_4\amp =\begin{amatrix}[rrr] 1\amp 0\amp 0\\
        0\amp 1\amp -3\\
        0\amp 0\amp 1\end{amatrix}
      </mrow>
      <mrow>E_5\amp =\begin{amatrix}[rrr]1\amp -2\amp 0\\
        0\amp 1\amp 0\\
        0\amp 0\amp 1\end{amatrix}</mrow>
    </md>
  </p>
  <p>
    Thus we have <m>A^{-1}=E_5E_4E_3E_2E_1</m> and <m>A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}</m>.
    This shows how both of these matrices can be written as products of elementary matrices
    (recall that each <m>E_i^{-1}</m> is also elementary).
  </p>
</section>
