<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_rank_nullity">
  <title>Rank-nullity theorem and fundamental spaces</title>
  <introduction>
    This section is in a sense just a long-format example of how to compute bases and dimensions of subspaces. Along the way, however we meet the <xref ref="th_rank-nullity" text="custom">rank-nullity theorem</xref> (sometimes called the <q>fundamental theorem of linear algebra</q>), and apply this theorem in the context of <em>fundamental spaces of matrices</em> (<xref ref="d_fundamental_space"/>)
  </introduction>
  <subsection xml:id="ss_rank-nullity">
    <title>The rank-nullity theorem</title>

<introduction>
  <p> The rank-nullity theorem relates the the dimensions of the null space and image of a linear transformation <m>T\colon V\rightarrow W</m>, assuming <m>V</m> is finite dimensional. Roughly speaking, it says that the bigger the null space, the smaller the image. More precisely, it tells us that
  <me>
    \dim V=\dim\NS T+\dim\im T
  </me>.
As we will see, this elegant result can be used to significantly simplify computations with linear transformations. In particular, it often comes in handy when proving claims about the image of a linear transformation. Furthermore, it allows us to prove some intuitively obvious properties of linear transformations. For example, suppose <m>V</m> is a finite-dimensional vector space. It seems obvious that if <m>\dim W> \dim V</m>, then there is no linear transformation mapping <m>V</m> surjectively onto <m>W</m>: <ie />, you shouldn't be able to map a <q>smaller</q> vector space onto a <q>bigger</q> one. Similarly, if <m>\dim W \lt \dim V</m>, then we expect that there is no injective linear transformation mapping <m>V</m> injectively into <m>W</m>. Both these results are easy consequences of the <xref ref="th_rank-nullity" text="custom"> rank-nullity theorem </xref>. </p>
<p>
  Before proving the theorem we give names to <m>\dim \NS T</m> and <m>\dim\im T</m>.
</p>
</introduction>
    <definition xml:id="d_rank_nullity">
      <title>Rank and nullity</title>
      <idx><h>rank</h><h>of a linear transformation</h></idx>
      <idx><h>nullity</h><h>of a linear transformation</h></idx>
      <notation>
        <usage><m>\rank T</m></usage>
        <description>the rank of <m>T</m></description>
      </notation>
      <notation>
        <usage><m>\nullity T</m></usage>
        <description>the nullity of <m>T</m></description>
      </notation>
      <statement>
        <p>
          Let <m>T\colon V\rightarrow W</m> be a linear transformation.
          <ul>
            <li>
              <p>
                The <term>rank</term> of <m>T</m>, denoted <m>\rank T</m>, is the dimension of <m>\im T</m>: <ie />,
                <me>
                \rank T=\dim\im T
                </me>.
              </p>
            </li>
            <li>
              <p>
                The <term>nullity</term> of <m>T</m>, denoted <m>\nullity T</m>, is the dimension of <m>\NS T</m>: <ie />,
                <me>
                \nullity T=\dim\NS T
                </me>.
              </p>
            </li>
          </ul>
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_rank-nullity">
      <title>Rank-nullity</title>
      <idx><h>rank-nullity theorem</h></idx>
      <statement>
        <p>
          Let <m>V</m> be a vector space of dimension <m>n</m>, and let <m>T\colon V\rightarrow W</m> be a linear transformation. Then
          <me>
          n=\dim\NS T+\dim\im T
          </me>,
          or alternatively,
          <me>
          n=\nullity T+\rank T
          </me>.

        </p>
      </statement>

    <proof>
      <p>
        Choose a basis <m>B'=\{\boldv_1, \boldv_2, \dots, \boldv_k\}</m> of <m>\NS T</m> and extend <m>B'</m> to a basis <m>B=\{\boldv_1, \boldv_2,\dots, \boldv_k,\boldv_{k+1},\dots, \boldv_n\}</m>, using <xref ref="th_basis_contract_expand"/>. Observe that <m>\dim\NS T=\nullity T=k</m> and <m>\dim V=n</m>.
      </p>
      <p>
        We claim that <m>B''=\{T(\boldv_{k+1}),T(\boldv_{k+2}),\dots, T(\boldv_{n})\}</m> is a basis of <m>\im T</m>.
      </p>
      <proof>
        <title>Proof of claim</title>
        <case>
         <title><m>B''</m> is linearly independent</title>
        <p>
          Suppose <m>a_kT(\boldv_k)+a_{k+1}T(\boldv_{k+1})+\cdots +a_nT(\boldv_n)=\boldzero</m>. Then the vector
          <m>\boldv=a_k\boldv_k+a_{k+1}\boldv_{k+1}+\cdots +a_n\boldv_n</m> satisfies <m>T(\boldv)=\boldzero</m> (using linearity of <m>T</m>), and hence <m>\boldv\in \NS T</m>. Then, using the fact that <m>B'</m> is a basis of <m>\NS T</m>, we have
          <me>
            b_1\boldv_1+b_2\boldv_2+\cdots +\boldv_k=\boldv=a_k\boldv_k+a_{k+1}\boldv_{k+1}+\cdots +a_n\boldv_n,
          </me>
          and hence
          <me>
            b_1\boldv_1+b_2\boldv_2+\cdots +\boldv_k-a_k\boldv_k-a_{k+1}\boldv_{k+1}-\cdots -a_n\boldv_n=\boldzero.
          </me>
          Since the set <m>B</m> is linearly independent, we conclude that <m>b_i=a_j=0</m> for all <m>1\leq i\leq k</m> and <m>k+1\leq j\leq n</m>. In particular, <m>a_{k+1}=a_{k+2}=\cdots=a_n=0</m>, as desired.
        </p>
        </case>
        <case>
         <title><m>B''</m> spans <m>\im T</m></title>
        <p>
        It is clear that <m>\Span B''\subseteq \im T</m> since <m>T(\boldv_i)\in \im T</m> for all <m>k+1\leq i\leq n</m> and <m>\im T</m> is closed under linear combinations.
        </p>
        <p>
          For the other direction, suppose <m>\boldw\in \im T</m>. Then there is a <m>\boldv\in V</m> such that <m>\boldw=T(\boldv)</m>. Since <m>B</m> is a basis of <m>V</m> we may write
          <me>
            \boldv=a_1\boldv_1+a_2\boldv_2+\cdots a_k\boldv_k+a_{k+1}\boldv_{k+1}+\cdots +a_n\boldv_n
          </me>,
          in which case
          <md>
            <mrow>\boldw=T(\boldv)\amp= T(a_1\boldv_1+a_2\boldv_2+\cdots a_k\boldv_k+a_{k+1}\boldv_{k+1}+\cdots +a_n\boldv_n)
            </mrow>
            <mrow> \amp=a_1T(\boldv_1)+a_2T(\boldv_2)+\cdots a_kT(\boldv_k)+a_{k+1}T(\boldv_{k+1})+\cdots +a_nT(\boldv_n)
            \amp (T \text{ is linear })
            </mrow>
            <mrow>  \amp=\boldzero +a_kT(\boldv_k)+a_{k+1}T(\boldv_{k+1})+\cdots +a_nT(\boldv_n) \amp (\boldv_i\in\NS T \text{ for } 1\leq i\leq k) </mrow>
          </md>.
          This shows that <m>\boldw=a_kT(\boldv_k)+a_{k+1}T(\boldv_{k+1})+\cdots +a_nT(\boldv_n)\in \Span B''</m>, as desired.
        </p>
        </case>
      </proof>
    Having shown <m>B''</m> is a basis for <m>\im T</m>, we conclude that <m>\dim \im T=\val{B''}=n-(k+1)+1=n-k</m>, and thus that
    <md>
      <mrow>\dim V \amp=k+(n-k) </mrow>
      <mrow> \amp=\dim\NS T+\dim \im T </mrow>
      <mrow>  \amp = \nullity T+\rank T</mrow>
    </md>.
    </proof>
</theorem>
<example>
  <title>Rank-nullity verification</title>

  <statement>
    <p>
      Verify the rank-nullity theorem for the linear transformation <m>T\colon \R^3\rightarrow \R^2</m> defined as <m>T(x,y,z)=(x+y+z,x+y+z)</m>.
    </p>
  </statement>
  <solution>
    <p>
      To verify the rank-nullity theorem, we must compute bases for <m>\NS T</m> and <m>\im T</m>. Consider first <m>\NS T</m>. We have
      <md>
        <mrow>\NS T \amp =\{(x,y,z)\colon x+y+z=0\}</mrow>
        <mrow> \amp =\{(-s-t,s,t)\colon s,t\in\R\} </mrow>
      </md>.
      Here the parametric description is obtained using our usual technique for solving systems of equations (<xref ref="th_solveSystem"/>). From the parametric description, it is clear that the set <m>B=\{(-1,1,0), (-1,0,1)\}</m> spans <m>\NS T</m>. Since <m>B</m> is clearly linearly independent, it is a basis for <m>\NS T</m>, and we conclude that <m>\dim \NS=\val{B}=2</m>. (Alternatively, the equation <m>x+y+z=0</m> defines a plane passing through the origin in <m>\R^3</m>, which we know has dimension two as a subspace.)
    </p>
    <p>
      Next it is fairly clearly that
      <m>\im T=\{(t,t)\colon t\in \R\}=\Span\{(1,1)\}</m>. Thus <m>B'=\{(1,1)\}</m> is a basis for <m>\im T</m> and <m>\dim\im T=\val{B'}=1</m>.
    </p>
    <p>
      Finally we observe that
      <me>
        \nullity T+\rank T=\dim\NS T+\dim\im T=2+1=3=\dim \R^3,
      </me>
      as predicted by the rank-nullity theorem. 
    </p>
  </solution>
</example>
  </subsection>
  <subsection xml:id="ss_fundamental_spaces">
    <title>Fundamental spaces of matrices</title>


    <definition xml:id="d_fundamental_space">
      <title>Fundamental spaces</title>
      <idx><h>fundamental space</h><h>of a matrix</h></idx>
      <idx><h>null space</h><h>of a matrix</h></idx>
      <idx><h>row space</h><h>of a matrix</h></idx>
      <idx><h>column space</h><h>of a matrix</h></idx>
      <idx><h>rank</h><h>of a matrix</h></idx>
      <idx><h>nullity</h><h>of a matrix</h></idx>
      <notation>
        <usage><m>\NS A</m></usage>
        <description>the null space of matrix <m>A</m></description>
      </notation>
      <notation>
        <usage><m>\RS A</m></usage>
        <description>the row space of a matrix <m>A</m></description>
      </notation>
      <notation>
        <usage><m>\CS A</m></usage>
        <description>the column space of a matrix <m>A</m></description>
      </notation>
      <notation>
        <usage><m>\rank A</m></usage>
        <description>the rank of a matrix <m>A</m></description>
      </notation>
      <notation>
        <usage><m>\nullity A</m></usage>
        <description>the nullity of a matrix <m>A</m></description>
      </notation>
      <statement>
        <p>
          Let <m>A</m> be a an <m>m\times n</m> matrix. Let <m>\boldr_1,\dots, \boldr_m</m> be the <m>m</m> rows of <m>A</m>, and let <m>\boldc_1,\dots \boldc_n</m> be its <m>n</m> columns. The following subspaces are called the <term>fundamental subspaces of <m>A</m></term>.
        </p>
        <ul>
          <li>
            <p>
              The <term>null space of <m>A</m></term>, denoted <m>\NS A</m> is defined as
              <me>
              \NS A =\{\boldx\in\R^n\colon A\boldx=\boldzero\}\subseteq \R^n.
              </me>.

            </p>
          </li>
          <li>
            <p>
              The <term>row space of <m>A</m></term>, denoted <m>\RS A</m>, is defined as
              <me>
              \RS A=\Span \{\boldr_1, \boldr_2, \dots, \boldr_m\}\subseteq \R^n
              </me>.

            </p>
          </li>
          <li>
            <p>
              The <term>column space of <m>A</m></term>, denoted <m>\CS A</m>, is defined as
              <me>
              \CS A=\Span \{\boldc_1, \boldc_2, \dots, \boldc_n\}\subseteq \R^m
              </me>.
            </p>
          </li>
        </ul>
        <p>
          The <term>rank</term> and <term>nullity</term> of <m>A</m>, denoted <m>\rank A</m> and <m>\nullity A</m>, respectively, are defined as <m>\rank A=\dim \CS A</m> and <m>\nullity A=\dim\NS A</m>.
        </p>
      </statement>
    </definition>
    <theorem xml:id="th_fundamental_spaces">
      <title>Fundamental spaces</title>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix. Suppose <m>A</m> is row equivalent to the row echelon matrix <m>U</m>.
        </p>
        <ol>
          <li>
            <p>
              Let <m>T_A\colon \R^n\rightarrow \R^m</m> be the matrix transformation associated to <m>A</m>. Then <m>\NS A=\NS T_A</m> and <m>\CS A=\im T_A</m>.
            </p>
          </li>
          <li>
            <p>
              We have
              <ul>
                <li>
                  <p>
                    <m>\NS A=\NS U</m>
                  </p>
                </li>
                <li>
                  <p>
                    <m>\RS A=\RS U</m>
                  </p>
                </li>
                <li>
                  <p>
                    <m>\dim\CS A=\dim \CS U</m> (but in general it is not the case that <m>\dim\CS A=\dim\CS U</m>).
                  </p>
                </li>
              </ul>
            </p>
          </li>
          <li>
            <p>
              Let <m>r </m> be the number of leading ones in <m>U</m>, and let <m>s=n-r</m>; <ie />, <m>r</m> and <m>k</m> are the number of leading and free variables, respectively, of the system corresponding to <m>\begin{amatrix}[r|r]U\amp \boldzero\end{amatrix}</m>. Then
              <md>
              <mrow>\rank A\amp =\dim\CS A=\dim \RS A=r </mrow>
              <mrow> \nullity A\amp=\dim\NS A=s </mrow>
              </md>.
            </p>
          </li>
          <li>
            <title>Rank-nullity for matrices</title>
            <p>
              We have
              <me>
              n=\dim\NS A+\dim\CS A=\dim\NS A+\dim\RS A
              </me>.
            </p>
          </li>
        </ol>
      </statement>
    </theorem>
<proof>
  <p>
    We prove each statement in turn.
  </p>
  <proof>
    <title>Proof of (1)</title>
    <p>
      Recall that <m>T_A</m> is defined as <m>T_A(\boldx)=A\boldx</m> for all <m>\boldx\in \mathbb{R}^n </m>. Then clearly <m>T_A(\boldx)=\boldzero</m> if and only if <m>A\boldx=\boldzero</m>, showing that <m>\NS T=\NS A</m>.
    </p>
    <p>
      Let <m>\boldc_1, \boldc_2,\dots, \boldc_n</m> be the columns of <m>A</m>. We have
      <md>
        <mrow> \boldy\in \im T_A \amp\iff \boldy=T_A(\boldx) \text{ for some } \boldx\in \mathbb{R}^n </mrow>
        <mrow> \amp\iff  \boldy=A \boldx \text{ for some } \boldx=(x_1,x_2,\dots, x_n)\in \mathbb{R}^n
        \amp (\text{definition of } T_A) </mrow>
        <mrow>  \amp \iff \boldy=x_1\boldc_1+x_2\boldc_2+\cdots +x_n\boldc_n \text{ for some } x_i\in \mathbb{\R} \amp (<xref ref="th_column_method"/>)</mrow>
        <mrow>  \amp \iff \boldy\in\CS A</mrow>
      </md>.
      Thus <m>\im T_A=\CS A</m>.
    </p>
  </proof>
  <proof>
    <title>Proof of (2)</title>
    <p>
    First observe that <m>A=\begin{bmatrix}1\amp 1\\ 1\amp 1 \end{bmatrix}</m> is row equivalent to <m>U=\begin{bmatrix}1\amp 1\\ 0 \amp 0\end{bmatrix}</m> and yet <m>\CS A=\Span\{(1,1)\}=\{(t,t)\colon t\in \R\}\ne \CS U=\Span\{(1,0)\}=\{(t,0)\colon t\in\R\}</m>. Thus we do not have <m>\CS A=\CS U</m> in general.
  </p>
  <p> Next, we show more generally that if <m>A</m> is row equivalent to <m>B</m>, then <m>\NS A=\NS B</m>, <m>\RS A=\RS B</m>, and <m>\dim\CS A=\dim \CS B</m>.
</p>
<p>
  Assume that <m>A</m> is row equivalent to <m>B</m>. Using the formulation of row reduction in terms of multiplication by elementary matrices, we see that there is an invertible matrix <m>Q</m> such that <m>B=QA</m>, and hence also <m>A=Q^{-1}B</m>. Then:
  <md>
    <mrow> \boldx\in\NS A\amp\iff A\boldx=\boldzero </mrow>
    <mrow> \amp\iff QA\boldx=Q\boldzero \amp (<xref ref="th_inverse_cancel"/>) </mrow>
    <mrow> \amp \iff B\boldx=\boldzero </mrow>
    <mrow>  \amp iff \boldx\in\NS B</mrow>
  </md> .
  This proves <m>\NS A=\NS B</m>.
</p>
<p>
  Next, by (1) we have <m>\NS A=\NS T_A</m>, <m>\CS A=\im T_A</m> and <m>\NS B=\NS T_B</m>, <m>\CS B=\im T_B</m>. Then
  <md>
    <mrow>\dim \CS A \amp=\dim\im T_A </mrow>
    <mrow> \amp=n-\dim\NS A \amp (<xref ref="th_rank-nullity"/>) </mrow>
    <mrow>  \amp=n-\dim NS B \amp (\NS A=\NS B)</mrow>
    <mrow>  \amp =\dim\im T_B \amp ( <xref ref="th_rank-nullity"/></mrow>
    <mrow>  \amp =\dim \CS B</mrow>
  </md>.
</p>
<p>
Lastly, we turn to the row spaces. We will show that each row of <m>B</m> is an element of <m>\RS A</m>, from whence it follows that <m>\RS B\subseteq \RS A</m>. Let <m>\boldr_i</m> be the <m>i</m>-th row of <m>B</m>, and let <m>\boldq_i</m> be the <m>i</m>-th column of <m>Q</m>. By <xref ref="th_row_method"/>, we have <m>\boldr_i=\boldq_i A</m>, and furthermore, <m>\boldq_i A</m> is the linear combination of the rows of <m>A</m> whose coefficients come from the entries of <m>\boldq_i</m>. Thus <m>\boldr_i\in\RS A</m>, as desired.
</p>
<p>
Having shown that <m>\RS B\subseteq \RS A</m>, we see that the same argument works <em>mutatis mutandis</em> (swapping the roles of <m>A</m> and <m>B</m> and using <m>Q^{-1}</m> in place of <m>Q</m>) to show that <m>\RS A\subseteq \RS B</m>. We conclude that <m>\RS A=\RS B</m>.
</p>
  </proof>
<proof>
  <title>Proof of (3)</title>
  <p>
   By (2) we know that <m>\NS A=\NS U, \RS A=\RS U</m>, and <m>\dim\CS A=\dim\CS U</m>. So it is enough to show that <m>\dim \NS U=\dim\RS U=r</m> and <m>\dim \NS U=s</m>.
  </p>
  <p>
    First, we will show that the <m>r</m> nonzero rows of <m>U</m> form a basis for <m>\RS U</m>, proving <m>\dim\RS U=r</m>. Clearly the nonzero rows span <m>\RS U</m>, since any linear combination of all the rows of <m>U</m> can be expressed as a linear combination of the nonzero rows. Furthermore, since <m>U</m> is in row echelon form, the staircase pattern of the leading ones appearing in the nonzero rows assures that these row vectors are linearly independent.
  </p>
  <p>
    Next, we show that the columns of <m>U</m> containing leading ones form a basis of <m>\CS U</m>.
      Let <m>\boldu_{i_1},\dots, \boldu_{i_r}</m> be the columns of <m>U</m> with leading ones, and let
      <m>\boldu_{j_1}, \boldu_{j_2}, \dots, \boldu_{j_s}</m> be the columns without leading ones.
      To prove the <m>\boldu_{i_k}</m> form a basis for <m>\CS U</m>, we will show that given any
      <m>\boldy\in \CS U</m> there is a <em>unique</em>
      choice of scalars <m>c_1, c_2,\dots,
      c_r</m> such that <m>c_1\boldu_{i_1}+\cdots +c_r\boldu_{i_r}=\boldy</m>.
      (Recall that the uniqueness of this choice implies linear independence.)
    </p>
    <p>
      So assume <m>\boldy\in \CS U</m>.
      Then we can find <m>\boldx\in\R^n</m> such that <m>U\boldx=\boldy</m>,
      which means the linear system with augmented matrix <m>[\ U\ \vert \ \boldy]</m> is consistent.
      Using our Gaussian elimination theory (specifically, <xref ref="th_solveSystem"/>),
      we know that the solutions
      <m>\boldx=(x_1,x_2,\dots,
      x_n)</m> to this system are in 1-1 correspondence with choices for the free variables <m>x_{j_1}=t_{j_1}, x_{j_2}=t_{j_2}, \dots,
      x_{j_s}=t_{j_s}</m>.
      (Remember that the columns
      <m>\boldu_{j_k}</m> without leading ones correspond to the free variables.)
      In particular, there is a unique solution to
      <m>U\boldx=\boldy</m> where we set all the free variables equal to 0.
      By the column method (<xref ref="th_column_method"/>),
      this gives us a unique linear combination of only the columns
      <m>\boldu_{i_k}</m> with leading ones equal to <m>\boldy</m>.
      This proves the claim, and shows that the columns with leading ones form a basis for <m>\CS U</m>. We conclude that <m>\dim\CS U=r</m>.
  </p>
  <p>
    Lastly,  again using the results of (1), we have
    <md>
      <mrow>\dim NS U \amp =\dim NS T_U </mrow>
      <mrow> \amp =n-\dim\im T_U \amp (<xref ref="th_rank-nullity"/>) </mrow>
      <mrow>  \amp =n-\dim\CS U</mrow>
      <mrow>  \amp =n-r </mrow>
      <mrow>  \amp =s </mrow>
    </md>,
    where the last equality uses the fact that the sum of the number of columns with leading ones (<m>r</m>) and the number of columns without leading ones (<m>s</m>) is <m>n</m>, the total number of columns.
  </p>
</proof>
<proof>
  <title>Proof of (4)</title>
  <p>
    We have
    <md>
      <mrow> n \amp =\dim\NS T_A+\dim\im T_A \amp (<xref ref="th_rank-nullity"/>)</mrow>
      <mrow> \amp =\dim\NS A+\dim\CS A \amp (\text{by (1)})\</mrow>
      <mrow>  \amp =\dim\NS A+\dim\RS A \amp (\text{ by (3)})</mrow>
    </md>.
  </p>
</proof>

</proof>


    <algorithm xml:id="proc_fund_spaces">
      <title>Computing bases of fundamental spaces</title>
      <statement>
        <p>
          To compute bases for the fundamental spaces of an <m>m\times n</m> matrix <m>A</m>, proceed as follow.
        </p>
        <ol>
          <li>
            <p>
              Row reduce <m>A</m> to a matrix <m>U</m> in row echelon form.
            </p>
          </li>
          <li>
            <p>
              We have <m>\NS A=\NS U</m>. Compute a parametric description of the solutions to the linear system <m>U\boldx=\boldzero</m> following <xref ref="th_solveSystem"/>. If the free variables are <m>t_1, t_2, \dots, t_k </m>, a basis <m>B=\{\boldv_1, \boldv_2, \dots, \boldv_k\}</m> of <m>\NS A</m> is obtained by letting <m>\boldv_i</m> be the solution corresponding to the choice <m>t_i=1</m> and <m>t_j=0</m> for <m>j\ne i</m>.
            </p>
          </li>
          <li>
            <p>
              We have <m>\RS A=\RS U</m>. The set of nonzero rows of <m>U</m> is a basis for <m>\RS A</m>.
            </p>
          </li>
          <li>
            <p>
              In general <m>\CS A\ne \CS U</m>. However, the columns of <m>U</m> containing leading ones form a basis of <m>\CS U</m>, and the <em>corresponding columns</em> of <m>A</m> form a basis for <m>\CS A</m>.
            </p>
          </li>
        </ol>
      </statement>
    </algorithm>
    <p>
      <xref ref="th_fundamental_spaces"/> allows us to add seven more equivalent statements to our invertibility theorem, bringing us to a total of fourteen! We have tried to sandwich in the new statements in places where the equivalence with a neighbor is fairly straightforward to show, or else already established.
    </p>
    <theorem xml:id="th_invertibility_supersized">
      <title>Invertibility theorem (supersized)</title>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> matrix.
          The following statements are equivalent.
        </p>
        <ol>
          <li>
            <p>
              <m>A</m> is invertible.
            </p>
          </li>

          <li>
            <p>
              The matrix equation <me>A\underset{n\times 1}{\boldx}=\underset{n\times 1}{\boldb}</me> has a <em>unique solution</em> for <em>any</em> column vector  <m>\boldb</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\NS A=\{\boldzero\}</m>
            </p>
          </li>
          <li>
            <p>
              <m>\nullity A=0</m>
            </p>
          </li>
          <li>
            <p>
              <m>\rank A=0</m>
            </p>
          </li>
          <li>
            <p>
              <m>\RS A=\R^n</m>
            </p>
          </li>
          <li>
            <p>
              <m>\CS A=\R^n</m>
            </p>
          </li>
          <li>
            <p>
              The matrix equation <me>A\underset{n\times 1}{\boldx}=\underset{n\times 1}{\boldb}</me> has a solution for <em>any</em> column vector  <m>\boldb</m>.
            </p>
          </li>
          <li>
            <p>
              The matrix equation <me>A\underset{n\times 1}{\boldx}=\underset{n\times 1}{\boldzero}</me> has a <em>unique solution</em>: namely, <m>\boldx=\boldzero_{n\times 1}</m>.
            </p>
          </li>
          <li>
            <p>
              Any of the following equivalent conditions about the set <m>S</m> of <em>columns</em> of <m>A</m> hold: <m>S</m> is a basis of <m>\R^n</m>; <m>S</m> spans <m>\R^n</m>; <m>S</m> is linearly independent.
            </p>
          </li>
          <li>
            <p>
              Any of the following equivalent conditions about the set <m>S</m> of <em>rows</em> of <m>A</m> hold: <m>S</m> is a basis of <m>\R^n</m>; <m>S</m> spans <m>\R^n</m>; <m>S</m> is linearly independent.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is row equivalent to <m>I_n</m>,
              the <m>n\times n</m> identity matrix.
            </p>
          </li>
          <li>
            <p>
              <m>A</m> is a product of elementary matrices.
            </p>
          </li>
          <li>
            <p>
              <m>\det A\ne 0</m>.
            </p>
          </li>
        </ol>
      </statement>
    </theorem>
  </subsection>
  <subsection xml:id="ss_expand_contract">
    <title>Contracting and expanding to bases</title>
    <p>
      Thanks to <xref ref="th_basis_dimension"/> we know that spanning sets can be contracted to bases, and linearly independent sets can be extended to bases; and we have already seen a few instances where this result has been put to good use. However, neither the theorem nor its proof provide a practical means of performing this contraction or extension. We would like a systematic way of determining which vectors to throw out (when contracting), or which vectors to chuck in (when extending). In the special case where <m>V=\R^n</m> for some <m>n</m>, we can adapt <xref ref="proc_fund_spaces"/> to our needs.
    </p>
    <algorithm xml:id="proc_contract_extend">
      <title>Contracting and extending to bases of <m>\R^n</m></title>
      <statement>
        <p>
          Let <m>S=\{\boldv_1, \boldv_2,\dots, \boldv_r\}\subseteq \R^n</m>.
        </p>
        <dl>
          <li>
            <title>Contracting to a basis</title>
            <p>
              Assume <m>S</m> spans <m>\R^n</m>. To contract <m>S</m> to a basis <m>B\subseteq S</m>, proceed as follows.
            </p>
            <ol>
              <li>
                <p>
                  Let <m>A</m> be the <m>n\times r</m> matrix whose <m>j</m>-th column is given by <m>\boldv_j</m> for all <m>1\leq j\leq r</m>.
                </p>
              </li>
              <li>
                <p>
                  Use the column space procedure (<xref ref="proc_fund_spaces" text="global"/>) to compute a basis <m>B</m> of <m>\CS A</m>, chosen from among the original columns of <m>A</m>.
                </p>
              </li>
              <li>
                <p>
                  The subset <m>B\subseteq S</m> is a basis for <m>\R^n</m>.
                </p>
              </li>
            </ol>
          </li>
          <li>
            <title>Extending to a basis</title>
            <p>
              Assume <m>S</m> is linearly independent. To extend <m>S</m> to a basis <m>B</m> of <m>\R^n</m> proceed as follows.
            </p>
            <ol>
              <li>
                <p>
                  Let <m>A</m> be the <m>n\times (r+n)</m> matrix whose first <m>r</m> columns are the elements of <m>S</m>, and whose remaining <m>n</m> columns consist of <m>\bolde_1, \bolde_2, \dots, \bolde_n</m>, the standard basis elements of <m>\R^n</m>.
                </p>
              </li>
              <li>
                <p>
                  Use the column space procedure (<xref ref="proc_fund_spaces" text="global"/>) to compute a basis <m>B</m> of <m>\CS A</m>, chosen from among the original columns of <m>A</m>.
                </p>
              </li>
              <li>
                <p>
                  The set <m>B</m> is a basis for <m>\R^n</m> containing <m>S</m>.
                </p>
              </li>
            </ol>
          </li>
        </dl>
      </statement>
      <proof>
        <p>
          Let's see why in both cases the procedure produces a basis of <m>\R^n</m> that is either a sub- or superset of <m>S</m>.
        </p>
        <case>
          <title>Contracting to a basis</title>
          <p>
            In this case we have <m>\CS A=\Span S=\R^n</m>. Thus <m>B</m> is a basis for <m>\R^n</m>. Since the column space procedure selects columns <em>from among</em> the original columns of <m>A</m>, we have <m>B\subseteq S</m>, as desired.
          </p>
        </case>
        <case>
          <title>Extending to a basis</title>
          <p>
            Since <m>\CS A</m> contains <m>\bolde_j</m> for all <m>1\leq j\leq n</m>, we have <m>\CS A=\R^n</m>. Thus <m>B</m> is a basis for <m>\R^n</m>. Since the first <m>r</m> columns of <m>A</m> are linearly independent (they are the elements of <m>S</m>), when we row reduce <m>A</m> to a matrix <m>U</m> in row echelon form, the first <m>r</m> columns of <m>U</m> will contain leading ones. (To see this, imagine row reducing the <m>n\times r</m> submatrix <m>A'</m> consisting of the first <m>r</m> columns of <m>A</m> to a row echelon matrix <m>U'</m>. Since these columns are linearly independent, they already form a basis for <m>\CS A'</m>. Thus the corresponding colmns of <m>U'</m> must all have leading ones. )
            It follows that the first <m>r</m> columns of <m>A</m> are selected to be in the basis <m>B</m>, and hence that <m>S\subseteq B</m>, as desired.
          </p>
        </case>
      </proof>

    </algorithm>
  </subsection>
  <xi:include href="./s_rank_nullity_ex.ptx"/>
</section>
