<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_transformation">
  <title>Linear transformations</title>
  <introduction>
    <p>
    As detailed in <xref ref="d_linear_transform"/> a <em>linear transformation</em> is a special type of function between two vector spaces: one that <em>respects</em> in some sense the vector operations of both spaces.
    </p>
    <p>
      This manner of theorizing is typical in mathematics:
      first we introduce a special class of objects defined axiomatically, then we introduce special functions or maps between these objects. Since the original objects of study (e.g. vector spaces) come equipped with special structural properties (e.g. vector operations), the functions we wish to study are the ones that somehow acknowledge this structure.
    </p>
    <p>
      You have already seen this principle at work in your study of calculus.
      First we give <m>\R</m> some structure by defining a notion of proximity (i.e., <m>x</m> is close to <m>y</m> if <m>\val{x-y}</m> is small),
      then we introduce a special family of functions that somehow respects this structure:
      these are precisely the <em>continuous</em> functions!
    </p>
    <p>
      As you will see, linear transformations are not just interesting objects of study in their own right, they also serve as invaluable tools in our continued exploration of the intrinsic properties of vector spaces.
    </p>
    <p>
      In the meantime rejoice in the fact that we can now give a succinct definition of linear algebra:
      it is the theory of vector spaces and the linear transformations between them. Go shout it from the rooftops!
    </p>
  </introduction>
  <subsection xml:id="ss_linear_transform">
    <title>Linear transformations</title>
    <p>
       First and foremost, a linear transformation is a function. Before continuing on in this section, you may want to reacquaint yourself with the basic function concepts and notation outlined in <xref ref="s_functions">Section</xref>.
    </p>
     
  <definition xml:id="d_linear_transform">
    <title>Linear transformations</title>

    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
        A function <m>T\colon V\rightarrow W</m> is a <term>linear transformation</term> (or <term>linear</term>)
        if it satisfies the following properties:
        <ol marker="i">
          <li>
            <p>
              For all <m>\boldv_1, \boldv_2\in V</m>, we have <m>T(\boldv_1+\boldv_2)=T(\boldv_1)+T(\boldv_2)</m>.
            </p>
          </li>
          <li>
            <p> For all <m>c\in \R</m> and <m>\boldv\in V</m> we have
              <m>T(c\boldv)=cT(\boldv)</m>.
            </p>
          </li>
        </ol>
      </p>
      <p>
        If a function between vector spaces is <term>nonlinear</term> if it is not a linear transformation.
      </p>
    </statement>
  </definition>
  <remark>
    <p>
      How precisely does a linear transformation <q>respect</q> vector space structure?
      In plain English: the image of a sum is the sum of the images,
      and the image of a scalar multiple is the scalar multiple of the image.
    </p>
</remark>
  <p>
    As our first examples of linear transformations, we define the <em>zero transformation</em> and <em>identity transformation</em> on a vector space.
  </p>
  <definition xml:id="d_transform_zero_identity">
    <title>Zero and identity transformation</title>
    <idx><h>linear transformation</h><h>zero transformation</h></idx>
    <idx><h>linear transformation</h><h>identity transformation</h></idx>
    <statement>
        <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
      </p>
      <p>
        The <term>zero transformation from <m>V</m> to <m>W</m></term>, denoted <m>T_0</m>, is defined as follows:
        <md>
          <mrow>T_0\colon V \amp\rightarrow W  </mrow>
          <mrow> \boldv \amp\mapsto T_0(\boldv)=\boldzero_W</mrow>
        </md>,
        where <m>\boldzero_W</m> is the zero vector of <m>W</m>. In other words, <m>T_0</m> is the function that maps all elements of <m>V</m> to the zero vector of <m>W</m>.
      </p>
      <p>
        The <term>identity transformation of <m>V</m></term>, denoted <m>\id_V</m>, is defined as follows:
        <md>
          <mrow> \id_V\colon V \amp\rightarrow V </mrow>
          <mrow> \boldv \amp\mapsto \id_V(\boldv)=\boldv</mrow>
        </md>.
        In other words, <m>\id_V(\boldv)=\boldv</m> for all <m>\boldv\in V</m>.
      </p>
    </statement>
  </definition>
  <example xml:id="rm_transform_zero_identity">
  <title>Model of linear transformation proof</title>
    <p>
    Let's show that the zero and identity functions are indeed linear transformations.
    </p>
    <p>
    Let <m>V</m> and <m>W</m> be vector spaces, and let <m>T_0\colon V\rightarrow W</m> be the zero function. We verify each defining property separately.
    <ol marker="i">
      <li>
        <p>
          Given <m>\boldv, \boldw\in V</m>, we have
          <md>
            <mrow>T_0(\boldv_1+\boldv_2)\amp =\boldzero_W  \amp (\text{by def.}) </mrow>
            <mrow> \amp =\boldzero_W+\boldzero_W </mrow>
            <mrow>  \amp = T_0(\boldv_1)+T_0(\boldv_2) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          Given <m>c\in \R</m> and <m>\boldv\in V</m>, we have
          <md>
            <mrow>T_0(c\boldv) \amp = \boldzero_W \amp (\text{by def.})</mrow>
            <mrow> \amp = c\boldzero_W \amp (<xref ref="th_vectorspace_props" text="global"/>) </mrow>
            <mrow>  \amp = cT_0(\boldv) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
    </ol>
    This proves that <m>T_0\colon V\rightarrow W</m> is a linear transformation.
    </p>
    <p>
    Now let <m>V</m> be a vector spaces, and let <m>\id_V\colon V\rightarrow V</m> be the identity function.
    <ol marker="i">
      <li>
        <p>
          Given <m>\boldv, \boldw\in V</m>, we have
          <md>
            <mrow>\id_V(\boldv_1+\boldv_2)\amp =\boldv_1+\boldv_2  \amp (\text{by def.})</mrow>
            <mrow> \amp = \id_V(\boldv_1)+\id_V(\boldv_2) \amp (\text{by def.})</mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          Given <m>c\in \R</m> and <m>\boldv\in V</m>, we have
          <md>
            <mrow>\id_V(c\boldv) \amp = c\boldv \amp (\text{by def.})</mrow>
            <mrow> \amp = c\id_V(\boldv) \amp (\text{by def.}) </mrow>
          </md>.
        </p>
      </li>
    </ol>
    This proves that <m>\id_V\colon V\rightarrow V</m> is a linear transformation.
    </p>
</example>
<theorem xml:id="th_transform_basic_props">
  <title>Basic properties of linear transformations</title>
  <statement>
    <p>
      Let <m>T\colon V\rightarrow W</m> be a linear transformation. Let <m>\boldzero_V</m> and <m>\boldzero_W</m> be the zero vectors of <m>V</m> and <m>W</m>, respectively.
    <ol>
      <li>
        <p>
          We have <m>T(\boldzero_V)=\boldzero_W</m>.
        </p>
      </li>
      <li>
        <p>
          For all <m>\boldv\in V</m>, we have <m>T(-\boldv)=-T(\boldv)</m>.
        </p>
      </li>
      <li>
        <p>
          For any linear combination
          <me>\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n\in V</me>
          we have
          <me>
            T(\boldv)=T(c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n)=c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots +c_nT(\boldv_n)
          </me>.
        </p>
      </li>
    </ol>
    </p>
  </statement>
  <proof>
    <p>
      <ol>
        <li>
          <p>
            We employ some similar trickery to what was done in the proof of <xref ref="th_vectorspace_props"/>. Assuming <m>T</m> is linear:
            <md>
              <mrow>T(\boldzero_V)  \amp= T(\boldzero_V+\boldzero_V)</mrow>
              <mrow> \amp =T(\boldzero_V)+T(\boldzero_V) \amp (<xref ref="d_linear_transform"/>) </mrow>
            </md>.
            Thus, whatever <m>T(\boldzero_V)\in W</m> may be, it satisfies
            <me>
              T(\boldzero_V)=T(\boldzero_V)+T(\boldzero_V)
            </me>.
            Canceling <m>T(\boldzero_V)</m> on both sides using <m>-T(\boldzero_V)</m>, we conclude
            <me>
              \boldzero_W=T(\boldzero_V)
            </me>.
          </p>
        </li>
        <li>
          <p>
            The argument is similar:
            <md>
              <mrow>\boldzero_W \amp= T(\boldzero_V) \amp (\text{by (1)})</mrow>
              <mrow> \amp =T(-\boldv+\boldv)</mrow>
              <mrow>  \amp = T(-\boldv)+T(\boldv)</mrow>
            </md>.
            Since <m>\boldzero_W=T(-\boldv)+T(\boldv)</m>, adding <m>-T(\boldv)</m> to both sides of the equation yields
            <me>
              -T(\boldv)=T(-\boldv)
            </me>.

          </p>
        </li>
        <li>
          <p>
            This is an easy proof by induction using the two defining properties of a linear transformation in tandem.
          </p>
        </li>
      </ol>
    </p>
  </proof>
</theorem>
  <remark xml:id="rm_transform_dist">
  <p>
  Statement (3) of <xref ref="th_transform_basic_props"/> provides a more algebraic interpretation of how linear transformations preserve vector space structure: namely, they <em>distribute</em> over linear combinations of vectors.
  </p>
</remark>

    <algorithm xml:id="proc_transform_onestep">
  <title>The one-step linear transformation proof</title>
  <statement>
    <p>
    As a sort of converse to (3) in <xref ref="th_transform_basic_props"/>, observe that if <m>T\colon V\rightarrow W</m> satisfies
    <me>
      T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)
    </me>
    for all <m>c, d\in \R</m> and <m>\boldv_1,\boldv_2\in V</m>, and <m>\boldw\in W</m>, then <m>T</m> is linear. Indeed, taking the special case <m>c=d=1</m> yields (1) of <xref ref="d_linear_transform"/>; and choosing <m>d=0</m> yields (2) of <xref ref="d_linear_transform"/>.
    </p>
    <p>
      This observation gives rise to an alternate one-step technique for proving linearity: given <m>T\colon V\rightarrow W</m>, show that
      <me>
        T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)
      </me>
      for all <m>c, d\in \R</m> and <m>\boldv_1,\boldv_2\in V</m>.
    </p>
  </statement>
</algorithm>

</subsection>

<subsection xml:id="ss_matrix_transforms">
  <title>Matrix transformations</title>
  <p>
    We now describe what turns out to be an entire family of examples of linear transformations: so-called <em>matrix transformations</em> of the form <m>T_A\colon \R^n\rightarrow \R^m</m>, where <m>A</m> is a given <m>m\times n</m> matrix. This is a good place to recall the <xref ref="princ_matrix_mantra" text='custom'> matrix mantra </xref>. Not only can a matrix represent a system of linear equations, it can represent a linear transformation. These are two very different concepts, and the <xref ref="princ_matrix_mantra" text='custom'> matrix mantra </xref> helps us to not confuse the two. In the end a matrix is just a matrix: a mathematical tool that can be employed to diverse ends. Observe that the definition of matrix multiplication marks the first point where <xref ref="declaration_tuples_columns"/> comes into play.
  </p>
  <definition xml:id="d_matrix_transform">
    <title>Matrix transformations</title>
    <idx><h>matrix transformation</h></idx>
    <notation>
      <usage><m>T_A</m></usage>
      <description>the matrix transformation associated to <m>A</m></description>
    </notation>
    <statement>
      <p>
      Let <m>A</m> be an <m>m\times n</m> matrix. The <term>matrix transformation associated to <m>A</m></term> is the function <m>T_A</m> defined as follows:
      <md>
        <mrow>T_A\colon \R^n \amp\rightarrow \R^m </mrow>
        <mrow> \boldx\amp\mapsto T_A(\boldx)=A\boldx </mrow>
      </md>.
      In other words, given input <m>\boldx\in \R^n</m>, the output <m>T(\boldx)</m> is defined as <m>A\boldx</m>.
      </p>
    </statement>
  </definition>
  <theorem xml:id="th_matrix_transform_i">
    <title>Matrix transformations I</title>
    <statement>
      <p>
        Let <m>A</m> be an <m>m\times n</m> matrix. The function <m>T_A\colon \R^n\rightarrow \R^m</m> is a linear transformation.
      </p>
    </statement>
    <proof>
      <p>
        We use the one-step technique. For any <m>c,d\in \R</m> and <m>\boldx_1, \boldx_2\in \R^n</m>, we have
        <md>
          <mrow>T_A(c\boldx_1+d\boldx_2) \amp =A(c\boldx_1+d\boldx_2)</mrow>
          <mrow> \amp =A(c\boldx_1)+A(d\boldx_2) \amp (<xref ref="th_matrix_alg_props"/>) </mrow>
          <mrow>  \amp =cA\boldx_1+dA\boldx_2 \amp (<xref ref="th_matrix_alg_props"/>)</mrow>
          <mrow>  \amp =cT_A(\boldx_1)+dT_A(\boldx_2)</mrow>
        </md>.
        This proves <m>T_A</m> is a linear transformation.
      </p>
    </proof>

  </theorem>
    <remark xml:id="rm_matrix_transform">
  <p>
    As the title of <xref ref="th_matrix_transform_i"/> suggests, there is a follow-up result (<xref ref="cor_matrix_transformations"/>), and this states that in fact <em>any</em> linear transformation <m>T\colon\R^n\rightarrow\R^m </m> is of the form <m>T=T_A</m> for some <m>m\times n</m> matrix <m>A</m>. In other words, all linear transformations from <m>\R^n</m> to <m>\R^m</m> are matrix transformations.
  </p>
  <p>
    As general as these two results are, mark well the <em>restriction</em> that remains: they apply only to functions with domain and codomain equal to a vector spaces of tuples. They say nothing for example about functions from <m>\R^\infty</m> to <m>F([0,1],\R)</m>.
  </p>
</remark>
    <remark xml:id="rm_matrix_transform_example">
  <p>
  <xref ref="th_matrix_transform_i"/> gives rise to an alternative technique for showing a function <m>T\colon \R^n\rightarrow \R^m</m> is a linear transformation: show that <m>T=T_A</m> for some matrix <m>A</m>.
  </p>
  <p>
    As an example, consider the function
    <md>
      <mrow>T\colon \R^2 \amp\rightarrow  \R^3 </mrow>
      <mrow> (x,y)\amp\mapsto (7x+2y, -y, x) </mrow>
    </md>.
  Conflating tuples with column vectors as described in <xref ref="d_matrix_transform"/> we see that <m>T=T_A</m> where
  <me>
    A=\begin{amatrix}[rr]7\amp 2\\ 0\amp -1\\ 1\amp 0  \end{amatrix}
  </me>.
  In other words, the original formula is just a description in terms of tuples of the function
  <me>
    \begin{amatrix}[c]x\\ y  \end{amatrix}\mapsto A\begin{amatrix}[c]x\\ y  \end{amatrix}=\begin{amatrix}[c]7x+2y\\ -y\\ x  \end{amatrix}
  </me>.
  It follows from <xref ref="th_matrix_transform_i"/> that <m>T=T_A</m> is linear.
  </p>
</remark>
</subsection>
<subsection xml:id="ss_rotations_reflections">
  <title>Rotations and reflections in the plane</title>
<p>
  We now introduce two very important geometric examples of linear transformations: rotations and reflections in the plane. These operations are described in detail below; we use <xref ref="th_matrix_transform_i"/> to prove these operations are in fact linear transformations.
 </p>
<definition xml:id="d_rotation">
  <title>Rotation in the plane</title>
  <idx><h>rotation</h><h>as linear transformation</h></idx>
  <notation>
    <usage><m>\rho_\alpha</m></usage>
    <description>rotation by <m>\alpha</m> in the plane</description>
  </notation>
  <statement>
    <p>
        Fix an angle <m>\alpha</m> and define
        <me>\rho_\alpha\colon \R^2\rightarrow \R^2
        </me>
        to be the function that takes an input <m>P=(x,y)</m>, considered as a point in the <m>xy</m>-plane, and returns the output <m>P'=(x',y')</m> obtained rotating <m>P</m> by an angle of <m>\alpha</m> about the origin.
        The function <m>\rho_\alpha</m> is called <term>rotation about the origin </term> by the angle <m>\alpha</m>.
      </p>
      <p>
      We can extract a formula from the rule defining <m>\rho_\alpha</m> by using polar coordinates: if <m>P</m> has polar coordinates <m>(r,\theta)</m>, then <m>P'=\rho_\alpha(P)</m> has polar coordinates <m>(r,\theta+\alpha)</m>.
      </p>
  </statement>
</definition>
<theorem xml:id="th_transform_rotation">
  <title>Rotation is a linear transformation</title>
  <statement>
    <p>
    Fix an angle <m>\alpha</m>. The rotation function <m>\rho_{\alpha}\colon \R^2\rightarrow \R^2</m> is a linear transformation. In fact, we have <m>\rho_\alpha=T_A</m>, where
    <me>
      A=\begin{amatrix}[rr]
        \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
    \end{amatrix}
    </me>.
    </p>
  </statement>
  <proof>
    <p>
      By  <xref ref="rm_matrix_transform_example"/>, we need only show that <m>\rho_\alpha=T_A</m> for the matrix indicated.
    </p>
    <p>
    If the point <m>P=(x,y)</m> has polar coordinates <m>(r,\theta)</m> (so that <m>x=r\cos\theta</m> and <m>y=r\sin\theta</m>), then its image <m>P'=\rho_{\alpha}(P)</m> under our rotation has polar coordinates <m>(r,\theta+\alpha)</m>. Translating back to rectangular coordinates, we see that
    <md>
      <mrow> \rho_\alpha(P)\amp= P' </mrow>
      <mrow>  \amp =\left(r\cos(\theta+\alpha),r\sin(\theta+\alpha)\right)</mrow>
      <mrow> \amp =(r\cos\theta\cos\alpha-r\sin\theta\sin\alpha, r\sin\theta\cos\alpha+r\cos\theta\sin\alpha)
      \amp (\text{trig. identities}) </mrow>
      <mrow>  \amp=(\cos\alpha\, x-\sin\alpha\, y, \sin\alpha\, x+\cos\alpha\, y)
      \amp (\text{since } x=r\cos\theta, y=r\sin\theta) </mrow>
    </md>.
    It follows that <m>\rho_{\alpha}=T_A</m>, where
    <me>
      A=\begin{amatrix}[rr]
        \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
    \end{amatrix}
    </me>,
    as claimed.
    </p>
  </proof>
</theorem>
    <remark xml:id="rm_rotation_matrix">
  <p>
      Observe that it is not at all obvious geometrically that the rotation operation is <em>linear</em>: <ie />, that it preserves addition and scalar multiplication of vectors in <m>\R^2</m>. Indeed, our proof does not even show this directly, but instead first gives a matrix formula for rotation and then uses <xref ref="th_transform_rotation"/>.
    </p>
    <p>
    Since matrices of the form
    <me>
    \begin{amatrix}[rr]
      \cos\alpha\amp -\sin\alpha\\ \sin\alpha \amp \cos\alpha
  \end{amatrix}
    </me>
    can be understood as defining rotations of the plane, we call them <em>rotation matrices</em>.
    </p>
</remark>
<example>
  <title>Rotation matrices</title>
  <statement>
    <p>
      Find formulas for <m>\rho_\pi\colon \R^2\rightarrow \R^2</m> and <m>\rho_{2\pi/3}\colon \R^2\rightarrow \R^2</m>, expressing your answer in terms of pairs (as opposed to column vectors).
    </p>
  </statement>
  <solution>
    <p>
    The rotation matrix corresponding to <m>\alpha=\pi</m> is
    <me>
      A=\begin{amatrix}[rr]\cos\pi\amp -\sin\pi\\ \sin\pi \amp \cos\pi  \end{amatrix}=
      \begin{amatrix}[rr]-1\amp 0\\ 0 \amp -1  \end{amatrix}
    </me>.
    Thus <m>\rho_\pi=T_A</m> has formula
    <me>
      \rho_{\pi}(x,y)=(-x,-y)=-(x,y)
    </me>.
    Note: this is as expected! Rotating by 180 degrees produces the inverse vector.
    </p>
    <p>
    The rotation matrix corresponding to <m>\alpha=2\pi/3</m> is
    <me>
      B=\begin{amatrix}[rr]\cos(2\pi/3)\amp -\sin(2\pi/3)\\ \sin(2\pi/3) \amp \cos(2\pi/3)  \end{amatrix}=
      \begin{amatrix}[rr]-\frac{1}{2}\amp -\frac{\sqrt{3}}{2}\\ \frac{\sqrt{3}}{2} \amp -\frac{1}{2}  \end{amatrix}
    </me>.
    Thus <m>\rho_{2\pi/3}=T_B</m> has formula
    <me>
      \rho_{2\pi/3}(x,y)=\frac{1}{2}(-x-\sqrt{3}y, \sqrt{3}x-y)
    </me>.
    Let's check our formula for <m>\rho_{2\pi/3}</m>for the vectors <m>(1,0)</m> and <m>(0,1)</m>:
    <md>
      <mrow>\rho_{2\pi/3}(1,0) \amp =(-1/2, \sqrt{3}/2) </mrow>
      <mrow>\rho_{2\pi/3}(0,1) \amp =(-\sqrt{3}/2, -1/2) </mrow>
    </md>.
    Confirm for yourself geometrically that these are the points you get by rotating <m>(1,0)</m> and <m>(0,1)</m> by an angle of <m>2\pi/3</m> about the origin.
    </p>

  </solution>
</example>
<p>
  A second example of a geometric linear transformation is furnished by reflection through a line in <m>\R^2</m>.
</p>
<definition xml:id="d_reflection">
  <title>Reflection through a line</title>
    <idx><h>reflection</h><h>through a line</h></idx>
  <statement>
      <p>
      Fix an angle <m>\alpha</m> with <m>0\leq \alpha \leq \pi </m>, and let <m>\ell_\alpha</m> be the line through the origin that makes an angle of <m>\alpha</m> with the positive <m>x</m>-axis.
      </p>
      <p>
        Define <m>r_\alpha\colon \R^2\rightarrow \R^2</m> to be the function that takes an input <m>P=(x,y)</m> and returns the output <m>P'=(x',y')</m> obtained by reflecting <m>P</m> through the line <m>\ell_\alpha</m>. In more detail: if <m>P</m> lies on <m>\ell_\alpha</m>, then <m>P'=P</m>; otherwise, <m>P'</m> is obtained by drawing the perpendicular through <m>\ell_\alpha</m> that passes through <m>P</m> and taking the point on the other side of this line whose distance to <m>\ell_\alpha</m> is equal to the distance from <m>P</m> to <m>\ell_\alpha</m>.
      </p>
    <p>
      The function <m>r_{\alpha}</m> is called <term>rotation through the line</term> <m>\ell_\alpha</m>.
    </p>
  </statement>
</definition>
<theorem xml:id="th_transform_reflection">
  <title>Reflection is a linear transformation</title>
  <statement>
    <p>
      Fix an angle <m>0\leq \alpha\leq \pi</m>. The reflection <m>r_\alpha\colon \R^2\rightarrow\R^2</m> is a linear transformation. In fact we have <m>r_{\alpha}=T_A</m>, where
      <me>
        A=\begin{amatrix}[rr]
          \cos 2\alpha\amp\sin 2\alpha \\
          \sin 2\alpha \amp -\cos 2\alpha
      \end{amatrix}
      </me>.
    </p>
  </statement>
  <proof>
    <p>
      See <xref ref="ex_transformation_reflection"/>.
    </p>
  </proof>

</theorem>
<example xml:id="eg_rot_refl">
  <title>Visualizing reflection and rotation</title>
  <statement>
    <p>
      The <url href="https://www.geogebra.org" visual="geogebra.org">GeoGebra</url> interactive below helps visualize the operation of rotations and reflections on the plane by showing how they act on the triangle <m>\triangle ABC</m>.
      <ul>
        <li>
          <p>
            Move or alter the triangle as you see fit.
          </p>
        </li>
        <li>
          <p>
            Check the box of the desired operation, rotation or reflection.
          </p>
        </li>
        <li>
          <p>
            If rotation is selected, the slider adjusts the angle <m>\alpha</m> of rotation.
          </p>
        </li>
        <li>
          <p>
            If reflection is selected, the slider adjusts the angle <m>\alpha</m> determining the line <m>\ell_\alpha</m> of reflection. Click the <q>Draw perps</q> box to see the the perpendicular lines used to define the reflections of vertices <m>A, B, C</m>.
          </p>
        </li>
      </ul>
    </p>
    <figure xml:id="fig_rot_refl">
      <title>Eigenspaces of a symmetric matrix are orthogonal</title>
      <interactive xml:id="geogebra_rot_refl" platform="geogebra" width="100%" aspect="4:3">
        <slate surface="geogebra" material="bwnxf7b9" aspect="4:3" marker="train-distance">
        enableLabelDrags(false);
        </slate>
      </interactive>
      <caption>Visualizing reflection and rotation. Made with <url href="https://www.geogebra.org" visual="geogebra.org">GeoGebra</url>.</caption>
    </figure>
  </statement>

</example>

</subsection>
<subsection xml:id="ss_transform_exotic_examples">
<title>Additional examples</title>
<p>
  We now proceed to some examples involving our more exotic vector spaces.
</p>
<example xml:id="eg_transform_transpose">
  <title>Transposition is linear</title>
  <statement>
    <p>
    Fix <m>m,n\geq 1</m>. Define the function <m>f\colon M_{mn}\rightarrow M_{nm}</m> as follows:
    <me>
      f(A)=A^T
    </me>.
    In other words, <m>f</m> maps a matrix to its transpose.
  </p>
  <p>
    Show that <m>f</m> is a linear transformation.
    </p>
  </statement>
  <solution>
    <p>
      We must show <m>f(cA+dB)=cf(A)+df(B)</m> for all scalars
      <m>c,d\in\R</m> and all matrices <m>A, B\in M_{mn}</m>.
      This follows easily from properties of transpose:
      <md>
        <mrow>f(cA+dB)\amp =(cA+dB)^T \amp \text{ (by def.) }</mrow>
        <mrow>\amp =(cA)^T+(dB)^T \amp (<xref ref="th_trans_props"/>)</mrow>
        <mrow>\amp =cA^T+dB^T \amp (<xref ref="th_trans_props"/>)</mrow>
        <mrow>\amp =cf(A)+df(B)</mrow>
      </md>
    </p>
  </solution>
</example>
<example>
  <title>Left-shift transformation</title>

  <statement>
    <p>
    Define the <term>left-shift operation</term>, <m>T_\ell\colon \R^\infty \rightarrow R^{\infty}</m> as follows:
    <me>
      T_\ell\left( (a_{i})_{i=1}^\infty\right)= (a_{i+1})_{i=1}^\infty
    </me>.
    In other words, we have
    <me>
      T_\ell \left( (a_1,a_2,a_3,\dots)\right)=(a_2,a_3,\dots)
    </me>.
    Show that <m>T_\ell</m> is a linear transformation.
    </p>
  </statement>
  <solution>
    <p>
    Let <m>\boldv=(a_i)_{i=1}^\infty</m> and <m>\boldw=(b_i)_{i=1}^\infty</m> be two infinite sequences in <m>\R^\infty</m>. For any <m>c,d\in\R</m> we have
    <md>
      <mrow>T_\ell(c\boldv+d\boldw) \amp=T_\ell\left((ca_i+db_i)_{i=1}^\infty \right)\amp (<xref ref="ex_vs_infinitesequences"/>) </mrow>
      <mrow> \amp= (ca_{i+1}+db_{i+1})_{i=1}^\infty \amp (\text{by def.})</mrow>
      <mrow>  \amp=c(a_{i+1})_{i=1}^\infty+d(b_{i+1})_{i=1}^\infty \amp (<xref ref="ex_vs_infinitesequences"/>)</mrow>
      <mrow>  \amp=cT_\ell(\boldv)+dT_\ell(\boldw)\amp (\text{by def.}) </mrow>
    </md>.
    This proves <m>T_\ell</m> is a linear transformation.
    </p>
  </solution>
</example>

</subsection>
<paragraphs xml:id="ss_transormation_video_examples">
  <title>Video examples: deciding if <m>T</m> is linear</title>
<p>
  <figure>
    <title>Video: deciding if <m>T</m> is linear</title>
  <caption>Video: deciding if <m>T</m> is linear</caption>
  <video xml:id="vid_transform_1" youtube="STYVF5cprEU" />
  </figure>
  <figure>
    <title>Video: deciding if <m>T</m> is linear</title>
  <caption>Video: deciding if <m>T</m> is linear</caption>
  <video xml:id="vid_transform_2" youtube="weETKP8p7cE" />
  </figure>
</p>
</paragraphs>
<subsection xml:id="ss_transform_composition">
  <title>Composition of linear transformations and matrix multiplication</title>
<p>
  We end by making good on a promise we made long ago to <em>retroactively</em> make sense of the definition of matrix multiplication. The key connecting concept, as it turns out, is composition of functions. We first need a result showing that composition preserves linearity.
</p>
<theorem xml:id="th_transform_composition">
  <title>Composition of linear transformations</title>
  <statement>
    <p>
    Let <m>V, W, U</m> be vector spaces, and suppose <m>T\colon W\rightarrow U</m> and <m>S\colon V\rightarrow W</m> are linear transformations. Then the composition
    <me>
      T\circ S\colon V\rightarrow U
    </me>
    is a linear transformation.
    </p>
  </statement>
  <proof>
    <p>
      Exercise.
    </p>
  </proof>
</theorem>
<p>
  Turning now to matrix multiplication, suppose <m>A</m> is <m>m\times n</m> and <m>B</m> is <m>n\times r</m>. Let <m>C=AB</m> be their product. These matrices give rise to linear transformations
  <md>
    <mrow>T_A\colon \R^n \amp\rightarrow \R^m \amp T_B\colon \R^r \amp\rightarrow \R^n \amp T_C\colon \R^r \amp\rightarrow \R^m  </mrow>
  </md>.
  According to <xref ref="th_transform_composition"/> the composition <m>T_A\circ T_B</m> is a linear transformation from <m>\R^r</m> (the domain of <m>T_B</m>) to <m>\R^m</m> (the codomain of <m>T_A</m>). We claim that <m>T_A\circ T_B=T_C</m>. Indeed, identifying elements of <m>\R^r</m> with column vectors, for all <m>\boldx\in \R^n</m> we have
  <md>
    <mrow> T_A\circ T_B(\boldx) \amp = T_A(T_B(\boldx)) \amp (<xref ref="d_function_composition"/>) </mrow>
    <mrow> \amp =T_A(B\boldx) \amp (<xref ref="d_matrix_transform"/>)</mrow>
    <mrow> \amp= A(B\boldx) \amp (<xref ref="d_matrix_transform"/>)</mrow>
    <mrow>  \amp = (AB)\boldx \amp (\text{assoc.})</mrow>
    <mrow>  \amp = T_C(\boldx) \amp (\text{since } C=AB)</mrow>
  </md>.
Thus, we can now understand the definition of matrix multiplication as being chosen precisely to encode how to compute the composition of two matrix transformations. The restriction on the dimension of the ingredient matrices is now understood as guaranteeing that the corresponding matrix transformations can be composed!
</p>

</subsection>

<xi:include href="./s_transformation_ex.ptx"/>


   </section>
