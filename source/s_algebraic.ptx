<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_algebraic">
  <title>Matrix algebra</title>
  <introduction>
    <p>The last section was devoted to what might be called the <em>arithmetic</em> of matrices. We learned the basic operations of adding, multiplying, scaling, and transposing matrices. In this section we tackle the <em>algebra</em> of matrices. We will investigate the properties enjoyed (and not enjoyed) by our matrix operations, and will show how to use these operations to solve matrix equations.
  </p>
  <p>
  As you learn about matrix algebra, always keep in mind your old friend, real number algebra. For the most part these two algebraic systems closely resemble one another, as <xref ref="th_matrix_alg_props"/> below makes clear. However, there are two crucial points where they differ (see <xref ref="th_matrix_abnormalities"/>): two important properties of real number algebra that do not hold for matrices. The consequences of these two simple aberrations are far-reaching and imbue matrix algebra with a fascinating richness in comparison to real number algebra.
  </p>
  </introduction>
  <theorem xml:id="th_matrix_alg_props">
      <title>Properties of matrix arithmetic</title>
      <statement>
        <p>
          The following properties hold for all matrices <m>A, B, C</m> and scalars
          <m>a,
          b, c\in \R</m> for which the given expression makes sense.
          <ol marker="(i)">
            <li xml:id="th_matrix_comm">
            <title>Addition commutative law</title>
              <p>
                <m>A+B=B+A</m>
              </p>
            </li>
            <li xml:id="th_matrixadd_assoc">
              <title>Addition associative law </title>
              <p>
                <m>A+(B+C)=(A+B)+C</m>
              </p>
            </li>
            <li xml:id="th_matrixmult_assoc">
              <title>Multiplication associative law</title>

              <p>
                <m>A(BC)=(AB)C</m>
              </p>
            </li>
            <li xml:id="th_matrix_leftdist">
              <title>Left-distributive law</title>

              <p>
                <m>A(B+C)=AB+AC</m>
              </p>
            </li>
            <li xml:id="th_matrix_rightdist">
              <title>Right-distributive law</title>
              <p>
                <m>(B+C)A=BA+CA</m>
              </p>
            </li>
            <li xml:id="th_matrixscale_dist">
              <title>Scaling distributive law</title>

              <p>
                <m>a(B+C)=aB+aC</m>
              </p>
            </li>
            <li xml:id="th_matrixscale_dist2">
              <title>Another scaling distributive law</title>

              <p>
                <m>(a+b)C=aC+bC</m>
              </p>
            </li>
            <li xml:id="th_matrixscale_assoc">
              <title>Scaling associative law</title>
              <p>
                <m>a(bC)=(ab)C</m>
              </p>
            </li>
            <li xml:id="th_matrixscale_comm">
              <title>Scaling commutative law</title>
              <p>
                <m>a(BC)=(aB)C=B(aC)</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>
    <p>
      How does one actually prove one of these properties?
      These are all <em>matrix equalities</em> of the form <m>X=Y</m>, so
      according to the <xref ref="d_matrix_equality" text='custom'> matrix equality definition</xref>
      we must show (1) that the matrices <m>X</m> and <m>Y</m> have the same dimension,
      and (2) that <m>(X)_{ij}=(Y)_{ij}</m> for all <m>(i,j)</m>. The proof below illustrates this technique for the <xref ref="th_matrixmult_assoc" text="custom">multiplication associative law</xref> of <xref ref="th_matrix_alg_props"/>.
    </p>

  <proof>
    <title>Proof of (iii)</title>
    <p>We prove only the <xref ref="th_matrixmult_assoc" text="custom">multiplication associative law</xref>.
    Let <m>A=[a_{ij}]_{m\times r}</m>,
      <m>B=[b_{ij}]_{r\times s}</m>,
      <m>C=[c_{ij}]_{s\times n}</m>.
      To show
      <me>
        A(BC)=(AB)C
      </me>,
      we must show (1) that <m>A(BC)</m> and <m>(AB)C</m> have the same dimension, and (2) that <me>[A(BC)]_{ij}=[(AB)C]_{ij}</me> for all possible <m>(i,j)</m>.
    </p>
      <p>
      (1) The usual observation about <q>inner</q> and <q>outer</q> dimensions shows that both <m>A(BC)</m> and <m>(AB)C</m> have dimension <m>m\times n</m>.
    </p><xref ref="d_matrix_mult"/>
    <p> (2) Given any  <m>(i,j)</m> with <m>1\leq i\leq m</m> and <m>1\leq j\leq n</m>, we have:
      <md>
        <mrow> [A(BC)]_{ij}\amp=\sum_{\ell=1}^ra_{i\ell}[BC]_{\ell j}  \amp (<xref ref="d_matrix_mult"/>)</mrow>
        <mrow>\amp=\sum_{\ell=1}^ra_{i\ell}\left(\sum_{k=1}^sb_{\ell k}c_{kj}\right) \amp (<xref ref="d_matrix_mult"/>)</mrow>
        <mrow>=\amp \sum_{\ell=1}^r\sum_{k=1}^sa_{i\ell}(b_{\ell k}c_{kj}) \amp \text{ (real dist.) }</mrow>
        <mrow>=\amp \sum_{k=1}^s\sum_{\ell=1}^r(a_{i\ell}b_{\ell k})c_{kj} \amp \text{ (real comm., real assoc.) }</mrow>
        <mrow>=\amp \sum_{k=1}^s\left(\sum_{\ell=1}^ra_{i\ell}b_{\ell k}\right)c_{kj} \amp \text{ (real dist.) }</mrow>
        <mrow>=\amp \sum_{k=1}^s[AB]_{ik}c_{kj}  =\Bigl[(AB)C]_{ij}</mrow>
      </md>
    </p>
    <p>
      This proves that all entries of the two matrices are equal,
      and hence <m>A(BC)=(AB)C</m>.
    </p>
  </proof>
  <p>Like real number algebra, we can identify some special matrices that act as <em>additive identities</em> and <em>multiplciative identities</em>; and every matrix has an <em>additive inverse</em>. What we mean here is spelled out in detail in <xref ref="th_matrix_add_mult_ident"/>.
</p>
<definition xml:id="d_matrix_add_inverse">
  <title>Additive inverse of a matrix</title>
  <idx><h>additive inverse</h><h>of a matrix</h></idx>
  <idx><h>matrix</h><h>additive inverse</h></idx>
  <notation>
    <usage><m>-A</m></usage>
    <description>Additive inverse of <m>A</m></description>
  </notation>
  <statement>
    <p>
    Given an <m>m\times n</m> matrix <m>A=[a_{ij}]</m>, its <term>additive inverse</term> <m>-A</m> is defined as
    <me>
      -A=(-1)A=[-a_{ij}]
    </me>.
    </p>
  </statement>
</definition>
<definition xml:id="d_identity_matrix">
  <title>Identity matrix</title>
  <idx><h>identity matrix</h></idx>
  <idx><h>matrix</h><h>identity matrix</h></idx>
  <notation>
    <usage><m>I</m></usage>
    <description>inverse matrix</description>
  </notation>
  <statement>
    <p>
      The <term><m>n\times n</m> identity matrix</term>
      is the square <m>n\times n</m> matrix
      <me>
        I_n=\begin{bmatrix}1\amp 0\amp 0 \amp \dots\amp 0\\ 0\amp 1 \amp 0 \amp \dots \amp 0\\
      \vdots \\
     0\amp 0\amp \dots \amp 0\amp 1\end{bmatrix}
      </me>
        with ones along the diagonal and zeros everywhere else. In other words, for all <m>1\leq i\leq n</m> and <m>1\leq j\leq n</m>, we have
      <me>
        (I_n)_{ij}=\begin{cases} 1 \amp \text{if } i=j \\ 0\amp \text{if } i\ne j
      \end{cases}
      </me>.
      When the size <m>n</m> of the identity matrix is not important, we will often denote it simply as <m>I</m>.
    </p>
  </statement>
  </definition>

  <theorem xml:id="th_matrix_add_mult_ident">
    <title>Additive identities, additive inverses, and multiplicative identities</title>
    <statement>
      <ol>
        <li xml:id="th_matrix_add_ident">
          <title>Additive identities</title>
          <p>The <m>m\times n</m> zero matrix <m>\boldzero_{m\times n}</m> is an <em>additive identity</em> for <m>m\times n</m> matrices in the following sense: for any <m>m\times n</m> matrix <m>A</m> we have
          <me>
            \boldzero_{m\times n}+A=A
          </me>.
          </p>
        </li>
        <li xml:id="th_matrix_add_inverse">
          <title>Additive inverses</title>
          <p>For any <m>m\times n</m> matrix <m>A</m> we have
          <me>
            A+-A=A+(-1)A=\boldzero_{m\times n}
          </me>.
          </p>
        </li>
        <li xml:id="th_matrix_mult_ident">
          <title>Multiplicative identities</title>
          <p>The <m>n\times n</m> identity matrix is a <em>multiplicative identity</em> for <m>n\times n</m> matrices in the following sense: for any <m>n\times n</m> matrix <m>A</m> we have
          <me>
            I_n\, A=A\, I_n=A
          </me>.
          </p>
        </li>
      </ol>

    </statement>
  </theorem>
  <proof>
    <p>
      Left as an exercise.
    </p>
  </proof>

  <corollary xml:id="c_matrix_additive_canc">
    <title>Additive cancellation of matrices</title>

    <statement>
      <p>
        Given <m>m\times n</m> matrices <m>A, B</m>, and <m>C</m>, we have <m>A+B=A+C</m> if and only if <m>B=C</m>. Using logical notation:
        <me>
          A+B=A+C \iff  B=C.
        </me>
      </p>
    </statement>
    <proof>
      <p>
        As simple as this claim might seem, remember that we are dealing with a completely new algebraic system here. We will prove both implications of <q>if and only if</q> statement separately.
      </p>
        <case>
         <title>Proof: <m>A+B=A+C\implies B=C</m></title>
        <p>
        We prove this via a chain of implications:
        <md>
          <mrow> A+B=A+C\amp \implies -A+(A+B)=-A+(A+C) </mrow>
          <mrow>  \amp\implies (-A+A)+B=(-A+A)+C\amp (<xref ref="th_matrixadd_assoc" text="title"/>) </mrow>
          <mrow>  \amp\implies \boldzero_{m\times n}+B=\boldzero_{m\times n}+C \amp (<xref ref="th_matrix_add_inverse"/>) </mrow>
          <mrow>  \amp \implies B=C \amp (<xref ref="th_matrix_add_ident"/>) </mrow>
        </md>.
      </p>
    </case>
      <case>
       <title>Proof: <m>B=C\implies A+B=A+C</m></title>
      <p>
      This direction is obvious: if <m>B</m> and <m>C</m> are equal matrices, then they remain equal when we add <m>A</m> to each of them.
      </p>
      </case>
    </proof>
  </corollary>


  <remark>
    <p>
      The algebraic importance of <xref ref="c_matrix_additive_canc"/> is that we can perform
      <em>additive cancellation</em>  in matrix equations just as we do in real number algebra. For example, we can solve the matrix equation <m>A+B=3A</m> for <m>B</m> as follows:
    </p>
  <md>
    <mrow>A+B\amp = 3A</mrow>
    <mrow>A+B\amp = (1+2)A</mrow>
    <mrow>A+B\amp = A+2A</mrow>
    <mrow>B\amp = 2A \amp (<xref ref="c_matrix_additive_canc"/>)</mrow>
  </md>.
</remark>
<warning>
<p>Though we can perform additive cancellation in matrix algebra, we can <em>not</em> always perform <em>multiplicative cancellation</em>. For example, consider the matrices
<me>
A=\begin{bmatrix}1\amp 1\\ 1\amp 1\end{bmatrix}, B=\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix}, C=\begin{bmatrix}0\amp 0\\ 1\amp 0 \end{bmatrix}
</me>.
Check for yourself that <m>AB=AC</m>, and yet <m>B\ne C</m>. In other words, we cannot always <q>cancel</q> <m>A</m> from the matrix equation <m>AB=AC</m>.
</p>
</warning>
<p>
The example in our warning above is but one instance of the general failure of the principle of multiplicative cancellation in matrix algebra. This in turn is a consequence of the following theorem, which identifies the two crucial places where matrix algebra differs significantly from real number algebra.
</p>
    <theorem xml:id="th_matrix_abnormalities">
      <title>Matrix algebra abnormalities</title>
      <statement>
        <p>
          <ol>
        <li xml:id="th_mult_not_commutative">
        <title>Matrix multiplication is not commutative</title>
          <p>
            For two <m>n\times n</m> matrices <m>A</m> and <m>B</m>, we do <em>not</em> necessarily have <m>AB=BA</m>.
          </p>
        </li>
        <li xml:id="th_mult_has_zero_divisors">
        <title>Products of nonzero matrices may be equal to zero</title>
        <p>
          If the product of two matrices is the zero matrix,
          we <em>cannot</em> conclude that one of matrices is the zero matrix. In logical notation:
          <men xml:id="eq_mult_has_zero_divisors">
            \underset{m\times n}{A}\, \underset{n\times r}{B}=\boldzero_{m\times r}\;\not\!\!\!\!\implies A=\boldzero_{m\times n} \text{ or }  B=\boldzero_{n\times r}
          </men>.
              </p>
            </li>
          </ol>
        </p>
        
      </statement>
    </theorem>
  <p>
      This is a good place to point out that to prove an identity does not hold, it suffices to provide a single counterexample to that effect. We do so for each failed identity of <xref ref="th_matrix_abnormalities"/> in turn. There is no significance to the particular counterexamples chosen here, and indeed there are infinitely many counterexamples to choose from in both cases.
    <ol>
      <li>
        <p>We have 
        <md>
          <mrow> \begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}\amp = \begin{bmatrix} 0\amp 1 \\ 0 \amp 0\end{bmatrix}</mrow>
          <mrow> \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} \amp = \begin{bmatrix} 0\amp 0\\ 0\amp 0 \end{bmatrix} </mrow>
        </md>
        and thus 
        <me>\begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} \ne \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix}
        </me>.
        </p>
      </li>
      <li>
        <p>
            Observe that 
            <me>
              \begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp 0 \end{bmatrix} =\begin{bmatrix}0\amp 0\\ 0\amp 0 \end{bmatrix} 
            </me>.
         This is an example of two nonzero matrices whose product is the zero matrix.
        </p>
      </li>
    </ol>
</p>
<p>
  An important consequence of the abnormality <xref ref="eq_mult_has_zero_divisors"/> is that matrix algebra does not enjoy the property of <em>multiplicative cancellation</em>. 
</p>
  <corollary xml:id="th_matrix_cancel">
    <title>Failure of multiplicative cancellation</title>
    <statement>
      <ol>
        <li>
          <p>
            Suppose matrices <m>A, B, C</m> satisfy <m>AB=AC</m> and <m>A\ne \boldzero</m>. We <em>cannot</em> conclude that <m>B=C</m>. In logical notation:
            <men xml:id="eq_no_left_cancel" >
              AB=AC \;\not\!\!\!\!\implies B=C
            </men>
          </p>
        </li>
        <li>
          <p>
            Suppose matrices <m>B, C, D</m> satisfy <m>BD=CD</m> and <m>D\ne \boldzero</m>. We <em>cannot</em> conclude that <m>B=C</m>. In logical notation:
            <men xml:id="eq_no_right_cancel">
              BD=CD \;\not\!\!\!\!\implies B=C
            </men>
          </p>
        </li>
      </ol>
    </statement>
  </corollary>
  <proof>
    <p>
      Again, we need only provide explicit counterexamples for each statement.
    </p>
    <ol>
      <li>
        <p> Let <m>A=\begin{bmatrix}1\amp 1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix}</m>, <m>B=\begin{bmatrix}1\amp 0\\ 0\amp 0\\ 0\amp 0 \end{bmatrix}</m>, <m>C=\begin{bmatrix}0\amp 0\\ 1\amp 0\\ 0\amp 0 \end{bmatrix}</m>. Verify for yourself that
          <me>
            \begin{bmatrix}1\amp 1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix} \begin{bmatrix}1\amp 0\\ 0\amp 0\\ 0\amp 0 \end{bmatrix} = \begin{bmatrix}1\amp 1\amp 0\\ 0\amp 0\amp 0 \end{bmatrix} \begin{bmatrix}0\amp 0\\ 1\amp 0\\ 0\amp 0 \end{bmatrix}=
              \begin{bmatrix}
                1\amp 0\\ 0\amp 0\\ 0\amp 0
              \end{bmatrix}
          </me>.
          Thus <m>AB=AC</m>, but clearly <m>B\ne C</m>.
        </p>
      </li>
      <li>
        <p>
          Let <m>B=\begin{bmatrix}2\amp 0\\ 0\amp 0 \end{bmatrix}</m>, <m>C=\begin{bmatrix}1\amp 1\\ 0\amp 0 \end{bmatrix}</m>, <m>D=\begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{bmatrix}</m>. We have
          <me>
            \begin{bmatrix}2\amp 0\\ 0\amp 0 \end{bmatrix} \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{bmatrix} = \begin{bmatrix}1\amp 1\\ 0\amp 0  \end{bmatrix} \begin{bmatrix}1\amp 1\amp 1\\ 1\amp 1\amp 1 \end{bmatrix}=
            \begin{bmatrix}
              2\amp 2\amp 2\\ 2\amp 2\amp 2
            \end{bmatrix}
          </me>.
          Thus <m>BD=CD</m>, but <m>B\ne C</m>.
        </p>
      </li>
    </ol>
  </proof>
  <remark xml:id="rm_cancel_failure">
    <p> Mark well this important abnormality of matrix algebra. Confronted with a real number equation of the form <m>ab=ac</m>, we have a deeply ingrained impulse to declare that either <m>a=0</m> or <m>b=c</m>. (If we're sloppy we may forget about that first possibility.) The corresponding maneuver for the matrix equation <m>AB=AC</m> is simply not available to us, unless we know something more about <m>A</m>.
  </p>
</remark>
<p>
  We end our foray into matrix algebra with some properties articulating how matrix transposition interacts with matrix addition, multiplication and scalar multiplication.
</p>
<theorem xml:id="th_trans_props">
  <title>Properties of matrix transposition</title>
  <p>
  The following properties hold for all matrices <m>A, B, C</m> and scalars
  <m>c\in \R</m> for which the given expression makes sense. </p>
      <statement>
        <ol marker="(i)">
          <li xml:id="th_trans_add" >
            <p>
              <m>(A+B)^T=A^T+B^T</m>
            </p>
          </li>
          <li xml:id="th_trans_scale" >
            <p>
              <m>(cA)^T=cA^T</m>
            </p>
          </li>
          <li xml:id="th_trans_prod">
            <p>
              <m>(AB)^T=B^TA^T</m>
            </p>
          </li>
          <li xml:id="th_trans_owninverse">
            <p>
              <m>\left(A^T\right)^T=A</m>
            </p>
          </li>
        </ol>
      </statement>
      <proof>
        <p> We prove only the first statement. First observe that if <m>A</m> is <m>m\times n</m>, then so is <m>B</m> and <m>A+B</m>. Then <m>(A+B)^T</m> is <m>n\times m</m> by <xref ref="d_transpose"/>. Similarly, we see that <m>A^T+B^T</m> is <m>n\times m</m>.
      </p>
        <p>
          Next, given any <m>(i,j)</m> with <m>1\leq i\leq n</m>, <m>1\leq j\leq m</m>, we have
        <md>
          <mrow>\left((A+B)^T\right)_{ij}\amp =(A+B)_{ji} \amp (<xref ref="d_transpose"/>)</mrow>
          <mrow>\amp = A_{ji}+B_{ji} \amp (<xref ref="d_matrix_add_subtract"/>)</mrow>
          <mrow>\amp =(A^T)_{ij}+(B^T)_{ij} \amp (<xref ref="d_transpose"/> </mrow>
          <mrow>\amp =(A^T+B^T)_{ij} \amp (<xref ref="d_matrix_add_subtract"/>)</mrow>
        </md>.
          Since the <m>ij</m>-entries of both matrices are equal for each <m>(i,j)</m>,
          it follows that <m>(A+B)^T=A^T+B^T</m>.
        </p>
      </proof>
    </theorem>
    <paragraphs xml:id="ss_algebra_video_examples">
      <title>Video examples: proving matrix equalities</title>

      <figure xml:id="fig_proof_assoc">
        <title>Video: matrix multiplication is associative</title>
        <caption>Video: matrix multiplication is associative</caption>
        <video xml:id="vid_proof_assoc" youtube="EDGxkEV5mm0" />
      </figure>

      <figure xml:id="fig_proof_transp">
        <title>Video: transpose property</title>
        <caption>Video: transpose property</caption>
        <video xml:id="vid_proof_transp" youtube="twq0_HdpC3w" />
      </figure>

    </paragraphs>
<xi:include  href="s_algebraic_ex.ptx"/>
</section>
