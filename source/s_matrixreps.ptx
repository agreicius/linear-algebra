<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="s_matrixreps">
  <title>Matrix representations of linear transformations</title>
  <introduction>
    <p>
      We have seen how the coordinate vector map can be used to translate a linear algebraic question posed about a finite-dimensional vector space <m>V</m> into a question about <m>\R^n</m>,
      where we have many computational algorithms at our disposal.
      We would like to extend this technique to linear transformations <m>T\colon V\rightarrow W</m>,
      where both <m>V</m> and <m>W</m> are
      <em>finite-dimensional</em>.
      The basic idea, to be fleshed out below,
      can be described as follows:
      <ol>
        <li>
          <p>
            Pick a basis <m>B</m> for <m>V</m>,
            and a basis <m>B'</m> for <m>W</m>.
          </p>
        </li>
        <li>
          <p>
            <q>Identify</q>
            <m>V</m> with <m>\R^n</m> and <m>W</m> with <m>\R^m</m> using the coordinate vector isomorphisms
            <m>[\hspace{5pt}]_B</m> and <m>[\hspace{5pt}]_{B'}</m>,
            respectively.
          </p>
        </li>
        <li>
          <p>
            <q>Model</q>
            the linear transformation <m>T\colon V\rightarrow W</m> with a certain linear transformation <m>T_A\colon \R^n\rightarrow \R^m</m>.
          </p>
        </li>
      </ol>
      The matrix <m>A</m> defining <m>T_A</m> will be called the
      <em>matrix representing <m>T</m> with respect to our choice of basis <m>B</m> for <m>V</m> and <m>B'</m> for <m>W</m></em>.
    </p>
    <p>
      In what sense does <m>A</m>
      <q>model</q>
      <m>T</m>?
      All the properties of <m>T</m> we are interested in (<m>\NS T</m>,
      <m>\nullity T</m>, <m>\im T</m>, <m>\rank T</m>,
      etc.) are perfectly mirrored by the matrix <m>A</m>.
      As a result,
      this technique allows us to answer questions about the original <m>T</m> essentially by applying a relevant matrix algorithm to <m>A</m>.
    </p>
  </introduction>
  <subsection xml:id="ss_matrix_reps">
    <title>Matrix representations of linear transformations</title>
    <p>
      Given a linear transformation <m>T\colon V\rightarrow W</m> and choice of ordered bases <m>B</m> and <m>B'</m> of <m>V</m> and <m>W</m>, respectively, we define the matrix <m>A=[T]_B^{B'}</m> representing <m>T</m> column by column, using a familiar looking formula.
    </p>
  <definition xml:id="d_matrix_representation">
    <title>Matrix representations of linear transformations</title>
    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces with ordered bases <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m> and <m>B'=(\boldw_1, \boldw_2, \dots, \boldw_m)</m>, respectively.  Given a linear transformation <m>T\colon V\rightarrow W</m>, the <term>matrix representing <m>T</m> with respect to <m>B</m> and <m>B'</m></term>, is the <m>m\times n</m> matrix <m>[T]_B^{B'}</m> whose <m>j</m>-th column is <m>[T(\boldv_j)]_{B'}</m>, considered as a column vector: <ie />,
        <men xml:id="eq_matrixrep_formula">
        [T]_B^{B'}=\begin{amatrix}[cccc]\vert \amp \vert \amp \amp \vert \\
        \left[T(\boldv_1)\right]_{B'}\amp [T(\boldv_2)]_{B'}\amp \dots \amp [T(\boldv_n)]_{B'} \\
        \vert \amp \vert \amp  \amp \vert
      \end{amatrix}
      </men>.
      In the special case where <m>W=V</m> and we pick <m>B'=B</m> we write simply <m>[T]_B</m>.
    </p>
  </statement>
</definition>
<example xml:id="eg_matrixreps_derivative">
  <statement>
    <p>
      Consider the linear transformation <m>T\colon P_{3}\rightarrow P_{2}</m> defined as <m>T(p(x))=p'(x)</m>.
      Compute <m>A=[T]_{B}^{B'}</m>,
      where <m>B</m> and <m>B'</m> are the standard bases for <m>P_3</m> and <m>P_2</m>,
      respectively.
    </p>
  </statement>
  <solution>
    <p>
      We have <m>B=(x^3, x^2, x, 1)</m> and <m>B'=(x^2, x, 1)</m>.
      Denote by <m>\boldc_j</m> the <m>j</m>-th column of <m>A</m>.
      We use <xref ref="eq_matrixrep_formula"/> to compute:
      <md>
      <mrow>\boldc_1\amp =[T(x^3)]_{B'}=[3x^2]_{B'}=\begin{bmatrix} 3</mrow>
      <mrow>0</mrow>
      <mrow>0 \end{bmatrix} \amp \boldc_2\amp =[T(x^2)]_{B'}=[2x]_{B'}=\begin{bmatrix} 0</mrow>
      <mrow>2</mrow>
      <mrow>0 \end{bmatrix}</mrow>
      <mrow>\boldc_3\amp =[T(x)]_{B'}=[1]_{B'}=\begin{bmatrix} 0</mrow>
      <mrow>0</mrow>
      <mrow>1 \end{bmatrix} \amp \boldc_4\amp =[T(1)]_{B'}=[0]_{B'}=\begin{bmatrix} 0</mrow>
      <mrow>0</mrow>
      <mrow>0 \end{bmatrix}</mrow>
      </md>.
      Thus <m>A=\begin{bmatrix}3\amp 0\amp 0\amp 0\\ 0\amp 2\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0 \end{bmatrix}</m>.
    </p>
  </solution>
</example>
<p>
  The formula for <m>[T]_{B}^{B'}</m> should remind you of the formula from <xref ref="cor_matrix_transformations"/> used for computing the standard matrix for a linear transformation <m>T\colon \R^n\rightarrow \R^m</m>: <ie />, the matrix <m>A</m> such that <m>T(\boldx)=A\boldx</m> for all <m>\boldx\in \R^n</m>. <xref ref="th_matrixreps_matrixtransforms"/> explicates this resemblance.
</p>
<theorem xml:id="th_matrixreps_matrixtransforms">
  <title>Standard matrix as a matrix representation</title>
  <statement>
    <p>
    Let <m>T\colon \R^n\rightarrow \R^m</m> be a linear transformation, and let <m>A</m> be its standard matrix: <ie />, <m>A</m> satisfies <m>T(\boldx)=A\boldx</m> for all <m>\boldx\in \R^n</m>. We have
    <me>
      A=[T]_{B}^{B'}
    </me>
    where <m>B</m> and <m>B'</m> are the standard bases of <m>\R^n</m> and <m>\R^m</m>, respectively. In other words, the matrix representing <m>T</m> with respect to the standard bases of <m>\R^n</m> and <m>\R^m</m> is none other than the standard matrix of <m>T</m>.
  </p>
</statement>
<proof>
  <p>
    According to the recipe in <xref ref="cor_matrix_transformations"/> we have
    <me>
    A= \begin{bmatrix}\vert\amp \vert\amp  \amp \vert \\ T(\bolde_1)\amp  T(\bolde_2)\amp \cdots \amp T(\bolde_n)\\ \vert\amp \vert\amp  \amp \vert \end{bmatrix}
    </me>.
    Let <m>B</m> and <m>B'</m> be the standard ordered bases of <m>\R^n</m> and <m>\R^m</m>, respectively. To see why <m>A=[T]_{B}^{B'}</m>, observe that for all <m>1\leq j\leq n</m> the <m>j</m>-th column of <m>A</m> is <m>T(\bolde_j)</m> and the <m>j</m>-th column of <m>[T]_B^{B'}</m> is <m>[T(\bolde_j)]_{B''}</m>. That these are equal is a result of the fact that for all vectors <m>\boldw\in \R^m</m> we have <m>[\boldw]_{B'}=\boldw</m>: that is, the coordinate vector of a vector <m>\boldw\in \R^m</m> with respect to the <em>standard basis</em> is just <m>\boldw</m> itself. (See <xref ref="eg_coordinatevector_standard"/>).
  </p>
</proof>
</theorem>
    <remark xml:id="rm_better_basis">
  <statement>
    <p>
      Let <m>T\colon \R^n\rightarrow \R^m</m> be a linear transformation, and let <m>A</m> be its standard matrix: <ie />, <m>A</m> is the <m>m\times n</m> matrix satisfying <m>T(\boldx)=A\boldx</m> for all <m>\boldx\in \R^n</m>. According to <xref ref="th_matrixreps_matrixtransforms"/>,  the standard matrix <m>A</m> is just one way of representing <m>T</m>: namely, the representation with respect to the standard bases of <m>\R^n</m> and <m>\R^m</m>. This begs the question of whether a different choice of bases might give rise to a more convenient matrix representation of <m>T</m>. The answer is yes, as we will see over the course of this chapter.
    </p>
  </statement>
</remark>
<example>
  <statement>
    <p>
      Define <m>T\colon \R^2\rightarrow \R^2</m> as  <m>T(x,y)=(4x-3y,2x-y)</m>.
    </p>
    <ol>
      <li>
        <p>
          Compute <m>[T]_B</m>, where <m>B=(\bolde_1, \bolde_2)</m> is the standard basis of <m>\R^2</m>.
        </p>
      </li>
      <li>
        <p>
          Compute <m>[T]_{B'}</m>, where <m>B'=((1,1), (1,-1))</m>.
        </p>
      </li>
    </ol>
  </statement>
  <solution>
    <ol>
      <li>
        <p>
          According to <xref ref="th_matrixreps_matrixtransforms"/>, since <m>B</m> is the standard basis <m>[T]_B</m> is the matrix <m>A</m> such that <m>T=T_A</m>:
          <md>
            <mrow>[T]_B\amp=\begin{bmatrix}
              \vert \amp \vert \\
              T(\bolde_1)\amp T(\bolde_2)\\
              \vert \amp \vert
            \end{bmatrix} </mrow>
            <mrow> \amp= \begin{amatrix}[rr]
              4\amp -3\\
              2\amp -1
            \end{amatrix} </mrow>
          </md>.
        </p>
      </li>
      <li>
        <p>
          We have
          <md>
            <mrow>[T]_{B'}=[T]_{B'}^{B'} \amp = \begin{bmatrix}
              \vert\amp \vert\\
              [T((1,1))]_{B'}\amp [T(1,-1)]_{B'}\\
              \vert \amp \vert
            \end{bmatrix} </mrow>
            <mrow> \amp = \begin{bmatrix}
              \vert\amp \vert\\
              [(1,1)]_{B'}\amp [(7,3)]_{B'}\\
              \vert \amp \vert
            \end{bmatrix}</mrow>
            <mrow>  \amp = \begin{amatrix}[rr]
              1\amp 5\\
              0\amp 2
            \end{amatrix}</mrow>
          </md>,
          where the last equality uses the fact that <m>[(1,1)]_{B'}=(1,0)</m> and <m>[(7,3)]_{B'}=(5,2)</m>, as you can verify yourself.
        </p>
      </li>
    </ol>
    <p>
    So we have <m>[T]_B=\begin{amatrix}[rr]4\amp -1\\ 2\amp -1  \end{amatrix}</m> and <m>[T]_{B'}=\begin{amatrix}[rr]1\amp 5\\ 0\amp 2  \end{amatrix}</m>. Moral: different choices of bases yield different matrix representations.
    </p>
  </solution>
</example>
</subsection>
<subsection xml:id="ss_matrixreps_models">
  <title>Matrix representations as models</title>
<p>
  Before moving to more examples, we describe in what precise sense the matrix <m>A=[T]_B^{B'}</m> models the original linear transformation <m>T\colon V\rightarrow W</m>, and how we can use <m>A</m> to answer questions about <m>T</m>. The next theorem is key to understanding this.
</p>
<theorem xml:id="th_matrixrep">
  <title>Defining property of matrix representation</title>
  <statement>
    <p>
      Let <m>T\colon V\rightarrow W</m> be a linear transformation, where <m>\dim V=n</m> and <m>\dim W=m</m>, and let <m>B, B'</m> be ordered bases for <m>V</m> and <m>W</m>, respectively.
      <ol>
        <li>
          <p>
            For all <m>\boldv\in V</m> we have
            <men xml:id="eq_matrixrep_prop">
              [T]_{B}^{B'}[\boldv]_B=[T(\boldv)]_{B'}
            </men>.
            As usual <m>[\boldv]_B</m> is treated here as an <m>n\times 1</m> column vector.
          </p>
        </li>
        <li>
          <p>
            Property <xref ref="eq_matrixrep_prop"/> defines <m>[T]_B^{B'}</m>: <ie />, if <m>A</m> is an <m>m\times n</m> matrix satisfying <m>A[\boldv]_B=[\boldv]_{B'}</m> for all <m>\boldv\in V</m>, then <m>A=[T]_{B}^{B''}</m>.
          </p>
        </li>
      </ol>
    </p>


  </statement>
  <proof>
    <p>
      Let <m>B=(\boldv_1, \boldv_2, \dots, \boldv_n)</m>.
    </p>
    <ol>
      <li>
        <p>
          By definition we have
          <me>
          [T]_B^{B'}=\begin{amatrix}[cccc]\vert \amp \vert \amp \amp \vert \\
          \left[T(\boldv_1)\right]_{B'}\amp [T(\boldv_2)]_{B'}\amp \dots \amp [T(\boldv_n)]_{B'} \\
          \vert \amp \vert \amp  \amp \vert
        \end{amatrix}
        </me>.
        Given any <m>\boldv\in V</m>, we can write
        <me>
        \boldv=c_1\boldv_1+c_2\boldv_2+\dots +c_n\boldv_n
        </me>
        for some <m>c_i\in \R</m>. Then
        <md>
        <mrow> [T]_{B}^{B'}[\boldv] \amp= [T]_{B}^{B'} \begin{bmatrix}
        c_1\\ c_2\\ \vdots \\ v_n
        \end{bmatrix} </mrow>
        <mrow> \amp=c_1[T(\boldv_1)]_{B'}+c_n[T(\boldv_n)]_{B'}+\cdots +c_n[T(\boldv_n)]_{B'} \amp (\text{column method})</mrow>
        <mrow>  \amp = [c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots +c_nT(\boldv_n)]_{B'} \amp (<xref ref="th_coordinates" text="global"/>)</mrow>
        <mrow>  \amp=[T(c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n)]_{B'} \amp (T \text{ is linear})</mrow>
        <mrow>  \amp =[T(\boldv)]_{B'}</mrow>
        </md>,
        as desired.
      </p>
    </li>
    <li>
      <p>
        Assume <m>A</m> satisfies
        <me>
        A[\boldv]_B=[T(\boldv)]_{B'}
        </me>
        for all <m>\boldv\in V</m>. Then in particular we have
        <men xml:id="eq_matrixrep_proof">
          A[\boldv_i]_B=[T(\boldv_i)]_{B'}
        </men>
        for all <m>1\leq i\leq n</m>. Since <m>\boldv_i</m> is the <m>i</m>-th element of <m>B</m>, we have <m>[\boldv_i]_B=\bolde_i</m>, the <m>i</m>-th standard basis element of <m>\R^n</m>. Using the column method (<xref ref="th_column_method" text="global"/>), we see that
        <me>
        A[\boldv_i]_B=A\bolde_i=\boldc_i,
        </me>
        where <m>\boldc_i</m> is the <m>i</m>-th column of <m>A</m>. Thus <xref ref="eq_matrixrep_proof"/> implies that the <m>i</m>-th column of <m>A</m> is equal to <m>[T(\boldv_i)]_{B}</m>, the <m>i</m>-th column of <m>[T]_B^{B'}</m>, for all <m>1\leq i\leq n</m>. Since <m>A</m> and <m>[T]_{B}^{B'}</m> have identical columns, we conclude that <m>A=[T]_{B}^{B'}</m>, as desired.
      </p>
    </li>
    </ol>
</proof>
</theorem>
<remark xml:id="rm_matrixreps_uniqueness">
  <title>Uniqueness of <m>[T]_B^{B'}</m></title>
  <statement>
    <p>
      The uniqueness property described in (2) of  <xref ref="th_matrixrep"/> provides an alternative way of computing <m>[T]_{B}^{B'}</m> that can be useful in certain situations: namely, simply provide an <m>m\times n</m> matrix <m>A</m> that satisfies the defining property
      <me>
        A[\boldv]_B=[T(\boldv)]_{B'}
      </me>
      for all <m>\boldv\in V</m>. Since there is only one such matrix, we must have <m>A=[T]_B^{B'}</m> in this case.
    </p>
  </statement>
</remark>
    <p>
      Let <m>T\colon V\rightarrow W</m>, <m>B</m>, and <m>B'</m> be as in <xref ref="th_matrixrep"/>. The defining property <xref ref="eq_matrixrep_prop"/> of <m>[T]_B^{B'}</m> can be summarized by saying that the following diagram is <em>commutative</em>.
    </p>
    <figure xml:id="fig_comm_diag">
      <title>Commutative diagram for <m>[T]_B^{B'}</m></title>
      <caption>Commutative diagram for <m>[T]_B^{B'}</m></caption>
      <image xml:id="im_comm_diag" width="30%">
        <latex-image>
          \begin{tikzcd}
          V \arrow[r, "T"] \arrow[d, leftrightarrow,"{[\hspace{.1in}]}_B"'] \amp W \arrow[d,leftrightarrow,"{[\hspace{.1in}]}_{B'}"] \\
          \R^n \arrow[r, "{[T]_B^{B'}}"'] \amp \R^m
          \end{tikzcd}
        </latex-image>
      </image>
    </figure>
    <!-- <figure xml:id="fig_comm_diag">
      <caption>Commutative diagram for <m>[T]_B^{B'}</m></caption>
      <image xml:id="im_comm_diag" width="30%" source="./images/im_comm_diag.svg" />
    </figure> -->
      <p>
        The diagram being commutative here means that starting with an element <m>\boldv\in V</m> in the top left of the diagram, whether we travel to the bottom right of the diagram either by first applying <m>T</m> and then applying <m>[\hspace{5pt}]_{B'}</m> (<q>go right, then down</q>), or else by first applying <m>[\hspace{5pt}]_B</m> and then applying <m>[T]_B^{B'}</m> (<q>go down, then right</q>), we get the same result! (The bottom map should technically be labeled <m>T_A</m>, where <m>A=[T]_B^{B'}</m>, but this would detract from the elegance of the diagram.)
    </p>
    <p>
      Besides commutativity, the other import feature of <xref ref="fig_comm_diag"/> is that the two vertical coordinate transformations identify the domain and codomain of <m>T</m> with the familiar spaces <m>\R^n</m> and <m>\R^m</m> in a one-to-one manner. (Using the language of <xref ref="s_isom"/>, these maps are isomorphisms.)
      These properties together allow us to translate any linear algebraic question about <m>T</m> to an equivalent question about the matrix <m>A</m>, as the following theorem indicates.
    </p>
      <theorem xml:id="th_matrixreps_model">
        <title>Computing with matrix representations</title>
        <statement>
          <p>
            Let <m>T\colon V\rightarrow W</m> be linear, and let <m>B</m> and <m>B'</m> be ordered bases of <m>V</m> and <m>W</m>, respectively. The matrix   <m>A=[T]_{B}^{B'}</m> satisfies the following properties:
          </p>
          <ol>
            <li>
              <p>
                <m>\boldv\in \NS T</m> if and only if <m>[\boldv]_B\in \NS A</m>;
              </p>
            </li>
            <li>
              <p>
                <m>\boldw\in \im T</m> if and only if <m>[\boldw]_{B'}\in \CS A</m>;
              </p>
            </li>
            <li>
              <p>
                <m>\{\boldv_1,\boldv_2,\dots, \boldv_r\}</m> is a basis of <m>\NS T</m> if and only if <m>\{[\boldv_1]_B, [\boldv_2]_B, \dots, [\boldv_r]_B\}</m> is a basis of <m>\NS A</m>;
              </p>
            </li>
            <li>
              <p>
                <m>\{\boldw_1,\boldw_2,\dots, \boldw_s\}</m> is a basis of <m>\im T</m> if and only if <m>\{[\boldw_1]_{B'}, [\boldw_2]_{B'}, \dots, [\boldv_s]_{B'}\}</m> is a basis of <m>\CS A</m>;
              </p>
            </li>
            <li>
              <p>
                <m>\nullity T=\nullity A</m> and <m>\rank T=\rank A</m>.
              </p>
            </li>
          </ol>
        </statement>
        <proof>
          <proof>
            <title>Proof of (1)</title>
            <p>
              We have
              <md>
                <mrow>\boldv\in \NS T \amp\iff T(\boldv)=\boldzero </mrow>
                <mrow> \amp\iff [T(\boldv)]_{B'}=[\boldzero]_{B'} \amp (<xref ref="th_coordinates"/>, (2))</mrow>
                <mrow>  \amp \iff [T]_{B}^{B'}[\boldv]_B=\boldzero \amp (<xref ref="eq_matrixrep_prop"/>)</mrow>
                <mrow>  \amp [\boldv]_B\in\NS A \amp (A=[T]_B^{B'}) </mrow>
              </md>.
            </p>
          </proof>
          <proof>
            <title>Proof of (2)</title>
            <p>
              We have
              <md>
                <mrow>\boldw\in \im T \amp\iff \boldw=T(\boldv) \text{ for some } \boldv\in V </mrow>
                <mrow>  \amp\iff [\boldw]_{B'}=[T(\boldv)]_{B'} \text{ for some } \boldv \in V\amp (<xref ref="th_coordinates"/>-(2)) </mrow>
                <mrow>  \amp\iff [\boldw]_{B'}=[T]_{B}^{B'}[\boldv]_B \text{ for some } \boldv\in V \amp (<xref ref="eq_matrixrep_prop"/>) </mrow>
                <mrow>  \amp\iff [\boldw]_{B'}=A\boldx \text{ for some } \boldx\in \R^n  \amp (A=[T]_B^{B'}, <xref ref="th_coordinates"/>-(3))</mrow>
                <mrow>  \amp \iff [\boldw]_{B'}\in \CS A</mrow>
              </md>.
              The last equivalence follows from the fact that
              <me>
                \CS A=\im T_A=\{\boldb\in \R^m\colon \boldb=A\boldx \text{ for some } \boldx\in \R^n\}
              </me>.

            </p>
          </proof>
          <proof>
            <title>Proof (3)-(4)</title>
            <p>
              These now follow from (1)-(2) and statement (4) of <xref ref="th_coordinates"/>.
            </p>
          </proof>


        </proof>

      </theorem>
<example xml:id="eg_matrixreps_derivative_contd">
  <statement>
    <p>
      Consider again <xref ref="eg_matrixreps_derivative"/>, where we modeled the linear transformation
      <md>
        <mrow>T\colon P_3 \amp\rightarrow P_2 </mrow>
        <mrow>  p(x)\amp\mapsto p'(x) </mrow>
      </md>
      as
      <me>
        [T]_B^{B'}=\begin{bmatrix}3\amp 0\amp 0\amp 0\\ 0\amp 2\amp 0\amp 0\\ 0\amp 0\amp 1\amp 0 \end{bmatrix}
      </me>.
      Here <m>B=(x^3, x^2, x, 1)</m> and <m>B'=(x^2, x, 1)</m> are the respective standard bases of <m>P_3</m> and <m>P_2</m>.
    </p>
    <p>
      Let <m>A=[T]_B^{B'}</m>. By inspection, we see easily that
      <md>
        <mrow>\NS \amp= \Span\{(0,0,0,1)\}\amp \CS A\amp =\Span \{(3,0,0), (0,2,0),(0,0,1)\}=\R^3 </mrow>
      </md>.
      Using <xref ref="th_matrixreps_model"/> we can lift these spanning sets back to <m>P_3</m> and <m>P_2</m> to conclude that
      <md>
        <mrow>\NS T \amp =\Span\{1\}\subseteq P_3 \amp \im T\amp =\Span\{3x^2, x, 1\}=P_2 </mrow>
      </md>.
      What do these results tell us about the differential operation <m>T(p)=p'</m>?
    </p>
      <p>
        From <m>\NS T=\Span\{1\}=\{p(x)\in P_2\colon p(x)=c \text{ for all x}\}</m>, we conclude that <m>T(p)=p'=\boldzero</m> if and only if <m>p(x)=c</m> for some <m>c\in \R</m>: <ie />, the polynomials whose derivative is equal to the zero function are precisely the constant polynomials.
      </p>
      <p>
        From <m>\im T=P_2</m>, we conclude that for all <m>q(x)\in P_2</m> there is a <m>p(x)\in P_3</m> such that <m>p'(x)=q(x)</m>. Using the language of calculus, this means that every polynomial of degree at most two has an antiderivative that itself is a polynomial of degree at most three.
      </p>
      <p>
        Linear algebra here reveals some properties of the derivative, in the restricted context of polynomial functions, that calculus shows to be true more generally: namely, that a function <m>f\in C^1(\R)</m> satisfies <m>f'=\boldzero</m> if and only if <m>f</m> is a constant function, and that every continuous function <m>g\in C(\R)</m> has an antiderivative.
      </p>
  </statement>
</example>
<paragraphs xml:id="ss_vid_eg_coordinatemaps">
  <title>Video example: matrix representations </title>
  <figure xml:id="fig_vid_matrix_reps">
    <title>Video: matrix representations</title>
  <caption>Video: matrix representations</caption>
  <video xml:id="vid_vid_matrix_reps" youtube="aIxeh_1Ztr4" />
  </figure>
</paragraphs>
</subsection>

<subsection>
  <title>Choice of basis</title>
  <p>
    Given a linear transformation <m>T\colon V\rightarrow W</m>, different choice of bases for <m>V</m> and <m>W</m> give rise to different matrix representations of <m>T</m>. This observation immediately gives rise to two questions: What is the precise relationship between two different matrix representations?; How should we choose our bases so that the resulting matrix representation is useful to us? We will take up these questions in earnest in the subsequent sections. For now we content ourselves with an illustrative, long-format example.
  </p>
  <example xml:id="eg_matrixreps_proj">
    <title>Two representations of orthogonal projection</title>

    <statement>
      <p>
        Let <m>W\subseteq \R^3</m> be the plane passing through the origin with normal vector <m>\boldn=(1,1,1)</m> (with respect to the dot product): <ie />,
        <me>
          W=\{(x,y,z)\in \R^3\colon x+y+z\}
        </me>.
        Consider the orthogonal projection transformation <m>\operatorname{proj}_W\colon \R^3\rightarrow \R^3</m>. With respect to the standard basis <m>B=(\bolde_1, \bolde_2, \bolde_3)</m> we know from <xref ref="th_matrixreps_matrixtransforms"/> that <m>[\operatorname{proj}_W]_B=[\operatorname{proj}_W]_B^B</m> is just the matrix <m>A</m> such that <m>\operatorname{proj}_W=T_A</m>. Using the general formaul for projection onto a plane derived in <xref ref="eg_projection_plane"/>, we conclude:
        <me>
          [T]_B=A=\frac{1}{3}\begin{amatrix}[rrr]
          2\amp -1\amp -1\\
          -1\amp 2\amp -1\\
          -1\amp -1\amp 2
        \end{amatrix}
        </me>.
        Now consider a nonstandard basis <m>B'</m> of <m>\R^3</m> constructed with an eye toward the geometry involved in the definition of this projection operator. Namely, we begin with a basis of the plane <m>W</m> and extend to a full basis of <m>\R^3</m> by adding the normal vector <m>(1,1,1)</m> defining <m>W</m>. Note that since <m>W</m> is a 2-dimensional subspace, a basis by formed by any two linearly independent members. As such a basis of <m>W</m> is easily produced by inspection. We choose <m>\boldv_1=(1,-1,0)</m> and <m>\boldv_2=(1,0,-1)</m> as our basis for <m>W</m>. Adding the normal vector <m>\boldv_3=(1,1,1)</m> then yields the ordered basis
        <me>
        B'=\left( \underset{\boldv_1}{(1,-1,0)}, \underset{\boldv_2}{(1,0,-1)}, \underset{\boldv_3}{(1,1,1)} \right)
        </me>
        of <m>\R^3</m>.
      </p>
      <p>
        Let <m>A'=[T]_B'</m>, and for all <m>1\leq i\leq 3</m> let <m>\boldc_i</m> be the <m>i</m>-th column of <m>A</m>. Using <xref ref="eq_matrixrep_formula"/> we compute
        <md>
          <mrow>\boldc_1 = [\proj{\boldv_1}{W}]_{B'}\amp =[\boldv_1]_{B'} \amp (\boldv_1\in W)</mrow>
          <mrow> \amp =(1,0,0) \amp (\boldv_1=1\boldv_1+0\boldv_2+0\boldv_3)</mrow>
          <mrow>\boldc_2 = [\proj{\boldv_2}{W}]_{B'}\amp =[\boldv_2]_{B'} \amp (\boldv_2\in W)</mrow>
          <mrow> \amp =(0,1,0) \amp (\boldv_2=0\boldv_1+1\boldv_2+0\boldv_3)</mrow>
          <mrow>\boldc_3 = [\proj{\boldv_3}{W}]_{B'}\amp =[\boldzero]_{B'} \amp (\boldv_3\in W^\perp)</mrow>
          <mrow> \amp =(0,0,0) \amp (\boldzero=0\boldv_1+0\boldv_2+0\boldv_3)</mrow>
        </md>.
        Thus
        <me>
          [T]_{B'}=A'=\begin{amatrix}[rrr]1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 0  \end{amatrix}
        </me>.
        Wow, <m>A'</m> is way simpler! How can the two very different matrices <m>A</m> and <m>A'</m> represent the same linear transformation? A useful way of thinking about this is to consider <m>A</m> and <m>A'</m> as two matrix formulas for <m>\operatorname{proj}_W</m> with respect to two different <em>coordinate systems</em>. This can be made precise by using the defining properties (<xref ref="eq_matrixrep_prop"/>) of <m>A=[T]_B</m> and <m>A'=[T]_{B'}</m>. For <m>A=[T]_B</m> we have
        <men xml:id="eq_matrixres_A">
          A[\boldx]_B=[\proj{\boldx}{W}]_B \implies A\boldx=\proj{\boldx}{W}
        </men>,
        where we have used the fact that for the <em>standard basis</em> <m>B</m> we have <m>[\boldy]_B=\boldy</m> for any <m>\boldy\in \R^3</m>.
        Thus we can compute <m>\proj{\boldx}{W}</m> directly with <m>A</m> as <m>A\boldx</m>. For <m>A'=[T]_{B'}</m>, on the other hand, property <xref ref="eq_matrixrep_prop"/> reads as
        <men xml:id="eq_matrixreps_Aalt">
          A'[\boldx]_{B'}=[\proj{\boldx}{W}]_{B'}
        </men>.
        Formula <xref ref="eq_matrixreps_Aalt"/> indicates that we cannot use <m>A'</m> directly to compute <m>\proj{\boldx}{W}</m>. Rather, we must first compute the <m>B'</m>-coordinates of <m>\boldx</m> and then multiply by <m>A'</m>, at which point the <m>B'</m>-coordinates of <m>\proj{\boldx}{W}</m> are returned. In other words, <m>A'</m> describes the operation of <m>\operatorname{proj}_W</m> with respect to <m>B'</m>-coordinates of vectors in <m>\R^3</m>. As such, <m>A</m> may be more useful to us in terms of computing <m>\operatorname{proj}_W</m> directly. However, <m>A'</m> has the advantage of giving us a clear <em>conceptual</em> picture of projection. For example, we see directly that <m>A'</m> has nullity one and rank 2, and thus the same is true of
        <m>\operatorname{proj}_W</m>.
        Furthermore, understanding that the columns of <m>A'</m> describe how <m>\operatorname{proj}_W</m> acts on the basis <m>B'</m>, we see easily that <m>\operatorname{proj}_W</m> acts as the identity on the 2-dimensional space spanned by the first two basis elements, and sends the subspace spanned by the third basis element to <m>\boldzero</m>. In other words, <m>A'</m> neatly encodes the conceptual picture of <m>\operatorname{proj}_W</m> as an operator that fixes the plane <m>W=\Span\{(1,-1,0),(1,0,-1)\}</m> and sends everything in <m>W^\perp=\Span\{(1,1,1)\}</m> to <m>\boldzero</m>.
      </p>
    </statement>
  </example>

  </subsection>
  <xi:include href="./s_matrixreps_ex.ptx"/>

  </section>
